import{S as vd,i as $d,s as Ed,e as s,k as d,w as m,t as r,M as qd,c as l,d as e,m as u,a as o,x as _,h as i,b as f,G as a,g as p,y as g,q as y,o as w,B as v,v as jd}from"../chunks/vendor-hf-doc-builder.js";import{T as js}from"../chunks/Tip-hf-doc-builder.js";import{I as N}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as L}from"../chunks/CodeBlock-hf-doc-builder.js";function bd(S){let h,q,c,$,k;return{c(){h=s("p"),q=r("To help you get started, try beginning with the dataset loading script "),c=s("a"),$=r("template"),k=r("!"),this.h()},l(E){h=l(E,"P",{});var b=o(h);q=i(b,"To help you get started, try beginning with the dataset loading script "),c=l(b,"A",{href:!0,rel:!0});var x=o(c);$=i(x,"template"),x.forEach(e),k=i(b,"!"),b.forEach(e),this.h()},h(){f(c,"href","https://github.com/huggingface/datasets/blob/master/templates/new_dataset_script.py"),f(c,"rel","nofollow")},m(E,b){p(E,h,b),a(h,q),a(h,c),a(c,$),a(h,k)},d(E){E&&e(h)}}}function kd(S){let h,q;return{c(){h=s("p"),q=r("Only use a default configuration when it makes sense. Don\u2019t set one because it may be more convenient for the user to not specify a configuration when they load your dataset. For example, multi-lingual datasets often have a separate configuration for each language. An appropriate default may be an aggregated configuration that loads all the languages of the dataset if the user doesn\u2019t request a particular one.")},l(c){h=l(c,"P",{});var $=o(h);q=i($,"Only use a default configuration when it makes sense. Don\u2019t set one because it may be more convenient for the user to not specify a configuration when they load your dataset. For example, multi-lingual datasets often have a separate configuration for each language. An appropriate default may be an aggregated configuration that loads all the languages of the dataset if the user doesn\u2019t request a particular one."),$.forEach(e)},m(c,$){p(c,h,$),a(h,q)},d(c){c&&e(h)}}}function xd(S){let h,q;return{c(){h=s("p"),q=r("If the data files live in the same folder or repository of the dataset script, you can just pass the relative paths to the files instead of URLs.")},l(c){h=l(c,"P",{});var $=o(h);q=i($,"If the data files live in the same folder or repository of the dataset script, you can just pass the relative paths to the files instead of URLs."),$.forEach(e)},m(c,$){p(c,h,$),a(h,q)},d(c){c&&e(h)}}}function Dd(S){let h,q,c,$,k,E,b,x;return{c(){h=s("p"),q=r("Make sure you run all of the following commands "),c=s("strong"),$=r("from the root"),k=r(" of your local "),E=s("code"),b=r("datasets"),x=r(" repository.")},l(A){h=l(A,"P",{});var j=o(h);q=i(j,"Make sure you run all of the following commands "),c=l(j,"STRONG",{});var O=o(c);$=i(O,"from the root"),O.forEach(e),k=i(j," of your local "),E=l(j,"CODE",{});var F=o(E);b=i(F,"datasets"),F.forEach(e),x=i(j," repository."),j.forEach(e)},m(A,j){p(A,h,j),a(h,q),a(h,c),a(c,$),a(h,k),a(h,E),a(E,b),a(h,x)},d(A){A&&e(h)}}}function Id(S){let h,q,c,$,k,E,b,x;return{c(){h=s("p"),q=r("Manually creating dummy data can be tricky. Make sure you follow the instructions from the command "),c=s("code"),$=r("datasets-cli dummy_data datasets/<your-dataset-folder>"),k=r(". If you are still unable to successfully generate dummy data, open a "),E=s("a"),b=r("Pull Request"),x=r(" and we will be happy to help you out!"),this.h()},l(A){h=l(A,"P",{});var j=o(h);q=i(j,"Manually creating dummy data can be tricky. Make sure you follow the instructions from the command "),c=l(j,"CODE",{});var O=o(c);$=i(O,"datasets-cli dummy_data datasets/<your-dataset-folder>"),O.forEach(e),k=i(j,". If you are still unable to successfully generate dummy data, open a "),E=l(j,"A",{href:!0,rel:!0});var F=o(E);b=i(F,"Pull Request"),F.forEach(e),x=i(j," and we will be happy to help you out!"),j.forEach(e),this.h()},h(){f(E,"href","https://github.com/huggingface/datasets/pulls"),f(E,"rel","nofollow")},m(A,j){p(A,h,j),a(h,q),a(h,c),a(c,$),a(h,k),a(h,E),a(E,b),a(h,x)},d(A){A&&e(h)}}}function Ad(S){let h,q,c,$,k,E,b,x,A,j,O,F,bs,ye,ho,ks,U,co,Xe,mo,_o,Je,go,yo,xs,Lt,Ds,we,wo,Is,D,Ke,vo,$o,Ze,Eo,qo,ta,jo,bo,ea,ko,xo,aa,Do,Io,sa,Ao,As,st,Lo,Tt,To,So,Ls,lt,Ts,M,ot,la,St,Oo,oa,Co,Ss,nt,Po,na,No,Ro,Os,rt,ra,ve,ia,Uo,Go,Vo,pa,it,da,Bo,zo,$e,Fo,Mo,Cs,Ot,Ps,H,ua,Ee,fa,Ho,Qo,Yo,ha,qe,ca,Wo,Xo,Ns,je,Jo,Rs,Ct,Us,Q,pt,ma,Pt,Ko,_a,Zo,Gs,G,tn,Nt,en,an,be,sn,ln,Vs,dt,on,Rt,nn,rn,Bs,ke,Ut,pn,xe,dn,un,zs,Gt,Fs,Vt,Bt,fn,ga,hn,cn,Ms,zt,Hs,Ft,Mt,mn,ya,_n,gn,Qs,Ht,Ys,Y,ut,wa,Qt,yn,va,wn,Ws,V,vn,$a,$n,En,Ea,qn,jn,Xs,Yt,Js,ft,Ks,W,ht,qa,Wt,bn,ja,kn,Zs,De,xn,tl,Ie,ba,Dn,el,Xt,al,ct,sl,Jt,Kt,mt,Ae,In,An,Le,Ln,Tn,Sn,Zt,ka,T,On,xa,Cn,Pn,Da,Nn,Rn,Ia,Un,Gn,Aa,Vn,Bn,zn,La,Te,Ta,Fn,Mn,ll,_t,Hn,Sa,Qn,Yn,ol,te,nl,X,gt,Oa,ee,Wn,Ca,Xn,rl,Se,Jn,il,B,Pa,Kn,Zn,Na,tr,er,Ra,ar,pl,Oe,sr,dl,yt,Ua,wt,Ga,lr,or,Va,nr,rr,ir,Ba,ae,pr,za,dr,ur,ul,se,fl,J,vt,Fa,le,fr,Ma,hr,hl,Ce,cr,cl,$t,ml,K,Et,Ha,oe,mr,Qa,_r,_l,Pe,ne,gr,Ya,yr,wr,gl,re,yl,ie,R,vr,Wa,$r,Er,Xa,qr,jr,Ja,br,kr,wl,Z,qt,Ka,pe,xr,Za,Dr,vl,Ne,Ir,$l,tt,jt,ts,de,Ar,es,Lr,El,Re,Tr,ql,I,as,Sr,Or,ss,Cr,Pr,ls,Nr,Rr,os,Ur,Gr,ns,Vr,Br,rs,zr,jl,Ue,Fr,bl,ue,kl,et,bt,is,fe,Mr,ps,Hr,xl,Ge,Qr,Dl,he,Il,kt,Al,Ve,Yr,Ll,xt,ds,Be,us,Wr,Xr,Jr,fs,ze,hs,Kr,Zr,Tl,at,Dt,cs,ce,ti,ms,ei,Sl,It,ai,_s,si,li,Ol,me,Cl,Fe,oi,Pl,_e,Nl,Me,ni,Rl;return E=new N({}),Lt=new L({props:{code:`from datasets import load_dataset
load_dataset("path/to/my_dataset")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>load_dataset(<span class="hljs-string">&quot;path/to/my_dataset&quot;</span>)`}}),lt=new js({props:{$$slots:{default:[bd]},$$scope:{ctx:S}}}),St=new N({}),Ot=new L({props:{code:`datasets.Features(
    {
        "id": datasets.Value("string"),
        "title": datasets.Value("string"),
        "context": datasets.Value("string"),
        "question": datasets.Value("string"),
        "answers": datasets.Sequence(
            {
                "text": datasets.Value("string"),
                "answer_start": datasets.Value("int32"),
            }
        ),
    }
)`,highlighted:`datasets.Features(
    {
        <span class="hljs-string">&quot;id&quot;</span>: datasets.Value(<span class="hljs-string">&quot;string&quot;</span>),
        <span class="hljs-string">&quot;title&quot;</span>: datasets.Value(<span class="hljs-string">&quot;string&quot;</span>),
        <span class="hljs-string">&quot;context&quot;</span>: datasets.Value(<span class="hljs-string">&quot;string&quot;</span>),
        <span class="hljs-string">&quot;question&quot;</span>: datasets.Value(<span class="hljs-string">&quot;string&quot;</span>),
        <span class="hljs-string">&quot;answers&quot;</span>: datasets.<span class="hljs-type">Sequence</span>(
            {
                <span class="hljs-string">&quot;text&quot;</span>: datasets.Value(<span class="hljs-string">&quot;string&quot;</span>),
                <span class="hljs-string">&quot;answer_start&quot;</span>: datasets.Value(<span class="hljs-string">&quot;int32&quot;</span>),
            }
        ),
    }
)`}}),Ct=new L({props:{code:`def _info(self):
    return datasets.DatasetInfo(
        description=_DESCRIPTION,
        features=datasets.Features(
            {
                "id": datasets.Value("string"),
                "title": datasets.Value("string"),
                "context": datasets.Value("string"),
                "question": datasets.Value("string"),
                "answers": datasets.features.Sequence(
                    {"text": datasets.Value("string"), "answer_start": datasets.Value("int32"),}
                ),
            }
        ),
        # No default supervised_keys (as we have to pass both question
        # and context as input).
        supervised_keys=None,
        homepage="https://rajpurkar.github.io/SQuAD-explorer/",
        citation=_CITATION,
    )`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">_info</span>(<span class="hljs-params">self</span>):
    <span class="hljs-keyword">return</span> datasets.DatasetInfo(
        description=_DESCRIPTION,
        features=datasets.Features(
            {
                <span class="hljs-string">&quot;id&quot;</span>: datasets.Value(<span class="hljs-string">&quot;string&quot;</span>),
                <span class="hljs-string">&quot;title&quot;</span>: datasets.Value(<span class="hljs-string">&quot;string&quot;</span>),
                <span class="hljs-string">&quot;context&quot;</span>: datasets.Value(<span class="hljs-string">&quot;string&quot;</span>),
                <span class="hljs-string">&quot;question&quot;</span>: datasets.Value(<span class="hljs-string">&quot;string&quot;</span>),
                <span class="hljs-string">&quot;answers&quot;</span>: datasets.features.<span class="hljs-type">Sequence</span>(
                    {<span class="hljs-string">&quot;text&quot;</span>: datasets.Value(<span class="hljs-string">&quot;string&quot;</span>), <span class="hljs-string">&quot;answer_start&quot;</span>: datasets.Value(<span class="hljs-string">&quot;int32&quot;</span>),}
                ),
            }
        ),
        <span class="hljs-comment"># No default supervised_keys (as we have to pass both question</span>
        <span class="hljs-comment"># and context as input).</span>
        supervised_keys=<span class="hljs-literal">None</span>,
        homepage=<span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;</span>,
        citation=_CITATION,
    )`}}),Pt=new N({}),Gt=new L({props:{code:`
`,highlighted:`<span class="hljs-keyword">class</span> <span class="hljs-title class_">SuperGlueConfig</span>(datasets.BuilderConfig):
    <span class="hljs-string">&quot;&quot;&quot;BuilderConfig for SuperGLUE.&quot;&quot;&quot;</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, features, data_url, citation, url, label_classes=(<span class="hljs-params"><span class="hljs-string">&quot;False&quot;</span>, <span class="hljs-string">&quot;True&quot;</span></span>), **kwargs</span>):
        <span class="hljs-string">&quot;&quot;&quot;BuilderConfig for SuperGLUE.

        Args:
        features: *list[string]*, list of the features that will appear in the
            feature dict. Should not include &quot;label&quot;.
        data_url: *string*, url to download the zip file from.
        citation: *string*, citation for the data set.
        url: *string*, url for information about the data set.
        label_classes: *list[string]*, the list of classes for the label if the
            label is present as a string. Non-string labels will be cast to either
            &#x27;False&#x27; or &#x27;True&#x27;.
        **kwargs: keyword arguments forwarded to super.
        &quot;&quot;&quot;</span>
        <span class="hljs-comment"># Version history:</span>
        <span class="hljs-comment"># 1.0.2: Fixed non-nondeterminism in ReCoRD.</span>
        <span class="hljs-comment"># 1.0.1: Change from the pre-release trial version of SuperGLUE (v1.9) to</span>
        <span class="hljs-comment">#        the full release (v2.0).</span>
        <span class="hljs-comment"># 1.0.0: S3 (new shuffling, sharding and slicing mechanism).</span>
        <span class="hljs-comment"># 0.0.2: Initial version.</span>
        <span class="hljs-built_in">super</span>(SuperGlueConfig, self).__init__(version=datasets.Version(<span class="hljs-string">&quot;1.0.2&quot;</span>), **kwargs)
        self.features = features
        self.label_classes = label_classes
        self.data_url = data_url
        self.citation = citation
        self.url = url`}}),zt=new L({props:{code:"",highlighted:`<span class="hljs-keyword">class</span> <span class="hljs-title class_">SuperGlue</span>(datasets.GeneratorBasedBuilder):
    <span class="hljs-string">&quot;&quot;&quot;The SuperGLUE benchmark.&quot;&quot;&quot;</span>

    BUILDER_CONFIGS = [
        SuperGlueConfig(
            name=<span class="hljs-string">&quot;boolq&quot;</span>,
            description=_BOOLQ_DESCRIPTION,
            features=[<span class="hljs-string">&quot;question&quot;</span>, <span class="hljs-string">&quot;passage&quot;</span>],
            data_url=<span class="hljs-string">&quot;https://dl.fbaipublicfiles.com/glue/superglue/data/v2/BoolQ.zip&quot;</span>,
            citation=_BOOLQ_CITATION,
            url=<span class="hljs-string">&quot;https://github.com/google-research-datasets/boolean-questions&quot;</span>,
        ),
        ...
        ...
        SuperGlueConfig(
            name=<span class="hljs-string">&quot;axg&quot;</span>,
            description=_AXG_DESCRIPTION,
            features=[<span class="hljs-string">&quot;premise&quot;</span>, <span class="hljs-string">&quot;hypothesis&quot;</span>],
            label_classes=[<span class="hljs-string">&quot;entailment&quot;</span>, <span class="hljs-string">&quot;not_entailment&quot;</span>],
            data_url=<span class="hljs-string">&quot;https://dl.fbaipublicfiles.com/glue/superglue/data/v2/AX-g.zip&quot;</span>,
            citation=_AXG_CITATION,
            url=<span class="hljs-string">&quot;https://github.com/rudinger/winogender-schemas&quot;</span>,
        ),`}}),Ht=new L({props:{code:`from datasets import load_dataset
dataset = load_dataset('super_glue', 'boolq')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;super_glue&#x27;</span>, <span class="hljs-string">&#x27;boolq&#x27;</span>)`}}),Qt=new N({}),Yt=new L({props:{code:`

`,highlighted:`<span class="hljs-keyword">class</span> <span class="hljs-title class_">NewDataset</span>(datasets.GeneratorBasedBuilder):

VERSION = datasets.Version(<span class="hljs-string">&quot;1.1.0&quot;</span>)

BUILDER_CONFIGS = [
    datasets.BuilderConfig(name=<span class="hljs-string">&quot;first_domain&quot;</span>, version=VERSION, description=<span class="hljs-string">&quot;This part of my dataset covers a first domain&quot;</span>),
    datasets.BuilderConfig(name=<span class="hljs-string">&quot;second_domain&quot;</span>, version=VERSION, description=<span class="hljs-string">&quot;This part of my dataset covers a second domain&quot;</span>),
]

DEFAULT_CONFIG_NAME = <span class="hljs-string">&quot;first_domain&quot;</span>`}}),ft=new js({props:{warning:!0,$$slots:{default:[kd]},$$scope:{ctx:S}}}),Wt=new N({}),Xt=new L({props:{code:`_URL = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
_URLS = {
    "train": _URL + "train-v1.1.json",
    "dev": _URL + "dev-v1.1.json",
}`,highlighted:`_URL = <span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;</span>
_URLS = {
    <span class="hljs-string">&quot;train&quot;</span>: _URL + <span class="hljs-string">&quot;train-v1.1.json&quot;</span>,
    <span class="hljs-string">&quot;dev&quot;</span>: _URL + <span class="hljs-string">&quot;dev-v1.1.json&quot;</span>,
}`}}),ct=new js({props:{$$slots:{default:[xd]},$$scope:{ctx:S}}}),te=new L({props:{code:"",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">_split_generators</span>(<span class="hljs-params">self, dl_manager: datasets.DownloadManager</span>) -&gt; <span class="hljs-type">List</span>[datasets.SplitGenerator]:
    urls_to_download = self._URLS
    downloaded_files = dl_manager.download_and_extract(urls_to_download)

    <span class="hljs-keyword">return</span> [
        datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={<span class="hljs-string">&quot;filepath&quot;</span>: downloaded_files[<span class="hljs-string">&quot;train&quot;</span>]}),
        datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs={<span class="hljs-string">&quot;filepath&quot;</span>: downloaded_files[<span class="hljs-string">&quot;dev&quot;</span>]}),
    ]`}}),ee=new N({}),se=new L({props:{code:`
`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">_generate_examples</span>(<span class="hljs-params">self, filepath</span>):
    <span class="hljs-string">&quot;&quot;&quot;This function returns the examples in the raw (text) form.&quot;&quot;&quot;</span>
    logger.info(<span class="hljs-string">&quot;generating examples from = %s&quot;</span>, filepath)
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filepath) <span class="hljs-keyword">as</span> f:
        squad = json.load(f)
        <span class="hljs-keyword">for</span> article <span class="hljs-keyword">in</span> squad[<span class="hljs-string">&quot;data&quot;</span>]:
            title = article.get(<span class="hljs-string">&quot;title&quot;</span>, <span class="hljs-string">&quot;&quot;</span>).strip()
            <span class="hljs-keyword">for</span> paragraph <span class="hljs-keyword">in</span> article[<span class="hljs-string">&quot;paragraphs&quot;</span>]:
                context = paragraph[<span class="hljs-string">&quot;context&quot;</span>].strip()
                <span class="hljs-keyword">for</span> qa <span class="hljs-keyword">in</span> paragraph[<span class="hljs-string">&quot;qas&quot;</span>]:
                    question = qa[<span class="hljs-string">&quot;question&quot;</span>].strip()
                    id_ = qa[<span class="hljs-string">&quot;id&quot;</span>]

                    answer_starts = [answer[<span class="hljs-string">&quot;answer_start&quot;</span>] <span class="hljs-keyword">for</span> answer <span class="hljs-keyword">in</span> qa[<span class="hljs-string">&quot;answers&quot;</span>]]
                    answers = [answer[<span class="hljs-string">&quot;text&quot;</span>].strip() <span class="hljs-keyword">for</span> answer <span class="hljs-keyword">in</span> qa[<span class="hljs-string">&quot;answers&quot;</span>]]

                    <span class="hljs-comment"># Features currently used are &quot;context&quot;, &quot;question&quot;, and &quot;answers&quot;.</span>
                    <span class="hljs-comment"># Others are extracted here for the ease of future expansions.</span>
                    <span class="hljs-keyword">yield</span> id_, {
                        <span class="hljs-string">&quot;title&quot;</span>: title,
                        <span class="hljs-string">&quot;context&quot;</span>: context,
                        <span class="hljs-string">&quot;question&quot;</span>: question,
                        <span class="hljs-string">&quot;id&quot;</span>: id_,
                        <span class="hljs-string">&quot;answers&quot;</span>: {<span class="hljs-string">&quot;answer_start&quot;</span>: answer_starts, <span class="hljs-string">&quot;text&quot;</span>: answers,},
                    }`}}),le=new N({}),$t=new js({props:{warning:!0,$$slots:{default:[Dd]},$$scope:{ctx:S}}}),oe=new N({}),re=new L({props:{code:"datasets-cli test datasets/<your-dataset-folder> --save_infos --all_configs",highlighted:'<span class="hljs-comment">datasets</span><span class="hljs-literal">-</span><span class="hljs-comment">cli</span> <span class="hljs-comment">test</span> <span class="hljs-comment">datasets/</span>&lt;<span class="hljs-comment">your</span><span class="hljs-literal">-</span><span class="hljs-comment">dataset</span><span class="hljs-literal">-</span><span class="hljs-comment">folder</span>&gt; --<span class="hljs-comment">save_infos</span> --<span class="hljs-comment">all_configs</span>'}}),pe=new N({}),de=new N({}),ue=new L({props:{code:"datasets-cli dummy_data datasets/<your-dataset-folder> --auto_generate",highlighted:'datasets-cli dummy_data datasets/&lt;your-dataset-<span class="hljs-built_in">folder</span>&gt; <span class="hljs-comment">--auto_generate</span>'}}),fe=new N({}),he=new L({props:{code:`






`,highlighted:`datasets-cli dummy_data datasets/<span class="hljs-symbol">&lt;your-dataset-folder&gt;</span>

==============================DUMMY DATA INSTRUCTIONS==============================
- In order <span class="hljs-keyword">to</span> create the dummy data <span class="hljs-keyword">for</span> my-dataset, please <span class="hljs-keyword">go</span> into the folder <span class="hljs-string">&#x27;./datasets/my-dataset/dummy/1.1.0&#x27;</span> with *<span class="hljs-keyword">cd</span> ./datasets/my-dataset/dummy/<span class="hljs-number">1.1</span>.<span class="hljs-number">0</span>* .

- Please create the following dummy data <span class="hljs-keyword">files</span> <span class="hljs-string">&#x27;dummy_data/TREC_10.label, dummy_data/train_5500.label&#x27;</span> from the folder <span class="hljs-string">&#x27;./datasets/my-dataset/dummy/1.1.0&#x27;</span>

- For each of the splits <span class="hljs-string">&#x27;train, test&#x27;</span>, <span class="hljs-keyword">make</span> sure that one <span class="hljs-built_in">or</span> more of the dummy data <span class="hljs-keyword">files</span> provide at least one example

- If the method *_generate_examples(...)* includes multiple *<span class="hljs-keyword">open</span>()* statements, you might have <span class="hljs-keyword">to</span> create other <span class="hljs-keyword">files</span> in addition <span class="hljs-keyword">to</span> <span class="hljs-string">&#x27;dummy_data/TREC_10.label, dummy_data/train_5500.label&#x27;</span>. In this case please refer <span class="hljs-keyword">to</span> the *_generate_examples(...)* method

- After <span class="hljs-keyword">all</span> dummy data <span class="hljs-keyword">files</span> are created, they should <span class="hljs-keyword">be</span> zipped recursively <span class="hljs-keyword">to</span> <span class="hljs-string">&#x27;dummy_data.zip&#x27;</span> with the <span class="hljs-keyword">command</span> *zip -r dummy_data.zip dummy_data/*

- You can now <span class="hljs-keyword">delete</span> the folder <span class="hljs-string">&#x27;dummy_data&#x27;</span> with the <span class="hljs-keyword">command</span> *rm -r dummy_data*

- To <span class="hljs-built_in">get</span> the folder <span class="hljs-string">&#x27;dummy_data&#x27;</span> back <span class="hljs-keyword">for</span> further <span class="hljs-keyword">changes</span> <span class="hljs-keyword">to</span> the dummy data, simply unzip dummy_data.zip with the <span class="hljs-keyword">command</span> *unzip dummy_data.zip*

- Make sure you have created the <span class="hljs-keyword">file</span> <span class="hljs-string">&#x27;dummy_data.zip&#x27;</span> in <span class="hljs-string">&#x27;./datasets/my-dataset/dummy/1.1.0&#x27;</span>
===================================================================================`}}),kt=new js({props:{$$slots:{default:[Id]},$$scope:{ctx:S}}}),ce=new N({}),me=new L({props:{code:"RUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_real_dataset_<your_dataset_name>",highlighted:'RUN_SLOW=<span class="hljs-number">1</span> pytest tests/test_dataset_common.py<span class="hljs-number">::</span>LocalDatasetTest<span class="hljs-number">::</span>test_load_real_dataset_&lt;your_dataset_name&gt;'}}),_e=new L({props:{code:"RUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_dataset_all_configs_<your_dataset_name>",highlighted:'RUN_SLOW=<span class="hljs-number">1</span> pytest tests/test_dataset_common.py<span class="hljs-number">::</span>LocalDatasetTest<span class="hljs-number">::</span>test_load_dataset_all_configs_&lt;your_dataset_name&gt;'}}),{c(){h=s("meta"),q=d(),c=s("h1"),$=s("a"),k=s("span"),m(E.$$.fragment),b=d(),x=s("span"),A=r("Create a dataset loading script"),j=d(),O=s("p"),F=r("Write a dataset script to load and share your own datasets. It is a Python file that defines the different configurations and splits of your dataset, as well as how to download and process the data."),bs=d(),ye=s("p"),ho=r("The script can download data files from any website, or from the same dataset repository."),ks=d(),U=s("p"),co=r("Any dataset script, for example "),Xe=s("code"),mo=r("my_dataset.py"),_o=r(", can be placed in a folder or a repository named "),Je=s("code"),go=r("my_dataset"),yo=r(" and be loaded with:"),xs=d(),m(Lt.$$.fragment),Ds=d(),we=s("p"),wo=r("The following guide includes instructions for dataset scripts for how to:"),Is=d(),D=s("ul"),Ke=s("li"),vo=r("Add dataset metadata."),$o=d(),Ze=s("li"),Eo=r("Download data files."),qo=d(),ta=s("li"),jo=r("Generate samples."),bo=d(),ea=s("li"),ko=r("Test if your dataset was generated correctly."),xo=d(),aa=s("li"),Do=r("Create a Dataset card."),Io=d(),sa=s("li"),Ao=r("Upload a dataset to the Hugging Face Hub or GitHub."),As=d(),st=s("p"),Lo=r("Open the "),Tt=s("a"),To=r("SQuAD dataset loading script"),So=r(" template to follow along on how to share a dataset."),Ls=d(),m(lt.$$.fragment),Ts=d(),M=s("h2"),ot=s("a"),la=s("span"),m(St.$$.fragment),Oo=d(),oa=s("span"),Co=r("Add dataset attributes"),Ss=d(),nt=s("p"),Po=r("The first step is to add some information, or attributes, about your dataset in "),na=s("code"),No=r("DatasetBuilder._info()"),Ro=r(". The most important attributes you should specify are:"),Os=d(),rt=s("ol"),ra=s("li"),ve=s("p"),ia=s("code"),Uo=r("DatasetInfo.description"),Go=r(" provides a concise description of your dataset. The description informs the user what\u2019s in the dataset, how it was collected, and how it can be used for a NLP task."),Vo=d(),pa=s("li"),it=s("p"),da=s("code"),Bo=r("DatasetInfo.features"),zo=r(" defines the name and type of each column in your dataset. This will also provide the structure for each example, so it is possible to create nested subfields in a column if you want. Take a look at "),$e=s("a"),Fo=r("Features"),Mo=r(" for a full list of feature types you can use."),Cs=d(),m(Ot.$$.fragment),Ps=d(),H=s("ol"),ua=s("li"),Ee=s("p"),fa=s("code"),Ho=r("DatasetInfo.homepage"),Qo=r(" contains the URL to the dataset homepage so users can find more details about the dataset."),Yo=d(),ha=s("li"),qe=s("p"),ca=s("code"),Wo=r("DatasetInfo.citation"),Xo=r(" contains a BibTeX citation for the dataset."),Ns=d(),je=s("p"),Jo=r("After you\u2019ve filled out all these fields in the template, it should look like the following example from the SQuAD loading script:"),Rs=d(),m(Ct.$$.fragment),Us=d(),Q=s("h3"),pt=s("a"),ma=s("span"),m(Pt.$$.fragment),Ko=d(),_a=s("span"),Zo=r("Multiple configurations"),Gs=d(),G=s("p"),tn=r("In some cases, your dataset may have multiple configurations. For example, the "),Nt=s("a"),en=r("SuperGLUE"),an=r(" dataset is a collection of 5 datasets designed to evaluate language understanding tasks. \u{1F917} Datasets provides "),be=s("a"),sn=r("BuilderConfig"),ln=r(" which allows you to create different configurations for the user to select from."),Vs=d(),dt=s("p"),on=r("Let\u2019s study the "),Rt=s("a"),nn=r("SuperGLUE loading script"),rn=r(" to see how you can define several configurations."),Bs=d(),ke=s("ol"),Ut=s("li"),pn=r("Create a "),xe=s("a"),dn=r("BuilderConfig"),un=r(" subclass with attributes about your dataset. These attributes can be the features of your dataset, label classes, and a URL to the data files."),zs=d(),m(Gt.$$.fragment),Fs=d(),Vt=s("ol"),Bt=s("li"),fn=r("Create instances of your config to specify the values of the attributes of each configuration. This gives you the flexibility to specify all the name and description of each configuration. These sub-class instances should be listed under "),ga=s("code"),hn=r("DatasetBuilder.BUILDER_CONFIGS"),cn=r(":"),Ms=d(),m(zt.$$.fragment),Hs=d(),Ft=s("ol"),Mt=s("li"),mn=r("Now, users can load a specific configuration of the dataset with the configuration "),ya=s("code"),_n=r("name"),gn=r(":"),Qs=d(),m(Ht.$$.fragment),Ys=d(),Y=s("h3"),ut=s("a"),wa=s("span"),m(Qt.$$.fragment),yn=d(),va=s("span"),wn=r("Default configurations"),Ws=d(),V=s("p"),vn=r("Users must specify a configuration name when they load a dataset with multiple configurations. Otherwise, \u{1F917} Datasets will raise a "),$a=s("code"),$n=r("ValueError"),En=r(", and prompt the user to select a configuration name. You can avoid this by setting a default dataset configuration with the "),Ea=s("code"),qn=r("DEFAULT_CONFIG_NAME"),jn=r(" attribute:"),Xs=d(),m(Yt.$$.fragment),Js=d(),m(ft.$$.fragment),Ks=d(),W=s("h2"),ht=s("a"),qa=s("span"),m(Wt.$$.fragment),bn=d(),ja=s("span"),kn=r("Download data files and organize splits"),Zs=d(),De=s("p"),xn=r("After you\u2019ve defined the attributes of your dataset, the next step is to download the data files and organize them according to their splits."),tl=d(),Ie=s("ol"),ba=s("li"),Dn=r("Create a dictionary of URLs in the loading script that point to the original SQuAD data files:"),el=d(),m(Xt.$$.fragment),al=d(),m(ct.$$.fragment),sl=d(),Jt=s("ol"),Kt=s("li"),mt=s("p"),Ae=s("a"),In=r("DownloadManager.download_and_extract()"),An=r(" takes this dictionary and downloads the data files. Once the files are downloaded, use "),Le=s("a"),Ln=r("SplitGenerator"),Tn=r(" to organize each split in the dataset. This is a simple class that contains:"),Sn=d(),Zt=s("ul"),ka=s("li"),T=s("p"),On=r("The "),xa=s("code"),Cn=r("name"),Pn=r(" of each split. You should use the standard split names: "),Da=s("code"),Nn=r("Split.TRAIN"),Rn=r(", "),Ia=s("code"),Un=r("Split.TEST"),Gn=r(", and "),Aa=s("code"),Vn=r("Split.VALIDATION"),Bn=r("."),zn=d(),La=s("li"),Te=s("p"),Ta=s("code"),Fn=r("gen_kwargs"),Mn=r(" provides the file paths to the data files to load for each split."),ll=d(),_t=s("p"),Hn=r("Your "),Sa=s("code"),Qn=r("DatasetBuilder._split_generator()"),Yn=r(" should look like this now:"),ol=d(),m(te.$$.fragment),nl=d(),X=s("h2"),gt=s("a"),Oa=s("span"),m(ee.$$.fragment),Wn=d(),Ca=s("span"),Xn=r("Generate samples"),rl=d(),Se=s("p"),Jn=r("At this point, you have:"),il=d(),B=s("ul"),Pa=s("li"),Kn=r("Added the dataset attributes."),Zn=d(),Na=s("li"),tr=r("Provided instructions for how to download the data files."),er=d(),Ra=s("li"),ar=r("Organized the splits."),pl=d(),Oe=s("p"),sr=r("The next step is to actually generate the samples in each split."),dl=d(),yt=s("ol"),Ua=s("li"),wt=s("p"),Ga=s("code"),lr=r("DatasetBuilder._generate_examples"),or=r(" takes the file path provided by "),Va=s("code"),nr=r("gen_kwargs"),rr=r(" to read and parse the data files. You need to write a function that loads the data files and extracts the columns."),ir=d(),Ba=s("li"),ae=s("p"),pr=r("Your function should yield a tuple of an "),za=s("code"),dr=r("id_"),ur=r(", and an example from the dataset."),ul=d(),m(se.$$.fragment),fl=d(),J=s("h2"),vt=s("a"),Fa=s("span"),m(le.$$.fragment),fr=d(),Ma=s("span"),hr=r("Testing data and checksum metadata"),hl=d(),Ce=s("p"),cr=r(`We strongly recommend adding testing data and checksum metadata to your dataset to verify and test its behavior. This ensures the generated dataset matches your expectations.
Testing data and checksum metadata are mandatory for datasets stored in the GitHub repository of the \u{1F917} Datasets library.`),cl=d(),m($t.$$.fragment),ml=d(),K=s("h3"),Et=s("a"),Ha=s("span"),m(oe.$$.fragment),mr=d(),Qa=s("span"),_r=r("Dataset metadata"),_l=d(),Pe=s("ol"),ne=s("li"),gr=r("Run the following command to create the metadata file, "),Ya=s("code"),yr=r("dataset_infos.json"),wr=r(". This will also test your new dataset loading script and make sure it works correctly."),gl=d(),m(re.$$.fragment),yl=d(),ie=s("ol"),R=s("li"),vr=r("If your dataset loading script passed the test, you should now have a "),Wa=s("code"),$r=r("dataset_infos.json"),Er=r(" file in your dataset folder. This file contains information about the dataset, like its "),Xa=s("code"),qr=r("features"),jr=r(" and "),Ja=s("code"),br=r("download_size"),kr=r("."),wl=d(),Z=s("h3"),qt=s("a"),Ka=s("span"),m(pe.$$.fragment),xr=d(),Za=s("span"),Dr=r("(Optional) Dummy data"),vl=d(),Ne=s("p"),Ir=r("If you want to be able to test your dataset script without downloading the full dataset, you need to create some dummy data for automated testing. There are two methods for generating dummy data: automatically and manually."),$l=d(),tt=s("h4"),jt=s("a"),ts=s("span"),m(de.$$.fragment),Ar=d(),es=s("span"),Lr=r("Automatic"),El=d(),Re=s("p"),Tr=r("If your data file is one of the following formats, then you can automatically generate the dummy data:"),ql=d(),I=s("ul"),as=s("li"),Sr=r("txt"),Or=d(),ss=s("li"),Cr=r("csv"),Pr=d(),ls=s("li"),Nr=r("tsv"),Rr=d(),os=s("li"),Ur=r("jsonl"),Gr=d(),ns=s("li"),Vr=r("json"),Br=d(),rs=s("li"),zr=r("xml"),jl=d(),Ue=s("p"),Fr=r("Run the command below to generate the dummy data:"),bl=d(),m(ue.$$.fragment),kl=d(),et=s("h4"),bt=s("a"),is=s("span"),m(fe.$$.fragment),Mr=d(),ps=s("span"),Hr=r("Manual"),xl=d(),Ge=s("p"),Qr=r("If your data files are not among the supported formats, you will need to generate your dummy data manually. Run the command below to output detailed instructions on how to create the dummy data:"),Dl=d(),m(he.$$.fragment),Il=d(),m(kt.$$.fragment),Al=d(),Ve=s("p"),Yr=r("There should be two new files in your dataset folder:"),Ll=d(),xt=s("ul"),ds=s("li"),Be=s("p"),us=s("code"),Wr=r("dataset_infos.json"),Xr=r(" stores the dataset metadata including the data file checksums, and the number of examples required to confirm the dataset was generated properly."),Jr=d(),fs=s("li"),ze=s("p"),hs=s("code"),Kr=r("dummy_data.zip"),Zr=r(" is a file used to test the behavior of the loading script without having to download the full dataset."),Tl=d(),at=s("h4"),Dt=s("a"),cs=s("span"),m(ce.$$.fragment),ti=d(),ms=s("span"),ei=r("Run the tests"),Sl=d(),It=s("p"),ai=r("The last step is to actually test dataset generation with the real and dummy data. Clone the "),_s=s("code"),si=r("huggingface/datasets"),li=r(" repository and run the following command to test the real data:"),Ol=d(),m(me.$$.fragment),Cl=d(),Fe=s("p"),oi=r("Test the dummy data:"),Pl=d(),m(_e.$$.fragment),Nl=d(),Me=s("p"),ni=r("If both tests pass, your dataset was generated correctly!"),this.h()},l(t){const n=qd('[data-svelte="svelte-1phssyn"]',document.head);h=l(n,"META",{name:!0,content:!0}),n.forEach(e),q=u(t),c=l(t,"H1",{class:!0});var ge=o(c);$=l(ge,"A",{id:!0,class:!0,href:!0});var gs=o($);k=l(gs,"SPAN",{});var ys=o(k);_(E.$$.fragment,ys),ys.forEach(e),gs.forEach(e),b=u(ge),x=l(ge,"SPAN",{});var ws=o(x);A=i(ws,"Create a dataset loading script"),ws.forEach(e),ge.forEach(e),j=u(t),O=l(t,"P",{});var vs=o(O);F=i(vs,"Write a dataset script to load and share your own datasets. It is a Python file that defines the different configurations and splits of your dataset, as well as how to download and process the data."),vs.forEach(e),bs=u(t),ye=l(t,"P",{});var hi=o(ye);ho=i(hi,"The script can download data files from any website, or from the same dataset repository."),hi.forEach(e),ks=u(t),U=l(t,"P",{});var He=o(U);co=i(He,"Any dataset script, for example "),Xe=l(He,"CODE",{});var ci=o(Xe);mo=i(ci,"my_dataset.py"),ci.forEach(e),_o=i(He,", can be placed in a folder or a repository named "),Je=l(He,"CODE",{});var mi=o(Je);go=i(mi,"my_dataset"),mi.forEach(e),yo=i(He," and be loaded with:"),He.forEach(e),xs=u(t),_(Lt.$$.fragment,t),Ds=u(t),we=l(t,"P",{});var _i=o(we);wo=i(_i,"The following guide includes instructions for dataset scripts for how to:"),_i.forEach(e),Is=u(t),D=l(t,"UL",{});var C=o(D);Ke=l(C,"LI",{});var gi=o(Ke);vo=i(gi,"Add dataset metadata."),gi.forEach(e),$o=u(C),Ze=l(C,"LI",{});var yi=o(Ze);Eo=i(yi,"Download data files."),yi.forEach(e),qo=u(C),ta=l(C,"LI",{});var wi=o(ta);jo=i(wi,"Generate samples."),wi.forEach(e),bo=u(C),ea=l(C,"LI",{});var vi=o(ea);ko=i(vi,"Test if your dataset was generated correctly."),vi.forEach(e),xo=u(C),aa=l(C,"LI",{});var $i=o(aa);Do=i($i,"Create a Dataset card."),$i.forEach(e),Io=u(C),sa=l(C,"LI",{});var Ei=o(sa);Ao=i(Ei,"Upload a dataset to the Hugging Face Hub or GitHub."),Ei.forEach(e),C.forEach(e),As=u(t),st=l(t,"P",{});var Ul=o(st);Lo=i(Ul,"Open the "),Tt=l(Ul,"A",{href:!0,rel:!0});var qi=o(Tt);To=i(qi,"SQuAD dataset loading script"),qi.forEach(e),So=i(Ul," template to follow along on how to share a dataset."),Ul.forEach(e),Ls=u(t),_(lt.$$.fragment,t),Ts=u(t),M=l(t,"H2",{class:!0});var Gl=o(M);ot=l(Gl,"A",{id:!0,class:!0,href:!0});var ji=o(ot);la=l(ji,"SPAN",{});var bi=o(la);_(St.$$.fragment,bi),bi.forEach(e),ji.forEach(e),Oo=u(Gl),oa=l(Gl,"SPAN",{});var ki=o(oa);Co=i(ki,"Add dataset attributes"),ki.forEach(e),Gl.forEach(e),Ss=u(t),nt=l(t,"P",{});var Vl=o(nt);Po=i(Vl,"The first step is to add some information, or attributes, about your dataset in "),na=l(Vl,"CODE",{});var xi=o(na);No=i(xi,"DatasetBuilder._info()"),xi.forEach(e),Ro=i(Vl,". The most important attributes you should specify are:"),Vl.forEach(e),Os=u(t),rt=l(t,"OL",{});var Bl=o(rt);ra=l(Bl,"LI",{});var Di=o(ra);ve=l(Di,"P",{});var ri=o(ve);ia=l(ri,"CODE",{});var Ii=o(ia);Uo=i(Ii,"DatasetInfo.description"),Ii.forEach(e),Go=i(ri," provides a concise description of your dataset. The description informs the user what\u2019s in the dataset, how it was collected, and how it can be used for a NLP task."),ri.forEach(e),Di.forEach(e),Vo=u(Bl),pa=l(Bl,"LI",{});var Ai=o(pa);it=l(Ai,"P",{});var $s=o(it);da=l($s,"CODE",{});var Li=o(da);Bo=i(Li,"DatasetInfo.features"),Li.forEach(e),zo=i($s," defines the name and type of each column in your dataset. This will also provide the structure for each example, so it is possible to create nested subfields in a column if you want. Take a look at "),$e=l($s,"A",{href:!0});var Ti=o($e);Fo=i(Ti,"Features"),Ti.forEach(e),Mo=i($s," for a full list of feature types you can use."),$s.forEach(e),Ai.forEach(e),Bl.forEach(e),Cs=u(t),_(Ot.$$.fragment,t),Ps=u(t),H=l(t,"OL",{start:!0});var zl=o(H);ua=l(zl,"LI",{});var Si=o(ua);Ee=l(Si,"P",{});var ii=o(Ee);fa=l(ii,"CODE",{});var Oi=o(fa);Ho=i(Oi,"DatasetInfo.homepage"),Oi.forEach(e),Qo=i(ii," contains the URL to the dataset homepage so users can find more details about the dataset."),ii.forEach(e),Si.forEach(e),Yo=u(zl),ha=l(zl,"LI",{});var Ci=o(ha);qe=l(Ci,"P",{});var pi=o(qe);ca=l(pi,"CODE",{});var Pi=o(ca);Wo=i(Pi,"DatasetInfo.citation"),Pi.forEach(e),Xo=i(pi," contains a BibTeX citation for the dataset."),pi.forEach(e),Ci.forEach(e),zl.forEach(e),Ns=u(t),je=l(t,"P",{});var Ni=o(je);Jo=i(Ni,"After you\u2019ve filled out all these fields in the template, it should look like the following example from the SQuAD loading script:"),Ni.forEach(e),Rs=u(t),_(Ct.$$.fragment,t),Us=u(t),Q=l(t,"H3",{class:!0});var Fl=o(Q);pt=l(Fl,"A",{id:!0,class:!0,href:!0});var Ri=o(pt);ma=l(Ri,"SPAN",{});var Ui=o(ma);_(Pt.$$.fragment,Ui),Ui.forEach(e),Ri.forEach(e),Ko=u(Fl),_a=l(Fl,"SPAN",{});var Gi=o(_a);Zo=i(Gi,"Multiple configurations"),Gi.forEach(e),Fl.forEach(e),Gs=u(t),G=l(t,"P",{});var Qe=o(G);tn=i(Qe,"In some cases, your dataset may have multiple configurations. For example, the "),Nt=l(Qe,"A",{href:!0,rel:!0});var Vi=o(Nt);en=i(Vi,"SuperGLUE"),Vi.forEach(e),an=i(Qe," dataset is a collection of 5 datasets designed to evaluate language understanding tasks. \u{1F917} Datasets provides "),be=l(Qe,"A",{href:!0});var Bi=o(be);sn=i(Bi,"BuilderConfig"),Bi.forEach(e),ln=i(Qe," which allows you to create different configurations for the user to select from."),Qe.forEach(e),Vs=u(t),dt=l(t,"P",{});var Ml=o(dt);on=i(Ml,"Let\u2019s study the "),Rt=l(Ml,"A",{href:!0,rel:!0});var zi=o(Rt);nn=i(zi,"SuperGLUE loading script"),zi.forEach(e),rn=i(Ml," to see how you can define several configurations."),Ml.forEach(e),Bs=u(t),ke=l(t,"OL",{});var Fi=o(ke);Ut=l(Fi,"LI",{});var Hl=o(Ut);pn=i(Hl,"Create a "),xe=l(Hl,"A",{href:!0});var Mi=o(xe);dn=i(Mi,"BuilderConfig"),Mi.forEach(e),un=i(Hl," subclass with attributes about your dataset. These attributes can be the features of your dataset, label classes, and a URL to the data files."),Hl.forEach(e),Fi.forEach(e),zs=u(t),_(Gt.$$.fragment,t),Fs=u(t),Vt=l(t,"OL",{start:!0});var Hi=o(Vt);Bt=l(Hi,"LI",{});var Ql=o(Bt);fn=i(Ql,"Create instances of your config to specify the values of the attributes of each configuration. This gives you the flexibility to specify all the name and description of each configuration. These sub-class instances should be listed under "),ga=l(Ql,"CODE",{});var Qi=o(ga);hn=i(Qi,"DatasetBuilder.BUILDER_CONFIGS"),Qi.forEach(e),cn=i(Ql,":"),Ql.forEach(e),Hi.forEach(e),Ms=u(t),_(zt.$$.fragment,t),Hs=u(t),Ft=l(t,"OL",{start:!0});var Yi=o(Ft);Mt=l(Yi,"LI",{});var Yl=o(Mt);mn=i(Yl,"Now, users can load a specific configuration of the dataset with the configuration "),ya=l(Yl,"CODE",{});var Wi=o(ya);_n=i(Wi,"name"),Wi.forEach(e),gn=i(Yl,":"),Yl.forEach(e),Yi.forEach(e),Qs=u(t),_(Ht.$$.fragment,t),Ys=u(t),Y=l(t,"H3",{class:!0});var Wl=o(Y);ut=l(Wl,"A",{id:!0,class:!0,href:!0});var Xi=o(ut);wa=l(Xi,"SPAN",{});var Ji=o(wa);_(Qt.$$.fragment,Ji),Ji.forEach(e),Xi.forEach(e),yn=u(Wl),va=l(Wl,"SPAN",{});var Ki=o(va);wn=i(Ki,"Default configurations"),Ki.forEach(e),Wl.forEach(e),Ws=u(t),V=l(t,"P",{});var Ye=o(V);vn=i(Ye,"Users must specify a configuration name when they load a dataset with multiple configurations. Otherwise, \u{1F917} Datasets will raise a "),$a=l(Ye,"CODE",{});var Zi=o($a);$n=i(Zi,"ValueError"),Zi.forEach(e),En=i(Ye,", and prompt the user to select a configuration name. You can avoid this by setting a default dataset configuration with the "),Ea=l(Ye,"CODE",{});var tp=o(Ea);qn=i(tp,"DEFAULT_CONFIG_NAME"),tp.forEach(e),jn=i(Ye," attribute:"),Ye.forEach(e),Xs=u(t),_(Yt.$$.fragment,t),Js=u(t),_(ft.$$.fragment,t),Ks=u(t),W=l(t,"H2",{class:!0});var Xl=o(W);ht=l(Xl,"A",{id:!0,class:!0,href:!0});var ep=o(ht);qa=l(ep,"SPAN",{});var ap=o(qa);_(Wt.$$.fragment,ap),ap.forEach(e),ep.forEach(e),bn=u(Xl),ja=l(Xl,"SPAN",{});var sp=o(ja);kn=i(sp,"Download data files and organize splits"),sp.forEach(e),Xl.forEach(e),Zs=u(t),De=l(t,"P",{});var lp=o(De);xn=i(lp,"After you\u2019ve defined the attributes of your dataset, the next step is to download the data files and organize them according to their splits."),lp.forEach(e),tl=u(t),Ie=l(t,"OL",{});var op=o(Ie);ba=l(op,"LI",{});var np=o(ba);Dn=i(np,"Create a dictionary of URLs in the loading script that point to the original SQuAD data files:"),np.forEach(e),op.forEach(e),el=u(t),_(Xt.$$.fragment,t),al=u(t),_(ct.$$.fragment,t),sl=u(t),Jt=l(t,"OL",{start:!0});var rp=o(Jt);Kt=l(rp,"LI",{});var Jl=o(Kt);mt=l(Jl,"P",{});var Es=o(mt);Ae=l(Es,"A",{href:!0});var ip=o(Ae);In=i(ip,"DownloadManager.download_and_extract()"),ip.forEach(e),An=i(Es," takes this dictionary and downloads the data files. Once the files are downloaded, use "),Le=l(Es,"A",{href:!0});var pp=o(Le);Ln=i(pp,"SplitGenerator"),pp.forEach(e),Tn=i(Es," to organize each split in the dataset. This is a simple class that contains:"),Es.forEach(e),Sn=u(Jl),Zt=l(Jl,"UL",{});var Kl=o(Zt);ka=l(Kl,"LI",{});var dp=o(ka);T=l(dp,"P",{});var z=o(T);On=i(z,"The "),xa=l(z,"CODE",{});var up=o(xa);Cn=i(up,"name"),up.forEach(e),Pn=i(z," of each split. You should use the standard split names: "),Da=l(z,"CODE",{});var fp=o(Da);Nn=i(fp,"Split.TRAIN"),fp.forEach(e),Rn=i(z,", "),Ia=l(z,"CODE",{});var hp=o(Ia);Un=i(hp,"Split.TEST"),hp.forEach(e),Gn=i(z,", and "),Aa=l(z,"CODE",{});var cp=o(Aa);Vn=i(cp,"Split.VALIDATION"),cp.forEach(e),Bn=i(z,"."),z.forEach(e),dp.forEach(e),zn=u(Kl),La=l(Kl,"LI",{});var mp=o(La);Te=l(mp,"P",{});var di=o(Te);Ta=l(di,"CODE",{});var _p=o(Ta);Fn=i(_p,"gen_kwargs"),_p.forEach(e),Mn=i(di," provides the file paths to the data files to load for each split."),di.forEach(e),mp.forEach(e),Kl.forEach(e),Jl.forEach(e),rp.forEach(e),ll=u(t),_t=l(t,"P",{});var Zl=o(_t);Hn=i(Zl,"Your "),Sa=l(Zl,"CODE",{});var gp=o(Sa);Qn=i(gp,"DatasetBuilder._split_generator()"),gp.forEach(e),Yn=i(Zl," should look like this now:"),Zl.forEach(e),ol=u(t),_(te.$$.fragment,t),nl=u(t),X=l(t,"H2",{class:!0});var to=o(X);gt=l(to,"A",{id:!0,class:!0,href:!0});var yp=o(gt);Oa=l(yp,"SPAN",{});var wp=o(Oa);_(ee.$$.fragment,wp),wp.forEach(e),yp.forEach(e),Wn=u(to),Ca=l(to,"SPAN",{});var vp=o(Ca);Xn=i(vp,"Generate samples"),vp.forEach(e),to.forEach(e),rl=u(t),Se=l(t,"P",{});var $p=o(Se);Jn=i($p,"At this point, you have:"),$p.forEach(e),il=u(t),B=l(t,"UL",{});var We=o(B);Pa=l(We,"LI",{});var Ep=o(Pa);Kn=i(Ep,"Added the dataset attributes."),Ep.forEach(e),Zn=u(We),Na=l(We,"LI",{});var qp=o(Na);tr=i(qp,"Provided instructions for how to download the data files."),qp.forEach(e),er=u(We),Ra=l(We,"LI",{});var jp=o(Ra);ar=i(jp,"Organized the splits."),jp.forEach(e),We.forEach(e),pl=u(t),Oe=l(t,"P",{});var bp=o(Oe);sr=i(bp,"The next step is to actually generate the samples in each split."),bp.forEach(e),dl=u(t),yt=l(t,"OL",{});var eo=o(yt);Ua=l(eo,"LI",{});var kp=o(Ua);wt=l(kp,"P",{});var qs=o(wt);Ga=l(qs,"CODE",{});var xp=o(Ga);lr=i(xp,"DatasetBuilder._generate_examples"),xp.forEach(e),or=i(qs," takes the file path provided by "),Va=l(qs,"CODE",{});var Dp=o(Va);nr=i(Dp,"gen_kwargs"),Dp.forEach(e),rr=i(qs," to read and parse the data files. You need to write a function that loads the data files and extracts the columns."),qs.forEach(e),kp.forEach(e),ir=u(eo),Ba=l(eo,"LI",{});var Ip=o(Ba);ae=l(Ip,"P",{});var ao=o(ae);pr=i(ao,"Your function should yield a tuple of an "),za=l(ao,"CODE",{});var Ap=o(za);dr=i(Ap,"id_"),Ap.forEach(e),ur=i(ao,", and an example from the dataset."),ao.forEach(e),Ip.forEach(e),eo.forEach(e),ul=u(t),_(se.$$.fragment,t),fl=u(t),J=l(t,"H2",{class:!0});var so=o(J);vt=l(so,"A",{id:!0,class:!0,href:!0});var Lp=o(vt);Fa=l(Lp,"SPAN",{});var Tp=o(Fa);_(le.$$.fragment,Tp),Tp.forEach(e),Lp.forEach(e),fr=u(so),Ma=l(so,"SPAN",{});var Sp=o(Ma);hr=i(Sp,"Testing data and checksum metadata"),Sp.forEach(e),so.forEach(e),hl=u(t),Ce=l(t,"P",{});var Op=o(Ce);cr=i(Op,`We strongly recommend adding testing data and checksum metadata to your dataset to verify and test its behavior. This ensures the generated dataset matches your expectations.
Testing data and checksum metadata are mandatory for datasets stored in the GitHub repository of the \u{1F917} Datasets library.`),Op.forEach(e),cl=u(t),_($t.$$.fragment,t),ml=u(t),K=l(t,"H3",{class:!0});var lo=o(K);Et=l(lo,"A",{id:!0,class:!0,href:!0});var Cp=o(Et);Ha=l(Cp,"SPAN",{});var Pp=o(Ha);_(oe.$$.fragment,Pp),Pp.forEach(e),Cp.forEach(e),mr=u(lo),Qa=l(lo,"SPAN",{});var Np=o(Qa);_r=i(Np,"Dataset metadata"),Np.forEach(e),lo.forEach(e),_l=u(t),Pe=l(t,"OL",{});var Rp=o(Pe);ne=l(Rp,"LI",{});var oo=o(ne);gr=i(oo,"Run the following command to create the metadata file, "),Ya=l(oo,"CODE",{});var Up=o(Ya);yr=i(Up,"dataset_infos.json"),Up.forEach(e),wr=i(oo,". This will also test your new dataset loading script and make sure it works correctly."),oo.forEach(e),Rp.forEach(e),gl=u(t),_(re.$$.fragment,t),yl=u(t),ie=l(t,"OL",{start:!0});var Gp=o(ie);R=l(Gp,"LI",{});var At=o(R);vr=i(At,"If your dataset loading script passed the test, you should now have a "),Wa=l(At,"CODE",{});var Vp=o(Wa);$r=i(Vp,"dataset_infos.json"),Vp.forEach(e),Er=i(At," file in your dataset folder. This file contains information about the dataset, like its "),Xa=l(At,"CODE",{});var Bp=o(Xa);qr=i(Bp,"features"),Bp.forEach(e),jr=i(At," and "),Ja=l(At,"CODE",{});var zp=o(Ja);br=i(zp,"download_size"),zp.forEach(e),kr=i(At,"."),At.forEach(e),Gp.forEach(e),wl=u(t),Z=l(t,"H3",{class:!0});var no=o(Z);qt=l(no,"A",{id:!0,class:!0,href:!0});var Fp=o(qt);Ka=l(Fp,"SPAN",{});var Mp=o(Ka);_(pe.$$.fragment,Mp),Mp.forEach(e),Fp.forEach(e),xr=u(no),Za=l(no,"SPAN",{});var Hp=o(Za);Dr=i(Hp,"(Optional) Dummy data"),Hp.forEach(e),no.forEach(e),vl=u(t),Ne=l(t,"P",{});var Qp=o(Ne);Ir=i(Qp,"If you want to be able to test your dataset script without downloading the full dataset, you need to create some dummy data for automated testing. There are two methods for generating dummy data: automatically and manually."),Qp.forEach(e),$l=u(t),tt=l(t,"H4",{class:!0});var ro=o(tt);jt=l(ro,"A",{id:!0,class:!0,href:!0});var Yp=o(jt);ts=l(Yp,"SPAN",{});var Wp=o(ts);_(de.$$.fragment,Wp),Wp.forEach(e),Yp.forEach(e),Ar=u(ro),es=l(ro,"SPAN",{});var Xp=o(es);Lr=i(Xp,"Automatic"),Xp.forEach(e),ro.forEach(e),El=u(t),Re=l(t,"P",{});var Jp=o(Re);Tr=i(Jp,"If your data file is one of the following formats, then you can automatically generate the dummy data:"),Jp.forEach(e),ql=u(t),I=l(t,"UL",{});var P=o(I);as=l(P,"LI",{});var Kp=o(as);Sr=i(Kp,"txt"),Kp.forEach(e),Or=u(P),ss=l(P,"LI",{});var Zp=o(ss);Cr=i(Zp,"csv"),Zp.forEach(e),Pr=u(P),ls=l(P,"LI",{});var td=o(ls);Nr=i(td,"tsv"),td.forEach(e),Rr=u(P),os=l(P,"LI",{});var ed=o(os);Ur=i(ed,"jsonl"),ed.forEach(e),Gr=u(P),ns=l(P,"LI",{});var ad=o(ns);Vr=i(ad,"json"),ad.forEach(e),Br=u(P),rs=l(P,"LI",{});var sd=o(rs);zr=i(sd,"xml"),sd.forEach(e),P.forEach(e),jl=u(t),Ue=l(t,"P",{});var ld=o(Ue);Fr=i(ld,"Run the command below to generate the dummy data:"),ld.forEach(e),bl=u(t),_(ue.$$.fragment,t),kl=u(t),et=l(t,"H4",{class:!0});var io=o(et);bt=l(io,"A",{id:!0,class:!0,href:!0});var od=o(bt);is=l(od,"SPAN",{});var nd=o(is);_(fe.$$.fragment,nd),nd.forEach(e),od.forEach(e),Mr=u(io),ps=l(io,"SPAN",{});var rd=o(ps);Hr=i(rd,"Manual"),rd.forEach(e),io.forEach(e),xl=u(t),Ge=l(t,"P",{});var id=o(Ge);Qr=i(id,"If your data files are not among the supported formats, you will need to generate your dummy data manually. Run the command below to output detailed instructions on how to create the dummy data:"),id.forEach(e),Dl=u(t),_(he.$$.fragment,t),Il=u(t),_(kt.$$.fragment,t),Al=u(t),Ve=l(t,"P",{});var pd=o(Ve);Yr=i(pd,"There should be two new files in your dataset folder:"),pd.forEach(e),Ll=u(t),xt=l(t,"UL",{});var po=o(xt);ds=l(po,"LI",{});var dd=o(ds);Be=l(dd,"P",{});var ui=o(Be);us=l(ui,"CODE",{});var ud=o(us);Wr=i(ud,"dataset_infos.json"),ud.forEach(e),Xr=i(ui," stores the dataset metadata including the data file checksums, and the number of examples required to confirm the dataset was generated properly."),ui.forEach(e),dd.forEach(e),Jr=u(po),fs=l(po,"LI",{});var fd=o(fs);ze=l(fd,"P",{});var fi=o(ze);hs=l(fi,"CODE",{});var hd=o(hs);Kr=i(hd,"dummy_data.zip"),hd.forEach(e),Zr=i(fi," is a file used to test the behavior of the loading script without having to download the full dataset."),fi.forEach(e),fd.forEach(e),po.forEach(e),Tl=u(t),at=l(t,"H4",{class:!0});var uo=o(at);Dt=l(uo,"A",{id:!0,class:!0,href:!0});var cd=o(Dt);cs=l(cd,"SPAN",{});var md=o(cs);_(ce.$$.fragment,md),md.forEach(e),cd.forEach(e),ti=u(uo),ms=l(uo,"SPAN",{});var _d=o(ms);ei=i(_d,"Run the tests"),_d.forEach(e),uo.forEach(e),Sl=u(t),It=l(t,"P",{});var fo=o(It);ai=i(fo,"The last step is to actually test dataset generation with the real and dummy data. Clone the "),_s=l(fo,"CODE",{});var gd=o(_s);si=i(gd,"huggingface/datasets"),gd.forEach(e),li=i(fo," repository and run the following command to test the real data:"),fo.forEach(e),Ol=u(t),_(me.$$.fragment,t),Cl=u(t),Fe=l(t,"P",{});var yd=o(Fe);oi=i(yd,"Test the dummy data:"),yd.forEach(e),Pl=u(t),_(_e.$$.fragment,t),Nl=u(t),Me=l(t,"P",{});var wd=o(Me);ni=i(wd,"If both tests pass, your dataset was generated correctly!"),wd.forEach(e),this.h()},h(){f(h,"name","hf:doc:metadata"),f(h,"content",JSON.stringify(Ld)),f($,"id","create-a-dataset-loading-script"),f($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f($,"href","#create-a-dataset-loading-script"),f(c,"class","relative group"),f(Tt,"href","https://github.com/huggingface/datasets/blob/master/datasets/squad/squad.py"),f(Tt,"rel","nofollow"),f(ot,"id","add-dataset-attributes"),f(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ot,"href","#add-dataset-attributes"),f(M,"class","relative group"),f($e,"href","/docs/datasets/pr_4530/en/package_reference/main_classes#datasets.Features"),f(H,"start","3"),f(pt,"id","multiple-configurations"),f(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(pt,"href","#multiple-configurations"),f(Q,"class","relative group"),f(Nt,"href","https://huggingface.co/datasets/super_glue"),f(Nt,"rel","nofollow"),f(be,"href","/docs/datasets/pr_4530/en/package_reference/builder_classes#datasets.BuilderConfig"),f(Rt,"href","https://github.com/huggingface/datasets/blob/master/datasets/super_glue/super_glue.py"),f(Rt,"rel","nofollow"),f(xe,"href","/docs/datasets/pr_4530/en/package_reference/builder_classes#datasets.BuilderConfig"),f(Vt,"start","2"),f(Ft,"start","3"),f(ut,"id","default-configurations"),f(ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ut,"href","#default-configurations"),f(Y,"class","relative group"),f(ht,"id","download-data-files-and-organize-splits"),f(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ht,"href","#download-data-files-and-organize-splits"),f(W,"class","relative group"),f(Ae,"href","/docs/datasets/pr_4530/en/package_reference/builder_classes#datasets.DownloadManager.download_and_extract"),f(Le,"href","/docs/datasets/pr_4530/en/package_reference/builder_classes#datasets.SplitGenerator"),f(Jt,"start","2"),f(gt,"id","generate-samples"),f(gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(gt,"href","#generate-samples"),f(X,"class","relative group"),f(vt,"id","testing-data-and-checksum-metadata"),f(vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(vt,"href","#testing-data-and-checksum-metadata"),f(J,"class","relative group"),f(Et,"id","dataset-metadata"),f(Et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Et,"href","#dataset-metadata"),f(K,"class","relative group"),f(ie,"start","2"),f(qt,"id","optional-dummy-data"),f(qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(qt,"href","#optional-dummy-data"),f(Z,"class","relative group"),f(jt,"id","automatic"),f(jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(jt,"href","#automatic"),f(tt,"class","relative group"),f(bt,"id","manual"),f(bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(bt,"href","#manual"),f(et,"class","relative group"),f(Dt,"id","run-the-tests"),f(Dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Dt,"href","#run-the-tests"),f(at,"class","relative group")},m(t,n){a(document.head,h),p(t,q,n),p(t,c,n),a(c,$),a($,k),g(E,k,null),a(c,b),a(c,x),a(x,A),p(t,j,n),p(t,O,n),a(O,F),p(t,bs,n),p(t,ye,n),a(ye,ho),p(t,ks,n),p(t,U,n),a(U,co),a(U,Xe),a(Xe,mo),a(U,_o),a(U,Je),a(Je,go),a(U,yo),p(t,xs,n),g(Lt,t,n),p(t,Ds,n),p(t,we,n),a(we,wo),p(t,Is,n),p(t,D,n),a(D,Ke),a(Ke,vo),a(D,$o),a(D,Ze),a(Ze,Eo),a(D,qo),a(D,ta),a(ta,jo),a(D,bo),a(D,ea),a(ea,ko),a(D,xo),a(D,aa),a(aa,Do),a(D,Io),a(D,sa),a(sa,Ao),p(t,As,n),p(t,st,n),a(st,Lo),a(st,Tt),a(Tt,To),a(st,So),p(t,Ls,n),g(lt,t,n),p(t,Ts,n),p(t,M,n),a(M,ot),a(ot,la),g(St,la,null),a(M,Oo),a(M,oa),a(oa,Co),p(t,Ss,n),p(t,nt,n),a(nt,Po),a(nt,na),a(na,No),a(nt,Ro),p(t,Os,n),p(t,rt,n),a(rt,ra),a(ra,ve),a(ve,ia),a(ia,Uo),a(ve,Go),a(rt,Vo),a(rt,pa),a(pa,it),a(it,da),a(da,Bo),a(it,zo),a(it,$e),a($e,Fo),a(it,Mo),p(t,Cs,n),g(Ot,t,n),p(t,Ps,n),p(t,H,n),a(H,ua),a(ua,Ee),a(Ee,fa),a(fa,Ho),a(Ee,Qo),a(H,Yo),a(H,ha),a(ha,qe),a(qe,ca),a(ca,Wo),a(qe,Xo),p(t,Ns,n),p(t,je,n),a(je,Jo),p(t,Rs,n),g(Ct,t,n),p(t,Us,n),p(t,Q,n),a(Q,pt),a(pt,ma),g(Pt,ma,null),a(Q,Ko),a(Q,_a),a(_a,Zo),p(t,Gs,n),p(t,G,n),a(G,tn),a(G,Nt),a(Nt,en),a(G,an),a(G,be),a(be,sn),a(G,ln),p(t,Vs,n),p(t,dt,n),a(dt,on),a(dt,Rt),a(Rt,nn),a(dt,rn),p(t,Bs,n),p(t,ke,n),a(ke,Ut),a(Ut,pn),a(Ut,xe),a(xe,dn),a(Ut,un),p(t,zs,n),g(Gt,t,n),p(t,Fs,n),p(t,Vt,n),a(Vt,Bt),a(Bt,fn),a(Bt,ga),a(ga,hn),a(Bt,cn),p(t,Ms,n),g(zt,t,n),p(t,Hs,n),p(t,Ft,n),a(Ft,Mt),a(Mt,mn),a(Mt,ya),a(ya,_n),a(Mt,gn),p(t,Qs,n),g(Ht,t,n),p(t,Ys,n),p(t,Y,n),a(Y,ut),a(ut,wa),g(Qt,wa,null),a(Y,yn),a(Y,va),a(va,wn),p(t,Ws,n),p(t,V,n),a(V,vn),a(V,$a),a($a,$n),a(V,En),a(V,Ea),a(Ea,qn),a(V,jn),p(t,Xs,n),g(Yt,t,n),p(t,Js,n),g(ft,t,n),p(t,Ks,n),p(t,W,n),a(W,ht),a(ht,qa),g(Wt,qa,null),a(W,bn),a(W,ja),a(ja,kn),p(t,Zs,n),p(t,De,n),a(De,xn),p(t,tl,n),p(t,Ie,n),a(Ie,ba),a(ba,Dn),p(t,el,n),g(Xt,t,n),p(t,al,n),g(ct,t,n),p(t,sl,n),p(t,Jt,n),a(Jt,Kt),a(Kt,mt),a(mt,Ae),a(Ae,In),a(mt,An),a(mt,Le),a(Le,Ln),a(mt,Tn),a(Kt,Sn),a(Kt,Zt),a(Zt,ka),a(ka,T),a(T,On),a(T,xa),a(xa,Cn),a(T,Pn),a(T,Da),a(Da,Nn),a(T,Rn),a(T,Ia),a(Ia,Un),a(T,Gn),a(T,Aa),a(Aa,Vn),a(T,Bn),a(Zt,zn),a(Zt,La),a(La,Te),a(Te,Ta),a(Ta,Fn),a(Te,Mn),p(t,ll,n),p(t,_t,n),a(_t,Hn),a(_t,Sa),a(Sa,Qn),a(_t,Yn),p(t,ol,n),g(te,t,n),p(t,nl,n),p(t,X,n),a(X,gt),a(gt,Oa),g(ee,Oa,null),a(X,Wn),a(X,Ca),a(Ca,Xn),p(t,rl,n),p(t,Se,n),a(Se,Jn),p(t,il,n),p(t,B,n),a(B,Pa),a(Pa,Kn),a(B,Zn),a(B,Na),a(Na,tr),a(B,er),a(B,Ra),a(Ra,ar),p(t,pl,n),p(t,Oe,n),a(Oe,sr),p(t,dl,n),p(t,yt,n),a(yt,Ua),a(Ua,wt),a(wt,Ga),a(Ga,lr),a(wt,or),a(wt,Va),a(Va,nr),a(wt,rr),a(yt,ir),a(yt,Ba),a(Ba,ae),a(ae,pr),a(ae,za),a(za,dr),a(ae,ur),p(t,ul,n),g(se,t,n),p(t,fl,n),p(t,J,n),a(J,vt),a(vt,Fa),g(le,Fa,null),a(J,fr),a(J,Ma),a(Ma,hr),p(t,hl,n),p(t,Ce,n),a(Ce,cr),p(t,cl,n),g($t,t,n),p(t,ml,n),p(t,K,n),a(K,Et),a(Et,Ha),g(oe,Ha,null),a(K,mr),a(K,Qa),a(Qa,_r),p(t,_l,n),p(t,Pe,n),a(Pe,ne),a(ne,gr),a(ne,Ya),a(Ya,yr),a(ne,wr),p(t,gl,n),g(re,t,n),p(t,yl,n),p(t,ie,n),a(ie,R),a(R,vr),a(R,Wa),a(Wa,$r),a(R,Er),a(R,Xa),a(Xa,qr),a(R,jr),a(R,Ja),a(Ja,br),a(R,kr),p(t,wl,n),p(t,Z,n),a(Z,qt),a(qt,Ka),g(pe,Ka,null),a(Z,xr),a(Z,Za),a(Za,Dr),p(t,vl,n),p(t,Ne,n),a(Ne,Ir),p(t,$l,n),p(t,tt,n),a(tt,jt),a(jt,ts),g(de,ts,null),a(tt,Ar),a(tt,es),a(es,Lr),p(t,El,n),p(t,Re,n),a(Re,Tr),p(t,ql,n),p(t,I,n),a(I,as),a(as,Sr),a(I,Or),a(I,ss),a(ss,Cr),a(I,Pr),a(I,ls),a(ls,Nr),a(I,Rr),a(I,os),a(os,Ur),a(I,Gr),a(I,ns),a(ns,Vr),a(I,Br),a(I,rs),a(rs,zr),p(t,jl,n),p(t,Ue,n),a(Ue,Fr),p(t,bl,n),g(ue,t,n),p(t,kl,n),p(t,et,n),a(et,bt),a(bt,is),g(fe,is,null),a(et,Mr),a(et,ps),a(ps,Hr),p(t,xl,n),p(t,Ge,n),a(Ge,Qr),p(t,Dl,n),g(he,t,n),p(t,Il,n),g(kt,t,n),p(t,Al,n),p(t,Ve,n),a(Ve,Yr),p(t,Ll,n),p(t,xt,n),a(xt,ds),a(ds,Be),a(Be,us),a(us,Wr),a(Be,Xr),a(xt,Jr),a(xt,fs),a(fs,ze),a(ze,hs),a(hs,Kr),a(ze,Zr),p(t,Tl,n),p(t,at,n),a(at,Dt),a(Dt,cs),g(ce,cs,null),a(at,ti),a(at,ms),a(ms,ei),p(t,Sl,n),p(t,It,n),a(It,ai),a(It,_s),a(_s,si),a(It,li),p(t,Ol,n),g(me,t,n),p(t,Cl,n),p(t,Fe,n),a(Fe,oi),p(t,Pl,n),g(_e,t,n),p(t,Nl,n),p(t,Me,n),a(Me,ni),Rl=!0},p(t,[n]){const ge={};n&2&&(ge.$$scope={dirty:n,ctx:t}),lt.$set(ge);const gs={};n&2&&(gs.$$scope={dirty:n,ctx:t}),ft.$set(gs);const ys={};n&2&&(ys.$$scope={dirty:n,ctx:t}),ct.$set(ys);const ws={};n&2&&(ws.$$scope={dirty:n,ctx:t}),$t.$set(ws);const vs={};n&2&&(vs.$$scope={dirty:n,ctx:t}),kt.$set(vs)},i(t){Rl||(y(E.$$.fragment,t),y(Lt.$$.fragment,t),y(lt.$$.fragment,t),y(St.$$.fragment,t),y(Ot.$$.fragment,t),y(Ct.$$.fragment,t),y(Pt.$$.fragment,t),y(Gt.$$.fragment,t),y(zt.$$.fragment,t),y(Ht.$$.fragment,t),y(Qt.$$.fragment,t),y(Yt.$$.fragment,t),y(ft.$$.fragment,t),y(Wt.$$.fragment,t),y(Xt.$$.fragment,t),y(ct.$$.fragment,t),y(te.$$.fragment,t),y(ee.$$.fragment,t),y(se.$$.fragment,t),y(le.$$.fragment,t),y($t.$$.fragment,t),y(oe.$$.fragment,t),y(re.$$.fragment,t),y(pe.$$.fragment,t),y(de.$$.fragment,t),y(ue.$$.fragment,t),y(fe.$$.fragment,t),y(he.$$.fragment,t),y(kt.$$.fragment,t),y(ce.$$.fragment,t),y(me.$$.fragment,t),y(_e.$$.fragment,t),Rl=!0)},o(t){w(E.$$.fragment,t),w(Lt.$$.fragment,t),w(lt.$$.fragment,t),w(St.$$.fragment,t),w(Ot.$$.fragment,t),w(Ct.$$.fragment,t),w(Pt.$$.fragment,t),w(Gt.$$.fragment,t),w(zt.$$.fragment,t),w(Ht.$$.fragment,t),w(Qt.$$.fragment,t),w(Yt.$$.fragment,t),w(ft.$$.fragment,t),w(Wt.$$.fragment,t),w(Xt.$$.fragment,t),w(ct.$$.fragment,t),w(te.$$.fragment,t),w(ee.$$.fragment,t),w(se.$$.fragment,t),w(le.$$.fragment,t),w($t.$$.fragment,t),w(oe.$$.fragment,t),w(re.$$.fragment,t),w(pe.$$.fragment,t),w(de.$$.fragment,t),w(ue.$$.fragment,t),w(fe.$$.fragment,t),w(he.$$.fragment,t),w(kt.$$.fragment,t),w(ce.$$.fragment,t),w(me.$$.fragment,t),w(_e.$$.fragment,t),Rl=!1},d(t){e(h),t&&e(q),t&&e(c),v(E),t&&e(j),t&&e(O),t&&e(bs),t&&e(ye),t&&e(ks),t&&e(U),t&&e(xs),v(Lt,t),t&&e(Ds),t&&e(we),t&&e(Is),t&&e(D),t&&e(As),t&&e(st),t&&e(Ls),v(lt,t),t&&e(Ts),t&&e(M),v(St),t&&e(Ss),t&&e(nt),t&&e(Os),t&&e(rt),t&&e(Cs),v(Ot,t),t&&e(Ps),t&&e(H),t&&e(Ns),t&&e(je),t&&e(Rs),v(Ct,t),t&&e(Us),t&&e(Q),v(Pt),t&&e(Gs),t&&e(G),t&&e(Vs),t&&e(dt),t&&e(Bs),t&&e(ke),t&&e(zs),v(Gt,t),t&&e(Fs),t&&e(Vt),t&&e(Ms),v(zt,t),t&&e(Hs),t&&e(Ft),t&&e(Qs),v(Ht,t),t&&e(Ys),t&&e(Y),v(Qt),t&&e(Ws),t&&e(V),t&&e(Xs),v(Yt,t),t&&e(Js),v(ft,t),t&&e(Ks),t&&e(W),v(Wt),t&&e(Zs),t&&e(De),t&&e(tl),t&&e(Ie),t&&e(el),v(Xt,t),t&&e(al),v(ct,t),t&&e(sl),t&&e(Jt),t&&e(ll),t&&e(_t),t&&e(ol),v(te,t),t&&e(nl),t&&e(X),v(ee),t&&e(rl),t&&e(Se),t&&e(il),t&&e(B),t&&e(pl),t&&e(Oe),t&&e(dl),t&&e(yt),t&&e(ul),v(se,t),t&&e(fl),t&&e(J),v(le),t&&e(hl),t&&e(Ce),t&&e(cl),v($t,t),t&&e(ml),t&&e(K),v(oe),t&&e(_l),t&&e(Pe),t&&e(gl),v(re,t),t&&e(yl),t&&e(ie),t&&e(wl),t&&e(Z),v(pe),t&&e(vl),t&&e(Ne),t&&e($l),t&&e(tt),v(de),t&&e(El),t&&e(Re),t&&e(ql),t&&e(I),t&&e(jl),t&&e(Ue),t&&e(bl),v(ue,t),t&&e(kl),t&&e(et),v(fe),t&&e(xl),t&&e(Ge),t&&e(Dl),v(he,t),t&&e(Il),v(kt,t),t&&e(Al),t&&e(Ve),t&&e(Ll),t&&e(xt),t&&e(Tl),t&&e(at),v(ce),t&&e(Sl),t&&e(It),t&&e(Ol),v(me,t),t&&e(Cl),t&&e(Fe),t&&e(Pl),v(_e,t),t&&e(Nl),t&&e(Me)}}}const Ld={local:"create-a-dataset-loading-script",sections:[{local:"add-dataset-attributes",sections:[{local:"multiple-configurations",title:"Multiple configurations"},{local:"default-configurations",title:"Default configurations"}],title:"Add dataset attributes"},{local:"download-data-files-and-organize-splits",title:"Download data files and organize splits"},{local:"generate-samples",title:"Generate samples"},{local:"testing-data-and-checksum-metadata",sections:[{local:"dataset-metadata",title:"Dataset metadata"},{local:"optional-dummy-data",sections:[{local:"automatic",title:"Automatic"},{local:"manual",title:"Manual"},{local:"run-the-tests",title:"Run the tests"}],title:"(Optional) Dummy data"}],title:"Testing data and checksum metadata"}],title:"Create a dataset loading script"};function Td(S){return jd(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Nd extends vd{constructor(h){super();$d(this,h,Td,Ad,Ed,{})}}export{Nd as default,Ld as metadata};
