import{S as Ef,i as qf,s as Pf,e as l,k as f,t as i,c as o,a as n,m as c,h as p,d as s,b as h,g as r,F as a,Q as Gl,q as b,l as ru,n as Mr,o as j,B as x,p as Vr,w as k,y as E,j as _u,G as vu,a0 as fu,x as P,a1 as $u,T as yu,Y as cu,Z as hu,M as wu,v as bu}from"../chunks/vendor-8138ceec.js";import{T as ya}from"../chunks/Tip-12722dfc.js";import{I as O}from"../chunks/IconCopyLink-2dd3a6ac.js";import{a as uu,C as D}from"../chunks/CodeBlock-fc89709f.js";import{b as mu,I as ju,a as xu}from"../chunks/IconTensorflow-7f573d67.js";function iu(q,d,g){const u=q.slice();return u[8]=d[g],u[10]=g,u}function pu(q){let d,g,u;var y=q[8].icon;function $(_){return{props:{classNames:"mr-1.5"}}}return y&&(d=new y($())),{c(){d&&k(d.$$.fragment),g=ru()},l(_){d&&P(d.$$.fragment,_),g=ru()},m(_,m){d&&E(d,_,m),r(_,g,m),u=!0},p(_,m){if(y!==(y=_[8].icon)){if(d){Mr();const w=d;j(w.$$.fragment,1,0,()=>{x(w,1)}),Vr()}y?(d=new y($()),k(d.$$.fragment),b(d.$$.fragment,1),E(d,g.parentNode,g)):d=null}},i(_){u||(d&&b(d.$$.fragment,_),u=!0)},o(_){d&&j(d.$$.fragment,_),u=!1},d(_){_&&s(g),d&&x(d,_)}}}function du(q){let d,g,u,y=q[8].name+"",$,_,m,w,v,A,S,T=q[8].icon&&pu(q);function Y(){return q[6](q[8])}return{c(){d=l("button"),T&&T.c(),g=f(),u=l("p"),$=i(y),m=f(),this.h()},l(I){d=o(I,"BUTTON",{class:!0});var N=n(d);T&&T.l(N),g=c(N),u=o(N,"P",{class:!0});var z=n(u);$=p(z,y),z.forEach(s),m=c(N),N.forEach(s),this.h()},h(){h(u,"class",_="!m-0 "+q[8].classNames),h(d,"class",w="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(q[10]?"r":"l")+" "+(q[8].group!==q[1]&&"text-gray-500 filter grayscale"))},m(I,N){r(I,d,N),T&&T.m(d,null),a(d,g),a(d,u),a(u,$),a(d,m),v=!0,A||(S=Gl(d,"click",Y),A=!0)},p(I,N){q=I,q[8].icon?T?(T.p(q,N),N&1&&b(T,1)):(T=pu(q),T.c(),b(T,1),T.m(d,g)):T&&(Mr(),j(T,1,1,()=>{T=null}),Vr()),(!v||N&1)&&y!==(y=q[8].name+"")&&_u($,y),(!v||N&1&&_!==(_="!m-0 "+q[8].classNames))&&h(u,"class",_),(!v||N&3&&w!==(w="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(q[10]?"r":"l")+" "+(q[8].group!==q[1]&&"text-gray-500 filter grayscale")))&&h(d,"class",w)},i(I){v||(b(T),v=!0)},o(I){j(T),v=!1},d(I){I&&s(d),T&&T.d(),A=!1,S()}}}function ku(q){let d,g,u,y=q[3].filter(q[5]),$=[];for(let m=0;m<y.length;m+=1)$[m]=du(iu(q,y,m));const _=m=>j($[m],1,1,()=>{$[m]=null});return{c(){d=l("div"),g=l("div");for(let m=0;m<$.length;m+=1)$[m].c();this.h()},l(m){d=o(m,"DIV",{});var w=n(d);g=o(w,"DIV",{class:!0});var v=n(g);for(let A=0;A<$.length;A+=1)$[A].l(v);v.forEach(s),w.forEach(s),this.h()},h(){h(g,"class","bg-white leading-none border border-gray-100 rounded-lg inline-flex p-0.5 text-sm mb-4 select-none")},m(m,w){r(m,d,w),a(d,g);for(let v=0;v<$.length;v+=1)$[v].m(g,null);u=!0},p(m,[w]){if(w&27){y=m[3].filter(m[5]);let v;for(v=0;v<y.length;v+=1){const A=iu(m,y,v);$[v]?($[v].p(A,w),b($[v],1)):($[v]=du(A),$[v].c(),b($[v],1),$[v].m(g,null))}for(Mr(),v=y.length;v<$.length;v+=1)_(v);Vr()}},i(m){if(!u){for(let w=0;w<y.length;w+=1)b($[w]);u=!0}},o(m){$=$.filter(Boolean);for(let w=0;w<$.length;w+=1)j($[w]);u=!1},d(m){m&&s(d),vu($,m)}}}function Eu(q,d,g){let u,{ids:y}=d;const $=y.join("-"),_=mu($);fu(q,_,S=>g(1,u=S));const m=[{id:"pt",classNames:"",icon:ju,name:"Pytorch",group:"group1"},{id:"tf",classNames:"",icon:xu,name:"TensorFlow",group:"group2"},{id:"stringapi",classNames:"text-blue-600",name:"String API",group:"group1"},{id:"readinstruction",classNames:"text-blue-600",name:"ReadInstruction",group:"group2"}];function w(S){$u(_,u=S,u)}const v=S=>y.includes(S.id),A=S=>w(S.group);return q.$$set=S=>{"ids"in S&&g(0,y=S.ids)},[y,u,_,m,w,v,A]}class gu extends Ef{constructor(d){super();qf(this,d,Eu,ku,Pf,{ids:0})}}function qu(q){let d,g,u,y,$,_,m=q[1].highlighted+"",w;return g=new uu({props:{classNames:"transition duration-200 ease-in-out "+(q[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:q[1].code}}),$=new gu({props:{ids:q[4]}}),{c(){d=l("div"),k(g.$$.fragment),u=f(),y=l("pre"),k($.$$.fragment),_=new cu,this.h()},l(v){d=o(v,"DIV",{class:!0});var A=n(d);P(g.$$.fragment,A),A.forEach(s),u=c(v),y=o(v,"PRE",{});var S=n(y);P($.$$.fragment,S),_=hu(S),S.forEach(s),this.h()},h(){h(d,"class","absolute top-2.5 right-4"),_.a=null},m(v,A){r(v,d,A),E(g,d,null),r(v,u,A),r(v,y,A),E($,y,null),_.m(m,y),w=!0},p(v,A){const S={};A&4&&(S.classNames="transition duration-200 ease-in-out "+(v[2]&&"opacity-0")),A&2&&(S.value=v[1].code),g.$set(S),(!w||A&2)&&m!==(m=v[1].highlighted+"")&&_.p(m)},i(v){w||(b(g.$$.fragment,v),b($.$$.fragment,v),w=!0)},o(v){j(g.$$.fragment,v),j($.$$.fragment,v),w=!1},d(v){v&&s(d),x(g),v&&s(u),v&&s(y),x($)}}}function Pu(q){let d,g,u,y,$,_,m=q[0].highlighted+"",w;return g=new uu({props:{classNames:"transition duration-200 ease-in-out "+(q[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:q[0].code}}),$=new gu({props:{ids:q[4]}}),{c(){d=l("div"),k(g.$$.fragment),u=f(),y=l("pre"),k($.$$.fragment),_=new cu,this.h()},l(v){d=o(v,"DIV",{class:!0});var A=n(d);P(g.$$.fragment,A),A.forEach(s),u=c(v),y=o(v,"PRE",{});var S=n(y);P($.$$.fragment,S),_=hu(S),S.forEach(s),this.h()},h(){h(d,"class","absolute top-2.5 right-4"),_.a=null},m(v,A){r(v,d,A),E(g,d,null),r(v,u,A),r(v,y,A),E($,y,null),_.m(m,y),w=!0},p(v,A){const S={};A&4&&(S.classNames="transition duration-200 ease-in-out "+(v[2]&&"opacity-0")),A&1&&(S.value=v[0].code),g.$set(S),(!w||A&1)&&m!==(m=v[0].highlighted+"")&&_.p(m)},i(v){w||(b(g.$$.fragment,v),b($.$$.fragment,v),w=!0)},o(v){j(g.$$.fragment,v),j($.$$.fragment,v),w=!1},d(v){v&&s(d),x(g),v&&s(u),v&&s(y),x($)}}}function Au(q){let d,g,u,y,$,_;const m=[Pu,qu],w=[];function v(A,S){return A[3]==="group1"?0:1}return g=v(q),u=w[g]=m[g](q),{c(){d=l("div"),u.c(),this.h()},l(A){d=o(A,"DIV",{class:!0});var S=n(d);u.l(S),S.forEach(s),this.h()},h(){h(d,"class","code-block relative")},m(A,S){r(A,d,S),w[g].m(d,null),y=!0,$||(_=[Gl(d,"mouseover",q[6]),Gl(d,"focus",q[6]),Gl(d,"mouseout",q[7]),Gl(d,"focus",q[7])],$=!0)},p(A,[S]){let T=g;g=v(A),g===T?w[g].p(A,S):(Mr(),j(w[T],1,1,()=>{w[T]=null}),Vr(),u=w[g],u?u.p(A,S):(u=w[g]=m[g](A),u.c()),b(u,1),u.m(d,null))},i(A){y||(b(u),y=!0)},o(A){j(u),y=!1},d(A){A&&s(d),w[g].d(),$=!1,yu(_)}}}function Su(q,d,g){let u,{group1:y}=d,{group2:$}=d;const _=[y.id,$.id],m=_.join("-"),w=mu(m);fu(q,w,T=>g(3,u=T));let v=!0;function A(){g(2,v=!1)}function S(){g(2,v=!0)}return q.$$set=T=>{"group1"in T&&g(0,y=T.group1),"group2"in T&&g(1,$=T.group2)},[y,$,v,u,_,w,A,S]}class Wl extends Ef{constructor(d){super();qf(this,d,Su,Au,Pf,{group1:0,group2:1})}}function Tu(q){let d,g,u,y,$;return{c(){d=l("p"),g=i("Refer to the "),u=l("a"),y=i("Upload"),$=i(" guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Refer to the "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"Upload"),w.forEach(s),$=p(m," guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),m.forEach(s),this.h()},h(){h(u,"href","./upload_dataset")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&s(d)}}}function Du(q){let d,g,u,y,$;return{c(){d=l("p"),g=i("If you don\u2019t specify which data files to use, "),u=l("code"),y=i("load_dataset"),$=i(" will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data.")},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"If you don\u2019t specify which data files to use, "),u=o(m,"CODE",{});var w=n(u);y=p(w,"load_dataset"),w.forEach(s),$=p(m," will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data."),m.forEach(s)},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&s(d)}}}function Nu(q){let d,g,u,y,$;return{c(){d=l("p"),g=i("Curious about how to load datasets for vision? Check out the image loading guide "),u=l("a"),y=i("here"),$=i("!"),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Curious about how to load datasets for vision? Check out the image loading guide "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"here"),w.forEach(s),$=p(m,"!"),m.forEach(s),this.h()},h(){h(u,"href","./image_process")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&s(d)}}}function Iu(q){let d,g,u,y,$,_,m,w,v,A,S,T,Y,I,N,z,C;return{c(){d=l("p"),g=i("An object data type in "),u=l("a"),y=i("pandas.Series"),$=i(" doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),_=l("a"),m=i("Features"),w=i(" using the "),v=l("code"),A=i("from_dict"),S=i(" or "),T=l("code"),Y=i("from_pandas"),I=i(" methods. See the "),N=l("a"),z=i("troubleshoot"),C=i(" for more details on how to explicitly specify your own features."),this.h()},l(J){d=o(J,"P",{});var H=n(d);g=p(H,"An object data type in "),u=o(H,"A",{href:!0,rel:!0});var wa=n(u);y=p(wa,"pandas.Series"),wa.forEach(s),$=p(H," doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),_=o(H,"A",{href:!0});var jt=n(_);m=p(jt,"Features"),jt.forEach(s),w=p(H," using the "),v=o(H,"CODE",{});var ba=n(v);A=p(ba,"from_dict"),ba.forEach(s),S=p(H," or "),T=o(H,"CODE",{});var ja=n(T);Y=p(ja,"from_pandas"),ja.forEach(s),I=p(H," methods. See the "),N=o(H,"A",{href:!0});var xt=n(N);z=p(xt,"troubleshoot"),xt.forEach(s),C=p(H," for more details on how to explicitly specify your own features."),H.forEach(s),this.h()},h(){h(u,"href","https://pandas.pydata.org/docs/reference/api/pandas.Series.html"),h(u,"rel","nofollow"),h(_,"href","/docs/datasets/pr_4337/en/package_reference/main_classes#datasets.Features"),h(N,"href","./loading#specify-features")},m(J,H){r(J,d,H),a(d,g),a(d,u),a(u,y),a(d,$),a(d,_),a(_,m),a(d,w),a(d,v),a(v,A),a(d,S),a(d,T),a(T,Y),a(d,I),a(d,N),a(N,z),a(d,C)},d(J){J&&s(d)}}}function Cu(q){let d,g,u,y,$;return{c(){d=l("p"),g=i("Using "),u=l("code"),y=i("pct1_dropremainder"),$=i(" rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100.")},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Using "),u=o(m,"CODE",{});var w=n(u);y=p(w,"pct1_dropremainder"),w.forEach(s),$=p(m," rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100."),m.forEach(s)},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&s(d)}}}function Ou(q){let d,g,u,y,$;return{c(){d=l("p"),g=i("See the "),u=l("a"),y=i("Metrics"),$=i(" guide for more details on how to write your own metric loading script."),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"See the "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"Metrics"),w.forEach(s),$=p(m," guide for more details on how to write your own metric loading script."),m.forEach(s),this.h()},h(){h(u,"href","./how_to_metrics#custom-metric-loading-script")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&s(d)}}}function Hu(q){let d,g,u,y,$;return{c(){d=l("p"),g=i("Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),u=l("a"),y=i("Metric.compute()"),$=i(" gathers all the predictions and references from the nodes, and computes the final metric."),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"Metric.compute()"),w.forEach(s),$=p(m," gathers all the predictions and references from the nodes, and computes the final metric."),m.forEach(s),this.h()},h(){h(u,"href","/docs/datasets/pr_4337/en/package_reference/main_classes#datasets.Metric.compute")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&s(d)}}}function Fu(q){let d,g,u,y,$,_,m,w,v,A,S,T,Y,I,N,z,C,J,H,wa,jt,ba,ja,xt,Ur,zr,je,Jr,Yr,xe,Br,Wr,ke,Gr,Ql,xa,Qr,Kl,ka,Xl,et,kt,Ee,cs,Kr,qe,Xr,Zl,Et,Zr,Pe,ti,si,to,B,ai,Ea,ei,li,hs,oi,ni,so,us,ao,qa,ri,eo,qt,ii,Ae,pi,di,lo,ms,oo,Pt,no,F,fi,Se,ci,hi,Te,ui,mi,De,gi,_i,Ne,vi,$i,Ie,yi,wi,ro,gs,io,At,po,W,bi,Ce,ji,xi,_s,ki,Ei,fo,vs,co,St,qi,Oe,Pi,Ai,ho,$s,uo,lt,Tt,He,ys,Si,Fe,Ti,mo,Dt,Di,Pa,Ni,Ii,go,Nt,Le,Ci,Oi,Re,Hi,_o,ws,vo,ot,It,Me,bs,Fi,Ve,Li,$o,L,Ri,Ue,Mi,Vi,ze,Ui,zi,Je,Ji,Yi,Ye,Bi,Wi,Aa,Gi,Qi,yo,Ct,wo,nt,Ot,Be,js,Ki,We,Xi,bo,Sa,Zi,jo,xs,xo,Ta,tp,ko,ks,Eo,Da,sp,qo,Es,Po,Na,ap,Ao,qs,So,Ia,ep,To,Ps,Do,rt,Ht,Ge,As,lp,Qe,op,No,Ft,np,Ca,rp,ip,Io,Ss,Co,Oa,pp,Oo,Ts,Ho,Lt,dp,Ke,fp,cp,Fo,Ds,Lo,Ha,hp,Ro,Ns,Mo,Fa,up,Vo,it,Rt,Xe,Is,mp,Ze,gp,Uo,La,_p,zo,Cs,Jo,Ra,vp,Yo,Os,Bo,pt,Mt,tl,Hs,$p,sl,yp,Wo,Ma,wp,Go,Fs,Qo,Va,bp,Ko,Ls,Xo,dt,Vt,al,Rs,jp,el,xp,Zo,Ut,kp,Ua,Ep,qp,tn,ft,zt,ll,Ms,Pp,ol,Ap,sn,Jt,Sp,za,Tp,Dp,an,Vs,en,ct,Yt,nl,Us,Np,rl,Ip,ln,Bt,Cp,Ja,Op,Hp,on,zs,nn,Wt,rn,ht,Gt,il,Js,Fp,pl,Lp,pn,Ya,Rp,dn,G,Mp,dl,Vp,Up,fl,zp,Jp,fn,ut,Qt,cl,Ys,Yp,hl,Bp,cn,Q,Wp,Ba,Gp,Qp,Wa,Kp,Xp,hn,K,Zp,ul,td,sd,ml,ad,ed,un,Bs,mn,Kt,ld,gl,od,nd,gn,Ws,_n,Ga,rd,vn,Gs,$n,Qa,id,yn,Qs,wn,Ka,pd,bn,Ks,jn,mt,Xt,_l,Xs,dd,vl,fd,xn,Xa,cd,kn,Zs,En,Zt,hd,$l,ud,md,qn,ta,Pn,ts,An,Za,Sn,gt,ss,yl,sa,gd,wl,_d,Tn,te,vd,Dn,_t,as,bl,aa,$d,jl,yd,Nn,M,wd,se,bd,jd,xl,xd,kd,kl,Ed,qd,In,es,Pd,ea,Ad,Sd,Cn,la,On,vt,ls,El,oa,Td,ql,Dd,Hn,X,Nd,ae,Id,Cd,na,Od,Hd,Fn,Z,Fd,ee,Ld,Rd,le,Md,Vd,Ln,ra,Rn,tt,Ud,Pl,zd,Jd,oe,Yd,Bd,Mn,ia,Vn,ne,Wd,Un,pa,zn,$t,os,Al,da,Gd,Sl,Qd,Jn,re,Kd,Yn,fa,Bn,ns,Wn,yt,rs,Tl,ca,Xd,Dl,Zd,Gn,st,tf,Nl,sf,af,ie,ef,lf,Qn,ha,Kn,wt,is,Il,ua,of,Cl,nf,Xn,pe,rf,Zn,de,pf,tr,at,Ol,ma,df,Hl,ff,cf,hf,Fl,bt,uf,Ll,mf,gf,Rl,_f,vf,$f,Ml,ga,yf,fe,wf,bf,sr,_a,ar,ps,er,ds,jf,Vl,xf,kf,lr,va,or;return _=new O({}),cs=new O({}),us=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('lhoestq/demo1')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;lhoestq/demo1&#x27;</span>)`}}),ms=new D({props:{code:`dataset = load_dataset(
  "lhoestq/custom_squad",
  revision="main"  # tag name, or branch name, or commit hash
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(
<span class="hljs-meta">&gt;&gt;&gt; </span>  <span class="hljs-string">&quot;lhoestq/custom_squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>  revision=<span class="hljs-string">&quot;main&quot;</span>  <span class="hljs-comment"># tag name, or branch name, or commit hash</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),Pt=new ya({props:{$$slots:{default:[Tu]},$$scope:{ctx:q}}}),gs=new D({props:{code:`data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;train.csv&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;test.csv&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;namespace/your_dataset_name&quot;</span>, data_files=data_files)`}}),At=new ya({props:{warning:!0,$$slots:{default:[Du]},$$scope:{ctx:q}}}),vs=new D({props:{code:`from datasets import load_dataset
c4_subset = load_dataset('allenai/c4', data_files='en/c4-train.0000*-of-01024.json.gz')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_subset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, data_files=<span class="hljs-string">&#x27;en/c4-train.0000*-of-01024.json.gz&#x27;</span>)`}}),$s=new D({props:{code:`data_files = {"validation": "en/c4-validation.*.json.gz"}
c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;validation&quot;</span>: <span class="hljs-string">&quot;en/c4-validation.*.json.gz&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_validation = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;validation&quot;</span>)`}}),ys=new O({}),ws=new D({props:{code:`dataset = load_dataset("path/to/local/loading_script/loading_script.py", split="train")
dataset = load_dataset("path/to/local/loading_script", split="train")  # equivalent because the file has the same name as the directory`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;path/to/local/loading_script/loading_script.py&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;path/to/local/loading_script&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)  <span class="hljs-comment"># equivalent because the file has the same name as the directory</span>`}}),bs=new O({}),Ct=new ya({props:{$$slots:{default:[Nu]},$$scope:{ctx:q}}}),js=new O({}),xs=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('csv', data_files='my_file.csv')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.csv&#x27;</span>)`}}),ks=new D({props:{code:"dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv'])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=[<span class="hljs-string">&#x27;my_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_2.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_3.csv&#x27;</span>])'}}),Es=new D({props:{code:"dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'], 'test': 'my_test_file.csv'})",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_train_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_train_file_2.csv&#x27;</span>], <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.csv&#x27;</span>})'}}),qs=new D({props:{code:`base_url = "https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/"
dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train.csv&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: base_url + <span class="hljs-string">&#x27;test.csv&#x27;</span>})`}}),Ps=new D({props:{code:`url = "https://domain.org/train_data.zip"
data_files = {"train": url}
dataset = load_dataset("csv", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://domain.org/train_data.zip&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: url}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files)`}}),As=new O({}),Ss=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('json', data_files='my_file.json')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>)`}}),Ts=new D({props:{code:`{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true}`,highlighted:`{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false}
{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}`}}),Ds=new D({props:{code:`{"version": "0.1.0",
    "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
            {"a": 4, "b": -5.5, "c": null, "d": true}]
}

>>> from datasets import load_dataset
>>> dataset = load_dataset('json', data_files='my_file.json', field='data')`,highlighted:`{<span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;0.1.0&quot;</span>,
    <span class="hljs-string">&quot;data&quot;</span>: [{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false},
            {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}]
}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>, field=<span class="hljs-string">&#x27;data&#x27;</span>)`}}),Ns=new D({props:{code:`base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
dataset = load_dataset('json', data_files={'train': base_url + 'train-v1.1.json', 'validation': base_url + 'dev-v1.1.json'}, field="data")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train-v1.1.json&#x27;</span>, <span class="hljs-string">&#x27;validation&#x27;</span>: base_url + <span class="hljs-string">&#x27;dev-v1.1.json&#x27;</span>}, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),Is=new O({}),Cs=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('text', data_files={'train': ['my_text_1.txt', 'my_text_2.txt'], 'test': 'my_test_file.txt'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_text_1.txt&#x27;</span>, <span class="hljs-string">&#x27;my_text_2.txt&#x27;</span>], <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.txt&#x27;</span>})`}}),Os=new D({props:{code:"dataset = load_dataset('text', data_files='https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files=<span class="hljs-string">&#x27;https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt&#x27;</span>)'}}),Hs=new O({}),Fs=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;train.parquet&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;test.parquet&#x27;</span>})`}}),Ls=new D({props:{code:`base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
data_files = {"train": base_url + "wikipedia-train.parquet"}
wiki = load_dataset("parquet", data_files=data_files, split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;wikipedia-train.parquet&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Rs=new O({}),Ms=new O({}),Vs=new D({props:{code:`from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>my_dict = {<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_dict(my_dict)`}}),Us=new O({}),zs=new D({props:{code:`from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-meta">&gt;&gt;&gt; </span>df = pd.DataFrame({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_pandas(df)`}}),Wt=new ya({props:{warning:!0,$$slots:{default:[Iu]},$$scope:{ctx:q}}}),Js=new O({}),Ys=new O({}),Bs=new Wl({props:{group1:{id:"stringapi",code:"train_test_ds = datasets.load_dataset('bookcorpus', split='train+test')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train+test&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = datasets.ReadInstruction('train') + datasets.ReadInstruction('test')
train_test_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Ws=new Wl({props:{group1:{id:"stringapi",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split='train[10:20]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[10:20]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', from_=10, to=20, unit='abs'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">10</span>, to=<span class="hljs-number">20</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>))'}}}),Gs=new Wl({props:{group1:{id:"stringapi",code:"train_10pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', to=10, unit='%'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))'}}}),Qs=new Wl({props:{group1:{id:"stringapi",code:"train_10_80pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]+train[-80%:]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]+train[-80%:]&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = (datasets.ReadInstruction('train', to=10, unit='%') + datasets.ReadInstruction('train', from_=-80, unit='%'))
train_10_80pct_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = (datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=-<span class="hljs-number">80</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Ks=new Wl({props:{group1:{id:"stringapi",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[<span class="hljs-subst">{k}</span>%:<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[:<span class="hljs-subst">{k}</span>%]+train[<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%:]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`},group2:{id:"readinstruction",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', [datasets.ReadInstruction('train', from_=k, to=k+10, unit='%') for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', [(datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')) for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [(datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}}),Xs=new O({}),Zs=new D({props:{code:`# Assuming *train* split contains 999 records.
# 19 records, from 500 (included) to 519 (excluded).
>>> train_50_52_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%]')
# 20 records, from 519 (included) to 539 (excluded).
>>> train_52_54_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%]')`,highlighted:`<span class="hljs-comment"># Assuming *train* split contains 999 records.</span>
<span class="hljs-comment"># 19 records, from 500 (included) to 519 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%]&#x27;</span>)
<span class="hljs-comment"># 20 records, from 519 (included) to 539 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%]&#x27;</span>)`}}),ta=new D({props:{code:`# 18 records, from 450 (included) to 468 (excluded).
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction( 'train', from_=50, to=52, unit='%', rounding='pct1_dropremainder'))
# 18 records, from 468 (included) to 486 (excluded).
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train',from_=52, to=54, unit='%', rounding='pct1_dropremainder'))
# Or equivalently:
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%](pct1_dropremainder)')
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%](pct1_dropremainder)')`,highlighted:`<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction( <span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># Or equivalently:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%](pct1_dropremainder)&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%](pct1_dropremainder)&#x27;</span>)`}}),ts=new ya({props:{warning:!0,$$slots:{default:[Cu]},$$scope:{ctx:q}}}),sa=new O({}),aa=new O({}),la=new D({props:{code:'dataset = load_dataset("matinf", "summarization")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;matinf&quot;</span>, <span class="hljs-string">&quot;summarization&quot;</span>)
Downloading <span class="hljs-keyword">and</span> preparing dataset matinf/summarization (download: Unknown size, generated: <span class="hljs-number">246.89</span> MiB, post-processed: Unknown size, total: <span class="hljs-number">246.89</span> MiB) to /root/.cache/huggingface/datasets/matinf/summarization/<span class="hljs-number">1.0</span><span class="hljs-number">.0</span>/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
AssertionError: The dataset matinf <span class="hljs-keyword">with</span> config summarization requires manual data. 
Please follow the manual download instructions: To use MATINF you have to download it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9). You will receive a download link <span class="hljs-keyword">and</span> a password once you complete the form. Please extract <span class="hljs-built_in">all</span> files <span class="hljs-keyword">in</span> one folder <span class="hljs-keyword">and</span> load the dataset <span class="hljs-keyword">with</span>: *datasets.load_dataset(<span class="hljs-string">&#x27;matinf&#x27;</span>, data_dir=<span class="hljs-string">&#x27;path/to/folder/folder_name&#x27;</span>)*. 
Manual data can be loaded <span class="hljs-keyword">with</span> \`datasets.load_dataset(matinf, data_dir=<span class="hljs-string">&#x27;&lt;path/to/manual/data&gt;&#x27;</span>) `}}),oa=new O({}),ra=new D({props:{code:`class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]
emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>class_names = [<span class="hljs-string">&quot;sadness&quot;</span>, <span class="hljs-string">&quot;joy&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;anger&quot;</span>, <span class="hljs-string">&quot;fear&quot;</span>, <span class="hljs-string">&quot;surprise&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>emotion_features = Features({<span class="hljs-string">&#x27;text&#x27;</span>: Value(<span class="hljs-string">&#x27;string&#x27;</span>), <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=class_names)})`}}),ia=new D({props:{code:"dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=file_dict, delimiter=<span class="hljs-string">&#x27;;&#x27;</span>, column_names=[<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>], features=emotion_features)'}}),pa=new D({props:{code:"dataset['train'].features",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&#x27;train&#x27;</span>].features
{<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">6</span>, names=[<span class="hljs-string">&#x27;sadness&#x27;</span>, <span class="hljs-string">&#x27;joy&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;anger&#x27;</span>, <span class="hljs-string">&#x27;fear&#x27;</span>, <span class="hljs-string">&#x27;surprise&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),da=new O({}),fa=new D({props:{code:`from datasets import load_metric
metric = load_metric('PATH/TO/MY/METRIC/SCRIPT')

# Example of typical usage
for batch in dataset:
    inputs, references = batch
    predictions = model(inputs)
    metric.add_batch(predictions=predictions, references=references)
score = metric.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;PATH/TO/MY/METRIC/SCRIPT&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example of typical usage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataset:
<span class="hljs-meta">... </span>    inputs, references = batch
<span class="hljs-meta">... </span>    predictions = model(inputs)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=references)
<span class="hljs-meta">&gt;&gt;&gt; </span>score = metric.compute()`}}),ns=new ya({props:{$$slots:{default:[Ou]},$$scope:{ctx:q}}}),ca=new O({}),ha=new D({props:{code:`from datasets import load_metric
metric = load_metric('bleurt', name='bleurt-base-128')
metric = load_metric('bleurt', name='bleurt-base-512')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-128&#x27;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-512&#x27;</span>)</span>`}}),ua=new O({}),_a=new D({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=rank)`}}),ps=new ya({props:{$$slots:{default:[Hu]},$$scope:{ctx:q}}}),va=new D({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=process_id, experiment_id=<span class="hljs-string">&quot;My_experiment_10&quot;</span>)`}}),{c(){d=l("meta"),g=f(),u=l("h1"),y=l("a"),$=l("span"),k(_.$$.fragment),m=f(),w=l("span"),v=i("Load"),A=f(),S=l("p"),T=i("You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),Y=f(),I=l("p"),N=i("This guide will show you how to load a dataset from:"),z=f(),C=l("ul"),J=l("li"),H=i("The Hub without a dataset loading script"),wa=f(),jt=l("li"),ba=i("Local loading script"),ja=f(),xt=l("li"),Ur=i("Local files"),zr=f(),je=l("li"),Jr=i("In-memory data"),Yr=f(),xe=l("li"),Br=i("Offline"),Wr=f(),ke=l("li"),Gr=i("A specific slice of a split"),Ql=f(),xa=l("p"),Qr=i("You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Kl=f(),ka=l("a"),Xl=f(),et=l("h2"),kt=l("a"),Ee=l("span"),k(cs.$$.fragment),Kr=f(),qe=l("span"),Xr=i("Hugging Face Hub"),Zl=f(),Et=l("p"),Zr=i("In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),Pe=l("strong"),ti=i("without"),si=i(" a loading script!"),to=f(),B=l("p"),ai=i("First, create a dataset repository and upload your data files. Then you can use "),Ea=l("a"),ei=i("load_dataset()"),li=i(" like you learned in the tutorial. For example, load the files from this "),hs=l("a"),oi=i("demo repository"),ni=i(" by providing the repository namespace and dataset name:"),so=f(),k(us.$$.fragment),ao=f(),qa=l("p"),ri=i("This dataset repository contains CSV files, and this code loads all the data from the CSV files."),eo=f(),qt=l("p"),ii=i("Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),Ae=l("code"),pi=i("revision"),di=i(" flag to specify which dataset version you want to load:"),lo=f(),k(ms.$$.fragment),oo=f(),k(Pt.$$.fragment),no=f(),F=l("p"),fi=i("If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),Se=l("code"),ci=i("train"),hi=i(" split. Use the "),Te=l("code"),ui=i("data_files"),mi=i(" parameter to map data files to splits like "),De=l("code"),gi=i("train"),_i=i(", "),Ne=l("code"),vi=i("validation"),$i=i(" and "),Ie=l("code"),yi=i("test"),wi=i(":"),ro=f(),k(gs.$$.fragment),io=f(),k(At.$$.fragment),po=f(),W=l("p"),bi=i("You can also load a specific subset of the files with the "),Ce=l("code"),ji=i("data_files"),xi=i(" parameter. The example below loads files from the "),_s=l("a"),ki=i("C4 dataset"),Ei=i(":"),fo=f(),k(vs.$$.fragment),co=f(),St=l("p"),qi=i("Specify a custom split with the "),Oe=l("code"),Pi=i("split"),Ai=i(" parameter:"),ho=f(),k($s.$$.fragment),uo=f(),lt=l("h2"),Tt=l("a"),He=l("span"),k(ys.$$.fragment),Si=f(),Fe=l("span"),Ti=i("Local loading script"),mo=f(),Dt=l("p"),Di=i("You may have a \u{1F917} Datasets loading script locally on your computer. You can load this dataset by passing to "),Pa=l("a"),Ni=i("load_dataset()"),Ii=i(" one of the following paths:"),go=f(),Nt=l("ul"),Le=l("li"),Ci=i("The local path to the loading script file."),Oi=f(),Re=l("li"),Hi=i("The local path to the directory containing the loading script file (only if the script file has the same name as the directory)."),_o=f(),k(ws.$$.fragment),vo=f(),ot=l("h2"),It=l("a"),Me=l("span"),k(bs.$$.fragment),Fi=f(),Ve=l("span"),Li=i("Local and remote files"),$o=f(),L=l("p"),Ri=i("\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Ue=l("code"),Mi=i("csv"),Vi=i(", "),ze=l("code"),Ui=i("json"),zi=i(", "),Je=l("code"),Ji=i("txt"),Yi=i(" or "),Ye=l("code"),Bi=i("parquet"),Wi=i(" file. The "),Aa=l("a"),Gi=i("load_dataset()"),Qi=i(" method is able to load each of these file types."),yo=f(),k(Ct.$$.fragment),wo=f(),nt=l("h3"),Ot=l("a"),Be=l("span"),k(js.$$.fragment),Ki=f(),We=l("span"),Xi=i("CSV"),bo=f(),Sa=l("p"),Zi=i("\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),jo=f(),k(xs.$$.fragment),xo=f(),Ta=l("p"),tp=i("If you have more than one CSV file:"),ko=f(),k(ks.$$.fragment),Eo=f(),Da=l("p"),sp=i("You can also map the training and test splits to specific CSV files:"),qo=f(),k(Es.$$.fragment),Po=f(),Na=l("p"),ap=i("To load remote CSV files via HTTP, you can pass the URLs:"),Ao=f(),k(qs.$$.fragment),So=f(),Ia=l("p"),ep=i("To load zipped CSV files:"),To=f(),k(Ps.$$.fragment),Do=f(),rt=l("h3"),Ht=l("a"),Ge=l("span"),k(As.$$.fragment),lp=f(),Qe=l("span"),op=i("JSON"),No=f(),Ft=l("p"),np=i("JSON files are loaded directly with "),Ca=l("a"),rp=i("load_dataset()"),ip=i(" as shown below:"),Io=f(),k(Ss.$$.fragment),Co=f(),Oa=l("p"),pp=i("JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),Oo=f(),k(Ts.$$.fragment),Ho=f(),Lt=l("p"),dp=i("Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Ke=l("code"),fp=i("field"),cp=i(" argument as shown in the following:"),Fo=f(),k(Ds.$$.fragment),Lo=f(),Ha=l("p"),hp=i("To load remote JSON files via HTTP, you can pass the URLs:"),Ro=f(),k(Ns.$$.fragment),Mo=f(),Fa=l("p"),up=i("While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Vo=f(),it=l("h3"),Rt=l("a"),Xe=l("span"),k(Is.$$.fragment),mp=f(),Ze=l("span"),gp=i("Text files"),Uo=f(),La=l("p"),_p=i("Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),zo=f(),k(Cs.$$.fragment),Jo=f(),Ra=l("p"),vp=i("To load remote TXT files via HTTP, you can pass the URLs:"),Yo=f(),k(Os.$$.fragment),Bo=f(),pt=l("h3"),Mt=l("a"),tl=l("span"),k(Hs.$$.fragment),$p=f(),sl=l("span"),yp=i("Parquet"),Wo=f(),Ma=l("p"),wp=i("Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Go=f(),k(Fs.$$.fragment),Qo=f(),Va=l("p"),bp=i("To load remote parquet files via HTTP, you can pass the URLs:"),Ko=f(),k(Ls.$$.fragment),Xo=f(),dt=l("h2"),Vt=l("a"),al=l("span"),k(Rs.$$.fragment),jp=f(),el=l("span"),xp=i("In-memory data"),Zo=f(),Ut=l("p"),kp=i("\u{1F917} Datasets will also allow you to create a "),Ua=l("a"),Ep=i("Dataset"),qp=i(" directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),tn=f(),ft=l("h3"),zt=l("a"),ll=l("span"),k(Ms.$$.fragment),Pp=f(),ol=l("span"),Ap=i("Python dictionary"),sn=f(),Jt=l("p"),Sp=i("Load Python dictionaries with "),za=l("a"),Tp=i("Dataset.from_dict()"),Dp=i(":"),an=f(),k(Vs.$$.fragment),en=f(),ct=l("h3"),Yt=l("a"),nl=l("span"),k(Us.$$.fragment),Np=f(),rl=l("span"),Ip=i("Pandas DataFrame"),ln=f(),Bt=l("p"),Cp=i("Load Pandas DataFrames with "),Ja=l("a"),Op=i("Dataset.from_pandas()"),Hp=i(":"),on=f(),k(zs.$$.fragment),nn=f(),k(Wt.$$.fragment),rn=f(),ht=l("h2"),Gt=l("a"),il=l("span"),k(Js.$$.fragment),Fp=f(),pl=l("span"),Lp=i("Offline"),pn=f(),Ya=l("p"),Rp=i("Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),dn=f(),G=l("p"),Mp=i("If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),dl=l("code"),Vp=i("HF_DATASETS_OFFLINE"),Up=i(" to "),fl=l("code"),zp=i("1"),Jp=i(" to enable full offline mode."),fn=f(),ut=l("h2"),Qt=l("a"),cl=l("span"),k(Ys.$$.fragment),Yp=f(),hl=l("span"),Bp=i("Slice splits"),cn=f(),Q=l("p"),Wp=i("For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Ba=l("a"),Gp=i("ReadInstruction"),Qp=i(". Strings are more compact and readable for simple cases, while "),Wa=l("a"),Kp=i("ReadInstruction"),Xp=i(" is easier to use with variable slicing parameters."),hn=f(),K=l("p"),Zp=i("Concatenate the "),ul=l("code"),td=i("train"),sd=i(" and "),ml=l("code"),ad=i("test"),ed=i(" split by:"),un=f(),k(Bs.$$.fragment),mn=f(),Kt=l("p"),ld=i("Select specific rows of the "),gl=l("code"),od=i("train"),nd=i(" split:"),gn=f(),k(Ws.$$.fragment),_n=f(),Ga=l("p"),rd=i("Or select a percentage of the split with:"),vn=f(),k(Gs.$$.fragment),$n=f(),Qa=l("p"),id=i("You can even select a combination of percentages from each split:"),yn=f(),k(Qs.$$.fragment),wn=f(),Ka=l("p"),pd=i("Finally, create cross-validated dataset splits by:"),bn=f(),k(Ks.$$.fragment),jn=f(),mt=l("h3"),Xt=l("a"),_l=l("span"),k(Xs.$$.fragment),dd=f(),vl=l("span"),fd=i("Percent slicing and rounding"),xn=f(),Xa=l("p"),cd=i("For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),kn=f(),k(Zs.$$.fragment),En=f(),Zt=l("p"),hd=i("If you want equal sized splits, use "),$l=l("code"),ud=i("pct1_dropremainder"),md=i(" rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),qn=f(),k(ta.$$.fragment),Pn=f(),k(ts.$$.fragment),An=f(),Za=l("a"),Sn=f(),gt=l("h2"),ss=l("a"),yl=l("span"),k(sa.$$.fragment),gd=f(),wl=l("span"),_d=i("Troubleshooting"),Tn=f(),te=l("p"),vd=i("Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),Dn=f(),_t=l("h3"),as=l("a"),bl=l("span"),k(aa.$$.fragment),$d=f(),jl=l("span"),yd=i("Manual download"),Nn=f(),M=l("p"),wd=i("Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),se=l("a"),bd=i("load_dataset()"),jd=i(" to throw an "),xl=l("code"),xd=i("AssertionError"),kd=i(". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),kl=l("code"),Ed=i("data_dir"),qd=i(" argument to specify the path to the files you just downloaded."),In=f(),es=l("p"),Pd=i("For example, if you try to download a configuration from the "),ea=l("a"),Ad=i("MATINF"),Sd=i(" dataset:"),Cn=f(),k(la.$$.fragment),On=f(),vt=l("h3"),ls=l("a"),El=l("span"),k(oa.$$.fragment),Td=f(),ql=l("span"),Dd=i("Specify features"),Hn=f(),X=l("p"),Nd=i("When you create a dataset from local files, the "),ae=l("a"),Id=i("Features"),Cd=i(" are automatically inferred by "),na=l("a"),Od=i("Apache Arrow"),Hd=i(". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),Fn=f(),Z=l("p"),Fd=i("The following example shows how you can add custom labels with "),ee=l("a"),Ld=i("ClassLabel"),Rd=i(". First, define your own labels using the "),le=l("a"),Md=i("Features"),Vd=i(" class:"),Ln=f(),k(ra.$$.fragment),Rn=f(),tt=l("p"),Ud=i("Next, specify the "),Pl=l("code"),zd=i("features"),Jd=i(" argument in "),oe=l("a"),Yd=i("load_dataset()"),Bd=i(" with the features you just created:"),Mn=f(),k(ia.$$.fragment),Vn=f(),ne=l("p"),Wd=i("Now when you look at your dataset features, you can see it uses the custom labels you defined:"),Un=f(),k(pa.$$.fragment),zn=f(),$t=l("h2"),os=l("a"),Al=l("span"),k(da.$$.fragment),Gd=f(),Sl=l("span"),Qd=i("Metrics"),Jn=f(),re=l("p"),Kd=i("When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),Yn=f(),k(fa.$$.fragment),Bn=f(),k(ns.$$.fragment),Wn=f(),yt=l("h3"),rs=l("a"),Tl=l("span"),k(ca.$$.fragment),Xd=f(),Dl=l("span"),Zd=i("Load configurations"),Gn=f(),st=l("p"),tf=i("It is possible for a metric to have different configurations. The configurations are stored in the "),Nl=l("code"),sf=i("config_name"),af=i(" parameter in "),ie=l("a"),ef=i("MetricInfo"),lf=i(" attribute. When you load a metric, provide the configuration name as shown in the following:"),Qn=f(),k(ha.$$.fragment),Kn=f(),wt=l("h3"),is=l("a"),Il=l("span"),k(ua.$$.fragment),of=f(),Cl=l("span"),nf=i("Distributed setup"),Xn=f(),pe=l("p"),rf=i("When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),Zn=f(),de=l("p"),pf=i("For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),tr=f(),at=l("ol"),Ol=l("li"),ma=l("p"),df=i("Define the total number of processes with the "),Hl=l("code"),ff=i("num_process"),cf=i(" argument."),hf=f(),Fl=l("li"),bt=l("p"),uf=i("Set the process "),Ll=l("code"),mf=i("rank"),gf=i(" as an integer between zero and "),Rl=l("code"),_f=i("num_process - 1"),vf=i("."),$f=f(),Ml=l("li"),ga=l("p"),yf=i("Load your metric with "),fe=l("a"),wf=i("load_metric()"),bf=i(" with these arguments:"),sr=f(),k(_a.$$.fragment),ar=f(),k(ps.$$.fragment),er=f(),ds=l("p"),jf=i("In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Vl=l("code"),xf=i("experiment_id"),kf=i(" to distinguish the separate evaluations:"),lr=f(),k(va.$$.fragment),this.h()},l(t){const e=wu('[data-svelte="svelte-1phssyn"]',document.head);d=o(e,"META",{name:!0,content:!0}),e.forEach(s),g=c(t),u=o(t,"H1",{class:!0});var $a=n(u);y=o($a,"A",{id:!0,class:!0,href:!0});var Ul=n(y);$=o(Ul,"SPAN",{});var zl=n($);P(_.$$.fragment,zl),zl.forEach(s),Ul.forEach(s),m=c($a),w=o($a,"SPAN",{});var Jl=n(w);v=p(Jl,"Load"),Jl.forEach(s),$a.forEach(s),A=c(t),S=o(t,"P",{});var Yl=n(S);T=p(Yl,"You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),Yl.forEach(s),Y=c(t),I=o(t,"P",{});var Bl=n(I);N=p(Bl,"This guide will show you how to load a dataset from:"),Bl.forEach(s),z=c(t),C=o(t,"UL",{});var R=n(C);J=o(R,"LI",{});var Af=n(J);H=p(Af,"The Hub without a dataset loading script"),Af.forEach(s),wa=c(R),jt=o(R,"LI",{});var Sf=n(jt);ba=p(Sf,"Local loading script"),Sf.forEach(s),ja=c(R),xt=o(R,"LI",{});var Tf=n(xt);Ur=p(Tf,"Local files"),Tf.forEach(s),zr=c(R),je=o(R,"LI",{});var Df=n(je);Jr=p(Df,"In-memory data"),Df.forEach(s),Yr=c(R),xe=o(R,"LI",{});var Nf=n(xe);Br=p(Nf,"Offline"),Nf.forEach(s),Wr=c(R),ke=o(R,"LI",{});var If=n(ke);Gr=p(If,"A specific slice of a split"),If.forEach(s),R.forEach(s),Ql=c(t),xa=o(t,"P",{});var Cf=n(xa);Qr=p(Cf,"You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Cf.forEach(s),Kl=c(t),ka=o(t,"A",{id:!0}),n(ka).forEach(s),Xl=c(t),et=o(t,"H2",{class:!0});var nr=n(et);kt=o(nr,"A",{id:!0,class:!0,href:!0});var Of=n(kt);Ee=o(Of,"SPAN",{});var Hf=n(Ee);P(cs.$$.fragment,Hf),Hf.forEach(s),Of.forEach(s),Kr=c(nr),qe=o(nr,"SPAN",{});var Ff=n(qe);Xr=p(Ff,"Hugging Face Hub"),Ff.forEach(s),nr.forEach(s),Zl=c(t),Et=o(t,"P",{});var rr=n(Et);Zr=p(rr,"In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),Pe=o(rr,"STRONG",{});var Lf=n(Pe);ti=p(Lf,"without"),Lf.forEach(s),si=p(rr," a loading script!"),rr.forEach(s),to=c(t),B=o(t,"P",{});var ce=n(B);ai=p(ce,"First, create a dataset repository and upload your data files. Then you can use "),Ea=o(ce,"A",{href:!0});var Rf=n(Ea);ei=p(Rf,"load_dataset()"),Rf.forEach(s),li=p(ce," like you learned in the tutorial. For example, load the files from this "),hs=o(ce,"A",{href:!0,rel:!0});var Mf=n(hs);oi=p(Mf,"demo repository"),Mf.forEach(s),ni=p(ce," by providing the repository namespace and dataset name:"),ce.forEach(s),so=c(t),P(us.$$.fragment,t),ao=c(t),qa=o(t,"P",{});var Vf=n(qa);ri=p(Vf,"This dataset repository contains CSV files, and this code loads all the data from the CSV files."),Vf.forEach(s),eo=c(t),qt=o(t,"P",{});var ir=n(qt);ii=p(ir,"Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),Ae=o(ir,"CODE",{});var Uf=n(Ae);pi=p(Uf,"revision"),Uf.forEach(s),di=p(ir," flag to specify which dataset version you want to load:"),ir.forEach(s),lo=c(t),P(ms.$$.fragment,t),oo=c(t),P(Pt.$$.fragment,t),no=c(t),F=o(t,"P",{});var V=n(F);fi=p(V,"If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),Se=o(V,"CODE",{});var zf=n(Se);ci=p(zf,"train"),zf.forEach(s),hi=p(V," split. Use the "),Te=o(V,"CODE",{});var Jf=n(Te);ui=p(Jf,"data_files"),Jf.forEach(s),mi=p(V," parameter to map data files to splits like "),De=o(V,"CODE",{});var Yf=n(De);gi=p(Yf,"train"),Yf.forEach(s),_i=p(V,", "),Ne=o(V,"CODE",{});var Bf=n(Ne);vi=p(Bf,"validation"),Bf.forEach(s),$i=p(V," and "),Ie=o(V,"CODE",{});var Wf=n(Ie);yi=p(Wf,"test"),Wf.forEach(s),wi=p(V,":"),V.forEach(s),ro=c(t),P(gs.$$.fragment,t),io=c(t),P(At.$$.fragment,t),po=c(t),W=o(t,"P",{});var he=n(W);bi=p(he,"You can also load a specific subset of the files with the "),Ce=o(he,"CODE",{});var Gf=n(Ce);ji=p(Gf,"data_files"),Gf.forEach(s),xi=p(he," parameter. The example below loads files from the "),_s=o(he,"A",{href:!0,rel:!0});var Qf=n(_s);ki=p(Qf,"C4 dataset"),Qf.forEach(s),Ei=p(he,":"),he.forEach(s),fo=c(t),P(vs.$$.fragment,t),co=c(t),St=o(t,"P",{});var pr=n(St);qi=p(pr,"Specify a custom split with the "),Oe=o(pr,"CODE",{});var Kf=n(Oe);Pi=p(Kf,"split"),Kf.forEach(s),Ai=p(pr," parameter:"),pr.forEach(s),ho=c(t),P($s.$$.fragment,t),uo=c(t),lt=o(t,"H2",{class:!0});var dr=n(lt);Tt=o(dr,"A",{id:!0,class:!0,href:!0});var Xf=n(Tt);He=o(Xf,"SPAN",{});var Zf=n(He);P(ys.$$.fragment,Zf),Zf.forEach(s),Xf.forEach(s),Si=c(dr),Fe=o(dr,"SPAN",{});var tc=n(Fe);Ti=p(tc,"Local loading script"),tc.forEach(s),dr.forEach(s),mo=c(t),Dt=o(t,"P",{});var fr=n(Dt);Di=p(fr,"You may have a \u{1F917} Datasets loading script locally on your computer. You can load this dataset by passing to "),Pa=o(fr,"A",{href:!0});var sc=n(Pa);Ni=p(sc,"load_dataset()"),sc.forEach(s),Ii=p(fr," one of the following paths:"),fr.forEach(s),go=c(t),Nt=o(t,"UL",{});var cr=n(Nt);Le=o(cr,"LI",{});var ac=n(Le);Ci=p(ac,"The local path to the loading script file."),ac.forEach(s),Oi=c(cr),Re=o(cr,"LI",{});var ec=n(Re);Hi=p(ec,"The local path to the directory containing the loading script file (only if the script file has the same name as the directory)."),ec.forEach(s),cr.forEach(s),_o=c(t),P(ws.$$.fragment,t),vo=c(t),ot=o(t,"H2",{class:!0});var hr=n(ot);It=o(hr,"A",{id:!0,class:!0,href:!0});var lc=n(It);Me=o(lc,"SPAN",{});var oc=n(Me);P(bs.$$.fragment,oc),oc.forEach(s),lc.forEach(s),Fi=c(hr),Ve=o(hr,"SPAN",{});var nc=n(Ve);Li=p(nc,"Local and remote files"),nc.forEach(s),hr.forEach(s),$o=c(t),L=o(t,"P",{});var U=n(L);Ri=p(U,"\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Ue=o(U,"CODE",{});var rc=n(Ue);Mi=p(rc,"csv"),rc.forEach(s),Vi=p(U,", "),ze=o(U,"CODE",{});var ic=n(ze);Ui=p(ic,"json"),ic.forEach(s),zi=p(U,", "),Je=o(U,"CODE",{});var pc=n(Je);Ji=p(pc,"txt"),pc.forEach(s),Yi=p(U," or "),Ye=o(U,"CODE",{});var dc=n(Ye);Bi=p(dc,"parquet"),dc.forEach(s),Wi=p(U," file. The "),Aa=o(U,"A",{href:!0});var fc=n(Aa);Gi=p(fc,"load_dataset()"),fc.forEach(s),Qi=p(U," method is able to load each of these file types."),U.forEach(s),yo=c(t),P(Ct.$$.fragment,t),wo=c(t),nt=o(t,"H3",{class:!0});var ur=n(nt);Ot=o(ur,"A",{id:!0,class:!0,href:!0});var cc=n(Ot);Be=o(cc,"SPAN",{});var hc=n(Be);P(js.$$.fragment,hc),hc.forEach(s),cc.forEach(s),Ki=c(ur),We=o(ur,"SPAN",{});var uc=n(We);Xi=p(uc,"CSV"),uc.forEach(s),ur.forEach(s),bo=c(t),Sa=o(t,"P",{});var mc=n(Sa);Zi=p(mc,"\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),mc.forEach(s),jo=c(t),P(xs.$$.fragment,t),xo=c(t),Ta=o(t,"P",{});var gc=n(Ta);tp=p(gc,"If you have more than one CSV file:"),gc.forEach(s),ko=c(t),P(ks.$$.fragment,t),Eo=c(t),Da=o(t,"P",{});var _c=n(Da);sp=p(_c,"You can also map the training and test splits to specific CSV files:"),_c.forEach(s),qo=c(t),P(Es.$$.fragment,t),Po=c(t),Na=o(t,"P",{});var vc=n(Na);ap=p(vc,"To load remote CSV files via HTTP, you can pass the URLs:"),vc.forEach(s),Ao=c(t),P(qs.$$.fragment,t),So=c(t),Ia=o(t,"P",{});var $c=n(Ia);ep=p($c,"To load zipped CSV files:"),$c.forEach(s),To=c(t),P(Ps.$$.fragment,t),Do=c(t),rt=o(t,"H3",{class:!0});var mr=n(rt);Ht=o(mr,"A",{id:!0,class:!0,href:!0});var yc=n(Ht);Ge=o(yc,"SPAN",{});var wc=n(Ge);P(As.$$.fragment,wc),wc.forEach(s),yc.forEach(s),lp=c(mr),Qe=o(mr,"SPAN",{});var bc=n(Qe);op=p(bc,"JSON"),bc.forEach(s),mr.forEach(s),No=c(t),Ft=o(t,"P",{});var gr=n(Ft);np=p(gr,"JSON files are loaded directly with "),Ca=o(gr,"A",{href:!0});var jc=n(Ca);rp=p(jc,"load_dataset()"),jc.forEach(s),ip=p(gr," as shown below:"),gr.forEach(s),Io=c(t),P(Ss.$$.fragment,t),Co=c(t),Oa=o(t,"P",{});var xc=n(Oa);pp=p(xc,"JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),xc.forEach(s),Oo=c(t),P(Ts.$$.fragment,t),Ho=c(t),Lt=o(t,"P",{});var _r=n(Lt);dp=p(_r,"Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Ke=o(_r,"CODE",{});var kc=n(Ke);fp=p(kc,"field"),kc.forEach(s),cp=p(_r," argument as shown in the following:"),_r.forEach(s),Fo=c(t),P(Ds.$$.fragment,t),Lo=c(t),Ha=o(t,"P",{});var Ec=n(Ha);hp=p(Ec,"To load remote JSON files via HTTP, you can pass the URLs:"),Ec.forEach(s),Ro=c(t),P(Ns.$$.fragment,t),Mo=c(t),Fa=o(t,"P",{});var qc=n(Fa);up=p(qc,"While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),qc.forEach(s),Vo=c(t),it=o(t,"H3",{class:!0});var vr=n(it);Rt=o(vr,"A",{id:!0,class:!0,href:!0});var Pc=n(Rt);Xe=o(Pc,"SPAN",{});var Ac=n(Xe);P(Is.$$.fragment,Ac),Ac.forEach(s),Pc.forEach(s),mp=c(vr),Ze=o(vr,"SPAN",{});var Sc=n(Ze);gp=p(Sc,"Text files"),Sc.forEach(s),vr.forEach(s),Uo=c(t),La=o(t,"P",{});var Tc=n(La);_p=p(Tc,"Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Tc.forEach(s),zo=c(t),P(Cs.$$.fragment,t),Jo=c(t),Ra=o(t,"P",{});var Dc=n(Ra);vp=p(Dc,"To load remote TXT files via HTTP, you can pass the URLs:"),Dc.forEach(s),Yo=c(t),P(Os.$$.fragment,t),Bo=c(t),pt=o(t,"H3",{class:!0});var $r=n(pt);Mt=o($r,"A",{id:!0,class:!0,href:!0});var Nc=n(Mt);tl=o(Nc,"SPAN",{});var Ic=n(tl);P(Hs.$$.fragment,Ic),Ic.forEach(s),Nc.forEach(s),$p=c($r),sl=o($r,"SPAN",{});var Cc=n(sl);yp=p(Cc,"Parquet"),Cc.forEach(s),$r.forEach(s),Wo=c(t),Ma=o(t,"P",{});var Oc=n(Ma);wp=p(Oc,"Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Oc.forEach(s),Go=c(t),P(Fs.$$.fragment,t),Qo=c(t),Va=o(t,"P",{});var Hc=n(Va);bp=p(Hc,"To load remote parquet files via HTTP, you can pass the URLs:"),Hc.forEach(s),Ko=c(t),P(Ls.$$.fragment,t),Xo=c(t),dt=o(t,"H2",{class:!0});var yr=n(dt);Vt=o(yr,"A",{id:!0,class:!0,href:!0});var Fc=n(Vt);al=o(Fc,"SPAN",{});var Lc=n(al);P(Rs.$$.fragment,Lc),Lc.forEach(s),Fc.forEach(s),jp=c(yr),el=o(yr,"SPAN",{});var Rc=n(el);xp=p(Rc,"In-memory data"),Rc.forEach(s),yr.forEach(s),Zo=c(t),Ut=o(t,"P",{});var wr=n(Ut);kp=p(wr,"\u{1F917} Datasets will also allow you to create a "),Ua=o(wr,"A",{href:!0});var Mc=n(Ua);Ep=p(Mc,"Dataset"),Mc.forEach(s),qp=p(wr," directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),wr.forEach(s),tn=c(t),ft=o(t,"H3",{class:!0});var br=n(ft);zt=o(br,"A",{id:!0,class:!0,href:!0});var Vc=n(zt);ll=o(Vc,"SPAN",{});var Uc=n(ll);P(Ms.$$.fragment,Uc),Uc.forEach(s),Vc.forEach(s),Pp=c(br),ol=o(br,"SPAN",{});var zc=n(ol);Ap=p(zc,"Python dictionary"),zc.forEach(s),br.forEach(s),sn=c(t),Jt=o(t,"P",{});var jr=n(Jt);Sp=p(jr,"Load Python dictionaries with "),za=o(jr,"A",{href:!0});var Jc=n(za);Tp=p(Jc,"Dataset.from_dict()"),Jc.forEach(s),Dp=p(jr,":"),jr.forEach(s),an=c(t),P(Vs.$$.fragment,t),en=c(t),ct=o(t,"H3",{class:!0});var xr=n(ct);Yt=o(xr,"A",{id:!0,class:!0,href:!0});var Yc=n(Yt);nl=o(Yc,"SPAN",{});var Bc=n(nl);P(Us.$$.fragment,Bc),Bc.forEach(s),Yc.forEach(s),Np=c(xr),rl=o(xr,"SPAN",{});var Wc=n(rl);Ip=p(Wc,"Pandas DataFrame"),Wc.forEach(s),xr.forEach(s),ln=c(t),Bt=o(t,"P",{});var kr=n(Bt);Cp=p(kr,"Load Pandas DataFrames with "),Ja=o(kr,"A",{href:!0});var Gc=n(Ja);Op=p(Gc,"Dataset.from_pandas()"),Gc.forEach(s),Hp=p(kr,":"),kr.forEach(s),on=c(t),P(zs.$$.fragment,t),nn=c(t),P(Wt.$$.fragment,t),rn=c(t),ht=o(t,"H2",{class:!0});var Er=n(ht);Gt=o(Er,"A",{id:!0,class:!0,href:!0});var Qc=n(Gt);il=o(Qc,"SPAN",{});var Kc=n(il);P(Js.$$.fragment,Kc),Kc.forEach(s),Qc.forEach(s),Fp=c(Er),pl=o(Er,"SPAN",{});var Xc=n(pl);Lp=p(Xc,"Offline"),Xc.forEach(s),Er.forEach(s),pn=c(t),Ya=o(t,"P",{});var Zc=n(Ya);Rp=p(Zc,"Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),Zc.forEach(s),dn=c(t),G=o(t,"P",{});var ue=n(G);Mp=p(ue,"If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),dl=o(ue,"CODE",{});var th=n(dl);Vp=p(th,"HF_DATASETS_OFFLINE"),th.forEach(s),Up=p(ue," to "),fl=o(ue,"CODE",{});var sh=n(fl);zp=p(sh,"1"),sh.forEach(s),Jp=p(ue," to enable full offline mode."),ue.forEach(s),fn=c(t),ut=o(t,"H2",{class:!0});var qr=n(ut);Qt=o(qr,"A",{id:!0,class:!0,href:!0});var ah=n(Qt);cl=o(ah,"SPAN",{});var eh=n(cl);P(Ys.$$.fragment,eh),eh.forEach(s),ah.forEach(s),Yp=c(qr),hl=o(qr,"SPAN",{});var lh=n(hl);Bp=p(lh,"Slice splits"),lh.forEach(s),qr.forEach(s),cn=c(t),Q=o(t,"P",{});var me=n(Q);Wp=p(me,"For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Ba=o(me,"A",{href:!0});var oh=n(Ba);Gp=p(oh,"ReadInstruction"),oh.forEach(s),Qp=p(me,". Strings are more compact and readable for simple cases, while "),Wa=o(me,"A",{href:!0});var nh=n(Wa);Kp=p(nh,"ReadInstruction"),nh.forEach(s),Xp=p(me," is easier to use with variable slicing parameters."),me.forEach(s),hn=c(t),K=o(t,"P",{});var ge=n(K);Zp=p(ge,"Concatenate the "),ul=o(ge,"CODE",{});var rh=n(ul);td=p(rh,"train"),rh.forEach(s),sd=p(ge," and "),ml=o(ge,"CODE",{});var ih=n(ml);ad=p(ih,"test"),ih.forEach(s),ed=p(ge," split by:"),ge.forEach(s),un=c(t),P(Bs.$$.fragment,t),mn=c(t),Kt=o(t,"P",{});var Pr=n(Kt);ld=p(Pr,"Select specific rows of the "),gl=o(Pr,"CODE",{});var ph=n(gl);od=p(ph,"train"),ph.forEach(s),nd=p(Pr," split:"),Pr.forEach(s),gn=c(t),P(Ws.$$.fragment,t),_n=c(t),Ga=o(t,"P",{});var dh=n(Ga);rd=p(dh,"Or select a percentage of the split with:"),dh.forEach(s),vn=c(t),P(Gs.$$.fragment,t),$n=c(t),Qa=o(t,"P",{});var fh=n(Qa);id=p(fh,"You can even select a combination of percentages from each split:"),fh.forEach(s),yn=c(t),P(Qs.$$.fragment,t),wn=c(t),Ka=o(t,"P",{});var ch=n(Ka);pd=p(ch,"Finally, create cross-validated dataset splits by:"),ch.forEach(s),bn=c(t),P(Ks.$$.fragment,t),jn=c(t),mt=o(t,"H3",{class:!0});var Ar=n(mt);Xt=o(Ar,"A",{id:!0,class:!0,href:!0});var hh=n(Xt);_l=o(hh,"SPAN",{});var uh=n(_l);P(Xs.$$.fragment,uh),uh.forEach(s),hh.forEach(s),dd=c(Ar),vl=o(Ar,"SPAN",{});var mh=n(vl);fd=p(mh,"Percent slicing and rounding"),mh.forEach(s),Ar.forEach(s),xn=c(t),Xa=o(t,"P",{});var gh=n(Xa);cd=p(gh,"For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),gh.forEach(s),kn=c(t),P(Zs.$$.fragment,t),En=c(t),Zt=o(t,"P",{});var Sr=n(Zt);hd=p(Sr,"If you want equal sized splits, use "),$l=o(Sr,"CODE",{});var _h=n($l);ud=p(_h,"pct1_dropremainder"),_h.forEach(s),md=p(Sr," rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),Sr.forEach(s),qn=c(t),P(ta.$$.fragment,t),Pn=c(t),P(ts.$$.fragment,t),An=c(t),Za=o(t,"A",{id:!0}),n(Za).forEach(s),Sn=c(t),gt=o(t,"H2",{class:!0});var Tr=n(gt);ss=o(Tr,"A",{id:!0,class:!0,href:!0});var vh=n(ss);yl=o(vh,"SPAN",{});var $h=n(yl);P(sa.$$.fragment,$h),$h.forEach(s),vh.forEach(s),gd=c(Tr),wl=o(Tr,"SPAN",{});var yh=n(wl);_d=p(yh,"Troubleshooting"),yh.forEach(s),Tr.forEach(s),Tn=c(t),te=o(t,"P",{});var wh=n(te);vd=p(wh,"Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),wh.forEach(s),Dn=c(t),_t=o(t,"H3",{class:!0});var Dr=n(_t);as=o(Dr,"A",{id:!0,class:!0,href:!0});var bh=n(as);bl=o(bh,"SPAN",{});var jh=n(bl);P(aa.$$.fragment,jh),jh.forEach(s),bh.forEach(s),$d=c(Dr),jl=o(Dr,"SPAN",{});var xh=n(jl);yd=p(xh,"Manual download"),xh.forEach(s),Dr.forEach(s),Nn=c(t),M=o(t,"P",{});var fs=n(M);wd=p(fs,"Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),se=o(fs,"A",{href:!0});var kh=n(se);bd=p(kh,"load_dataset()"),kh.forEach(s),jd=p(fs," to throw an "),xl=o(fs,"CODE",{});var Eh=n(xl);xd=p(Eh,"AssertionError"),Eh.forEach(s),kd=p(fs,". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),kl=o(fs,"CODE",{});var qh=n(kl);Ed=p(qh,"data_dir"),qh.forEach(s),qd=p(fs," argument to specify the path to the files you just downloaded."),fs.forEach(s),In=c(t),es=o(t,"P",{});var Nr=n(es);Pd=p(Nr,"For example, if you try to download a configuration from the "),ea=o(Nr,"A",{href:!0,rel:!0});var Ph=n(ea);Ad=p(Ph,"MATINF"),Ph.forEach(s),Sd=p(Nr," dataset:"),Nr.forEach(s),Cn=c(t),P(la.$$.fragment,t),On=c(t),vt=o(t,"H3",{class:!0});var Ir=n(vt);ls=o(Ir,"A",{id:!0,class:!0,href:!0});var Ah=n(ls);El=o(Ah,"SPAN",{});var Sh=n(El);P(oa.$$.fragment,Sh),Sh.forEach(s),Ah.forEach(s),Td=c(Ir),ql=o(Ir,"SPAN",{});var Th=n(ql);Dd=p(Th,"Specify features"),Th.forEach(s),Ir.forEach(s),Hn=c(t),X=o(t,"P",{});var _e=n(X);Nd=p(_e,"When you create a dataset from local files, the "),ae=o(_e,"A",{href:!0});var Dh=n(ae);Id=p(Dh,"Features"),Dh.forEach(s),Cd=p(_e," are automatically inferred by "),na=o(_e,"A",{href:!0,rel:!0});var Nh=n(na);Od=p(Nh,"Apache Arrow"),Nh.forEach(s),Hd=p(_e,". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),_e.forEach(s),Fn=c(t),Z=o(t,"P",{});var ve=n(Z);Fd=p(ve,"The following example shows how you can add custom labels with "),ee=o(ve,"A",{href:!0});var Ih=n(ee);Ld=p(Ih,"ClassLabel"),Ih.forEach(s),Rd=p(ve,". First, define your own labels using the "),le=o(ve,"A",{href:!0});var Ch=n(le);Md=p(Ch,"Features"),Ch.forEach(s),Vd=p(ve," class:"),ve.forEach(s),Ln=c(t),P(ra.$$.fragment,t),Rn=c(t),tt=o(t,"P",{});var $e=n(tt);Ud=p($e,"Next, specify the "),Pl=o($e,"CODE",{});var Oh=n(Pl);zd=p(Oh,"features"),Oh.forEach(s),Jd=p($e," argument in "),oe=o($e,"A",{href:!0});var Hh=n(oe);Yd=p(Hh,"load_dataset()"),Hh.forEach(s),Bd=p($e," with the features you just created:"),$e.forEach(s),Mn=c(t),P(ia.$$.fragment,t),Vn=c(t),ne=o(t,"P",{});var Fh=n(ne);Wd=p(Fh,"Now when you look at your dataset features, you can see it uses the custom labels you defined:"),Fh.forEach(s),Un=c(t),P(pa.$$.fragment,t),zn=c(t),$t=o(t,"H2",{class:!0});var Cr=n($t);os=o(Cr,"A",{id:!0,class:!0,href:!0});var Lh=n(os);Al=o(Lh,"SPAN",{});var Rh=n(Al);P(da.$$.fragment,Rh),Rh.forEach(s),Lh.forEach(s),Gd=c(Cr),Sl=o(Cr,"SPAN",{});var Mh=n(Sl);Qd=p(Mh,"Metrics"),Mh.forEach(s),Cr.forEach(s),Jn=c(t),re=o(t,"P",{});var Vh=n(re);Kd=p(Vh,"When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),Vh.forEach(s),Yn=c(t),P(fa.$$.fragment,t),Bn=c(t),P(ns.$$.fragment,t),Wn=c(t),yt=o(t,"H3",{class:!0});var Or=n(yt);rs=o(Or,"A",{id:!0,class:!0,href:!0});var Uh=n(rs);Tl=o(Uh,"SPAN",{});var zh=n(Tl);P(ca.$$.fragment,zh),zh.forEach(s),Uh.forEach(s),Xd=c(Or),Dl=o(Or,"SPAN",{});var Jh=n(Dl);Zd=p(Jh,"Load configurations"),Jh.forEach(s),Or.forEach(s),Gn=c(t),st=o(t,"P",{});var ye=n(st);tf=p(ye,"It is possible for a metric to have different configurations. The configurations are stored in the "),Nl=o(ye,"CODE",{});var Yh=n(Nl);sf=p(Yh,"config_name"),Yh.forEach(s),af=p(ye," parameter in "),ie=o(ye,"A",{href:!0});var Bh=n(ie);ef=p(Bh,"MetricInfo"),Bh.forEach(s),lf=p(ye," attribute. When you load a metric, provide the configuration name as shown in the following:"),ye.forEach(s),Qn=c(t),P(ha.$$.fragment,t),Kn=c(t),wt=o(t,"H3",{class:!0});var Hr=n(wt);is=o(Hr,"A",{id:!0,class:!0,href:!0});var Wh=n(is);Il=o(Wh,"SPAN",{});var Gh=n(Il);P(ua.$$.fragment,Gh),Gh.forEach(s),Wh.forEach(s),of=c(Hr),Cl=o(Hr,"SPAN",{});var Qh=n(Cl);nf=p(Qh,"Distributed setup"),Qh.forEach(s),Hr.forEach(s),Xn=c(t),pe=o(t,"P",{});var Kh=n(pe);rf=p(Kh,"When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),Kh.forEach(s),Zn=c(t),de=o(t,"P",{});var Xh=n(de);pf=p(Xh,"For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),Xh.forEach(s),tr=c(t),at=o(t,"OL",{});var we=n(at);Ol=o(we,"LI",{});var Zh=n(Ol);ma=o(Zh,"P",{});var Fr=n(ma);df=p(Fr,"Define the total number of processes with the "),Hl=o(Fr,"CODE",{});var tu=n(Hl);ff=p(tu,"num_process"),tu.forEach(s),cf=p(Fr," argument."),Fr.forEach(s),Zh.forEach(s),hf=c(we),Fl=o(we,"LI",{});var su=n(Fl);bt=o(su,"P",{});var be=n(bt);uf=p(be,"Set the process "),Ll=o(be,"CODE",{});var au=n(Ll);mf=p(au,"rank"),au.forEach(s),gf=p(be," as an integer between zero and "),Rl=o(be,"CODE",{});var eu=n(Rl);_f=p(eu,"num_process - 1"),eu.forEach(s),vf=p(be,"."),be.forEach(s),su.forEach(s),$f=c(we),Ml=o(we,"LI",{});var lu=n(Ml);ga=o(lu,"P",{});var Lr=n(ga);yf=p(Lr,"Load your metric with "),fe=o(Lr,"A",{href:!0});var ou=n(fe);wf=p(ou,"load_metric()"),ou.forEach(s),bf=p(Lr," with these arguments:"),Lr.forEach(s),lu.forEach(s),we.forEach(s),sr=c(t),P(_a.$$.fragment,t),ar=c(t),P(ps.$$.fragment,t),er=c(t),ds=o(t,"P",{});var Rr=n(ds);jf=p(Rr,"In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Vl=o(Rr,"CODE",{});var nu=n(Vl);xf=p(nu,"experiment_id"),nu.forEach(s),kf=p(Rr," to distinguish the separate evaluations:"),Rr.forEach(s),lr=c(t),P(va.$$.fragment,t),this.h()},h(){h(d,"name","hf:doc:metadata"),h(d,"content",JSON.stringify(Lu)),h(y,"id","load"),h(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(y,"href","#load"),h(u,"class","relative group"),h(ka,"id","load-from-the-hub"),h(kt,"id","hugging-face-hub"),h(kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(kt,"href","#hugging-face-hub"),h(et,"class","relative group"),h(Ea,"href","/docs/datasets/pr_4337/en/package_reference/loading_methods#datasets.load_dataset"),h(hs,"href","https://huggingface.co/datasets/lhoestq/demo1"),h(hs,"rel","nofollow"),h(_s,"href","https://huggingface.co/datasets/allenai/c4"),h(_s,"rel","nofollow"),h(Tt,"id","local-loading-script"),h(Tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Tt,"href","#local-loading-script"),h(lt,"class","relative group"),h(Pa,"href","/docs/datasets/pr_4337/en/package_reference/loading_methods#datasets.load_dataset"),h(It,"id","local-and-remote-files"),h(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(It,"href","#local-and-remote-files"),h(ot,"class","relative group"),h(Aa,"href","/docs/datasets/pr_4337/en/package_reference/loading_methods#datasets.load_dataset"),h(Ot,"id","csv"),h(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ot,"href","#csv"),h(nt,"class","relative group"),h(Ht,"id","json"),h(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ht,"href","#json"),h(rt,"class","relative group"),h(Ca,"href","/docs/datasets/pr_4337/en/package_reference/loading_methods#datasets.load_dataset"),h(Rt,"id","text-files"),h(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Rt,"href","#text-files"),h(it,"class","relative group"),h(Mt,"id","parquet"),h(Mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Mt,"href","#parquet"),h(pt,"class","relative group"),h(Vt,"id","inmemory-data"),h(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Vt,"href","#inmemory-data"),h(dt,"class","relative group"),h(Ua,"href","/docs/datasets/pr_4337/en/package_reference/main_classes#datasets.Dataset"),h(zt,"id","python-dictionary"),h(zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(zt,"href","#python-dictionary"),h(ft,"class","relative group"),h(za,"href","/docs/datasets/pr_4337/en/package_reference/main_classes#datasets.Dataset.from_dict"),h(Yt,"id","pandas-dataframe"),h(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Yt,"href","#pandas-dataframe"),h(ct,"class","relative group"),h(Ja,"href","/docs/datasets/pr_4337/en/package_reference/main_classes#datasets.Dataset.from_pandas"),h(Gt,"id","offline"),h(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Gt,"href","#offline"),h(ht,"class","relative group"),h(Qt,"id","slice-splits"),h(Qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Qt,"href","#slice-splits"),h(ut,"class","relative group"),h(Ba,"href","/docs/datasets/pr_4337/en/package_reference/builder_classes#datasets.ReadInstruction"),h(Wa,"href","/docs/datasets/pr_4337/en/package_reference/builder_classes#datasets.ReadInstruction"),h(Xt,"id","percent-slicing-and-rounding"),h(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Xt,"href","#percent-slicing-and-rounding"),h(mt,"class","relative group"),h(Za,"id","troubleshoot"),h(ss,"id","troubleshooting"),h(ss,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ss,"href","#troubleshooting"),h(gt,"class","relative group"),h(as,"id","manual-download"),h(as,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(as,"href","#manual-download"),h(_t,"class","relative group"),h(se,"href","/docs/datasets/pr_4337/en/package_reference/loading_methods#datasets.load_dataset"),h(ea,"href","https://huggingface.co/datasets/matinf"),h(ea,"rel","nofollow"),h(ls,"id","specify-features"),h(ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ls,"href","#specify-features"),h(vt,"class","relative group"),h(ae,"href","/docs/datasets/pr_4337/en/package_reference/main_classes#datasets.Features"),h(na,"href","https://arrow.apache.org/docs/"),h(na,"rel","nofollow"),h(ee,"href","/docs/datasets/pr_4337/en/package_reference/main_classes#datasets.ClassLabel"),h(le,"href","/docs/datasets/pr_4337/en/package_reference/main_classes#datasets.Features"),h(oe,"href","/docs/datasets/pr_4337/en/package_reference/loading_methods#datasets.load_dataset"),h(os,"id","metrics"),h(os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(os,"href","#metrics"),h($t,"class","relative group"),h(rs,"id","load-configurations"),h(rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(rs,"href","#load-configurations"),h(yt,"class","relative group"),h(ie,"href","/docs/datasets/pr_4337/en/package_reference/main_classes#datasets.MetricInfo"),h(is,"id","distributed-setup"),h(is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(is,"href","#distributed-setup"),h(wt,"class","relative group"),h(fe,"href","/docs/datasets/pr_4337/en/package_reference/loading_methods#datasets.load_metric")},m(t,e){a(document.head,d),r(t,g,e),r(t,u,e),a(u,y),a(y,$),E(_,$,null),a(u,m),a(u,w),a(w,v),r(t,A,e),r(t,S,e),a(S,T),r(t,Y,e),r(t,I,e),a(I,N),r(t,z,e),r(t,C,e),a(C,J),a(J,H),a(C,wa),a(C,jt),a(jt,ba),a(C,ja),a(C,xt),a(xt,Ur),a(C,zr),a(C,je),a(je,Jr),a(C,Yr),a(C,xe),a(xe,Br),a(C,Wr),a(C,ke),a(ke,Gr),r(t,Ql,e),r(t,xa,e),a(xa,Qr),r(t,Kl,e),r(t,ka,e),r(t,Xl,e),r(t,et,e),a(et,kt),a(kt,Ee),E(cs,Ee,null),a(et,Kr),a(et,qe),a(qe,Xr),r(t,Zl,e),r(t,Et,e),a(Et,Zr),a(Et,Pe),a(Pe,ti),a(Et,si),r(t,to,e),r(t,B,e),a(B,ai),a(B,Ea),a(Ea,ei),a(B,li),a(B,hs),a(hs,oi),a(B,ni),r(t,so,e),E(us,t,e),r(t,ao,e),r(t,qa,e),a(qa,ri),r(t,eo,e),r(t,qt,e),a(qt,ii),a(qt,Ae),a(Ae,pi),a(qt,di),r(t,lo,e),E(ms,t,e),r(t,oo,e),E(Pt,t,e),r(t,no,e),r(t,F,e),a(F,fi),a(F,Se),a(Se,ci),a(F,hi),a(F,Te),a(Te,ui),a(F,mi),a(F,De),a(De,gi),a(F,_i),a(F,Ne),a(Ne,vi),a(F,$i),a(F,Ie),a(Ie,yi),a(F,wi),r(t,ro,e),E(gs,t,e),r(t,io,e),E(At,t,e),r(t,po,e),r(t,W,e),a(W,bi),a(W,Ce),a(Ce,ji),a(W,xi),a(W,_s),a(_s,ki),a(W,Ei),r(t,fo,e),E(vs,t,e),r(t,co,e),r(t,St,e),a(St,qi),a(St,Oe),a(Oe,Pi),a(St,Ai),r(t,ho,e),E($s,t,e),r(t,uo,e),r(t,lt,e),a(lt,Tt),a(Tt,He),E(ys,He,null),a(lt,Si),a(lt,Fe),a(Fe,Ti),r(t,mo,e),r(t,Dt,e),a(Dt,Di),a(Dt,Pa),a(Pa,Ni),a(Dt,Ii),r(t,go,e),r(t,Nt,e),a(Nt,Le),a(Le,Ci),a(Nt,Oi),a(Nt,Re),a(Re,Hi),r(t,_o,e),E(ws,t,e),r(t,vo,e),r(t,ot,e),a(ot,It),a(It,Me),E(bs,Me,null),a(ot,Fi),a(ot,Ve),a(Ve,Li),r(t,$o,e),r(t,L,e),a(L,Ri),a(L,Ue),a(Ue,Mi),a(L,Vi),a(L,ze),a(ze,Ui),a(L,zi),a(L,Je),a(Je,Ji),a(L,Yi),a(L,Ye),a(Ye,Bi),a(L,Wi),a(L,Aa),a(Aa,Gi),a(L,Qi),r(t,yo,e),E(Ct,t,e),r(t,wo,e),r(t,nt,e),a(nt,Ot),a(Ot,Be),E(js,Be,null),a(nt,Ki),a(nt,We),a(We,Xi),r(t,bo,e),r(t,Sa,e),a(Sa,Zi),r(t,jo,e),E(xs,t,e),r(t,xo,e),r(t,Ta,e),a(Ta,tp),r(t,ko,e),E(ks,t,e),r(t,Eo,e),r(t,Da,e),a(Da,sp),r(t,qo,e),E(Es,t,e),r(t,Po,e),r(t,Na,e),a(Na,ap),r(t,Ao,e),E(qs,t,e),r(t,So,e),r(t,Ia,e),a(Ia,ep),r(t,To,e),E(Ps,t,e),r(t,Do,e),r(t,rt,e),a(rt,Ht),a(Ht,Ge),E(As,Ge,null),a(rt,lp),a(rt,Qe),a(Qe,op),r(t,No,e),r(t,Ft,e),a(Ft,np),a(Ft,Ca),a(Ca,rp),a(Ft,ip),r(t,Io,e),E(Ss,t,e),r(t,Co,e),r(t,Oa,e),a(Oa,pp),r(t,Oo,e),E(Ts,t,e),r(t,Ho,e),r(t,Lt,e),a(Lt,dp),a(Lt,Ke),a(Ke,fp),a(Lt,cp),r(t,Fo,e),E(Ds,t,e),r(t,Lo,e),r(t,Ha,e),a(Ha,hp),r(t,Ro,e),E(Ns,t,e),r(t,Mo,e),r(t,Fa,e),a(Fa,up),r(t,Vo,e),r(t,it,e),a(it,Rt),a(Rt,Xe),E(Is,Xe,null),a(it,mp),a(it,Ze),a(Ze,gp),r(t,Uo,e),r(t,La,e),a(La,_p),r(t,zo,e),E(Cs,t,e),r(t,Jo,e),r(t,Ra,e),a(Ra,vp),r(t,Yo,e),E(Os,t,e),r(t,Bo,e),r(t,pt,e),a(pt,Mt),a(Mt,tl),E(Hs,tl,null),a(pt,$p),a(pt,sl),a(sl,yp),r(t,Wo,e),r(t,Ma,e),a(Ma,wp),r(t,Go,e),E(Fs,t,e),r(t,Qo,e),r(t,Va,e),a(Va,bp),r(t,Ko,e),E(Ls,t,e),r(t,Xo,e),r(t,dt,e),a(dt,Vt),a(Vt,al),E(Rs,al,null),a(dt,jp),a(dt,el),a(el,xp),r(t,Zo,e),r(t,Ut,e),a(Ut,kp),a(Ut,Ua),a(Ua,Ep),a(Ut,qp),r(t,tn,e),r(t,ft,e),a(ft,zt),a(zt,ll),E(Ms,ll,null),a(ft,Pp),a(ft,ol),a(ol,Ap),r(t,sn,e),r(t,Jt,e),a(Jt,Sp),a(Jt,za),a(za,Tp),a(Jt,Dp),r(t,an,e),E(Vs,t,e),r(t,en,e),r(t,ct,e),a(ct,Yt),a(Yt,nl),E(Us,nl,null),a(ct,Np),a(ct,rl),a(rl,Ip),r(t,ln,e),r(t,Bt,e),a(Bt,Cp),a(Bt,Ja),a(Ja,Op),a(Bt,Hp),r(t,on,e),E(zs,t,e),r(t,nn,e),E(Wt,t,e),r(t,rn,e),r(t,ht,e),a(ht,Gt),a(Gt,il),E(Js,il,null),a(ht,Fp),a(ht,pl),a(pl,Lp),r(t,pn,e),r(t,Ya,e),a(Ya,Rp),r(t,dn,e),r(t,G,e),a(G,Mp),a(G,dl),a(dl,Vp),a(G,Up),a(G,fl),a(fl,zp),a(G,Jp),r(t,fn,e),r(t,ut,e),a(ut,Qt),a(Qt,cl),E(Ys,cl,null),a(ut,Yp),a(ut,hl),a(hl,Bp),r(t,cn,e),r(t,Q,e),a(Q,Wp),a(Q,Ba),a(Ba,Gp),a(Q,Qp),a(Q,Wa),a(Wa,Kp),a(Q,Xp),r(t,hn,e),r(t,K,e),a(K,Zp),a(K,ul),a(ul,td),a(K,sd),a(K,ml),a(ml,ad),a(K,ed),r(t,un,e),E(Bs,t,e),r(t,mn,e),r(t,Kt,e),a(Kt,ld),a(Kt,gl),a(gl,od),a(Kt,nd),r(t,gn,e),E(Ws,t,e),r(t,_n,e),r(t,Ga,e),a(Ga,rd),r(t,vn,e),E(Gs,t,e),r(t,$n,e),r(t,Qa,e),a(Qa,id),r(t,yn,e),E(Qs,t,e),r(t,wn,e),r(t,Ka,e),a(Ka,pd),r(t,bn,e),E(Ks,t,e),r(t,jn,e),r(t,mt,e),a(mt,Xt),a(Xt,_l),E(Xs,_l,null),a(mt,dd),a(mt,vl),a(vl,fd),r(t,xn,e),r(t,Xa,e),a(Xa,cd),r(t,kn,e),E(Zs,t,e),r(t,En,e),r(t,Zt,e),a(Zt,hd),a(Zt,$l),a($l,ud),a(Zt,md),r(t,qn,e),E(ta,t,e),r(t,Pn,e),E(ts,t,e),r(t,An,e),r(t,Za,e),r(t,Sn,e),r(t,gt,e),a(gt,ss),a(ss,yl),E(sa,yl,null),a(gt,gd),a(gt,wl),a(wl,_d),r(t,Tn,e),r(t,te,e),a(te,vd),r(t,Dn,e),r(t,_t,e),a(_t,as),a(as,bl),E(aa,bl,null),a(_t,$d),a(_t,jl),a(jl,yd),r(t,Nn,e),r(t,M,e),a(M,wd),a(M,se),a(se,bd),a(M,jd),a(M,xl),a(xl,xd),a(M,kd),a(M,kl),a(kl,Ed),a(M,qd),r(t,In,e),r(t,es,e),a(es,Pd),a(es,ea),a(ea,Ad),a(es,Sd),r(t,Cn,e),E(la,t,e),r(t,On,e),r(t,vt,e),a(vt,ls),a(ls,El),E(oa,El,null),a(vt,Td),a(vt,ql),a(ql,Dd),r(t,Hn,e),r(t,X,e),a(X,Nd),a(X,ae),a(ae,Id),a(X,Cd),a(X,na),a(na,Od),a(X,Hd),r(t,Fn,e),r(t,Z,e),a(Z,Fd),a(Z,ee),a(ee,Ld),a(Z,Rd),a(Z,le),a(le,Md),a(Z,Vd),r(t,Ln,e),E(ra,t,e),r(t,Rn,e),r(t,tt,e),a(tt,Ud),a(tt,Pl),a(Pl,zd),a(tt,Jd),a(tt,oe),a(oe,Yd),a(tt,Bd),r(t,Mn,e),E(ia,t,e),r(t,Vn,e),r(t,ne,e),a(ne,Wd),r(t,Un,e),E(pa,t,e),r(t,zn,e),r(t,$t,e),a($t,os),a(os,Al),E(da,Al,null),a($t,Gd),a($t,Sl),a(Sl,Qd),r(t,Jn,e),r(t,re,e),a(re,Kd),r(t,Yn,e),E(fa,t,e),r(t,Bn,e),E(ns,t,e),r(t,Wn,e),r(t,yt,e),a(yt,rs),a(rs,Tl),E(ca,Tl,null),a(yt,Xd),a(yt,Dl),a(Dl,Zd),r(t,Gn,e),r(t,st,e),a(st,tf),a(st,Nl),a(Nl,sf),a(st,af),a(st,ie),a(ie,ef),a(st,lf),r(t,Qn,e),E(ha,t,e),r(t,Kn,e),r(t,wt,e),a(wt,is),a(is,Il),E(ua,Il,null),a(wt,of),a(wt,Cl),a(Cl,nf),r(t,Xn,e),r(t,pe,e),a(pe,rf),r(t,Zn,e),r(t,de,e),a(de,pf),r(t,tr,e),r(t,at,e),a(at,Ol),a(Ol,ma),a(ma,df),a(ma,Hl),a(Hl,ff),a(ma,cf),a(at,hf),a(at,Fl),a(Fl,bt),a(bt,uf),a(bt,Ll),a(Ll,mf),a(bt,gf),a(bt,Rl),a(Rl,_f),a(bt,vf),a(at,$f),a(at,Ml),a(Ml,ga),a(ga,yf),a(ga,fe),a(fe,wf),a(ga,bf),r(t,sr,e),E(_a,t,e),r(t,ar,e),E(ps,t,e),r(t,er,e),r(t,ds,e),a(ds,jf),a(ds,Vl),a(Vl,xf),a(ds,kf),r(t,lr,e),E(va,t,e),or=!0},p(t,[e]){const $a={};e&2&&($a.$$scope={dirty:e,ctx:t}),Pt.$set($a);const Ul={};e&2&&(Ul.$$scope={dirty:e,ctx:t}),At.$set(Ul);const zl={};e&2&&(zl.$$scope={dirty:e,ctx:t}),Ct.$set(zl);const Jl={};e&2&&(Jl.$$scope={dirty:e,ctx:t}),Wt.$set(Jl);const Yl={};e&2&&(Yl.$$scope={dirty:e,ctx:t}),ts.$set(Yl);const Bl={};e&2&&(Bl.$$scope={dirty:e,ctx:t}),ns.$set(Bl);const R={};e&2&&(R.$$scope={dirty:e,ctx:t}),ps.$set(R)},i(t){or||(b(_.$$.fragment,t),b(cs.$$.fragment,t),b(us.$$.fragment,t),b(ms.$$.fragment,t),b(Pt.$$.fragment,t),b(gs.$$.fragment,t),b(At.$$.fragment,t),b(vs.$$.fragment,t),b($s.$$.fragment,t),b(ys.$$.fragment,t),b(ws.$$.fragment,t),b(bs.$$.fragment,t),b(Ct.$$.fragment,t),b(js.$$.fragment,t),b(xs.$$.fragment,t),b(ks.$$.fragment,t),b(Es.$$.fragment,t),b(qs.$$.fragment,t),b(Ps.$$.fragment,t),b(As.$$.fragment,t),b(Ss.$$.fragment,t),b(Ts.$$.fragment,t),b(Ds.$$.fragment,t),b(Ns.$$.fragment,t),b(Is.$$.fragment,t),b(Cs.$$.fragment,t),b(Os.$$.fragment,t),b(Hs.$$.fragment,t),b(Fs.$$.fragment,t),b(Ls.$$.fragment,t),b(Rs.$$.fragment,t),b(Ms.$$.fragment,t),b(Vs.$$.fragment,t),b(Us.$$.fragment,t),b(zs.$$.fragment,t),b(Wt.$$.fragment,t),b(Js.$$.fragment,t),b(Ys.$$.fragment,t),b(Bs.$$.fragment,t),b(Ws.$$.fragment,t),b(Gs.$$.fragment,t),b(Qs.$$.fragment,t),b(Ks.$$.fragment,t),b(Xs.$$.fragment,t),b(Zs.$$.fragment,t),b(ta.$$.fragment,t),b(ts.$$.fragment,t),b(sa.$$.fragment,t),b(aa.$$.fragment,t),b(la.$$.fragment,t),b(oa.$$.fragment,t),b(ra.$$.fragment,t),b(ia.$$.fragment,t),b(pa.$$.fragment,t),b(da.$$.fragment,t),b(fa.$$.fragment,t),b(ns.$$.fragment,t),b(ca.$$.fragment,t),b(ha.$$.fragment,t),b(ua.$$.fragment,t),b(_a.$$.fragment,t),b(ps.$$.fragment,t),b(va.$$.fragment,t),or=!0)},o(t){j(_.$$.fragment,t),j(cs.$$.fragment,t),j(us.$$.fragment,t),j(ms.$$.fragment,t),j(Pt.$$.fragment,t),j(gs.$$.fragment,t),j(At.$$.fragment,t),j(vs.$$.fragment,t),j($s.$$.fragment,t),j(ys.$$.fragment,t),j(ws.$$.fragment,t),j(bs.$$.fragment,t),j(Ct.$$.fragment,t),j(js.$$.fragment,t),j(xs.$$.fragment,t),j(ks.$$.fragment,t),j(Es.$$.fragment,t),j(qs.$$.fragment,t),j(Ps.$$.fragment,t),j(As.$$.fragment,t),j(Ss.$$.fragment,t),j(Ts.$$.fragment,t),j(Ds.$$.fragment,t),j(Ns.$$.fragment,t),j(Is.$$.fragment,t),j(Cs.$$.fragment,t),j(Os.$$.fragment,t),j(Hs.$$.fragment,t),j(Fs.$$.fragment,t),j(Ls.$$.fragment,t),j(Rs.$$.fragment,t),j(Ms.$$.fragment,t),j(Vs.$$.fragment,t),j(Us.$$.fragment,t),j(zs.$$.fragment,t),j(Wt.$$.fragment,t),j(Js.$$.fragment,t),j(Ys.$$.fragment,t),j(Bs.$$.fragment,t),j(Ws.$$.fragment,t),j(Gs.$$.fragment,t),j(Qs.$$.fragment,t),j(Ks.$$.fragment,t),j(Xs.$$.fragment,t),j(Zs.$$.fragment,t),j(ta.$$.fragment,t),j(ts.$$.fragment,t),j(sa.$$.fragment,t),j(aa.$$.fragment,t),j(la.$$.fragment,t),j(oa.$$.fragment,t),j(ra.$$.fragment,t),j(ia.$$.fragment,t),j(pa.$$.fragment,t),j(da.$$.fragment,t),j(fa.$$.fragment,t),j(ns.$$.fragment,t),j(ca.$$.fragment,t),j(ha.$$.fragment,t),j(ua.$$.fragment,t),j(_a.$$.fragment,t),j(ps.$$.fragment,t),j(va.$$.fragment,t),or=!1},d(t){s(d),t&&s(g),t&&s(u),x(_),t&&s(A),t&&s(S),t&&s(Y),t&&s(I),t&&s(z),t&&s(C),t&&s(Ql),t&&s(xa),t&&s(Kl),t&&s(ka),t&&s(Xl),t&&s(et),x(cs),t&&s(Zl),t&&s(Et),t&&s(to),t&&s(B),t&&s(so),x(us,t),t&&s(ao),t&&s(qa),t&&s(eo),t&&s(qt),t&&s(lo),x(ms,t),t&&s(oo),x(Pt,t),t&&s(no),t&&s(F),t&&s(ro),x(gs,t),t&&s(io),x(At,t),t&&s(po),t&&s(W),t&&s(fo),x(vs,t),t&&s(co),t&&s(St),t&&s(ho),x($s,t),t&&s(uo),t&&s(lt),x(ys),t&&s(mo),t&&s(Dt),t&&s(go),t&&s(Nt),t&&s(_o),x(ws,t),t&&s(vo),t&&s(ot),x(bs),t&&s($o),t&&s(L),t&&s(yo),x(Ct,t),t&&s(wo),t&&s(nt),x(js),t&&s(bo),t&&s(Sa),t&&s(jo),x(xs,t),t&&s(xo),t&&s(Ta),t&&s(ko),x(ks,t),t&&s(Eo),t&&s(Da),t&&s(qo),x(Es,t),t&&s(Po),t&&s(Na),t&&s(Ao),x(qs,t),t&&s(So),t&&s(Ia),t&&s(To),x(Ps,t),t&&s(Do),t&&s(rt),x(As),t&&s(No),t&&s(Ft),t&&s(Io),x(Ss,t),t&&s(Co),t&&s(Oa),t&&s(Oo),x(Ts,t),t&&s(Ho),t&&s(Lt),t&&s(Fo),x(Ds,t),t&&s(Lo),t&&s(Ha),t&&s(Ro),x(Ns,t),t&&s(Mo),t&&s(Fa),t&&s(Vo),t&&s(it),x(Is),t&&s(Uo),t&&s(La),t&&s(zo),x(Cs,t),t&&s(Jo),t&&s(Ra),t&&s(Yo),x(Os,t),t&&s(Bo),t&&s(pt),x(Hs),t&&s(Wo),t&&s(Ma),t&&s(Go),x(Fs,t),t&&s(Qo),t&&s(Va),t&&s(Ko),x(Ls,t),t&&s(Xo),t&&s(dt),x(Rs),t&&s(Zo),t&&s(Ut),t&&s(tn),t&&s(ft),x(Ms),t&&s(sn),t&&s(Jt),t&&s(an),x(Vs,t),t&&s(en),t&&s(ct),x(Us),t&&s(ln),t&&s(Bt),t&&s(on),x(zs,t),t&&s(nn),x(Wt,t),t&&s(rn),t&&s(ht),x(Js),t&&s(pn),t&&s(Ya),t&&s(dn),t&&s(G),t&&s(fn),t&&s(ut),x(Ys),t&&s(cn),t&&s(Q),t&&s(hn),t&&s(K),t&&s(un),x(Bs,t),t&&s(mn),t&&s(Kt),t&&s(gn),x(Ws,t),t&&s(_n),t&&s(Ga),t&&s(vn),x(Gs,t),t&&s($n),t&&s(Qa),t&&s(yn),x(Qs,t),t&&s(wn),t&&s(Ka),t&&s(bn),x(Ks,t),t&&s(jn),t&&s(mt),x(Xs),t&&s(xn),t&&s(Xa),t&&s(kn),x(Zs,t),t&&s(En),t&&s(Zt),t&&s(qn),x(ta,t),t&&s(Pn),x(ts,t),t&&s(An),t&&s(Za),t&&s(Sn),t&&s(gt),x(sa),t&&s(Tn),t&&s(te),t&&s(Dn),t&&s(_t),x(aa),t&&s(Nn),t&&s(M),t&&s(In),t&&s(es),t&&s(Cn),x(la,t),t&&s(On),t&&s(vt),x(oa),t&&s(Hn),t&&s(X),t&&s(Fn),t&&s(Z),t&&s(Ln),x(ra,t),t&&s(Rn),t&&s(tt),t&&s(Mn),x(ia,t),t&&s(Vn),t&&s(ne),t&&s(Un),x(pa,t),t&&s(zn),t&&s($t),x(da),t&&s(Jn),t&&s(re),t&&s(Yn),x(fa,t),t&&s(Bn),x(ns,t),t&&s(Wn),t&&s(yt),x(ca),t&&s(Gn),t&&s(st),t&&s(Qn),x(ha,t),t&&s(Kn),t&&s(wt),x(ua),t&&s(Xn),t&&s(pe),t&&s(Zn),t&&s(de),t&&s(tr),t&&s(at),t&&s(sr),x(_a,t),t&&s(ar),x(ps,t),t&&s(er),t&&s(ds),t&&s(lr),x(va,t)}}}const Lu={local:"load",sections:[{local:"hugging-face-hub",title:"Hugging Face Hub"},{local:"local-loading-script",title:"Local loading script"},{local:"local-and-remote-files",sections:[{local:"csv",title:"CSV"},{local:"json",title:"JSON"},{local:"text-files",title:"Text files"},{local:"parquet",title:"Parquet"}],title:"Local and remote files"},{local:"inmemory-data",sections:[{local:"python-dictionary",title:"Python dictionary"},{local:"pandas-dataframe",title:"Pandas DataFrame"}],title:"In-memory data"},{local:"offline",title:"Offline"},{local:"slice-splits",sections:[{local:"percent-slicing-and-rounding",title:"Percent slicing and rounding"}],title:"Slice splits"},{local:"troubleshooting",sections:[{local:"manual-download",title:"Manual download"},{local:"specify-features",title:"Specify features"}],title:"Troubleshooting"},{local:"metrics",sections:[{local:"load-configurations",title:"Load configurations"},{local:"distributed-setup",title:"Distributed setup"}],title:"Metrics"}],title:"Load"};function Ru(q){return bu(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Yu extends Ef{constructor(d){super();qf(this,d,Ru,Fu,Pf,{})}}export{Yu as default,Lu as metadata};
