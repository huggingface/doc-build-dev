import{S as Of,i as Lf,s as Hf,e as l,k as f,t as r,c as o,a as n,m as c,h as i,d as s,b as u,g as p,G as a,Q as no,q as w,l as $h,n as Zr,o as b,B as q,p as Xr,w as k,y as E,j as Ah,K as Sh,a0 as jh,x as P,a1 as Dh,T as Th,Y as qh,Z as kh,M as Nh,v as Ih}from"../chunks/vendor-hf-doc-builder.js";import{T as Ca}from"../chunks/Tip-hf-doc-builder.js";import{I as L}from"../chunks/IconCopyLink-hf-doc-builder.js";import{a as Eh,C as T}from"../chunks/CodeBlock-hf-doc-builder.js";import{b as xh,I as Ch,a as Oh}from"../chunks/IconTensorflow-hf-doc-builder.js";function yh(x,d,m){const h=x.slice();return h[8]=d[m],h[10]=m,h}function wh(x){let d,m,h;var v=x[8].icon;function _(y){return{props:{classNames:"mr-1.5"}}}return v&&(d=new v(_())),{c(){d&&k(d.$$.fragment),m=$h()},l(y){d&&P(d.$$.fragment,y),m=$h()},m(y,$){d&&E(d,y,$),p(y,m,$),h=!0},p(y,$){if(v!==(v=y[8].icon)){if(d){Zr();const j=d;b(j.$$.fragment,1,0,()=>{q(j,1)}),Xr()}v?(d=new v(_()),k(d.$$.fragment),w(d.$$.fragment,1),E(d,m.parentNode,m)):d=null}},i(y){h||(d&&w(d.$$.fragment,y),h=!0)},o(y){d&&b(d.$$.fragment,y),h=!1},d(y){y&&s(m),d&&q(d,y)}}}function bh(x){let d,m,h,v=x[8].name+"",_,y,$,j,g,A,S,D=x[8].icon&&wh(x);function Z(){return x[6](x[8])}return{c(){d=l("button"),D&&D.c(),m=f(),h=l("p"),_=r(v),$=f(),this.h()},l(C){d=o(C,"BUTTON",{class:!0});var N=n(d);D&&D.l(N),m=c(N),h=o(N,"P",{class:!0});var Y=n(h);_=i(Y,v),Y.forEach(s),$=c(N),N.forEach(s),this.h()},h(){u(h,"class",y="!m-0 "+x[8].classNames),u(d,"class",j="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(x[10]?"r":"l")+" "+(x[8].group!==x[1]&&"text-gray-500 filter grayscale"))},m(C,N){p(C,d,N),D&&D.m(d,null),a(d,m),a(d,h),a(h,_),a(d,$),g=!0,A||(S=no(d,"click",Z),A=!0)},p(C,N){x=C,x[8].icon?D?(D.p(x,N),N&1&&w(D,1)):(D=wh(x),D.c(),w(D,1),D.m(d,m)):D&&(Zr(),b(D,1,1,()=>{D=null}),Xr()),(!g||N&1)&&v!==(v=x[8].name+"")&&Ah(_,v),(!g||N&1&&y!==(y="!m-0 "+x[8].classNames))&&u(h,"class",y),(!g||N&3&&j!==(j="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(x[10]?"r":"l")+" "+(x[8].group!==x[1]&&"text-gray-500 filter grayscale")))&&u(d,"class",j)},i(C){g||(w(D),g=!0)},o(C){b(D),g=!1},d(C){C&&s(d),D&&D.d(),A=!1,S()}}}function Lh(x){let d,m,h,v=x[3].filter(x[5]),_=[];for(let $=0;$<v.length;$+=1)_[$]=bh(yh(x,v,$));const y=$=>b(_[$],1,1,()=>{_[$]=null});return{c(){d=l("div"),m=l("div");for(let $=0;$<_.length;$+=1)_[$].c();this.h()},l($){d=o($,"DIV",{});var j=n(d);m=o(j,"DIV",{class:!0});var g=n(m);for(let A=0;A<_.length;A+=1)_[A].l(g);g.forEach(s),j.forEach(s),this.h()},h(){u(m,"class","bg-white leading-none border border-gray-100 rounded-lg inline-flex p-0.5 text-sm mb-4 select-none")},m($,j){p($,d,j),a(d,m);for(let g=0;g<_.length;g+=1)_[g].m(m,null);h=!0},p($,[j]){if(j&27){v=$[3].filter($[5]);let g;for(g=0;g<v.length;g+=1){const A=yh($,v,g);_[g]?(_[g].p(A,j),w(_[g],1)):(_[g]=bh(A),_[g].c(),w(_[g],1),_[g].m(m,null))}for(Zr(),g=v.length;g<_.length;g+=1)y(g);Xr()}},i($){if(!h){for(let j=0;j<v.length;j+=1)w(_[j]);h=!0}},o($){_=_.filter(Boolean);for(let j=0;j<_.length;j+=1)b(_[j]);h=!1},d($){$&&s(d),Sh(_,$)}}}function Hh(x,d,m){let h,{ids:v}=d;const _=v.join("-"),y=xh(_);jh(x,y,S=>m(1,h=S));const $=[{id:"pt",classNames:"",icon:Ch,name:"Pytorch",group:"group1"},{id:"tf",classNames:"",icon:Oh,name:"TensorFlow",group:"group2"},{id:"stringapi",classNames:"text-blue-600",name:"String API",group:"group1"},{id:"readinstruction",classNames:"text-blue-600",name:"ReadInstruction",group:"group2"}];function j(S){Dh(y,h=S,h)}const g=S=>v.includes(S.id),A=S=>j(S.group);return x.$$set=S=>{"ids"in S&&m(0,v=S.ids)},[v,h,y,$,j,g,A]}class Ph extends Of{constructor(d){super();Lf(this,d,Hh,Lh,Hf,{ids:0})}}function Fh(x){let d,m,h,v,_,y,$=x[1].highlighted+"",j;return m=new Eh({props:{classNames:"transition duration-200 ease-in-out "+(x[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:x[1].code}}),_=new Ph({props:{ids:x[4]}}),{c(){d=l("div"),k(m.$$.fragment),h=f(),v=l("pre"),k(_.$$.fragment),y=new qh,this.h()},l(g){d=o(g,"DIV",{class:!0});var A=n(d);P(m.$$.fragment,A),A.forEach(s),h=c(g),v=o(g,"PRE",{});var S=n(v);P(_.$$.fragment,S),y=kh(S),S.forEach(s),this.h()},h(){u(d,"class","absolute top-2.5 right-4"),y.a=null},m(g,A){p(g,d,A),E(m,d,null),p(g,h,A),p(g,v,A),E(_,v,null),y.m($,v),j=!0},p(g,A){const S={};A&4&&(S.classNames="transition duration-200 ease-in-out "+(g[2]&&"opacity-0")),A&2&&(S.value=g[1].code),m.$set(S),(!j||A&2)&&$!==($=g[1].highlighted+"")&&y.p($)},i(g){j||(w(m.$$.fragment,g),w(_.$$.fragment,g),j=!0)},o(g){b(m.$$.fragment,g),b(_.$$.fragment,g),j=!1},d(g){g&&s(d),q(m),g&&s(h),g&&s(v),q(_)}}}function Rh(x){let d,m,h,v,_,y,$=x[0].highlighted+"",j;return m=new Eh({props:{classNames:"transition duration-200 ease-in-out "+(x[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:x[0].code}}),_=new Ph({props:{ids:x[4]}}),{c(){d=l("div"),k(m.$$.fragment),h=f(),v=l("pre"),k(_.$$.fragment),y=new qh,this.h()},l(g){d=o(g,"DIV",{class:!0});var A=n(d);P(m.$$.fragment,A),A.forEach(s),h=c(g),v=o(g,"PRE",{});var S=n(v);P(_.$$.fragment,S),y=kh(S),S.forEach(s),this.h()},h(){u(d,"class","absolute top-2.5 right-4"),y.a=null},m(g,A){p(g,d,A),E(m,d,null),p(g,h,A),p(g,v,A),E(_,v,null),y.m($,v),j=!0},p(g,A){const S={};A&4&&(S.classNames="transition duration-200 ease-in-out "+(g[2]&&"opacity-0")),A&1&&(S.value=g[0].code),m.$set(S),(!j||A&1)&&$!==($=g[0].highlighted+"")&&y.p($)},i(g){j||(w(m.$$.fragment,g),w(_.$$.fragment,g),j=!0)},o(g){b(m.$$.fragment,g),b(_.$$.fragment,g),j=!1},d(g){g&&s(d),q(m),g&&s(h),g&&s(v),q(_)}}}function Mh(x){let d,m,h,v,_,y;const $=[Rh,Fh],j=[];function g(A,S){return A[3]==="group1"?0:1}return m=g(x),h=j[m]=$[m](x),{c(){d=l("div"),h.c(),this.h()},l(A){d=o(A,"DIV",{class:!0});var S=n(d);h.l(S),S.forEach(s),this.h()},h(){u(d,"class","code-block relative")},m(A,S){p(A,d,S),j[m].m(d,null),v=!0,_||(y=[no(d,"mouseover",x[6]),no(d,"focus",x[6]),no(d,"mouseout",x[7]),no(d,"focus",x[7])],_=!0)},p(A,[S]){let D=m;m=g(A),m===D?j[m].p(A,S):(Zr(),b(j[D],1,1,()=>{j[D]=null}),Xr(),h=j[m],h?h.p(A,S):(h=j[m]=$[m](A),h.c()),w(h,1),h.m(d,null))},i(A){v||(w(h),v=!0)},o(A){b(h),v=!1},d(A){A&&s(d),j[m].d(),_=!1,Th(y)}}}function Vh(x,d,m){let h,{group1:v}=d,{group2:_}=d;const y=[v.id,_.id],$=y.join("-"),j=xh($);jh(x,j,D=>m(3,h=D));let g=!0;function A(){m(2,g=!1)}function S(){m(2,g=!0)}return x.$$set=D=>{"group1"in D&&m(0,v=D.group1),"group2"in D&&m(1,_=D.group2)},[v,_,g,h,y,j,A,S]}class oo extends Of{constructor(d){super();Lf(this,d,Vh,Mh,Hf,{group1:0,group2:1})}}function zh(x){let d,m,h,v,_;return{c(){d=l("p"),m=r("Refer to the "),h=l("a"),v=r("Upload a dataset to the Hub"),_=r(" tutorial for more details on how to create a dataset repository on the Hub, and how to upload your data files."),this.h()},l(y){d=o(y,"P",{});var $=n(d);m=i($,"Refer to the "),h=o($,"A",{href:!0});var j=n(h);v=i(j,"Upload a dataset to the Hub"),j.forEach(s),_=i($," tutorial for more details on how to create a dataset repository on the Hub, and how to upload your data files."),$.forEach(s),this.h()},h(){u(h,"href","./upload_dataset")},m(y,$){p(y,d,$),a(d,m),a(d,h),a(h,v),a(d,_)},d(y){y&&s(d)}}}function Uh(x){let d,m,h,v,_;return{c(){d=l("p"),m=r("If you don\u2019t specify which data files to use, "),h=l("a"),v=r("load_dataset()"),_=r(" will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data."),this.h()},l(y){d=o(y,"P",{});var $=n(d);m=i($,"If you don\u2019t specify which data files to use, "),h=o($,"A",{href:!0});var j=n(h);v=i(j,"load_dataset()"),j.forEach(s),_=i($," will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data."),$.forEach(s),this.h()},h(){u(h,"href","/docs/datasets/pr_4519/en/package_reference/loading_methods#datasets.load_dataset")},m(y,$){p(y,d,$),a(d,m),a(d,h),a(h,v),a(d,_)},d(y){y&&s(d)}}}function Jh(x){let d,m,h,v,_,y,$,j,g,A,S,D,Z,C,N,Y,O,G,vs,$s,Q,ys,ws,V,bs,js;return{c(){d=l("p"),m=r("An object data type in "),h=l("a"),v=r("pandas.Series"),_=r(" doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length "),y=l("code"),$=r("0"),j=r(" or the Series only contains "),g=l("code"),A=r("None/NaN"),S=r(" objects, the type is set to "),D=l("code"),Z=r("null"),C=r(". Avoid potential errors by constructing an explicit schema with "),N=l("a"),Y=r("Features"),O=r(" using the "),G=l("code"),vs=r("from_dict"),$s=r(" or "),Q=l("code"),ys=r("from_pandas"),ws=r(" methods. See the "),V=l("a"),bs=r("troubleshoot"),js=r(" for more details on how to explicitly specify your own features."),this.h()},l(K){d=o(K,"P",{});var I=n(d);m=i(I,"An object data type in "),h=o(I,"A",{href:!0,rel:!0});var Oa=n(h);v=i(Oa,"pandas.Series"),Oa.forEach(s),_=i(I," doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length "),y=o(I,"CODE",{});var Et=n(y);$=i(Et,"0"),Et.forEach(s),j=i(I," or the Series only contains "),g=o(I,"CODE",{});var La=n(g);A=i(La,"None/NaN"),La.forEach(s),S=i(I," objects, the type is set to "),D=o(I,"CODE",{});var Ha=n(D);Z=i(Ha,"null"),Ha.forEach(s),C=i(I,". Avoid potential errors by constructing an explicit schema with "),N=o(I,"A",{href:!0});var xt=n(N);Y=i(xt,"Features"),xt.forEach(s),O=i(I," using the "),G=o(I,"CODE",{});var Fa=n(G);vs=i(Fa,"from_dict"),Fa.forEach(s),$s=i(I," or "),Q=o(I,"CODE",{});var qs=n(Q);ys=i(qs,"from_pandas"),qs.forEach(s),ws=i(I," methods. See the "),V=o(I,"A",{href:!0});var M=n(V);bs=i(M,"troubleshoot"),M.forEach(s),js=i(I," for more details on how to explicitly specify your own features."),I.forEach(s),this.h()},h(){u(h,"href","https://pandas.pydata.org/docs/reference/api/pandas.Series.html"),u(h,"rel","nofollow"),u(N,"href","/docs/datasets/pr_4519/en/package_reference/main_classes#datasets.Features"),u(V,"href","./loading#specify-features")},m(K,I){p(K,d,I),a(d,m),a(d,h),a(h,v),a(d,_),a(d,y),a(y,$),a(d,j),a(d,g),a(g,A),a(d,S),a(d,D),a(D,Z),a(d,C),a(d,N),a(N,Y),a(d,O),a(d,G),a(G,vs),a(d,$s),a(d,Q),a(Q,ys),a(d,ws),a(d,V),a(V,bs),a(d,js)},d(K){K&&s(d)}}}function Bh(x){let d,m,h,v;return{c(){d=l("p"),m=l("code"),h=r("pct1_dropremainder"),v=r(" rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100.")},l(_){d=o(_,"P",{});var y=n(d);m=o(y,"CODE",{});var $=n(m);h=i($,"pct1_dropremainder"),$.forEach(s),v=i(y," rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100."),y.forEach(s)},m(_,y){p(_,d,y),a(d,m),a(m,h),a(d,v)},d(_){_&&s(d)}}}function Wh(x){let d,m;return{c(){d=l("p"),m=r("Metrics will soon be deprecated in \u{1F917} Datasets. To learn more about how to use metrics, take a look at our newest library \u{1F917} Evaluate! In addition to metrics, we\u2019ve also added more tools for evaluating models and datasets.")},l(h){d=o(h,"P",{});var v=n(d);m=i(v,"Metrics will soon be deprecated in \u{1F917} Datasets. To learn more about how to use metrics, take a look at our newest library \u{1F917} Evaluate! In addition to metrics, we\u2019ve also added more tools for evaluating models and datasets."),v.forEach(s)},m(h,v){p(h,d,v),a(d,m)},d(h){h&&s(d)}}}function Yh(x){let d,m,h,v,_;return{c(){d=l("p"),m=r("See the "),h=l("a"),v=r("Metrics"),_=r(" guide for more details on how to write your own metric loading script."),this.h()},l(y){d=o(y,"P",{});var $=n(d);m=i($,"See the "),h=o($,"A",{href:!0});var j=n(h);v=i(j,"Metrics"),j.forEach(s),_=i($," guide for more details on how to write your own metric loading script."),$.forEach(s),this.h()},h(){u(h,"href","./how_to_metrics#custom-metric-loading-script")},m(y,$){p(y,d,$),a(d,m),a(d,h),a(h,v),a(d,_)},d(y){y&&s(d)}}}function Gh(x){let d,m,h,v,_;return{c(){d=l("p"),m=r("Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),h=l("a"),v=r("Metric.compute()"),_=r(" gathers all the predictions and references from the nodes, and computes the final metric."),this.h()},l(y){d=o(y,"P",{});var $=n(d);m=i($,"Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),h=o($,"A",{href:!0});var j=n(h);v=i(j,"Metric.compute()"),j.forEach(s),_=i($," gathers all the predictions and references from the nodes, and computes the final metric."),$.forEach(s),this.h()},h(){u(h,"href","/docs/datasets/pr_4519/en/package_reference/main_classes#datasets.Metric.compute")},m(y,$){p(y,d,$),a(d,m),a(d,h),a(h,v),a(d,_)},d(y){y&&s(d)}}}function Qh(x){let d,m,h,v,_,y,$,j,g,A,S,D,Z,C,N,Y,O,G,vs,$s,Q,ys,ws,V,bs,js,K,I,Oa,Et,La,Ha,xt,Fa,qs,M,ti,ks,si,ai,Es,ei,li,ro,Ra,io,nt,Pt,Le,xs,oi,He,ni,po,z,ri,Fe,ii,pi,Ma,di,fi,Va,ci,ui,fo,At,hi,Ps,mi,gi,co,As,uo,St,_i,Re,vi,$i,ho,Ss,mo,Dt,go,H,yi,Me,wi,bi,Ve,ji,qi,ze,ki,Ei,Ue,xi,Pi,Je,Ai,Si,_o,Ds,vo,Tt,$o,X,Di,Be,Ti,Ni,Ts,Ii,Ci,yo,Ns,wo,Nt,Oi,We,Li,Hi,bo,Is,jo,rt,It,Ye,Cs,Fi,Ge,Ri,qo,Ct,Mi,za,Vi,zi,ko,Ot,Qe,Ui,Ji,Ke,Bi,Eo,Os,xo,it,Lt,Ze,Ls,Wi,Xe,Yi,Po,F,Gi,tl,Qi,Ki,sl,Zi,Xi,al,tp,sp,el,ap,ep,Ua,lp,op,Ao,pt,Ht,ll,Hs,np,ol,rp,So,Ja,ip,Do,Fs,To,Ba,pp,No,Rs,Io,Wa,dp,Co,Ms,Oo,Ya,fp,Lo,Vs,Ho,Ga,cp,Fo,zs,Ro,dt,Ft,nl,Us,up,rl,hp,Mo,Rt,mp,Qa,gp,_p,Vo,Js,zo,Ka,vp,Uo,Bs,Jo,Mt,$p,il,yp,wp,Bo,Ws,Wo,Za,bp,Yo,Ys,Go,Xa,jp,Qo,ft,Vt,pl,Gs,qp,dl,kp,Ko,te,Ep,Zo,Qs,Xo,se,xp,tn,Ks,sn,ct,zt,fl,Zs,Pp,cl,Ap,an,ae,Sp,en,ee,Dp,ln,Xs,on,le,Tp,nn,ta,rn,ut,Ut,ul,sa,Np,hl,Ip,pn,Jt,Cp,oe,Op,Lp,dn,ht,Bt,ml,aa,Hp,gl,Fp,fn,Wt,Rp,ne,Mp,Vp,cn,ea,un,mt,Yt,_l,la,zp,vl,Up,hn,Gt,Jp,re,Bp,Wp,mn,oa,gn,Qt,_n,gt,Kt,$l,na,Yp,yl,Gp,vn,ie,Qp,$n,tt,Kp,wl,Zp,Xp,bl,td,sd,yn,_t,Zt,jl,ra,ad,ql,ed,wn,st,ld,pe,od,nd,de,rd,id,bn,at,pd,kl,dd,fd,El,cd,ud,jn,ia,qn,Xt,hd,xl,md,gd,kn,pa,En,fe,_d,xn,da,Pn,ce,vd,An,fa,Sn,ue,$d,Dn,ca,Tn,vt,ts,Pl,ua,yd,Al,wd,Nn,he,bd,In,ha,Cn,ss,jd,Sl,qd,kd,On,ma,Ln,as,Hn,me,Fn,$t,es,Dl,ga,Ed,Tl,xd,Rn,ge,Pd,Mn,yt,ls,Nl,_a,Ad,Il,Sd,Vn,U,Dd,_e,Td,Nd,Cl,Id,Cd,Ol,Od,Ld,zn,os,Hd,va,Fd,Rd,Un,$a,Jn,wt,ns,Ll,ya,Md,Hl,Vd,Bn,J,zd,ve,Ud,Jd,wa,Bd,Wd,$e,Yd,Gd,Wn,rs,Qd,ye,Kd,Zd,Yn,ba,Gn,et,Xd,Fl,tf,sf,we,af,ef,Qn,ja,Kn,be,lf,Zn,qa,Xn,bt,is,Rl,ka,of,Ml,nf,tr,ps,sr,je,rf,ar,Ea,er,ds,lr,jt,fs,Vl,xa,pf,zl,df,or,lt,ff,Ul,cf,uf,qe,hf,mf,nr,Pa,rr,qt,cs,Jl,Aa,gf,Bl,_f,ir,ke,vf,pr,Ee,$f,dr,ot,Wl,Sa,yf,Yl,wf,bf,jf,Gl,kt,qf,Ql,kf,Ef,Kl,xf,Pf,Af,Zl,Da,Sf,xe,Df,Tf,fr,Ta,cr,us,ur,hs,Nf,Xl,If,Cf,hr,Na,mr;return y=new L({}),xs=new L({}),As=new T({props:{code:`from datasets import load_dataset
dataset = load_dataset("lhoestq/demo1")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;lhoestq/demo1&quot;</span>)`}}),Ss=new T({props:{code:`dataset = load_dataset(
  "lhoestq/custom_squad",
  revision="main"  # tag name, or branch name, or commit hash
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(
<span class="hljs-meta">... </span>  <span class="hljs-string">&quot;lhoestq/custom_squad&quot;</span>,
<span class="hljs-meta">... </span>  revision=<span class="hljs-string">&quot;main&quot;</span>  <span class="hljs-comment"># tag name, or branch name, or commit hash</span>
<span class="hljs-meta">... </span>)`}}),Dt=new Ca({props:{$$slots:{default:[zh]},$$scope:{ctx:x}}}),Ds=new T({props:{code:`data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;train.csv&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;test.csv&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;namespace/your_dataset_name&quot;</span>, data_files=data_files)`}}),Tt=new Ca({props:{warning:!0,$$slots:{default:[Uh]},$$scope:{ctx:x}}}),Ns=new T({props:{code:`from datasets import load_dataset
c4_subset = load_dataset("allenai/c4", data_files="en/c4-train.0000*-of-01024.json.gz")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_subset = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=<span class="hljs-string">&quot;en/c4-train.0000*-of-01024.json.gz&quot;</span>)`}}),Is=new T({props:{code:`data_files = {"validation": "en/c4-validation.*.json.gz"}
c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;validation&quot;</span>: <span class="hljs-string">&quot;en/c4-validation.*.json.gz&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_validation = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;validation&quot;</span>)`}}),Cs=new L({}),Os=new T({props:{code:`dataset = load_dataset("path/to/local/loading_script/loading_script.py", split="train")
dataset = load_dataset("path/to/local/loading_script", split="train")  # equivalent because the file has the same name as the directory`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;path/to/local/loading_script/loading_script.py&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;path/to/local/loading_script&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)  <span class="hljs-comment"># equivalent because the file has the same name as the directory</span>`}}),Ls=new L({}),Hs=new L({}),Fs=new T({props:{code:`from datasets import load_dataset
dataset = load_dataset("csv", data_files="my_file.csv")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=<span class="hljs-string">&quot;my_file.csv&quot;</span>)`}}),Rs=new T({props:{code:'dataset = load_dataset("csv", data_files=["my_file_1.csv", "my_file_2.csv", "my_file_3.csv"])',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=[<span class="hljs-string">&quot;my_file_1.csv&quot;</span>, <span class="hljs-string">&quot;my_file_2.csv&quot;</span>, <span class="hljs-string">&quot;my_file_3.csv&quot;</span>])'}}),Ms=new T({props:{code:'dataset = load_dataset("csv", data_files={"train": ["my_train_file_1.csv", "my_train_file_2.csv"], "test": "my_test_file.csv"})',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files={<span class="hljs-string">&quot;train&quot;</span>: [<span class="hljs-string">&quot;my_train_file_1.csv&quot;</span>, <span class="hljs-string">&quot;my_train_file_2.csv&quot;</span>], <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;my_test_file.csv&quot;</span>})'}}),Vs=new T({props:{code:`base_url = "https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/"
dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train.csv&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: base_url + <span class="hljs-string">&#x27;test.csv&#x27;</span>})`}}),zs=new T({props:{code:`url = "https://domain.org/train_data.zip"
data_files = {"train": url}
dataset = load_dataset("csv", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://domain.org/train_data.zip&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: url}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files)`}}),Us=new L({}),Js=new T({props:{code:`from datasets import load_dataset
dataset = load_dataset("json", data_files="my_file.json")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=<span class="hljs-string">&quot;my_file.json&quot;</span>)`}}),Bs=new T({props:{code:`{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true}`,highlighted:`<span class="hljs-punctuation">{</span><span class="hljs-attr">&quot;a&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;b&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">2.0</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;c&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;foo&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;d&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-keyword">false</span><span class="hljs-punctuation">}</span>
<span class="hljs-punctuation">{</span><span class="hljs-attr">&quot;a&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;b&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">-5.5</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;c&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-keyword">null</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;d&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-keyword">true</span><span class="hljs-punctuation">}</span>`}}),Ws=new T({props:{code:`{"version": "0.1.0",
 "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
          {"a": 4, "b": -5.5, "c": null, "d": true}]
}

>>> from datasets import load_dataset
>>> dataset = load_dataset("json", data_files="my_file.json", field="data")`,highlighted:`{<span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;0.1.0&quot;</span>,
 <span class="hljs-string">&quot;data&quot;</span>: [{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false},
          {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}]
}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=<span class="hljs-string">&quot;my_file.json&quot;</span>, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),Ys=new T({props:{code:`base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
dataset = load_dataset("json", data_files={"train": base_url + "train-v1.1.json", "validation": base_url + "dev-v1.1.json"}, field="data")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files={<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;train-v1.1.json&quot;</span>, <span class="hljs-string">&quot;validation&quot;</span>: base_url + <span class="hljs-string">&quot;dev-v1.1.json&quot;</span>}, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),Gs=new L({}),Qs=new T({props:{code:`from datasets import load_dataset
dataset = load_dataset("text", data_files={"train": ["my_text_1.txt", "my_text_2.txt"], "test": "my_test_file.txt"})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;text&quot;</span>, data_files={<span class="hljs-string">&quot;train&quot;</span>: [<span class="hljs-string">&quot;my_text_1.txt&quot;</span>, <span class="hljs-string">&quot;my_text_2.txt&quot;</span>], <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;my_test_file.txt&quot;</span>})`}}),Ks=new T({props:{code:'dataset = load_dataset("text", data_files="https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;text&quot;</span>, data_files=<span class="hljs-string">&quot;https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt&quot;</span>)'}}),Zs=new L({}),Xs=new T({props:{code:`from datasets import load_dataset
dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;train.parquet&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;test.parquet&#x27;</span>})`}}),ta=new T({props:{code:`base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
data_files = {"train": base_url + "wikipedia-train.parquet"}
wiki = load_dataset("parquet", data_files=data_files, split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;wikipedia-train.parquet&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),sa=new L({}),aa=new L({}),ea=new T({props:{code:`from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>my_dict = {<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_dict(my_dict)`}}),la=new L({}),oa=new T({props:{code:`from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-meta">&gt;&gt;&gt; </span>df = pd.DataFrame({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_pandas(df)`}}),Qt=new Ca({props:{warning:!0,$$slots:{default:[Jh]},$$scope:{ctx:x}}}),na=new L({}),ra=new L({}),ia=new oo({props:{group1:{id:"stringapi",code:'train_test_ds = datasets.load_dataset("bookcorpus", split="train+test")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train+test&quot;</span>)'},group2:{id:"readinstruction",code:`ri = datasets.ReadInstruction("train") + datasets.ReadInstruction("test")
train_test_ds = datasets.load_dataset("bookcorpus", split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>) + datasets.ReadInstruction(<span class="hljs-string">&quot;test&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=ri)`}}}),pa=new oo({props:{group1:{id:"stringapi",code:'train_10_20_ds = datasets.load_dataset("bookcorpus", split="train[10:20]")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train[10:20]&quot;</span>)'},group2:{id:"readinstruction",code:`train_10_20_ds = datasets.load_dataset("bookcorpu"', split=datasets.ReadInstruction("train", from_=10, to=20, unit="abs"))`,highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpu&quot;</span><span class="hljs-string">&#x27;, split=datasets.ReadInstruction(&quot;train&quot;, from_=10, to=20, unit=&quot;abs&quot;))</span>'}}}),da=new oo({props:{group1:{id:"stringapi",code:'train_10pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10pct_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train[:10%]&quot;</span>)'},group2:{id:"readinstruction",code:'train_10_20_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train", to=10, unit="%"))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&quot;%&quot;</span>))'}}}),fa=new oo({props:{group1:{id:"stringapi",code:'train_10_80pct_ds = datasets.load_dataset("bookcorpus", split="tr"in[:10%]+train[-80%:]")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;tr&quot;</span><span class="hljs-keyword">in</span>[:<span class="hljs-number">10</span>%]+train[-<span class="hljs-number">80</span>%:]<span class="hljs-string">&quot;)</span>'},group2:{id:"readinstruction",code:`ri = (datasets.ReadInstruction("train", to=10, unit="%") + datasets.ReadInstruction("train", from_=-80, unit="%"))
train_10_80pct_ds = datasets.load_dataset("bookcorpus", split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = (datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&quot;%&quot;</span>) + datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>, from_=-<span class="hljs-number">80</span>, unit=<span class="hljs-string">&quot;%&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=ri)`}}}),ca=new oo({props:{group1:{id:"stringapi",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
>>> vals_ds = datasets.load_dataset("bookcorpus", split=[f"train[{k}%:{k+10}%]" for k in range(0, 100, 10)])
>>> trains_ds = datasets.load_dataset("bookcorpus", split=[f"train[:{k}%]+train[{k+10}%:]" for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=[<span class="hljs-string">f&quot;train[<span class="hljs-subst">{k}</span>%:<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%]&quot;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=[<span class="hljs-string">f&quot;train[:<span class="hljs-subst">{k}</span>%]+train[<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%:]&quot;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`},group2:{id:"readinstruction",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
>>> vals_ds = datasets.load_dataset("bookcorpus", [datasets.ReadInstruction("train", from_=k, to=k+10, unit="%") for k in range(0, 100, 10)])
>>> trains_ds = datasets.load_dataset("bookcorpus", [(datasets.ReadInstruction("train", to=k, unit="%") + datasets.ReadInstruction("train", from_=k+10, unit="%")) for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, [datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&quot;%&quot;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, [(datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>, to=k, unit=<span class="hljs-string">&quot;%&quot;</span>) + datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&quot;%&quot;</span>)) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}}),ua=new L({}),ha=new T({props:{code:`# Assuming train split contains 999 records.
# 19 records, from 500 (included) to 519 (excluded).
>>> train_50_52_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%]")
# 20 records, from 519 (included) to 539 (excluded).
>>> train_52_54_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%]")`,highlighted:`<span class="hljs-comment"># Assuming train split contains 999 records.</span>
<span class="hljs-comment"># 19 records, from 500 (included) to 519 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train[50%:52%]&quot;</span>)
<span class="hljs-comment"># 20 records, from 519 (included) to 539 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train[52%:54%]&quot;</span>)`}}),ma=new T({props:{code:`# 18 records, from 450 (included) to 468 (excluded).
>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train", from_=50, to=52, unit="%", rounding="pct1_dropremainder"))
# 18 records, from 468 (included) to 486 (excluded).
>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train",from_=52, to=54, unit="%", rounding="pct1_dropremainder"))
# Or equivalently:
>>> train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%](pct1_dropremainder)")
>>> train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%](pct1_dropremainder)")`,highlighted:`<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">&quot;%&quot;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">&quot;%&quot;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))
<span class="hljs-comment"># Or equivalently:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train[50%:52%](pct1_dropremainder)&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train[52%:54%](pct1_dropremainder)&quot;</span>)`}}),as=new Ca({props:{warning:!0,$$slots:{default:[Bh]},$$scope:{ctx:x}}}),ga=new L({}),_a=new L({}),$a=new T({props:{code:'dataset = load_dataset("matinf", "summarization")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;matinf&quot;</span>, <span class="hljs-string">&quot;summarization&quot;</span>)
Downloading <span class="hljs-keyword">and</span> preparing dataset matinf/summarization (download: Unknown size, generated: <span class="hljs-number">246.89</span> MiB, post-processed: Unknown size, total: <span class="hljs-number">246.89</span> MiB) to /root/.cache/huggingface/datasets/matinf/summarization/<span class="hljs-number">1.0</span><span class="hljs-number">.0</span>/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
AssertionError: The dataset matinf <span class="hljs-keyword">with</span> config summarization requires manual data. 
Please follow the manual download instructions: To use MATINF you have to download it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9). You will receive a download link <span class="hljs-keyword">and</span> a password once you complete the form. Please extract <span class="hljs-built_in">all</span> files <span class="hljs-keyword">in</span> one folder <span class="hljs-keyword">and</span> load the dataset <span class="hljs-keyword">with</span>: *datasets.load_dataset(<span class="hljs-string">&#x27;matinf&#x27;</span>, data_dir=<span class="hljs-string">&#x27;path/to/folder/folder_name&#x27;</span>)*. 
Manual data can be loaded <span class="hljs-keyword">with</span> \`datasets.load_dataset(matinf, data_dir=<span class="hljs-string">&#x27;&lt;path/to/manual/data&gt;&#x27;</span>) `}}),ya=new L({}),ba=new T({props:{code:`class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]
emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>class_names = [<span class="hljs-string">&quot;sadness&quot;</span>, <span class="hljs-string">&quot;joy&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;anger&quot;</span>, <span class="hljs-string">&quot;fear&quot;</span>, <span class="hljs-string">&quot;surprise&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>emotion_features = Features({<span class="hljs-string">&#x27;text&#x27;</span>: Value(<span class="hljs-string">&#x27;string&#x27;</span>), <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=class_names)})`}}),ja=new T({props:{code:"dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=file_dict, delimiter=<span class="hljs-string">&#x27;;&#x27;</span>, column_names=[<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>], features=emotion_features)'}}),qa=new T({props:{code:"dataset['train'].features",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&#x27;train&#x27;</span>].features
{<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">6</span>, names=[<span class="hljs-string">&#x27;sadness&#x27;</span>, <span class="hljs-string">&#x27;joy&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;anger&#x27;</span>, <span class="hljs-string">&#x27;fear&#x27;</span>, <span class="hljs-string">&#x27;surprise&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),ka=new L({}),ps=new Ca({props:{warning:!0,$$slots:{default:[Wh]},$$scope:{ctx:x}}}),Ea=new T({props:{code:`from datasets import load_metric
metric = load_metric('PATH/TO/MY/METRIC/SCRIPT')

# Example of typical usage
for batch in dataset:
    inputs, references = batch
    predictions = model(inputs)
    metric.add_batch(predictions=predictions, references=references)
score = metric.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;PATH/TO/MY/METRIC/SCRIPT&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example of typical usage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataset:
<span class="hljs-meta">... </span>    inputs, references = batch
<span class="hljs-meta">... </span>    predictions = model(inputs)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=references)
<span class="hljs-meta">&gt;&gt;&gt; </span>score = metric.compute()`}}),ds=new Ca({props:{$$slots:{default:[Yh]},$$scope:{ctx:x}}}),xa=new L({}),Pa=new T({props:{code:`from datasets import load_metric
metric = load_metric('bleurt', name='bleurt-base-128')
metric = load_metric('bleurt', name='bleurt-base-512')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-128&#x27;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-512&#x27;</span>)</span>`}}),Aa=new L({}),Ta=new T({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=rank)`}}),us=new Ca({props:{$$slots:{default:[Gh]},$$scope:{ctx:x}}}),Na=new T({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=process_id, experiment_id=<span class="hljs-string">&quot;My_experiment_10&quot;</span>)`}}),{c(){d=l("meta"),m=f(),h=l("h1"),v=l("a"),_=l("span"),k(y.$$.fragment),$=f(),j=l("span"),g=r("Load"),A=f(),S=l("p"),D=r("Datasets are stored in various places; a dataset can be on your local machine\u2019s disk, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever a dataset is stored, \u{1F917} Datasets can help you load it."),Z=f(),C=l("p"),N=r("This guide will show you how to load a dataset from:"),Y=f(),O=l("ul"),G=l("li"),vs=r("The Hub without a dataset loading script"),$s=f(),Q=l("li"),ys=r("Local loading script"),ws=f(),V=l("li"),bs=r("Local files"),js=f(),K=l("li"),I=r("In-memory data"),Oa=f(),Et=l("li"),La=r("Offline"),Ha=f(),xt=l("li"),Fa=r("A specific slice of a split"),qs=f(),M=l("p"),ti=r("For more details specific to loading audio datasets, take a look at the "),ks=l("a"),si=r("load audio data"),ai=r(" guide. If you\u2019re working with image datasets, check out the "),Es=l("a"),ei=r("load image data"),li=r(" guide."),ro=f(),Ra=l("a"),io=f(),nt=l("h2"),Pt=l("a"),Le=l("span"),k(xs.$$.fragment),oi=f(),He=l("span"),ni=r("Hugging Face Hub"),po=f(),z=l("p"),ri=r("Datasets are loaded from a dataset loading script that downloads and generates the dataset. However, you can also load a dataset from any dataset repository on the Hub "),Fe=l("strong"),ii=r("without"),pi=r(" a loading script! Begin by "),Ma=l("a"),di=r("creating a dataset repository"),fi=r(" and upload your data files. Now you can use the "),Va=l("a"),ci=r("load_dataset()"),ui=r(" function to load the dataset."),fo=f(),At=l("p"),hi=r("For example, try loading the files from this "),Ps=l("a"),mi=r("demo repository"),gi=r(" by providing the repository namespace and dataset name. This dataset repository contains CSV files, and the code below loads the dataset from the CSV files:"),co=f(),k(As.$$.fragment),uo=f(),St=l("p"),_i=r("Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),Re=l("code"),vi=r("revision"),$i=r(" parameter to specify the dataset version you want to load:"),ho=f(),k(Ss.$$.fragment),mo=f(),k(Dt.$$.fragment),go=f(),H=l("p"),yi=r("A dataset without a loading script will by default load all the data into the "),Me=l("code"),wi=r("train"),bi=r(" split. Use the "),Ve=l("code"),ji=r("data_files"),qi=r(" parameter to map data files to splits like "),ze=l("code"),ki=r("train"),Ei=r(", "),Ue=l("code"),xi=r("validation"),Pi=r(" and "),Je=l("code"),Ai=r("test"),Si=r(":"),_o=f(),k(Ds.$$.fragment),vo=f(),k(Tt.$$.fragment),$o=f(),X=l("p"),Di=r("You can also load a specific subset of the files with the "),Be=l("code"),Ti=r("data_files"),Ni=r(" parameter. The example below only loads a single file from the "),Ts=l("a"),Ii=r("C4 dataset"),Ci=r(":"),yo=f(),k(Ns.$$.fragment),wo=f(),Nt=l("p"),Oi=r("The "),We=l("code"),Li=r("split"),Hi=r(" parameter can also map a data file to a specific split:"),bo=f(),k(Is.$$.fragment),jo=f(),rt=l("h2"),It=l("a"),Ye=l("span"),k(Cs.$$.fragment),Fi=f(),Ge=l("span"),Ri=r("Local loading script"),qo=f(),Ct=l("p"),Mi=r("You may have a \u{1F917} Datasets loading script locally on your computer. In this case, load the dataset by passing one of the following paths to "),za=l("a"),Vi=r("load_dataset()"),zi=r(":"),ko=f(),Ot=l("ul"),Qe=l("li"),Ui=r("The local path to the loading script file."),Ji=f(),Ke=l("li"),Bi=r("The local path to the directory containing the loading script file (only if the script file has the same name as the directory)."),Eo=f(),k(Os.$$.fragment),xo=f(),it=l("h2"),Lt=l("a"),Ze=l("span"),k(Ls.$$.fragment),Wi=f(),Xe=l("span"),Yi=r("Local and remote files"),Po=f(),F=l("p"),Gi=r("Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),tl=l("code"),Qi=r("csv"),Ki=r(", "),sl=l("code"),Zi=r("json"),Xi=r(", "),al=l("code"),tp=r("txt"),sp=r(" or "),el=l("code"),ap=r("parquet"),ep=r(" file. The "),Ua=l("a"),lp=r("load_dataset()"),op=r(" function is able to load each of these file types."),Ao=f(),pt=l("h3"),Ht=l("a"),ll=l("span"),k(Hs.$$.fragment),np=f(),ol=l("span"),rp=r("CSV"),So=f(),Ja=l("p"),ip=r("\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),Do=f(),k(Fs.$$.fragment),To=f(),Ba=l("p"),pp=r("If you have more than one CSV file:"),No=f(),k(Rs.$$.fragment),Io=f(),Wa=l("p"),dp=r("You can also map the training and test splits to specific CSV files:"),Co=f(),k(Ms.$$.fragment),Oo=f(),Ya=l("p"),fp=r("To load remote CSV files via HTTP, pass the URLs instead:"),Lo=f(),k(Vs.$$.fragment),Ho=f(),Ga=l("p"),cp=r("To load zipped CSV files:"),Fo=f(),k(zs.$$.fragment),Ro=f(),dt=l("h3"),Ft=l("a"),nl=l("span"),k(Us.$$.fragment),up=f(),rl=l("span"),hp=r("JSON"),Mo=f(),Rt=l("p"),mp=r("JSON files are loaded directly with "),Qa=l("a"),gp=r("load_dataset()"),_p=r(" as shown below:"),Vo=f(),k(Js.$$.fragment),zo=f(),Ka=l("p"),vp=r("JSON files have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),Uo=f(),k(Bs.$$.fragment),Jo=f(),Mt=l("p"),$p=r("Another JSON format you may encounter is a nested field, in which case you will need to specify the "),il=l("code"),yp=r("field"),wp=r(" argument as shown in the following:"),Bo=f(),k(Ws.$$.fragment),Wo=f(),Za=l("p"),bp=r("To load remote JSON files via HTTP, pass the URLs instead:"),Yo=f(),k(Ys.$$.fragment),Go=f(),Xa=l("p"),jp=r("While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Qo=f(),ft=l("h3"),Vt=l("a"),pl=l("span"),k(Gs.$$.fragment),qp=f(),dl=l("span"),kp=r("Text files"),Ko=f(),te=l("p"),Ep=r("Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Zo=f(),k(Qs.$$.fragment),Xo=f(),se=l("p"),xp=r("To load remote text files via HTTP, pass the URLs instead:"),tn=f(),k(Ks.$$.fragment),sn=f(),ct=l("h3"),zt=l("a"),fl=l("span"),k(Zs.$$.fragment),Pp=f(),cl=l("span"),Ap=r("Parquet"),an=f(),ae=l("p"),Sp=r("Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query."),en=f(),ee=l("p"),Dp=r("To load a Parquet file:"),ln=f(),k(Xs.$$.fragment),on=f(),le=l("p"),Tp=r("To load remote Parquet files via HTTP, pass the URLs instead:"),nn=f(),k(ta.$$.fragment),rn=f(),ut=l("h2"),Ut=l("a"),ul=l("span"),k(sa.$$.fragment),Np=f(),hl=l("span"),Ip=r("In-memory data"),pn=f(),Jt=l("p"),Cp=r("\u{1F917} Datasets will also allow you to create a "),oe=l("a"),Op=r("Dataset"),Lp=r(" directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),dn=f(),ht=l("h3"),Bt=l("a"),ml=l("span"),k(aa.$$.fragment),Hp=f(),gl=l("span"),Fp=r("Python dictionary"),fn=f(),Wt=l("p"),Rp=r("Load Python dictionaries with "),ne=l("a"),Mp=r("from_dict()"),Vp=r(":"),cn=f(),k(ea.$$.fragment),un=f(),mt=l("h3"),Yt=l("a"),_l=l("span"),k(la.$$.fragment),zp=f(),vl=l("span"),Up=r("Pandas DataFrame"),hn=f(),Gt=l("p"),Jp=r("Load Pandas DataFrames with "),re=l("a"),Bp=r("from_pandas()"),Wp=r(":"),mn=f(),k(oa.$$.fragment),gn=f(),k(Qt.$$.fragment),_n=f(),gt=l("h2"),Kt=l("a"),$l=l("span"),k(na.$$.fragment),Yp=f(),yl=l("span"),Gp=r("Offline"),vn=f(),ie=l("p"),Qp=r("Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),$n=f(),tt=l("p"),Kp=r("If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),wl=l("code"),Zp=r("HF_DATASETS_OFFLINE"),Xp=r(" to "),bl=l("code"),td=r("1"),sd=r(" to enable full offline mode."),yn=f(),_t=l("h2"),Zt=l("a"),jl=l("span"),k(ra.$$.fragment),ad=f(),ql=l("span"),ed=r("Slice splits"),wn=f(),st=l("p"),ld=r("For more granular loading, you can load specific slices of a split. There are two options for slicing a split: using strings or the "),pe=l("a"),od=r("ReadInstruction"),nd=r(" API. Strings are more compact and readable for simple cases, while "),de=l("a"),rd=r("ReadInstruction"),id=r(" is easier to use with variable slicing parameters."),bn=f(),at=l("p"),pd=r("Concatenate a "),kl=l("code"),dd=r("train"),fd=r(" and "),El=l("code"),cd=r("test"),ud=r(" split by:"),jn=f(),k(ia.$$.fragment),qn=f(),Xt=l("p"),hd=r("Select specific rows of the "),xl=l("code"),md=r("train"),gd=r(" split:"),kn=f(),k(pa.$$.fragment),En=f(),fe=l("p"),_d=r("Or select a percentage of a split with:"),xn=f(),k(da.$$.fragment),Pn=f(),ce=l("p"),vd=r("You can even select a combination of percentages from each split:"),An=f(),k(fa.$$.fragment),Sn=f(),ue=l("p"),$d=r("Finally, create cross-validated splits by:"),Dn=f(),k(ca.$$.fragment),Tn=f(),vt=l("h3"),ts=l("a"),Pl=l("span"),k(ua.$$.fragment),yd=f(),Al=l("span"),wd=r("Percent slicing and rounding"),Nn=f(),he=l("p"),bd=r("For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown below:"),In=f(),k(ha.$$.fragment),Cn=f(),ss=l("p"),jd=r("If you want equal sized splits, use "),Sl=l("code"),qd=r("pct1_dropremainder"),kd=r(" rounding instead. This treats the specified percentage boundaries as multiples of 1%."),On=f(),k(ma.$$.fragment),Ln=f(),k(as.$$.fragment),Hn=f(),me=l("a"),Fn=f(),$t=l("h2"),es=l("a"),Dl=l("span"),k(ga.$$.fragment),Ed=f(),Tl=l("span"),xd=r("Troubleshooting"),Rn=f(),ge=l("p"),Pd=r("Sometimes, you may get unexpected results when you load a dataset. Two of the most common issues you may encounter are manually downloading a dataset, and specifying features of a dataset."),Mn=f(),yt=l("h3"),ls=l("a"),Nl=l("span"),k(_a.$$.fragment),Ad=f(),Il=l("span"),Sd=r("Manual download"),Vn=f(),U=l("p"),Dd=r("Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This causes "),_e=l("a"),Td=r("load_dataset()"),Nd=r(" to throw an "),Cl=l("code"),Id=r("AssertionError"),Cd=r(". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),Ol=l("code"),Od=r("data_dir"),Ld=r(" argument to specify the path to the files you just downloaded."),zn=f(),os=l("p"),Hd=r("For example, if you try to download a configuration from the "),va=l("a"),Fd=r("MATINF"),Rd=r(" dataset:"),Un=f(),k($a.$$.fragment),Jn=f(),wt=l("h3"),ns=l("a"),Ll=l("span"),k(ya.$$.fragment),Md=f(),Hl=l("span"),Vd=r("Specify features"),Bn=f(),J=l("p"),zd=r("When you create a dataset from local files, the "),ve=l("a"),Ud=r("Features"),Jd=r(" are automatically inferred by "),wa=l("a"),Bd=r("Apache Arrow"),Wd=r(". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself. The following example shows how you can add custom labels with the "),$e=l("a"),Yd=r("ClassLabel"),Gd=r(" feature."),Wn=f(),rs=l("p"),Qd=r("Start by defining your own labels with the "),ye=l("a"),Kd=r("Features"),Zd=r(" class:"),Yn=f(),k(ba.$$.fragment),Gn=f(),et=l("p"),Xd=r("Next, specify the "),Fl=l("code"),tf=r("features"),sf=r(" parameter in "),we=l("a"),af=r("load_dataset()"),ef=r(" with the features you just created:"),Qn=f(),k(ja.$$.fragment),Kn=f(),be=l("p"),lf=r("Now when you look at your dataset features, you can see it uses the custom labels you defined:"),Zn=f(),k(qa.$$.fragment),Xn=f(),bt=l("h2"),is=l("a"),Rl=l("span"),k(ka.$$.fragment),of=f(),Ml=l("span"),nf=r("Metrics"),tr=f(),k(ps.$$.fragment),sr=f(),je=l("p"),rf=r("When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),ar=f(),k(Ea.$$.fragment),er=f(),k(ds.$$.fragment),lr=f(),jt=l("h3"),fs=l("a"),Vl=l("span"),k(xa.$$.fragment),pf=f(),zl=l("span"),df=r("Load configurations"),or=f(),lt=l("p"),ff=r("It is possible for a metric to have different configurations. The configurations are stored in the "),Ul=l("code"),cf=r("config_name"),uf=r(" parameter in "),qe=l("a"),hf=r("MetricInfo"),mf=r(" attribute. When you load a metric, provide the configuration name as shown in the following:"),nr=f(),k(Pa.$$.fragment),rr=f(),qt=l("h3"),cs=l("a"),Jl=l("span"),k(Aa.$$.fragment),gf=f(),Bl=l("span"),_f=r("Distributed setup"),ir=f(),ke=l("p"),vf=r("When working in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),pr=f(),Ee=l("p"),$f=r("For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),dr=f(),ot=l("ol"),Wl=l("li"),Sa=l("p"),yf=r("Define the total number of processes with the "),Yl=l("code"),wf=r("num_process"),bf=r(" argument."),jf=f(),Gl=l("li"),kt=l("p"),qf=r("Set the process "),Ql=l("code"),kf=r("rank"),Ef=r(" as an integer between zero and "),Kl=l("code"),xf=r("num_process - 1"),Pf=r("."),Af=f(),Zl=l("li"),Da=l("p"),Sf=r("Load your metric with "),xe=l("a"),Df=r("load_metric()"),Tf=r(" with these arguments:"),fr=f(),k(Ta.$$.fragment),cr=f(),k(us.$$.fragment),ur=f(),hs=l("p"),Nf=r("In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Xl=l("code"),If=r("experiment_id"),Cf=r(" to distinguish the separate evaluations:"),hr=f(),k(Na.$$.fragment),this.h()},l(t){const e=Nh('[data-svelte="svelte-1phssyn"]',document.head);d=o(e,"META",{name:!0,content:!0}),e.forEach(s),m=c(t),h=o(t,"H1",{class:!0});var Ia=n(h);v=o(Ia,"A",{id:!0,class:!0,href:!0});var to=n(v);_=o(to,"SPAN",{});var so=n(_);P(y.$$.fragment,so),so.forEach(s),to.forEach(s),$=c(Ia),j=o(Ia,"SPAN",{});var ao=n(j);g=i(ao,"Load"),ao.forEach(s),Ia.forEach(s),A=c(t),S=o(t,"P",{});var eo=n(S);D=i(eo,"Datasets are stored in various places; a dataset can be on your local machine\u2019s disk, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever a dataset is stored, \u{1F917} Datasets can help you load it."),eo.forEach(s),Z=c(t),C=o(t,"P",{});var lo=n(C);N=i(lo,"This guide will show you how to load a dataset from:"),lo.forEach(s),Y=c(t),O=o(t,"UL",{});var R=n(O);G=o(R,"LI",{});var Ff=n(G);vs=i(Ff,"The Hub without a dataset loading script"),Ff.forEach(s),$s=c(R),Q=o(R,"LI",{});var Rf=n(Q);ys=i(Rf,"Local loading script"),Rf.forEach(s),ws=c(R),V=o(R,"LI",{});var Mf=n(V);bs=i(Mf,"Local files"),Mf.forEach(s),js=c(R),K=o(R,"LI",{});var Vf=n(K);I=i(Vf,"In-memory data"),Vf.forEach(s),Oa=c(R),Et=o(R,"LI",{});var zf=n(Et);La=i(zf,"Offline"),zf.forEach(s),Ha=c(R),xt=o(R,"LI",{});var Uf=n(xt);Fa=i(Uf,"A specific slice of a split"),Uf.forEach(s),R.forEach(s),qs=c(t),M=o(t,"P",{});var Pe=n(M);ti=i(Pe,"For more details specific to loading audio datasets, take a look at the "),ks=o(Pe,"A",{class:!0,href:!0});var Jf=n(ks);si=i(Jf,"load audio data"),Jf.forEach(s),ai=i(Pe," guide. If you\u2019re working with image datasets, check out the "),Es=o(Pe,"A",{class:!0,href:!0});var Bf=n(Es);ei=i(Bf,"load image data"),Bf.forEach(s),li=i(Pe," guide."),Pe.forEach(s),ro=c(t),Ra=o(t,"A",{id:!0}),n(Ra).forEach(s),io=c(t),nt=o(t,"H2",{class:!0});var gr=n(nt);Pt=o(gr,"A",{id:!0,class:!0,href:!0});var Wf=n(Pt);Le=o(Wf,"SPAN",{});var Yf=n(Le);P(xs.$$.fragment,Yf),Yf.forEach(s),Wf.forEach(s),oi=c(gr),He=o(gr,"SPAN",{});var Gf=n(He);ni=i(Gf,"Hugging Face Hub"),Gf.forEach(s),gr.forEach(s),po=c(t),z=o(t,"P",{});var ms=n(z);ri=i(ms,"Datasets are loaded from a dataset loading script that downloads and generates the dataset. However, you can also load a dataset from any dataset repository on the Hub "),Fe=o(ms,"STRONG",{});var Qf=n(Fe);ii=i(Qf,"without"),Qf.forEach(s),pi=i(ms," a loading script! Begin by "),Ma=o(ms,"A",{href:!0});var Kf=n(Ma);di=i(Kf,"creating a dataset repository"),Kf.forEach(s),fi=i(ms," and upload your data files. Now you can use the "),Va=o(ms,"A",{href:!0});var Zf=n(Va);ci=i(Zf,"load_dataset()"),Zf.forEach(s),ui=i(ms," function to load the dataset."),ms.forEach(s),fo=c(t),At=o(t,"P",{});var _r=n(At);hi=i(_r,"For example, try loading the files from this "),Ps=o(_r,"A",{href:!0,rel:!0});var Xf=n(Ps);mi=i(Xf,"demo repository"),Xf.forEach(s),gi=i(_r," by providing the repository namespace and dataset name. This dataset repository contains CSV files, and the code below loads the dataset from the CSV files:"),_r.forEach(s),co=c(t),P(As.$$.fragment,t),uo=c(t),St=o(t,"P",{});var vr=n(St);_i=i(vr,"Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),Re=o(vr,"CODE",{});var tc=n(Re);vi=i(tc,"revision"),tc.forEach(s),$i=i(vr," parameter to specify the dataset version you want to load:"),vr.forEach(s),ho=c(t),P(Ss.$$.fragment,t),mo=c(t),P(Dt.$$.fragment,t),go=c(t),H=o(t,"P",{});var B=n(H);yi=i(B,"A dataset without a loading script will by default load all the data into the "),Me=o(B,"CODE",{});var sc=n(Me);wi=i(sc,"train"),sc.forEach(s),bi=i(B," split. Use the "),Ve=o(B,"CODE",{});var ac=n(Ve);ji=i(ac,"data_files"),ac.forEach(s),qi=i(B," parameter to map data files to splits like "),ze=o(B,"CODE",{});var ec=n(ze);ki=i(ec,"train"),ec.forEach(s),Ei=i(B,", "),Ue=o(B,"CODE",{});var lc=n(Ue);xi=i(lc,"validation"),lc.forEach(s),Pi=i(B," and "),Je=o(B,"CODE",{});var oc=n(Je);Ai=i(oc,"test"),oc.forEach(s),Si=i(B,":"),B.forEach(s),_o=c(t),P(Ds.$$.fragment,t),vo=c(t),P(Tt.$$.fragment,t),$o=c(t),X=o(t,"P",{});var Ae=n(X);Di=i(Ae,"You can also load a specific subset of the files with the "),Be=o(Ae,"CODE",{});var nc=n(Be);Ti=i(nc,"data_files"),nc.forEach(s),Ni=i(Ae," parameter. The example below only loads a single file from the "),Ts=o(Ae,"A",{href:!0,rel:!0});var rc=n(Ts);Ii=i(rc,"C4 dataset"),rc.forEach(s),Ci=i(Ae,":"),Ae.forEach(s),yo=c(t),P(Ns.$$.fragment,t),wo=c(t),Nt=o(t,"P",{});var $r=n(Nt);Oi=i($r,"The "),We=o($r,"CODE",{});var ic=n(We);Li=i(ic,"split"),ic.forEach(s),Hi=i($r," parameter can also map a data file to a specific split:"),$r.forEach(s),bo=c(t),P(Is.$$.fragment,t),jo=c(t),rt=o(t,"H2",{class:!0});var yr=n(rt);It=o(yr,"A",{id:!0,class:!0,href:!0});var pc=n(It);Ye=o(pc,"SPAN",{});var dc=n(Ye);P(Cs.$$.fragment,dc),dc.forEach(s),pc.forEach(s),Fi=c(yr),Ge=o(yr,"SPAN",{});var fc=n(Ge);Ri=i(fc,"Local loading script"),fc.forEach(s),yr.forEach(s),qo=c(t),Ct=o(t,"P",{});var wr=n(Ct);Mi=i(wr,"You may have a \u{1F917} Datasets loading script locally on your computer. In this case, load the dataset by passing one of the following paths to "),za=o(wr,"A",{href:!0});var cc=n(za);Vi=i(cc,"load_dataset()"),cc.forEach(s),zi=i(wr,":"),wr.forEach(s),ko=c(t),Ot=o(t,"UL",{});var br=n(Ot);Qe=o(br,"LI",{});var uc=n(Qe);Ui=i(uc,"The local path to the loading script file."),uc.forEach(s),Ji=c(br),Ke=o(br,"LI",{});var hc=n(Ke);Bi=i(hc,"The local path to the directory containing the loading script file (only if the script file has the same name as the directory)."),hc.forEach(s),br.forEach(s),Eo=c(t),P(Os.$$.fragment,t),xo=c(t),it=o(t,"H2",{class:!0});var jr=n(it);Lt=o(jr,"A",{id:!0,class:!0,href:!0});var mc=n(Lt);Ze=o(mc,"SPAN",{});var gc=n(Ze);P(Ls.$$.fragment,gc),gc.forEach(s),mc.forEach(s),Wi=c(jr),Xe=o(jr,"SPAN",{});var _c=n(Xe);Yi=i(_c,"Local and remote files"),_c.forEach(s),jr.forEach(s),Po=c(t),F=o(t,"P",{});var W=n(F);Gi=i(W,"Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),tl=o(W,"CODE",{});var vc=n(tl);Qi=i(vc,"csv"),vc.forEach(s),Ki=i(W,", "),sl=o(W,"CODE",{});var $c=n(sl);Zi=i($c,"json"),$c.forEach(s),Xi=i(W,", "),al=o(W,"CODE",{});var yc=n(al);tp=i(yc,"txt"),yc.forEach(s),sp=i(W," or "),el=o(W,"CODE",{});var wc=n(el);ap=i(wc,"parquet"),wc.forEach(s),ep=i(W," file. The "),Ua=o(W,"A",{href:!0});var bc=n(Ua);lp=i(bc,"load_dataset()"),bc.forEach(s),op=i(W," function is able to load each of these file types."),W.forEach(s),Ao=c(t),pt=o(t,"H3",{class:!0});var qr=n(pt);Ht=o(qr,"A",{id:!0,class:!0,href:!0});var jc=n(Ht);ll=o(jc,"SPAN",{});var qc=n(ll);P(Hs.$$.fragment,qc),qc.forEach(s),jc.forEach(s),np=c(qr),ol=o(qr,"SPAN",{});var kc=n(ol);rp=i(kc,"CSV"),kc.forEach(s),qr.forEach(s),So=c(t),Ja=o(t,"P",{});var Ec=n(Ja);ip=i(Ec,"\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),Ec.forEach(s),Do=c(t),P(Fs.$$.fragment,t),To=c(t),Ba=o(t,"P",{});var xc=n(Ba);pp=i(xc,"If you have more than one CSV file:"),xc.forEach(s),No=c(t),P(Rs.$$.fragment,t),Io=c(t),Wa=o(t,"P",{});var Pc=n(Wa);dp=i(Pc,"You can also map the training and test splits to specific CSV files:"),Pc.forEach(s),Co=c(t),P(Ms.$$.fragment,t),Oo=c(t),Ya=o(t,"P",{});var Ac=n(Ya);fp=i(Ac,"To load remote CSV files via HTTP, pass the URLs instead:"),Ac.forEach(s),Lo=c(t),P(Vs.$$.fragment,t),Ho=c(t),Ga=o(t,"P",{});var Sc=n(Ga);cp=i(Sc,"To load zipped CSV files:"),Sc.forEach(s),Fo=c(t),P(zs.$$.fragment,t),Ro=c(t),dt=o(t,"H3",{class:!0});var kr=n(dt);Ft=o(kr,"A",{id:!0,class:!0,href:!0});var Dc=n(Ft);nl=o(Dc,"SPAN",{});var Tc=n(nl);P(Us.$$.fragment,Tc),Tc.forEach(s),Dc.forEach(s),up=c(kr),rl=o(kr,"SPAN",{});var Nc=n(rl);hp=i(Nc,"JSON"),Nc.forEach(s),kr.forEach(s),Mo=c(t),Rt=o(t,"P",{});var Er=n(Rt);mp=i(Er,"JSON files are loaded directly with "),Qa=o(Er,"A",{href:!0});var Ic=n(Qa);gp=i(Ic,"load_dataset()"),Ic.forEach(s),_p=i(Er," as shown below:"),Er.forEach(s),Vo=c(t),P(Js.$$.fragment,t),zo=c(t),Ka=o(t,"P",{});var Cc=n(Ka);vp=i(Cc,"JSON files have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),Cc.forEach(s),Uo=c(t),P(Bs.$$.fragment,t),Jo=c(t),Mt=o(t,"P",{});var xr=n(Mt);$p=i(xr,"Another JSON format you may encounter is a nested field, in which case you will need to specify the "),il=o(xr,"CODE",{});var Oc=n(il);yp=i(Oc,"field"),Oc.forEach(s),wp=i(xr," argument as shown in the following:"),xr.forEach(s),Bo=c(t),P(Ws.$$.fragment,t),Wo=c(t),Za=o(t,"P",{});var Lc=n(Za);bp=i(Lc,"To load remote JSON files via HTTP, pass the URLs instead:"),Lc.forEach(s),Yo=c(t),P(Ys.$$.fragment,t),Go=c(t),Xa=o(t,"P",{});var Hc=n(Xa);jp=i(Hc,"While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Hc.forEach(s),Qo=c(t),ft=o(t,"H3",{class:!0});var Pr=n(ft);Vt=o(Pr,"A",{id:!0,class:!0,href:!0});var Fc=n(Vt);pl=o(Fc,"SPAN",{});var Rc=n(pl);P(Gs.$$.fragment,Rc),Rc.forEach(s),Fc.forEach(s),qp=c(Pr),dl=o(Pr,"SPAN",{});var Mc=n(dl);kp=i(Mc,"Text files"),Mc.forEach(s),Pr.forEach(s),Ko=c(t),te=o(t,"P",{});var Vc=n(te);Ep=i(Vc,"Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Vc.forEach(s),Zo=c(t),P(Qs.$$.fragment,t),Xo=c(t),se=o(t,"P",{});var zc=n(se);xp=i(zc,"To load remote text files via HTTP, pass the URLs instead:"),zc.forEach(s),tn=c(t),P(Ks.$$.fragment,t),sn=c(t),ct=o(t,"H3",{class:!0});var Ar=n(ct);zt=o(Ar,"A",{id:!0,class:!0,href:!0});var Uc=n(zt);fl=o(Uc,"SPAN",{});var Jc=n(fl);P(Zs.$$.fragment,Jc),Jc.forEach(s),Uc.forEach(s),Pp=c(Ar),cl=o(Ar,"SPAN",{});var Bc=n(cl);Ap=i(Bc,"Parquet"),Bc.forEach(s),Ar.forEach(s),an=c(t),ae=o(t,"P",{});var Wc=n(ae);Sp=i(Wc,"Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query."),Wc.forEach(s),en=c(t),ee=o(t,"P",{});var Yc=n(ee);Dp=i(Yc,"To load a Parquet file:"),Yc.forEach(s),ln=c(t),P(Xs.$$.fragment,t),on=c(t),le=o(t,"P",{});var Gc=n(le);Tp=i(Gc,"To load remote Parquet files via HTTP, pass the URLs instead:"),Gc.forEach(s),nn=c(t),P(ta.$$.fragment,t),rn=c(t),ut=o(t,"H2",{class:!0});var Sr=n(ut);Ut=o(Sr,"A",{id:!0,class:!0,href:!0});var Qc=n(Ut);ul=o(Qc,"SPAN",{});var Kc=n(ul);P(sa.$$.fragment,Kc),Kc.forEach(s),Qc.forEach(s),Np=c(Sr),hl=o(Sr,"SPAN",{});var Zc=n(hl);Ip=i(Zc,"In-memory data"),Zc.forEach(s),Sr.forEach(s),pn=c(t),Jt=o(t,"P",{});var Dr=n(Jt);Cp=i(Dr,"\u{1F917} Datasets will also allow you to create a "),oe=o(Dr,"A",{href:!0});var Xc=n(oe);Op=i(Xc,"Dataset"),Xc.forEach(s),Lp=i(Dr," directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),Dr.forEach(s),dn=c(t),ht=o(t,"H3",{class:!0});var Tr=n(ht);Bt=o(Tr,"A",{id:!0,class:!0,href:!0});var tu=n(Bt);ml=o(tu,"SPAN",{});var su=n(ml);P(aa.$$.fragment,su),su.forEach(s),tu.forEach(s),Hp=c(Tr),gl=o(Tr,"SPAN",{});var au=n(gl);Fp=i(au,"Python dictionary"),au.forEach(s),Tr.forEach(s),fn=c(t),Wt=o(t,"P",{});var Nr=n(Wt);Rp=i(Nr,"Load Python dictionaries with "),ne=o(Nr,"A",{href:!0});var eu=n(ne);Mp=i(eu,"from_dict()"),eu.forEach(s),Vp=i(Nr,":"),Nr.forEach(s),cn=c(t),P(ea.$$.fragment,t),un=c(t),mt=o(t,"H3",{class:!0});var Ir=n(mt);Yt=o(Ir,"A",{id:!0,class:!0,href:!0});var lu=n(Yt);_l=o(lu,"SPAN",{});var ou=n(_l);P(la.$$.fragment,ou),ou.forEach(s),lu.forEach(s),zp=c(Ir),vl=o(Ir,"SPAN",{});var nu=n(vl);Up=i(nu,"Pandas DataFrame"),nu.forEach(s),Ir.forEach(s),hn=c(t),Gt=o(t,"P",{});var Cr=n(Gt);Jp=i(Cr,"Load Pandas DataFrames with "),re=o(Cr,"A",{href:!0});var ru=n(re);Bp=i(ru,"from_pandas()"),ru.forEach(s),Wp=i(Cr,":"),Cr.forEach(s),mn=c(t),P(oa.$$.fragment,t),gn=c(t),P(Qt.$$.fragment,t),_n=c(t),gt=o(t,"H2",{class:!0});var Or=n(gt);Kt=o(Or,"A",{id:!0,class:!0,href:!0});var iu=n(Kt);$l=o(iu,"SPAN",{});var pu=n($l);P(na.$$.fragment,pu),pu.forEach(s),iu.forEach(s),Yp=c(Or),yl=o(Or,"SPAN",{});var du=n(yl);Gp=i(du,"Offline"),du.forEach(s),Or.forEach(s),vn=c(t),ie=o(t,"P",{});var fu=n(ie);Qp=i(fu,"Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),fu.forEach(s),$n=c(t),tt=o(t,"P",{});var Se=n(tt);Kp=i(Se,"If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),wl=o(Se,"CODE",{});var cu=n(wl);Zp=i(cu,"HF_DATASETS_OFFLINE"),cu.forEach(s),Xp=i(Se," to "),bl=o(Se,"CODE",{});var uu=n(bl);td=i(uu,"1"),uu.forEach(s),sd=i(Se," to enable full offline mode."),Se.forEach(s),yn=c(t),_t=o(t,"H2",{class:!0});var Lr=n(_t);Zt=o(Lr,"A",{id:!0,class:!0,href:!0});var hu=n(Zt);jl=o(hu,"SPAN",{});var mu=n(jl);P(ra.$$.fragment,mu),mu.forEach(s),hu.forEach(s),ad=c(Lr),ql=o(Lr,"SPAN",{});var gu=n(ql);ed=i(gu,"Slice splits"),gu.forEach(s),Lr.forEach(s),wn=c(t),st=o(t,"P",{});var De=n(st);ld=i(De,"For more granular loading, you can load specific slices of a split. There are two options for slicing a split: using strings or the "),pe=o(De,"A",{href:!0});var _u=n(pe);od=i(_u,"ReadInstruction"),_u.forEach(s),nd=i(De," API. Strings are more compact and readable for simple cases, while "),de=o(De,"A",{href:!0});var vu=n(de);rd=i(vu,"ReadInstruction"),vu.forEach(s),id=i(De," is easier to use with variable slicing parameters."),De.forEach(s),bn=c(t),at=o(t,"P",{});var Te=n(at);pd=i(Te,"Concatenate a "),kl=o(Te,"CODE",{});var $u=n(kl);dd=i($u,"train"),$u.forEach(s),fd=i(Te," and "),El=o(Te,"CODE",{});var yu=n(El);cd=i(yu,"test"),yu.forEach(s),ud=i(Te," split by:"),Te.forEach(s),jn=c(t),P(ia.$$.fragment,t),qn=c(t),Xt=o(t,"P",{});var Hr=n(Xt);hd=i(Hr,"Select specific rows of the "),xl=o(Hr,"CODE",{});var wu=n(xl);md=i(wu,"train"),wu.forEach(s),gd=i(Hr," split:"),Hr.forEach(s),kn=c(t),P(pa.$$.fragment,t),En=c(t),fe=o(t,"P",{});var bu=n(fe);_d=i(bu,"Or select a percentage of a split with:"),bu.forEach(s),xn=c(t),P(da.$$.fragment,t),Pn=c(t),ce=o(t,"P",{});var ju=n(ce);vd=i(ju,"You can even select a combination of percentages from each split:"),ju.forEach(s),An=c(t),P(fa.$$.fragment,t),Sn=c(t),ue=o(t,"P",{});var qu=n(ue);$d=i(qu,"Finally, create cross-validated splits by:"),qu.forEach(s),Dn=c(t),P(ca.$$.fragment,t),Tn=c(t),vt=o(t,"H3",{class:!0});var Fr=n(vt);ts=o(Fr,"A",{id:!0,class:!0,href:!0});var ku=n(ts);Pl=o(ku,"SPAN",{});var Eu=n(Pl);P(ua.$$.fragment,Eu),Eu.forEach(s),ku.forEach(s),yd=c(Fr),Al=o(Fr,"SPAN",{});var xu=n(Al);wd=i(xu,"Percent slicing and rounding"),xu.forEach(s),Fr.forEach(s),Nn=c(t),he=o(t,"P",{});var Pu=n(he);bd=i(Pu,"For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown below:"),Pu.forEach(s),In=c(t),P(ha.$$.fragment,t),Cn=c(t),ss=o(t,"P",{});var Rr=n(ss);jd=i(Rr,"If you want equal sized splits, use "),Sl=o(Rr,"CODE",{});var Au=n(Sl);qd=i(Au,"pct1_dropremainder"),Au.forEach(s),kd=i(Rr," rounding instead. This treats the specified percentage boundaries as multiples of 1%."),Rr.forEach(s),On=c(t),P(ma.$$.fragment,t),Ln=c(t),P(as.$$.fragment,t),Hn=c(t),me=o(t,"A",{id:!0}),n(me).forEach(s),Fn=c(t),$t=o(t,"H2",{class:!0});var Mr=n($t);es=o(Mr,"A",{id:!0,class:!0,href:!0});var Su=n(es);Dl=o(Su,"SPAN",{});var Du=n(Dl);P(ga.$$.fragment,Du),Du.forEach(s),Su.forEach(s),Ed=c(Mr),Tl=o(Mr,"SPAN",{});var Tu=n(Tl);xd=i(Tu,"Troubleshooting"),Tu.forEach(s),Mr.forEach(s),Rn=c(t),ge=o(t,"P",{});var Nu=n(ge);Pd=i(Nu,"Sometimes, you may get unexpected results when you load a dataset. Two of the most common issues you may encounter are manually downloading a dataset, and specifying features of a dataset."),Nu.forEach(s),Mn=c(t),yt=o(t,"H3",{class:!0});var Vr=n(yt);ls=o(Vr,"A",{id:!0,class:!0,href:!0});var Iu=n(ls);Nl=o(Iu,"SPAN",{});var Cu=n(Nl);P(_a.$$.fragment,Cu),Cu.forEach(s),Iu.forEach(s),Ad=c(Vr),Il=o(Vr,"SPAN",{});var Ou=n(Il);Sd=i(Ou,"Manual download"),Ou.forEach(s),Vr.forEach(s),Vn=c(t),U=o(t,"P",{});var gs=n(U);Dd=i(gs,"Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This causes "),_e=o(gs,"A",{href:!0});var Lu=n(_e);Td=i(Lu,"load_dataset()"),Lu.forEach(s),Nd=i(gs," to throw an "),Cl=o(gs,"CODE",{});var Hu=n(Cl);Id=i(Hu,"AssertionError"),Hu.forEach(s),Cd=i(gs,". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),Ol=o(gs,"CODE",{});var Fu=n(Ol);Od=i(Fu,"data_dir"),Fu.forEach(s),Ld=i(gs," argument to specify the path to the files you just downloaded."),gs.forEach(s),zn=c(t),os=o(t,"P",{});var zr=n(os);Hd=i(zr,"For example, if you try to download a configuration from the "),va=o(zr,"A",{href:!0,rel:!0});var Ru=n(va);Fd=i(Ru,"MATINF"),Ru.forEach(s),Rd=i(zr," dataset:"),zr.forEach(s),Un=c(t),P($a.$$.fragment,t),Jn=c(t),wt=o(t,"H3",{class:!0});var Ur=n(wt);ns=o(Ur,"A",{id:!0,class:!0,href:!0});var Mu=n(ns);Ll=o(Mu,"SPAN",{});var Vu=n(Ll);P(ya.$$.fragment,Vu),Vu.forEach(s),Mu.forEach(s),Md=c(Ur),Hl=o(Ur,"SPAN",{});var zu=n(Hl);Vd=i(zu,"Specify features"),zu.forEach(s),Ur.forEach(s),Bn=c(t),J=o(t,"P",{});var _s=n(J);zd=i(_s,"When you create a dataset from local files, the "),ve=o(_s,"A",{href:!0});var Uu=n(ve);Ud=i(Uu,"Features"),Uu.forEach(s),Jd=i(_s," are automatically inferred by "),wa=o(_s,"A",{href:!0,rel:!0});var Ju=n(wa);Bd=i(Ju,"Apache Arrow"),Ju.forEach(s),Wd=i(_s,". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself. The following example shows how you can add custom labels with the "),$e=o(_s,"A",{href:!0});var Bu=n($e);Yd=i(Bu,"ClassLabel"),Bu.forEach(s),Gd=i(_s," feature."),_s.forEach(s),Wn=c(t),rs=o(t,"P",{});var Jr=n(rs);Qd=i(Jr,"Start by defining your own labels with the "),ye=o(Jr,"A",{href:!0});var Wu=n(ye);Kd=i(Wu,"Features"),Wu.forEach(s),Zd=i(Jr," class:"),Jr.forEach(s),Yn=c(t),P(ba.$$.fragment,t),Gn=c(t),et=o(t,"P",{});var Ne=n(et);Xd=i(Ne,"Next, specify the "),Fl=o(Ne,"CODE",{});var Yu=n(Fl);tf=i(Yu,"features"),Yu.forEach(s),sf=i(Ne," parameter in "),we=o(Ne,"A",{href:!0});var Gu=n(we);af=i(Gu,"load_dataset()"),Gu.forEach(s),ef=i(Ne," with the features you just created:"),Ne.forEach(s),Qn=c(t),P(ja.$$.fragment,t),Kn=c(t),be=o(t,"P",{});var Qu=n(be);lf=i(Qu,"Now when you look at your dataset features, you can see it uses the custom labels you defined:"),Qu.forEach(s),Zn=c(t),P(qa.$$.fragment,t),Xn=c(t),bt=o(t,"H2",{class:!0});var Br=n(bt);is=o(Br,"A",{id:!0,class:!0,href:!0});var Ku=n(is);Rl=o(Ku,"SPAN",{});var Zu=n(Rl);P(ka.$$.fragment,Zu),Zu.forEach(s),Ku.forEach(s),of=c(Br),Ml=o(Br,"SPAN",{});var Xu=n(Ml);nf=i(Xu,"Metrics"),Xu.forEach(s),Br.forEach(s),tr=c(t),P(ps.$$.fragment,t),sr=c(t),je=o(t,"P",{});var th=n(je);rf=i(th,"When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),th.forEach(s),ar=c(t),P(Ea.$$.fragment,t),er=c(t),P(ds.$$.fragment,t),lr=c(t),jt=o(t,"H3",{class:!0});var Wr=n(jt);fs=o(Wr,"A",{id:!0,class:!0,href:!0});var sh=n(fs);Vl=o(sh,"SPAN",{});var ah=n(Vl);P(xa.$$.fragment,ah),ah.forEach(s),sh.forEach(s),pf=c(Wr),zl=o(Wr,"SPAN",{});var eh=n(zl);df=i(eh,"Load configurations"),eh.forEach(s),Wr.forEach(s),or=c(t),lt=o(t,"P",{});var Ie=n(lt);ff=i(Ie,"It is possible for a metric to have different configurations. The configurations are stored in the "),Ul=o(Ie,"CODE",{});var lh=n(Ul);cf=i(lh,"config_name"),lh.forEach(s),uf=i(Ie," parameter in "),qe=o(Ie,"A",{href:!0});var oh=n(qe);hf=i(oh,"MetricInfo"),oh.forEach(s),mf=i(Ie," attribute. When you load a metric, provide the configuration name as shown in the following:"),Ie.forEach(s),nr=c(t),P(Pa.$$.fragment,t),rr=c(t),qt=o(t,"H3",{class:!0});var Yr=n(qt);cs=o(Yr,"A",{id:!0,class:!0,href:!0});var nh=n(cs);Jl=o(nh,"SPAN",{});var rh=n(Jl);P(Aa.$$.fragment,rh),rh.forEach(s),nh.forEach(s),gf=c(Yr),Bl=o(Yr,"SPAN",{});var ih=n(Bl);_f=i(ih,"Distributed setup"),ih.forEach(s),Yr.forEach(s),ir=c(t),ke=o(t,"P",{});var ph=n(ke);vf=i(ph,"When working in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),ph.forEach(s),pr=c(t),Ee=o(t,"P",{});var dh=n(Ee);$f=i(dh,"For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),dh.forEach(s),dr=c(t),ot=o(t,"OL",{});var Ce=n(ot);Wl=o(Ce,"LI",{});var fh=n(Wl);Sa=o(fh,"P",{});var Gr=n(Sa);yf=i(Gr,"Define the total number of processes with the "),Yl=o(Gr,"CODE",{});var ch=n(Yl);wf=i(ch,"num_process"),ch.forEach(s),bf=i(Gr," argument."),Gr.forEach(s),fh.forEach(s),jf=c(Ce),Gl=o(Ce,"LI",{});var uh=n(Gl);kt=o(uh,"P",{});var Oe=n(kt);qf=i(Oe,"Set the process "),Ql=o(Oe,"CODE",{});var hh=n(Ql);kf=i(hh,"rank"),hh.forEach(s),Ef=i(Oe," as an integer between zero and "),Kl=o(Oe,"CODE",{});var mh=n(Kl);xf=i(mh,"num_process - 1"),mh.forEach(s),Pf=i(Oe,"."),Oe.forEach(s),uh.forEach(s),Af=c(Ce),Zl=o(Ce,"LI",{});var gh=n(Zl);Da=o(gh,"P",{});var Qr=n(Da);Sf=i(Qr,"Load your metric with "),xe=o(Qr,"A",{href:!0});var _h=n(xe);Df=i(_h,"load_metric()"),_h.forEach(s),Tf=i(Qr," with these arguments:"),Qr.forEach(s),gh.forEach(s),Ce.forEach(s),fr=c(t),P(Ta.$$.fragment,t),cr=c(t),P(us.$$.fragment,t),ur=c(t),hs=o(t,"P",{});var Kr=n(hs);Nf=i(Kr,"In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Xl=o(Kr,"CODE",{});var vh=n(Xl);If=i(vh,"experiment_id"),vh.forEach(s),Cf=i(Kr," to distinguish the separate evaluations:"),Kr.forEach(s),hr=c(t),P(Na.$$.fragment,t),this.h()},h(){u(d,"name","hf:doc:metadata"),u(d,"content",JSON.stringify(Kh)),u(v,"id","load"),u(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(v,"href","#load"),u(h,"class","relative group"),u(ks,"class","bg-yellow-200 text-yellow-900 dark:bg-yellow-500 px-1 rounded font-bold"),u(ks,"href","./audio_load"),u(Es,"class","bg-green-200 text-green-900 dark:bg-green-500 px-1 rounded font-bold"),u(Es,"href","./image_load"),u(Ra,"id","load-from-the-hub"),u(Pt,"id","hugging-face-hub"),u(Pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Pt,"href","#hugging-face-hub"),u(nt,"class","relative group"),u(Ma,"href","share#create-the-repository"),u(Va,"href","/docs/datasets/pr_4519/en/package_reference/loading_methods#datasets.load_dataset"),u(Ps,"href","https://huggingface.co/datasets/lhoestq/demo1"),u(Ps,"rel","nofollow"),u(Ts,"href","https://huggingface.co/datasets/allenai/c4/tree/main/en"),u(Ts,"rel","nofollow"),u(It,"id","local-loading-script"),u(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(It,"href","#local-loading-script"),u(rt,"class","relative group"),u(za,"href","/docs/datasets/pr_4519/en/package_reference/loading_methods#datasets.load_dataset"),u(Lt,"id","local-and-remote-files"),u(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Lt,"href","#local-and-remote-files"),u(it,"class","relative group"),u(Ua,"href","/docs/datasets/pr_4519/en/package_reference/loading_methods#datasets.load_dataset"),u(Ht,"id","csv"),u(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Ht,"href","#csv"),u(pt,"class","relative group"),u(Ft,"id","json"),u(Ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Ft,"href","#json"),u(dt,"class","relative group"),u(Qa,"href","/docs/datasets/pr_4519/en/package_reference/loading_methods#datasets.load_dataset"),u(Vt,"id","text-files"),u(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Vt,"href","#text-files"),u(ft,"class","relative group"),u(zt,"id","parquet"),u(zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(zt,"href","#parquet"),u(ct,"class","relative group"),u(Ut,"id","inmemory-data"),u(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Ut,"href","#inmemory-data"),u(ut,"class","relative group"),u(oe,"href","/docs/datasets/pr_4519/en/package_reference/main_classes#datasets.Dataset"),u(Bt,"id","python-dictionary"),u(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Bt,"href","#python-dictionary"),u(ht,"class","relative group"),u(ne,"href","/docs/datasets/pr_4519/en/package_reference/main_classes#datasets.Dataset.from_dict"),u(Yt,"id","pandas-dataframe"),u(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Yt,"href","#pandas-dataframe"),u(mt,"class","relative group"),u(re,"href","/docs/datasets/pr_4519/en/package_reference/main_classes#datasets.Dataset.from_pandas"),u(Kt,"id","offline"),u(Kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Kt,"href","#offline"),u(gt,"class","relative group"),u(Zt,"id","slice-splits"),u(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Zt,"href","#slice-splits"),u(_t,"class","relative group"),u(pe,"href","/docs/datasets/pr_4519/en/package_reference/builder_classes#datasets.ReadInstruction"),u(de,"href","/docs/datasets/pr_4519/en/package_reference/builder_classes#datasets.ReadInstruction"),u(ts,"id","percent-slicing-and-rounding"),u(ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ts,"href","#percent-slicing-and-rounding"),u(vt,"class","relative group"),u(me,"id","troubleshoot"),u(es,"id","troubleshooting"),u(es,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(es,"href","#troubleshooting"),u($t,"class","relative group"),u(ls,"id","manual-download"),u(ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ls,"href","#manual-download"),u(yt,"class","relative group"),u(_e,"href","/docs/datasets/pr_4519/en/package_reference/loading_methods#datasets.load_dataset"),u(va,"href","https://huggingface.co/datasets/matinf"),u(va,"rel","nofollow"),u(ns,"id","specify-features"),u(ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ns,"href","#specify-features"),u(wt,"class","relative group"),u(ve,"href","/docs/datasets/pr_4519/en/package_reference/main_classes#datasets.Features"),u(wa,"href","https://arrow.apache.org/docs/"),u(wa,"rel","nofollow"),u($e,"href","/docs/datasets/pr_4519/en/package_reference/main_classes#datasets.ClassLabel"),u(ye,"href","/docs/datasets/pr_4519/en/package_reference/main_classes#datasets.Features"),u(we,"href","/docs/datasets/pr_4519/en/package_reference/loading_methods#datasets.load_dataset"),u(is,"id","metrics"),u(is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(is,"href","#metrics"),u(bt,"class","relative group"),u(fs,"id","load-configurations"),u(fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(fs,"href","#load-configurations"),u(jt,"class","relative group"),u(qe,"href","/docs/datasets/pr_4519/en/package_reference/main_classes#datasets.MetricInfo"),u(cs,"id","distributed-setup"),u(cs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(cs,"href","#distributed-setup"),u(qt,"class","relative group"),u(xe,"href","/docs/datasets/pr_4519/en/package_reference/loading_methods#datasets.load_metric")},m(t,e){a(document.head,d),p(t,m,e),p(t,h,e),a(h,v),a(v,_),E(y,_,null),a(h,$),a(h,j),a(j,g),p(t,A,e),p(t,S,e),a(S,D),p(t,Z,e),p(t,C,e),a(C,N),p(t,Y,e),p(t,O,e),a(O,G),a(G,vs),a(O,$s),a(O,Q),a(Q,ys),a(O,ws),a(O,V),a(V,bs),a(O,js),a(O,K),a(K,I),a(O,Oa),a(O,Et),a(Et,La),a(O,Ha),a(O,xt),a(xt,Fa),p(t,qs,e),p(t,M,e),a(M,ti),a(M,ks),a(ks,si),a(M,ai),a(M,Es),a(Es,ei),a(M,li),p(t,ro,e),p(t,Ra,e),p(t,io,e),p(t,nt,e),a(nt,Pt),a(Pt,Le),E(xs,Le,null),a(nt,oi),a(nt,He),a(He,ni),p(t,po,e),p(t,z,e),a(z,ri),a(z,Fe),a(Fe,ii),a(z,pi),a(z,Ma),a(Ma,di),a(z,fi),a(z,Va),a(Va,ci),a(z,ui),p(t,fo,e),p(t,At,e),a(At,hi),a(At,Ps),a(Ps,mi),a(At,gi),p(t,co,e),E(As,t,e),p(t,uo,e),p(t,St,e),a(St,_i),a(St,Re),a(Re,vi),a(St,$i),p(t,ho,e),E(Ss,t,e),p(t,mo,e),E(Dt,t,e),p(t,go,e),p(t,H,e),a(H,yi),a(H,Me),a(Me,wi),a(H,bi),a(H,Ve),a(Ve,ji),a(H,qi),a(H,ze),a(ze,ki),a(H,Ei),a(H,Ue),a(Ue,xi),a(H,Pi),a(H,Je),a(Je,Ai),a(H,Si),p(t,_o,e),E(Ds,t,e),p(t,vo,e),E(Tt,t,e),p(t,$o,e),p(t,X,e),a(X,Di),a(X,Be),a(Be,Ti),a(X,Ni),a(X,Ts),a(Ts,Ii),a(X,Ci),p(t,yo,e),E(Ns,t,e),p(t,wo,e),p(t,Nt,e),a(Nt,Oi),a(Nt,We),a(We,Li),a(Nt,Hi),p(t,bo,e),E(Is,t,e),p(t,jo,e),p(t,rt,e),a(rt,It),a(It,Ye),E(Cs,Ye,null),a(rt,Fi),a(rt,Ge),a(Ge,Ri),p(t,qo,e),p(t,Ct,e),a(Ct,Mi),a(Ct,za),a(za,Vi),a(Ct,zi),p(t,ko,e),p(t,Ot,e),a(Ot,Qe),a(Qe,Ui),a(Ot,Ji),a(Ot,Ke),a(Ke,Bi),p(t,Eo,e),E(Os,t,e),p(t,xo,e),p(t,it,e),a(it,Lt),a(Lt,Ze),E(Ls,Ze,null),a(it,Wi),a(it,Xe),a(Xe,Yi),p(t,Po,e),p(t,F,e),a(F,Gi),a(F,tl),a(tl,Qi),a(F,Ki),a(F,sl),a(sl,Zi),a(F,Xi),a(F,al),a(al,tp),a(F,sp),a(F,el),a(el,ap),a(F,ep),a(F,Ua),a(Ua,lp),a(F,op),p(t,Ao,e),p(t,pt,e),a(pt,Ht),a(Ht,ll),E(Hs,ll,null),a(pt,np),a(pt,ol),a(ol,rp),p(t,So,e),p(t,Ja,e),a(Ja,ip),p(t,Do,e),E(Fs,t,e),p(t,To,e),p(t,Ba,e),a(Ba,pp),p(t,No,e),E(Rs,t,e),p(t,Io,e),p(t,Wa,e),a(Wa,dp),p(t,Co,e),E(Ms,t,e),p(t,Oo,e),p(t,Ya,e),a(Ya,fp),p(t,Lo,e),E(Vs,t,e),p(t,Ho,e),p(t,Ga,e),a(Ga,cp),p(t,Fo,e),E(zs,t,e),p(t,Ro,e),p(t,dt,e),a(dt,Ft),a(Ft,nl),E(Us,nl,null),a(dt,up),a(dt,rl),a(rl,hp),p(t,Mo,e),p(t,Rt,e),a(Rt,mp),a(Rt,Qa),a(Qa,gp),a(Rt,_p),p(t,Vo,e),E(Js,t,e),p(t,zo,e),p(t,Ka,e),a(Ka,vp),p(t,Uo,e),E(Bs,t,e),p(t,Jo,e),p(t,Mt,e),a(Mt,$p),a(Mt,il),a(il,yp),a(Mt,wp),p(t,Bo,e),E(Ws,t,e),p(t,Wo,e),p(t,Za,e),a(Za,bp),p(t,Yo,e),E(Ys,t,e),p(t,Go,e),p(t,Xa,e),a(Xa,jp),p(t,Qo,e),p(t,ft,e),a(ft,Vt),a(Vt,pl),E(Gs,pl,null),a(ft,qp),a(ft,dl),a(dl,kp),p(t,Ko,e),p(t,te,e),a(te,Ep),p(t,Zo,e),E(Qs,t,e),p(t,Xo,e),p(t,se,e),a(se,xp),p(t,tn,e),E(Ks,t,e),p(t,sn,e),p(t,ct,e),a(ct,zt),a(zt,fl),E(Zs,fl,null),a(ct,Pp),a(ct,cl),a(cl,Ap),p(t,an,e),p(t,ae,e),a(ae,Sp),p(t,en,e),p(t,ee,e),a(ee,Dp),p(t,ln,e),E(Xs,t,e),p(t,on,e),p(t,le,e),a(le,Tp),p(t,nn,e),E(ta,t,e),p(t,rn,e),p(t,ut,e),a(ut,Ut),a(Ut,ul),E(sa,ul,null),a(ut,Np),a(ut,hl),a(hl,Ip),p(t,pn,e),p(t,Jt,e),a(Jt,Cp),a(Jt,oe),a(oe,Op),a(Jt,Lp),p(t,dn,e),p(t,ht,e),a(ht,Bt),a(Bt,ml),E(aa,ml,null),a(ht,Hp),a(ht,gl),a(gl,Fp),p(t,fn,e),p(t,Wt,e),a(Wt,Rp),a(Wt,ne),a(ne,Mp),a(Wt,Vp),p(t,cn,e),E(ea,t,e),p(t,un,e),p(t,mt,e),a(mt,Yt),a(Yt,_l),E(la,_l,null),a(mt,zp),a(mt,vl),a(vl,Up),p(t,hn,e),p(t,Gt,e),a(Gt,Jp),a(Gt,re),a(re,Bp),a(Gt,Wp),p(t,mn,e),E(oa,t,e),p(t,gn,e),E(Qt,t,e),p(t,_n,e),p(t,gt,e),a(gt,Kt),a(Kt,$l),E(na,$l,null),a(gt,Yp),a(gt,yl),a(yl,Gp),p(t,vn,e),p(t,ie,e),a(ie,Qp),p(t,$n,e),p(t,tt,e),a(tt,Kp),a(tt,wl),a(wl,Zp),a(tt,Xp),a(tt,bl),a(bl,td),a(tt,sd),p(t,yn,e),p(t,_t,e),a(_t,Zt),a(Zt,jl),E(ra,jl,null),a(_t,ad),a(_t,ql),a(ql,ed),p(t,wn,e),p(t,st,e),a(st,ld),a(st,pe),a(pe,od),a(st,nd),a(st,de),a(de,rd),a(st,id),p(t,bn,e),p(t,at,e),a(at,pd),a(at,kl),a(kl,dd),a(at,fd),a(at,El),a(El,cd),a(at,ud),p(t,jn,e),E(ia,t,e),p(t,qn,e),p(t,Xt,e),a(Xt,hd),a(Xt,xl),a(xl,md),a(Xt,gd),p(t,kn,e),E(pa,t,e),p(t,En,e),p(t,fe,e),a(fe,_d),p(t,xn,e),E(da,t,e),p(t,Pn,e),p(t,ce,e),a(ce,vd),p(t,An,e),E(fa,t,e),p(t,Sn,e),p(t,ue,e),a(ue,$d),p(t,Dn,e),E(ca,t,e),p(t,Tn,e),p(t,vt,e),a(vt,ts),a(ts,Pl),E(ua,Pl,null),a(vt,yd),a(vt,Al),a(Al,wd),p(t,Nn,e),p(t,he,e),a(he,bd),p(t,In,e),E(ha,t,e),p(t,Cn,e),p(t,ss,e),a(ss,jd),a(ss,Sl),a(Sl,qd),a(ss,kd),p(t,On,e),E(ma,t,e),p(t,Ln,e),E(as,t,e),p(t,Hn,e),p(t,me,e),p(t,Fn,e),p(t,$t,e),a($t,es),a(es,Dl),E(ga,Dl,null),a($t,Ed),a($t,Tl),a(Tl,xd),p(t,Rn,e),p(t,ge,e),a(ge,Pd),p(t,Mn,e),p(t,yt,e),a(yt,ls),a(ls,Nl),E(_a,Nl,null),a(yt,Ad),a(yt,Il),a(Il,Sd),p(t,Vn,e),p(t,U,e),a(U,Dd),a(U,_e),a(_e,Td),a(U,Nd),a(U,Cl),a(Cl,Id),a(U,Cd),a(U,Ol),a(Ol,Od),a(U,Ld),p(t,zn,e),p(t,os,e),a(os,Hd),a(os,va),a(va,Fd),a(os,Rd),p(t,Un,e),E($a,t,e),p(t,Jn,e),p(t,wt,e),a(wt,ns),a(ns,Ll),E(ya,Ll,null),a(wt,Md),a(wt,Hl),a(Hl,Vd),p(t,Bn,e),p(t,J,e),a(J,zd),a(J,ve),a(ve,Ud),a(J,Jd),a(J,wa),a(wa,Bd),a(J,Wd),a(J,$e),a($e,Yd),a(J,Gd),p(t,Wn,e),p(t,rs,e),a(rs,Qd),a(rs,ye),a(ye,Kd),a(rs,Zd),p(t,Yn,e),E(ba,t,e),p(t,Gn,e),p(t,et,e),a(et,Xd),a(et,Fl),a(Fl,tf),a(et,sf),a(et,we),a(we,af),a(et,ef),p(t,Qn,e),E(ja,t,e),p(t,Kn,e),p(t,be,e),a(be,lf),p(t,Zn,e),E(qa,t,e),p(t,Xn,e),p(t,bt,e),a(bt,is),a(is,Rl),E(ka,Rl,null),a(bt,of),a(bt,Ml),a(Ml,nf),p(t,tr,e),E(ps,t,e),p(t,sr,e),p(t,je,e),a(je,rf),p(t,ar,e),E(Ea,t,e),p(t,er,e),E(ds,t,e),p(t,lr,e),p(t,jt,e),a(jt,fs),a(fs,Vl),E(xa,Vl,null),a(jt,pf),a(jt,zl),a(zl,df),p(t,or,e),p(t,lt,e),a(lt,ff),a(lt,Ul),a(Ul,cf),a(lt,uf),a(lt,qe),a(qe,hf),a(lt,mf),p(t,nr,e),E(Pa,t,e),p(t,rr,e),p(t,qt,e),a(qt,cs),a(cs,Jl),E(Aa,Jl,null),a(qt,gf),a(qt,Bl),a(Bl,_f),p(t,ir,e),p(t,ke,e),a(ke,vf),p(t,pr,e),p(t,Ee,e),a(Ee,$f),p(t,dr,e),p(t,ot,e),a(ot,Wl),a(Wl,Sa),a(Sa,yf),a(Sa,Yl),a(Yl,wf),a(Sa,bf),a(ot,jf),a(ot,Gl),a(Gl,kt),a(kt,qf),a(kt,Ql),a(Ql,kf),a(kt,Ef),a(kt,Kl),a(Kl,xf),a(kt,Pf),a(ot,Af),a(ot,Zl),a(Zl,Da),a(Da,Sf),a(Da,xe),a(xe,Df),a(Da,Tf),p(t,fr,e),E(Ta,t,e),p(t,cr,e),E(us,t,e),p(t,ur,e),p(t,hs,e),a(hs,Nf),a(hs,Xl),a(Xl,If),a(hs,Cf),p(t,hr,e),E(Na,t,e),mr=!0},p(t,[e]){const Ia={};e&2&&(Ia.$$scope={dirty:e,ctx:t}),Dt.$set(Ia);const to={};e&2&&(to.$$scope={dirty:e,ctx:t}),Tt.$set(to);const so={};e&2&&(so.$$scope={dirty:e,ctx:t}),Qt.$set(so);const ao={};e&2&&(ao.$$scope={dirty:e,ctx:t}),as.$set(ao);const eo={};e&2&&(eo.$$scope={dirty:e,ctx:t}),ps.$set(eo);const lo={};e&2&&(lo.$$scope={dirty:e,ctx:t}),ds.$set(lo);const R={};e&2&&(R.$$scope={dirty:e,ctx:t}),us.$set(R)},i(t){mr||(w(y.$$.fragment,t),w(xs.$$.fragment,t),w(As.$$.fragment,t),w(Ss.$$.fragment,t),w(Dt.$$.fragment,t),w(Ds.$$.fragment,t),w(Tt.$$.fragment,t),w(Ns.$$.fragment,t),w(Is.$$.fragment,t),w(Cs.$$.fragment,t),w(Os.$$.fragment,t),w(Ls.$$.fragment,t),w(Hs.$$.fragment,t),w(Fs.$$.fragment,t),w(Rs.$$.fragment,t),w(Ms.$$.fragment,t),w(Vs.$$.fragment,t),w(zs.$$.fragment,t),w(Us.$$.fragment,t),w(Js.$$.fragment,t),w(Bs.$$.fragment,t),w(Ws.$$.fragment,t),w(Ys.$$.fragment,t),w(Gs.$$.fragment,t),w(Qs.$$.fragment,t),w(Ks.$$.fragment,t),w(Zs.$$.fragment,t),w(Xs.$$.fragment,t),w(ta.$$.fragment,t),w(sa.$$.fragment,t),w(aa.$$.fragment,t),w(ea.$$.fragment,t),w(la.$$.fragment,t),w(oa.$$.fragment,t),w(Qt.$$.fragment,t),w(na.$$.fragment,t),w(ra.$$.fragment,t),w(ia.$$.fragment,t),w(pa.$$.fragment,t),w(da.$$.fragment,t),w(fa.$$.fragment,t),w(ca.$$.fragment,t),w(ua.$$.fragment,t),w(ha.$$.fragment,t),w(ma.$$.fragment,t),w(as.$$.fragment,t),w(ga.$$.fragment,t),w(_a.$$.fragment,t),w($a.$$.fragment,t),w(ya.$$.fragment,t),w(ba.$$.fragment,t),w(ja.$$.fragment,t),w(qa.$$.fragment,t),w(ka.$$.fragment,t),w(ps.$$.fragment,t),w(Ea.$$.fragment,t),w(ds.$$.fragment,t),w(xa.$$.fragment,t),w(Pa.$$.fragment,t),w(Aa.$$.fragment,t),w(Ta.$$.fragment,t),w(us.$$.fragment,t),w(Na.$$.fragment,t),mr=!0)},o(t){b(y.$$.fragment,t),b(xs.$$.fragment,t),b(As.$$.fragment,t),b(Ss.$$.fragment,t),b(Dt.$$.fragment,t),b(Ds.$$.fragment,t),b(Tt.$$.fragment,t),b(Ns.$$.fragment,t),b(Is.$$.fragment,t),b(Cs.$$.fragment,t),b(Os.$$.fragment,t),b(Ls.$$.fragment,t),b(Hs.$$.fragment,t),b(Fs.$$.fragment,t),b(Rs.$$.fragment,t),b(Ms.$$.fragment,t),b(Vs.$$.fragment,t),b(zs.$$.fragment,t),b(Us.$$.fragment,t),b(Js.$$.fragment,t),b(Bs.$$.fragment,t),b(Ws.$$.fragment,t),b(Ys.$$.fragment,t),b(Gs.$$.fragment,t),b(Qs.$$.fragment,t),b(Ks.$$.fragment,t),b(Zs.$$.fragment,t),b(Xs.$$.fragment,t),b(ta.$$.fragment,t),b(sa.$$.fragment,t),b(aa.$$.fragment,t),b(ea.$$.fragment,t),b(la.$$.fragment,t),b(oa.$$.fragment,t),b(Qt.$$.fragment,t),b(na.$$.fragment,t),b(ra.$$.fragment,t),b(ia.$$.fragment,t),b(pa.$$.fragment,t),b(da.$$.fragment,t),b(fa.$$.fragment,t),b(ca.$$.fragment,t),b(ua.$$.fragment,t),b(ha.$$.fragment,t),b(ma.$$.fragment,t),b(as.$$.fragment,t),b(ga.$$.fragment,t),b(_a.$$.fragment,t),b($a.$$.fragment,t),b(ya.$$.fragment,t),b(ba.$$.fragment,t),b(ja.$$.fragment,t),b(qa.$$.fragment,t),b(ka.$$.fragment,t),b(ps.$$.fragment,t),b(Ea.$$.fragment,t),b(ds.$$.fragment,t),b(xa.$$.fragment,t),b(Pa.$$.fragment,t),b(Aa.$$.fragment,t),b(Ta.$$.fragment,t),b(us.$$.fragment,t),b(Na.$$.fragment,t),mr=!1},d(t){s(d),t&&s(m),t&&s(h),q(y),t&&s(A),t&&s(S),t&&s(Z),t&&s(C),t&&s(Y),t&&s(O),t&&s(qs),t&&s(M),t&&s(ro),t&&s(Ra),t&&s(io),t&&s(nt),q(xs),t&&s(po),t&&s(z),t&&s(fo),t&&s(At),t&&s(co),q(As,t),t&&s(uo),t&&s(St),t&&s(ho),q(Ss,t),t&&s(mo),q(Dt,t),t&&s(go),t&&s(H),t&&s(_o),q(Ds,t),t&&s(vo),q(Tt,t),t&&s($o),t&&s(X),t&&s(yo),q(Ns,t),t&&s(wo),t&&s(Nt),t&&s(bo),q(Is,t),t&&s(jo),t&&s(rt),q(Cs),t&&s(qo),t&&s(Ct),t&&s(ko),t&&s(Ot),t&&s(Eo),q(Os,t),t&&s(xo),t&&s(it),q(Ls),t&&s(Po),t&&s(F),t&&s(Ao),t&&s(pt),q(Hs),t&&s(So),t&&s(Ja),t&&s(Do),q(Fs,t),t&&s(To),t&&s(Ba),t&&s(No),q(Rs,t),t&&s(Io),t&&s(Wa),t&&s(Co),q(Ms,t),t&&s(Oo),t&&s(Ya),t&&s(Lo),q(Vs,t),t&&s(Ho),t&&s(Ga),t&&s(Fo),q(zs,t),t&&s(Ro),t&&s(dt),q(Us),t&&s(Mo),t&&s(Rt),t&&s(Vo),q(Js,t),t&&s(zo),t&&s(Ka),t&&s(Uo),q(Bs,t),t&&s(Jo),t&&s(Mt),t&&s(Bo),q(Ws,t),t&&s(Wo),t&&s(Za),t&&s(Yo),q(Ys,t),t&&s(Go),t&&s(Xa),t&&s(Qo),t&&s(ft),q(Gs),t&&s(Ko),t&&s(te),t&&s(Zo),q(Qs,t),t&&s(Xo),t&&s(se),t&&s(tn),q(Ks,t),t&&s(sn),t&&s(ct),q(Zs),t&&s(an),t&&s(ae),t&&s(en),t&&s(ee),t&&s(ln),q(Xs,t),t&&s(on),t&&s(le),t&&s(nn),q(ta,t),t&&s(rn),t&&s(ut),q(sa),t&&s(pn),t&&s(Jt),t&&s(dn),t&&s(ht),q(aa),t&&s(fn),t&&s(Wt),t&&s(cn),q(ea,t),t&&s(un),t&&s(mt),q(la),t&&s(hn),t&&s(Gt),t&&s(mn),q(oa,t),t&&s(gn),q(Qt,t),t&&s(_n),t&&s(gt),q(na),t&&s(vn),t&&s(ie),t&&s($n),t&&s(tt),t&&s(yn),t&&s(_t),q(ra),t&&s(wn),t&&s(st),t&&s(bn),t&&s(at),t&&s(jn),q(ia,t),t&&s(qn),t&&s(Xt),t&&s(kn),q(pa,t),t&&s(En),t&&s(fe),t&&s(xn),q(da,t),t&&s(Pn),t&&s(ce),t&&s(An),q(fa,t),t&&s(Sn),t&&s(ue),t&&s(Dn),q(ca,t),t&&s(Tn),t&&s(vt),q(ua),t&&s(Nn),t&&s(he),t&&s(In),q(ha,t),t&&s(Cn),t&&s(ss),t&&s(On),q(ma,t),t&&s(Ln),q(as,t),t&&s(Hn),t&&s(me),t&&s(Fn),t&&s($t),q(ga),t&&s(Rn),t&&s(ge),t&&s(Mn),t&&s(yt),q(_a),t&&s(Vn),t&&s(U),t&&s(zn),t&&s(os),t&&s(Un),q($a,t),t&&s(Jn),t&&s(wt),q(ya),t&&s(Bn),t&&s(J),t&&s(Wn),t&&s(rs),t&&s(Yn),q(ba,t),t&&s(Gn),t&&s(et),t&&s(Qn),q(ja,t),t&&s(Kn),t&&s(be),t&&s(Zn),q(qa,t),t&&s(Xn),t&&s(bt),q(ka),t&&s(tr),q(ps,t),t&&s(sr),t&&s(je),t&&s(ar),q(Ea,t),t&&s(er),q(ds,t),t&&s(lr),t&&s(jt),q(xa),t&&s(or),t&&s(lt),t&&s(nr),q(Pa,t),t&&s(rr),t&&s(qt),q(Aa),t&&s(ir),t&&s(ke),t&&s(pr),t&&s(Ee),t&&s(dr),t&&s(ot),t&&s(fr),q(Ta,t),t&&s(cr),q(us,t),t&&s(ur),t&&s(hs),t&&s(hr),q(Na,t)}}}const Kh={local:"load",sections:[{local:"hugging-face-hub",title:"Hugging Face Hub"},{local:"local-loading-script",title:"Local loading script"},{local:"local-and-remote-files",sections:[{local:"csv",title:"CSV"},{local:"json",title:"JSON"},{local:"text-files",title:"Text files"},{local:"parquet",title:"Parquet"}],title:"Local and remote files"},{local:"inmemory-data",sections:[{local:"python-dictionary",title:"Python dictionary"},{local:"pandas-dataframe",title:"Pandas DataFrame"}],title:"In-memory data"},{local:"offline",title:"Offline"},{local:"slice-splits",sections:[{local:"percent-slicing-and-rounding",title:"Percent slicing and rounding"}],title:"Slice splits"},{local:"troubleshooting",sections:[{local:"manual-download",title:"Manual download"},{local:"specify-features",title:"Specify features"}],title:"Troubleshooting"},{local:"metrics",sections:[{local:"load-configurations",title:"Load configurations"},{local:"distributed-setup",title:"Distributed setup"}],title:"Metrics"}],title:"Load"};function Zh(x){return Ih(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class lm extends Of{constructor(d){super();Lf(this,d,Zh,Qh,Hf,{})}}export{lm as default,Kh as metadata};
