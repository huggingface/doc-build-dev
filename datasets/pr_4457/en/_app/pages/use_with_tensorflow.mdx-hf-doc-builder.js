import{S as kr,i as Tr,s as xr,e as o,k as d,w as u,t as s,M as Ar,c as l,d as e,m as c,a as r,x as m,h as n,b as h,G as a,g as p,y as g,q as y,o as v,B as w,v as qr}from"../chunks/vendor-hf-doc-builder.js";import{T as Cr}from"../chunks/Tip-hf-doc-builder.js";import{I as Y}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as Z}from"../chunks/CodeBlock-hf-doc-builder.js";function Pr(pe){let _,tt,E,P,W;return{c(){_=o("p"),tt=s("A "),E=o("a"),P=s("Dataset"),W=s(" object is a wrapper of an Arrow table, which allows fast reads from arrays in the dataset to TensorFlow tensors."),this.h()},l(q){_=l(q,"P",{});var S=r(_);tt=n(S,"A "),E=l(S,"A",{href:!0});var at=r(E);P=n(at,"Dataset"),at.forEach(e),W=n(S," object is a wrapper of an Arrow table, which allows fast reads from arrays in the dataset to TensorFlow tensors."),S.forEach(e),this.h()},h(){h(E,"href","/docs/datasets/pr_4457/en/package_reference/main_classes#datasets.Dataset")},m(q,S){p(q,_,S),a(_,tt),a(_,E),a(E,P),a(_,W)},d(q){q&&e(_)}}}function Or(pe){let _,tt,E,P,W,q,S,at,is,he,M,et,ea,yt,ps,sa,hs,de,z,st,na,vt,ds,oa,cs,ce,Wt,fs,fe,Mt,us,ue,wt,me,nt,ge,I,ms,la,gs,ys,ra,vs,ws,ye,_t,ve,B,ot,ia,bt,_s,pa,bs,we,lt,js,ha,$s,Es,_e,jt,be,zt,Ds,je,$t,$e,K,rt,da,Et,ks,ca,Ts,Ee,Dt,Bt,xs,As,De,kt,ke,Kt,qs,Te,Tt,xe,Gt,Cs,Ae,xt,qe,H,Ps,Jt,Os,Fs,Qt,Ns,Ss,Ce,G,it,fa,At,Is,ua,Hs,Pe,J,pt,ma,qt,Ls,ga,Us,Oe,D,Rs,ya,Ys,Ws,va,Ms,zs,wa,Bs,Ks,_a,Gs,Js,ba,Qs,Vs,Fe,b,Xs,ja,Zs,tn,$a,an,en,Ea,sn,nn,Da,on,ln,ka,rn,pn,Ta,hn,dn,Ne,j,cn,xa,fn,un,Aa,mn,gn,qa,yn,vn,Ct,wn,_n,Pt,bn,jn,Ca,$n,En,Se,Q,ht,Pa,Ot,Dn,Vt,kn,Oa,Tn,Ie,dt,xn,Fa,An,qn,He,Ft,Le,O,Cn,Na,Pn,On,Sa,Fn,Nn,Ia,Sn,In,Ue,Nt,Re,$,Hn,Ha,Ln,Un,St,Rn,Yn,La,Wn,Mn,Ua,zn,Bn,It,Kn,Gn,Ht,Jn,Qn,Ye,V,ct,Ra,Lt,Vn,Ya,Xn,We,f,Zn,Wa,to,ao,Ma,eo,so,za,no,oo,Ba,lo,ro,Ka,io,po,Ga,ho,co,Ja,fo,uo,Me,L,mo,Qa,go,yo,Va,vo,wo,ze,U,Ut,_o,Xa,bo,jo,$o,N,Eo,Za,Do,ko,te,To,xo,ae,Ao,qo,Co,C,Po,ee,Oo,Fo,se,No,So,ne,Io,Ho,oe,Lo,Uo,Be,X,ft,le,Rt,Ro,re,Yo,Ke,ut,Wo,ie,Mo,zo,Ge;return q=new Y({}),yt=new Y({}),vt=new Y({}),wt=new Z({props:{code:`from datasets import Dataset
data = [[1, 2],[3, 4]]
ds = Dataset.from_dict({"data": [[1, 2],[3, 4]]})
ds = ds.with_format("tf")
ds[0]
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=int64, numpy=array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])&gt;}
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
       [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])&gt;}`}}),nt=new Cr({props:{$$slots:{default:[Pr]},$$scope:{ctx:pe}}}),_t=new Z({props:{code:"ds[:]",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
       [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])&gt;}`}}),bt=new Y({}),jt=new Z({props:{code:`from datasets import Dataset
data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]
ds = Dataset.from_dict({"data": data})
ds = ds.with_format("tf")
ds[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]],[[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>],[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]]
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: data})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.RaggedTensor [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]&gt;}`}}),$t=new Z({props:{code:`from datasets import Dataset, Features, Array2D
data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]
features = Features({"data": Array2D(shape=(2, 2), dtype='int32')})
ds = Dataset.from_dict({"data": data}, features=features)
ds = ds.with_format("tf")
ds[0]
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, Features, Array2D
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]],[[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>],[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]]
<span class="hljs-meta">&gt;&gt;&gt; </span>features = Features({<span class="hljs-string">&quot;data&quot;</span>: Array2D(shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=<span class="hljs-string">&#x27;int32&#x27;</span>)})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: data}, features=features)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
 array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
        [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])&gt;}
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
 array([[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
         [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]],
 
        [[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
         [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]])&gt;}`}}),Et=new Y({}),kt=new Z({props:{code:`from datasets import Dataset, Features, ClassLabel
data = [0, 0, 1]
features = Features({"data": ClassLabel(names=["negative", "positive"])})
ds = Dataset.from_dict({"data": data}, features=features) 
ds = ds.with_format("tf")  
ds[:3]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, Features, ClassLabel
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>features = Features({<span class="hljs-string">&quot;data&quot;</span>: ClassLabel(names=[<span class="hljs-string">&quot;negative&quot;</span>, <span class="hljs-string">&quot;positive&quot;</span>])})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: data}, features=features) 
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)  
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">3</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">3</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])&gt;`}}),Tt=new Z({props:{code:`from datasets import Dataset, Features 
text = ["foo", "bar"]
data = [0, 1] 
ds = Dataset.from_dict({"text": text, "data": data})  
ds = ds.with_format("tf") 
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, Features 
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [<span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;bar&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>] 
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;text&quot;</span>: text, <span class="hljs-string">&quot;data&quot;</span>: data})  
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>) 
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;text&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=string, numpy=array([<span class="hljs-string">b&#x27;foo&#x27;</span>, <span class="hljs-string">b&#x27;bar&#x27;</span>], dtype=<span class="hljs-built_in">object</span>)&gt;,
 <span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])&gt;}`}}),xt=new Z({props:{code:`ds = ds.with_format("tf", columns=["data"], output_all_columns=True)
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>, columns=[<span class="hljs-string">&quot;data&quot;</span>], output_all_columns=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])&gt;,
 <span class="hljs-string">&#x27;text&#x27;</span>: [<span class="hljs-string">&#x27;foo&#x27;</span>, <span class="hljs-string">&#x27;bar&#x27;</span>]}`}}),At=new Y({}),qt=new Y({}),Ot=new Y({}),Ft=new Z({props:{code:`from datasets import Dataset
data = {"inputs": [[1, 2],[3, 4]], "labels": [0, 1]}
ds = Dataset.from_dict(data)
tf_ds = ds.to_tf_dataset(`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>data = {<span class="hljs-string">&quot;inputs&quot;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]], <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict(data)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_ds = ds.to_tf_dataset(
            columns=[<span class="hljs-string">&quot;inputs&quot;</span>],
            label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
            batch_size=<span class="hljs-number">2</span>,
            shuffle=<span class="hljs-literal">True</span>
            )`}}),Nt=new Z({props:{code:"model.fit(tf_ds, epochs=2)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(tf_ds, epochs=<span class="hljs-number">2</span>)'}}),Lt=new Y({}),Rt=new Y({}),{c(){_=o("meta"),tt=d(),E=o("h1"),P=o("a"),W=o("span"),u(q.$$.fragment),S=d(),at=o("span"),is=s("Using Datasets with TensorFlow"),he=d(),M=o("h2"),et=o("a"),ea=o("span"),u(yt.$$.fragment),ps=d(),sa=o("span"),hs=s("Tensors"),de=d(),z=o("h3"),st=o("a"),na=o("span"),u(vt.$$.fragment),ds=d(),oa=o("span"),cs=s("Dataset format"),ce=d(),Wt=o("p"),fs=s("By default, datasets return regular Python objects: integers, floats, strings, lists, etc."),fe=d(),Mt=o("p"),us=s("To get TensorFlow tensors instead, you can set the format of the dataset to \u201Ctf\u201D:"),ue=d(),u(wt.$$.fragment),me=d(),u(nt.$$.fragment),ge=d(),I=o("p"),ms=s("This can be useful for converting your dataset to a dict of "),la=o("code"),gs=s("Tensor"),ys=s(` objects, or for writing a generator to load TF
samples from it. If you wish to convert the entire dataset to `),ra=o("code"),vs=s("Tensor"),ws=s(", simply query the full dataset:"),ye=d(),u(_t.$$.fragment),ve=d(),B=o("h3"),ot=o("a"),ia=o("span"),u(bt.$$.fragment),_s=d(),pa=o("span"),bs=s("N-dimensional arrays"),we=d(),lt=o("p"),js=s(`If your dataset consists of N-dimensional arrays, you will see that by default they are considered as nested lists.
In particular, a TensorFlow formatted dataset outputs a `),ha=o("code"),$s=s("RaggedTensor"),Es=s(" instead of one single tensor:"),_e=d(),u(jt.$$.fragment),be=d(),zt=o("p"),Ds=s("To get one single tensor, you must explicitly use the Array feature type and specify the shape of your tensors:"),je=d(),u($t.$$.fragment),$e=d(),K=o("h3"),rt=o("a"),da=o("span"),u(Et.$$.fragment),ks=d(),ca=o("span"),Ts=s("Other feature types"),Ee=d(),Dt=o("p"),Bt=o("a"),xs=s("ClassLabel"),As=s(" data are properly converted to tensors:"),De=d(),u(kt.$$.fragment),ke=d(),Kt=o("p"),qs=s("Strings are also supported:"),Te=d(),u(Tt.$$.fragment),xe=d(),Gt=o("p"),Cs=s("But if you want, you can explicitly format certain columns and leave the other columns unformatted:"),Ae=d(),u(xt.$$.fragment),qe=d(),H=o("p"),Ps=s("The "),Jt=o("a"),Os=s("Image"),Fs=s(" and "),Qt=o("a"),Ns=s("Audio"),Ss=s(" feature types are not supported yet."),Ce=d(),G=o("h2"),it=o("a"),fa=o("span"),u(At.$$.fragment),Is=d(),ua=o("span"),Hs=s("Data loading"),Pe=d(),J=o("h3"),pt=o("a"),ma=o("span"),u(qt.$$.fragment),Ls=d(),ga=o("span"),Us=s("Interoperability with tf.data"),Oe=d(),D=o("p"),Rs=s(`Although you can load individual samples and batches just by indexing into your dataset, this won\u2019t work if you want
to use Keras methods like `),ya=o("code"),Ys=s("fit()"),Ws=s(" and "),va=o("code"),Ms=s("predict()"),zs=s(`. You could write a generator function that shuffles and loads batches
from your dataset and `),wa=o("code"),Bs=s("fit()"),Ks=s(` on that, but that sounds like a lot of unnecessary work. Instead, if you want to stream
data from your dataset on-the-fly, we recommend converting your dataset to a `),_a=o("code"),Gs=s("tf.data.Dataset"),Js=s(` using the
`),ba=o("code"),Qs=s("to_tf_dataset()"),Vs=s(" method."),Fe=d(),b=o("p"),Xs=s("The "),ja=o("code"),Zs=s("tf.data.Dataset"),tn=s(` class covers a wide range of use-cases - it is often created from Tensors in memory, or using a load function to read files on disc
or external storage. The dataset can be transformed arbitrarily with the `),$a=o("code"),an=s("map()"),en=s(" method, or methods like "),Ea=o("code"),sn=s("batch()"),nn=s(`
and `),Da=o("code"),on=s("shuffle()"),ln=s(` can be used to create a dataset that\u2019s ready for training. These methods do not modify the stored data
in any way - instead, the methods build a data pipeline graph that will be executed when the dataset is iterated over,
usually during model training or inference. This is different from the `),ka=o("code"),rn=s("map()"),pn=s(" method of Hugging Face "),Ta=o("code"),hn=s("Dataset"),dn=s(` objects,
which runs the map function immediately and saves the new or changed columns.`),Ne=d(),j=o("p"),cn=s("Since the entire data preprocessing pipeline can be compiled in a "),xa=o("code"),fn=s("tf.data.Dataset"),un=s(`, this approach allows for massively
parallel, asynchronous data loading and training. However, the requirement for graph compilation can be a limitation,
particularly for HuggingFace tokenizers, which are usually not (yet!) compilable as part of a TF graph. As a result,
we usually advise pre-processing the dataset as a Hugging Face dataset, where arbitrary Python functions can be
used, and then converting to `),Aa=o("code"),mn=s("tf.data.Dataset"),gn=s(" afterwards using "),qa=o("code"),yn=s("to_tf_dataset()"),vn=s(` to get a batched dataset ready for
training. To see examples of this approach, please see the `),Ct=o("a"),wn=s("examples"),_n=s(" or "),Pt=o("a"),bn=s("notebooks"),jn=s(" for "),Ca=o("code"),$n=s("transformers"),En=s("."),Se=d(),Q=o("h3"),ht=o("a"),Pa=o("span"),u(Ot.$$.fragment),Dn=d(),Vt=o("span"),kn=s("Using "),Oa=o("code"),Tn=s("to_tf_dataset()"),Ie=d(),dt=o("p"),xn=s("Using "),Fa=o("code"),An=s("to_tf_dataset()"),qn=s(" is straightforward. Once your dataset is preprocessed and ready, simply call it like so:"),He=d(),u(Ft.$$.fragment),Le=d(),O=o("p"),Cn=s("The returned "),Na=o("code"),Pn=s("tf_ds"),On=s(" object here is now fully ready to train on, and can be passed directly to "),Sa=o("code"),Fn=s("model.fit()"),Nn=s(`! Note
that you set the batch size when creating the dataset, and so you don\u2019t need to specify it when calling `),Ia=o("code"),Sn=s("fit()"),In=s(":"),Ue=d(),u(Nt.$$.fragment),Re=d(),$=o("p"),Hn=s("For a full description of the arguments, please see the "),Ha=o("code"),Ln=s("to_tf_dataset()"),Un=d(),St=o("a"),Rn=s("documentation"),Yn=s(`. In many cases,
you will also need to add a `),La=o("code"),Wn=s("collate_fn"),Mn=s(` to your call. This is a function that takes multiple elements of the dataset
and combines them into a single batch. When all elements have the same length, the built-in default collator will
suffice, but for more complex tasks a custom collator may be necessary. In particular, many tasks have samples
with varying sequence lengths which will require a data collator that can pad batches correctly. You can see examples
of this in the `),Ua=o("code"),zn=s("transformers"),Bn=s(" NLP "),It=o("a"),Kn=s("examples"),Gn=s(` and
`),Ht=o("a"),Jn=s("notebooks"),Qn=s(", where variable sequence lengths are very common."),Ye=d(),V=o("h3"),ct=o("a"),Ra=o("span"),u(Lt.$$.fragment),Vn=d(),Ya=o("span"),Xn=s("When to use to_tf_dataset"),We=d(),f=o("p"),Zn=s(`The astute reader may have noticed at this point that we have offered two approaches to achieve the same goal - if you
want to pass your dataset to a TensorFlow model, you can either convert the dataset to a `),Wa=o("code"),to=s("Tensor"),ao=s(" or "),Ma=o("code"),eo=s("dict"),so=s(" of "),za=o("code"),no=s("Tensors"),oo=s(`
using `),Ba=o("code"),lo=s(".with_format('tf')"),ro=s(", or you can convert the dataset to a "),Ka=o("code"),io=s("tf.data.Dataset"),po=s(" with "),Ga=o("code"),ho=s("to_tf_dataset()"),co=s(`. Either of these
can be passed to `),Ja=o("code"),fo=s("model.fit()"),uo=s(", so which should you choose?"),Me=d(),L=o("p"),mo=s("The key thing to recognize is that when you convert the whole dataset to "),Qa=o("code"),go=s("Tensor"),yo=s(`s, it is static and fully loaded into
RAM. This is simple and convenient, but if any of the following apply, you should probably use `),Va=o("code"),vo=s("to_tf_dataset()"),wo=s(`
instead:`),ze=d(),U=o("ul"),Ut=o("li"),_o=s("Your dataset is too large to fit in RAM. "),Xa=o("code"),bo=s("to_tf_dataset()"),jo=s(` streams only one batch at a time, so even very large
datasets can be handled with this method.`),$o=d(),N=o("li"),Eo=s("You want to apply random transformations using "),Za=o("code"),Do=s("dataset.with_transform()"),ko=s(" or the "),te=o("code"),To=s("collate_fn"),xo=s(`. This is
common in several modalities, such as image augmentations when training vision models, or random masking when training
masked language models. Using `),ae=o("code"),Ao=s("to_tf_dataset()"),qo=s(` will apply those transformations
at the moment when a batch is loaded, which means the same samples will get different augmentations each time
they are loaded. This is usually what you want.`),Co=d(),C=o("li"),Po=s(`Your data has a variable dimension, such as input texts in NLP that consist of varying
numbers of tokens. When you create a batch with samples with a variable dimension, the standard solution is to
pad the shorter samples to the length of the longest one. When you stream samples from a dataset with `),ee=o("code"),Oo=s("to_tf_dataset"),Fo=s(`,
you can apply this padding to each batch via your `),se=o("code"),No=s("collate_fn"),So=s(`. However, if you want to convert
such a dataset to dense `),ne=o("code"),Io=s("Tensor"),Ho=s("s, then you will have to pad samples to the length of the longest sample in "),oe=o("em"),Lo=s(`the
entire dataset!`),Uo=s(" This can result in huge amounts of padding, which wastes memory and reduces your model\u2019s speed."),Be=d(),X=o("h3"),ft=o("a"),le=o("span"),u(Rt.$$.fragment),Ro=d(),re=o("span"),Yo=s("Caveats and limitations"),Ke=d(),ut=o("p"),Wo=s("Right now, "),ie=o("code"),Mo=s("to_tf_dataset()"),zo=s(" always return a batched dataset - we will add support for unbatched datasets soon!"),this.h()},l(t){const i=Ar('[data-svelte="svelte-1phssyn"]',document.head);_=l(i,"META",{name:!0,content:!0}),i.forEach(e),tt=c(t),E=l(t,"H1",{class:!0});var Yt=r(E);P=l(Yt,"A",{id:!0,class:!0,href:!0});var Go=r(P);W=l(Go,"SPAN",{});var Jo=r(W);m(q.$$.fragment,Jo),Jo.forEach(e),Go.forEach(e),S=c(Yt),at=l(Yt,"SPAN",{});var Qo=r(at);is=n(Qo,"Using Datasets with TensorFlow"),Qo.forEach(e),Yt.forEach(e),he=c(t),M=l(t,"H2",{class:!0});var Je=r(M);et=l(Je,"A",{id:!0,class:!0,href:!0});var Vo=r(et);ea=l(Vo,"SPAN",{});var Xo=r(ea);m(yt.$$.fragment,Xo),Xo.forEach(e),Vo.forEach(e),ps=c(Je),sa=l(Je,"SPAN",{});var Zo=r(sa);hs=n(Zo,"Tensors"),Zo.forEach(e),Je.forEach(e),de=c(t),z=l(t,"H3",{class:!0});var Qe=r(z);st=l(Qe,"A",{id:!0,class:!0,href:!0});var tl=r(st);na=l(tl,"SPAN",{});var al=r(na);m(vt.$$.fragment,al),al.forEach(e),tl.forEach(e),ds=c(Qe),oa=l(Qe,"SPAN",{});var el=r(oa);cs=n(el,"Dataset format"),el.forEach(e),Qe.forEach(e),ce=c(t),Wt=l(t,"P",{});var sl=r(Wt);fs=n(sl,"By default, datasets return regular Python objects: integers, floats, strings, lists, etc."),sl.forEach(e),fe=c(t),Mt=l(t,"P",{});var nl=r(Mt);us=n(nl,"To get TensorFlow tensors instead, you can set the format of the dataset to \u201Ctf\u201D:"),nl.forEach(e),ue=c(t),m(wt.$$.fragment,t),me=c(t),m(nt.$$.fragment,t),ge=c(t),I=l(t,"P",{});var Xt=r(I);ms=n(Xt,"This can be useful for converting your dataset to a dict of "),la=l(Xt,"CODE",{});var ol=r(la);gs=n(ol,"Tensor"),ol.forEach(e),ys=n(Xt,` objects, or for writing a generator to load TF
samples from it. If you wish to convert the entire dataset to `),ra=l(Xt,"CODE",{});var ll=r(ra);vs=n(ll,"Tensor"),ll.forEach(e),ws=n(Xt,", simply query the full dataset:"),Xt.forEach(e),ye=c(t),m(_t.$$.fragment,t),ve=c(t),B=l(t,"H3",{class:!0});var Ve=r(B);ot=l(Ve,"A",{id:!0,class:!0,href:!0});var rl=r(ot);ia=l(rl,"SPAN",{});var il=r(ia);m(bt.$$.fragment,il),il.forEach(e),rl.forEach(e),_s=c(Ve),pa=l(Ve,"SPAN",{});var pl=r(pa);bs=n(pl,"N-dimensional arrays"),pl.forEach(e),Ve.forEach(e),we=c(t),lt=l(t,"P",{});var Xe=r(lt);js=n(Xe,`If your dataset consists of N-dimensional arrays, you will see that by default they are considered as nested lists.
In particular, a TensorFlow formatted dataset outputs a `),ha=l(Xe,"CODE",{});var hl=r(ha);$s=n(hl,"RaggedTensor"),hl.forEach(e),Es=n(Xe," instead of one single tensor:"),Xe.forEach(e),_e=c(t),m(jt.$$.fragment,t),be=c(t),zt=l(t,"P",{});var dl=r(zt);Ds=n(dl,"To get one single tensor, you must explicitly use the Array feature type and specify the shape of your tensors:"),dl.forEach(e),je=c(t),m($t.$$.fragment,t),$e=c(t),K=l(t,"H3",{class:!0});var Ze=r(K);rt=l(Ze,"A",{id:!0,class:!0,href:!0});var cl=r(rt);da=l(cl,"SPAN",{});var fl=r(da);m(Et.$$.fragment,fl),fl.forEach(e),cl.forEach(e),ks=c(Ze),ca=l(Ze,"SPAN",{});var ul=r(ca);Ts=n(ul,"Other feature types"),ul.forEach(e),Ze.forEach(e),Ee=c(t),Dt=l(t,"P",{});var Bo=r(Dt);Bt=l(Bo,"A",{href:!0});var ml=r(Bt);xs=n(ml,"ClassLabel"),ml.forEach(e),As=n(Bo," data are properly converted to tensors:"),Bo.forEach(e),De=c(t),m(kt.$$.fragment,t),ke=c(t),Kt=l(t,"P",{});var gl=r(Kt);qs=n(gl,"Strings are also supported:"),gl.forEach(e),Te=c(t),m(Tt.$$.fragment,t),xe=c(t),Gt=l(t,"P",{});var yl=r(Gt);Cs=n(yl,"But if you want, you can explicitly format certain columns and leave the other columns unformatted:"),yl.forEach(e),Ae=c(t),m(xt.$$.fragment,t),qe=c(t),H=l(t,"P",{});var Zt=r(H);Ps=n(Zt,"The "),Jt=l(Zt,"A",{href:!0});var vl=r(Jt);Os=n(vl,"Image"),vl.forEach(e),Fs=n(Zt," and "),Qt=l(Zt,"A",{href:!0});var wl=r(Qt);Ns=n(wl,"Audio"),wl.forEach(e),Ss=n(Zt," feature types are not supported yet."),Zt.forEach(e),Ce=c(t),G=l(t,"H2",{class:!0});var ts=r(G);it=l(ts,"A",{id:!0,class:!0,href:!0});var _l=r(it);fa=l(_l,"SPAN",{});var bl=r(fa);m(At.$$.fragment,bl),bl.forEach(e),_l.forEach(e),Is=c(ts),ua=l(ts,"SPAN",{});var jl=r(ua);Hs=n(jl,"Data loading"),jl.forEach(e),ts.forEach(e),Pe=c(t),J=l(t,"H3",{class:!0});var as=r(J);pt=l(as,"A",{id:!0,class:!0,href:!0});var $l=r(pt);ma=l($l,"SPAN",{});var El=r(ma);m(qt.$$.fragment,El),El.forEach(e),$l.forEach(e),Ls=c(as),ga=l(as,"SPAN",{});var Dl=r(ga);Us=n(Dl,"Interoperability with tf.data"),Dl.forEach(e),as.forEach(e),Oe=c(t),D=l(t,"P",{});var F=r(D);Rs=n(F,`Although you can load individual samples and batches just by indexing into your dataset, this won\u2019t work if you want
to use Keras methods like `),ya=l(F,"CODE",{});var kl=r(ya);Ys=n(kl,"fit()"),kl.forEach(e),Ws=n(F," and "),va=l(F,"CODE",{});var Tl=r(va);Ms=n(Tl,"predict()"),Tl.forEach(e),zs=n(F,`. You could write a generator function that shuffles and loads batches
from your dataset and `),wa=l(F,"CODE",{});var xl=r(wa);Bs=n(xl,"fit()"),xl.forEach(e),Ks=n(F,` on that, but that sounds like a lot of unnecessary work. Instead, if you want to stream
data from your dataset on-the-fly, we recommend converting your dataset to a `),_a=l(F,"CODE",{});var Al=r(_a);Gs=n(Al,"tf.data.Dataset"),Al.forEach(e),Js=n(F,` using the
`),ba=l(F,"CODE",{});var ql=r(ba);Qs=n(ql,"to_tf_dataset()"),ql.forEach(e),Vs=n(F," method."),F.forEach(e),Fe=c(t),b=l(t,"P",{});var T=r(b);Xs=n(T,"The "),ja=l(T,"CODE",{});var Cl=r(ja);Zs=n(Cl,"tf.data.Dataset"),Cl.forEach(e),tn=n(T,` class covers a wide range of use-cases - it is often created from Tensors in memory, or using a load function to read files on disc
or external storage. The dataset can be transformed arbitrarily with the `),$a=l(T,"CODE",{});var Pl=r($a);an=n(Pl,"map()"),Pl.forEach(e),en=n(T," method, or methods like "),Ea=l(T,"CODE",{});var Ol=r(Ea);sn=n(Ol,"batch()"),Ol.forEach(e),nn=n(T,`
and `),Da=l(T,"CODE",{});var Fl=r(Da);on=n(Fl,"shuffle()"),Fl.forEach(e),ln=n(T,` can be used to create a dataset that\u2019s ready for training. These methods do not modify the stored data
in any way - instead, the methods build a data pipeline graph that will be executed when the dataset is iterated over,
usually during model training or inference. This is different from the `),ka=l(T,"CODE",{});var Nl=r(ka);rn=n(Nl,"map()"),Nl.forEach(e),pn=n(T," method of Hugging Face "),Ta=l(T,"CODE",{});var Sl=r(Ta);hn=n(Sl,"Dataset"),Sl.forEach(e),dn=n(T,` objects,
which runs the map function immediately and saves the new or changed columns.`),T.forEach(e),Ne=c(t),j=l(t,"P",{});var x=r(j);cn=n(x,"Since the entire data preprocessing pipeline can be compiled in a "),xa=l(x,"CODE",{});var Il=r(xa);fn=n(Il,"tf.data.Dataset"),Il.forEach(e),un=n(x,`, this approach allows for massively
parallel, asynchronous data loading and training. However, the requirement for graph compilation can be a limitation,
particularly for HuggingFace tokenizers, which are usually not (yet!) compilable as part of a TF graph. As a result,
we usually advise pre-processing the dataset as a Hugging Face dataset, where arbitrary Python functions can be
used, and then converting to `),Aa=l(x,"CODE",{});var Hl=r(Aa);mn=n(Hl,"tf.data.Dataset"),Hl.forEach(e),gn=n(x," afterwards using "),qa=l(x,"CODE",{});var Ll=r(qa);yn=n(Ll,"to_tf_dataset()"),Ll.forEach(e),vn=n(x,` to get a batched dataset ready for
training. To see examples of this approach, please see the `),Ct=l(x,"A",{href:!0,rel:!0});var Ul=r(Ct);wn=n(Ul,"examples"),Ul.forEach(e),_n=n(x," or "),Pt=l(x,"A",{href:!0,rel:!0});var Rl=r(Pt);bn=n(Rl,"notebooks"),Rl.forEach(e),jn=n(x," for "),Ca=l(x,"CODE",{});var Yl=r(Ca);$n=n(Yl,"transformers"),Yl.forEach(e),En=n(x,"."),x.forEach(e),Se=c(t),Q=l(t,"H3",{class:!0});var es=r(Q);ht=l(es,"A",{id:!0,class:!0,href:!0});var Wl=r(ht);Pa=l(Wl,"SPAN",{});var Ml=r(Pa);m(Ot.$$.fragment,Ml),Ml.forEach(e),Wl.forEach(e),Dn=c(es),Vt=l(es,"SPAN",{});var Ko=r(Vt);kn=n(Ko,"Using "),Oa=l(Ko,"CODE",{});var zl=r(Oa);Tn=n(zl,"to_tf_dataset()"),zl.forEach(e),Ko.forEach(e),es.forEach(e),Ie=c(t),dt=l(t,"P",{});var ss=r(dt);xn=n(ss,"Using "),Fa=l(ss,"CODE",{});var Bl=r(Fa);An=n(Bl,"to_tf_dataset()"),Bl.forEach(e),qn=n(ss," is straightforward. Once your dataset is preprocessed and ready, simply call it like so:"),ss.forEach(e),He=c(t),m(Ft.$$.fragment,t),Le=c(t),O=l(t,"P",{});var mt=r(O);Cn=n(mt,"The returned "),Na=l(mt,"CODE",{});var Kl=r(Na);Pn=n(Kl,"tf_ds"),Kl.forEach(e),On=n(mt," object here is now fully ready to train on, and can be passed directly to "),Sa=l(mt,"CODE",{});var Gl=r(Sa);Fn=n(Gl,"model.fit()"),Gl.forEach(e),Nn=n(mt,`! Note
that you set the batch size when creating the dataset, and so you don\u2019t need to specify it when calling `),Ia=l(mt,"CODE",{});var Jl=r(Ia);Sn=n(Jl,"fit()"),Jl.forEach(e),In=n(mt,":"),mt.forEach(e),Ue=c(t),m(Nt.$$.fragment,t),Re=c(t),$=l(t,"P",{});var A=r($);Hn=n(A,"For a full description of the arguments, please see the "),Ha=l(A,"CODE",{});var Ql=r(Ha);Ln=n(Ql,"to_tf_dataset()"),Ql.forEach(e),Un=c(A),St=l(A,"A",{href:!0,rel:!0});var Vl=r(St);Rn=n(Vl,"documentation"),Vl.forEach(e),Yn=n(A,`. In many cases,
you will also need to add a `),La=l(A,"CODE",{});var Xl=r(La);Wn=n(Xl,"collate_fn"),Xl.forEach(e),Mn=n(A,` to your call. This is a function that takes multiple elements of the dataset
and combines them into a single batch. When all elements have the same length, the built-in default collator will
suffice, but for more complex tasks a custom collator may be necessary. In particular, many tasks have samples
with varying sequence lengths which will require a data collator that can pad batches correctly. You can see examples
of this in the `),Ua=l(A,"CODE",{});var Zl=r(Ua);zn=n(Zl,"transformers"),Zl.forEach(e),Bn=n(A," NLP "),It=l(A,"A",{href:!0,rel:!0});var tr=r(It);Kn=n(tr,"examples"),tr.forEach(e),Gn=n(A,` and
`),Ht=l(A,"A",{href:!0,rel:!0});var ar=r(Ht);Jn=n(ar,"notebooks"),ar.forEach(e),Qn=n(A,", where variable sequence lengths are very common."),A.forEach(e),Ye=c(t),V=l(t,"H3",{class:!0});var ns=r(V);ct=l(ns,"A",{id:!0,class:!0,href:!0});var er=r(ct);Ra=l(er,"SPAN",{});var sr=r(Ra);m(Lt.$$.fragment,sr),sr.forEach(e),er.forEach(e),Vn=c(ns),Ya=l(ns,"SPAN",{});var nr=r(Ya);Xn=n(nr,"When to use to_tf_dataset"),nr.forEach(e),ns.forEach(e),We=c(t),f=l(t,"P",{});var k=r(f);Zn=n(k,`The astute reader may have noticed at this point that we have offered two approaches to achieve the same goal - if you
want to pass your dataset to a TensorFlow model, you can either convert the dataset to a `),Wa=l(k,"CODE",{});var or=r(Wa);to=n(or,"Tensor"),or.forEach(e),ao=n(k," or "),Ma=l(k,"CODE",{});var lr=r(Ma);eo=n(lr,"dict"),lr.forEach(e),so=n(k," of "),za=l(k,"CODE",{});var rr=r(za);no=n(rr,"Tensors"),rr.forEach(e),oo=n(k,`
using `),Ba=l(k,"CODE",{});var ir=r(Ba);lo=n(ir,".with_format('tf')"),ir.forEach(e),ro=n(k,", or you can convert the dataset to a "),Ka=l(k,"CODE",{});var pr=r(Ka);io=n(pr,"tf.data.Dataset"),pr.forEach(e),po=n(k," with "),Ga=l(k,"CODE",{});var hr=r(Ga);ho=n(hr,"to_tf_dataset()"),hr.forEach(e),co=n(k,`. Either of these
can be passed to `),Ja=l(k,"CODE",{});var dr=r(Ja);fo=n(dr,"model.fit()"),dr.forEach(e),uo=n(k,", so which should you choose?"),k.forEach(e),Me=c(t),L=l(t,"P",{});var ta=r(L);mo=n(ta,"The key thing to recognize is that when you convert the whole dataset to "),Qa=l(ta,"CODE",{});var cr=r(Qa);go=n(cr,"Tensor"),cr.forEach(e),yo=n(ta,`s, it is static and fully loaded into
RAM. This is simple and convenient, but if any of the following apply, you should probably use `),Va=l(ta,"CODE",{});var fr=r(Va);vo=n(fr,"to_tf_dataset()"),fr.forEach(e),wo=n(ta,`
instead:`),ta.forEach(e),ze=c(t),U=l(t,"UL",{});var aa=r(U);Ut=l(aa,"LI",{});var os=r(Ut);_o=n(os,"Your dataset is too large to fit in RAM. "),Xa=l(os,"CODE",{});var ur=r(Xa);bo=n(ur,"to_tf_dataset()"),ur.forEach(e),jo=n(os,` streams only one batch at a time, so even very large
datasets can be handled with this method.`),os.forEach(e),$o=c(aa),N=l(aa,"LI",{});var gt=r(N);Eo=n(gt,"You want to apply random transformations using "),Za=l(gt,"CODE",{});var mr=r(Za);Do=n(mr,"dataset.with_transform()"),mr.forEach(e),ko=n(gt," or the "),te=l(gt,"CODE",{});var gr=r(te);To=n(gr,"collate_fn"),gr.forEach(e),xo=n(gt,`. This is
common in several modalities, such as image augmentations when training vision models, or random masking when training
masked language models. Using `),ae=l(gt,"CODE",{});var yr=r(ae);Ao=n(yr,"to_tf_dataset()"),yr.forEach(e),qo=n(gt,` will apply those transformations
at the moment when a batch is loaded, which means the same samples will get different augmentations each time
they are loaded. This is usually what you want.`),gt.forEach(e),Co=c(aa),C=l(aa,"LI",{});var R=r(C);Po=n(R,`Your data has a variable dimension, such as input texts in NLP that consist of varying
numbers of tokens. When you create a batch with samples with a variable dimension, the standard solution is to
pad the shorter samples to the length of the longest one. When you stream samples from a dataset with `),ee=l(R,"CODE",{});var vr=r(ee);Oo=n(vr,"to_tf_dataset"),vr.forEach(e),Fo=n(R,`,
you can apply this padding to each batch via your `),se=l(R,"CODE",{});var wr=r(se);No=n(wr,"collate_fn"),wr.forEach(e),So=n(R,`. However, if you want to convert
such a dataset to dense `),ne=l(R,"CODE",{});var _r=r(ne);Io=n(_r,"Tensor"),_r.forEach(e),Ho=n(R,"s, then you will have to pad samples to the length of the longest sample in "),oe=l(R,"EM",{});var br=r(oe);Lo=n(br,`the
entire dataset!`),br.forEach(e),Uo=n(R," This can result in huge amounts of padding, which wastes memory and reduces your model\u2019s speed."),R.forEach(e),aa.forEach(e),Be=c(t),X=l(t,"H3",{class:!0});var ls=r(X);ft=l(ls,"A",{id:!0,class:!0,href:!0});var jr=r(ft);le=l(jr,"SPAN",{});var $r=r(le);m(Rt.$$.fragment,$r),$r.forEach(e),jr.forEach(e),Ro=c(ls),re=l(ls,"SPAN",{});var Er=r(re);Yo=n(Er,"Caveats and limitations"),Er.forEach(e),ls.forEach(e),Ke=c(t),ut=l(t,"P",{});var rs=r(ut);Wo=n(rs,"Right now, "),ie=l(rs,"CODE",{});var Dr=r(ie);Mo=n(Dr,"to_tf_dataset()"),Dr.forEach(e),zo=n(rs," always return a batched dataset - we will add support for unbatched datasets soon!"),rs.forEach(e),this.h()},h(){h(_,"name","hf:doc:metadata"),h(_,"content",JSON.stringify(Fr)),h(P,"id","using-datasets-with-tensorflow"),h(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(P,"href","#using-datasets-with-tensorflow"),h(E,"class","relative group"),h(et,"id","tensors"),h(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(et,"href","#tensors"),h(M,"class","relative group"),h(st,"id","dataset-format"),h(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(st,"href","#dataset-format"),h(z,"class","relative group"),h(ot,"id","ndimensional-arrays"),h(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ot,"href","#ndimensional-arrays"),h(B,"class","relative group"),h(rt,"id","other-feature-types"),h(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(rt,"href","#other-feature-types"),h(K,"class","relative group"),h(Bt,"href","/docs/datasets/pr_4457/en/package_reference/main_classes#datasets.ClassLabel"),h(Jt,"href","/docs/datasets/pr_4457/en/package_reference/main_classes#datasets.Image"),h(Qt,"href","/docs/datasets/pr_4457/en/package_reference/main_classes#datasets.Audio"),h(it,"id","data-loading"),h(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(it,"href","#data-loading"),h(G,"class","relative group"),h(pt,"id","interoperability-with-tfdata"),h(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(pt,"href","#interoperability-with-tfdata"),h(J,"class","relative group"),h(Ct,"href","https://github.com/huggingface/transformers/tree/main/examples"),h(Ct,"rel","nofollow"),h(Pt,"href","https://huggingface.co/docs/transformers/notebooks"),h(Pt,"rel","nofollow"),h(ht,"id","using-totfdataset"),h(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ht,"href","#using-totfdataset"),h(Q,"class","relative group"),h(St,"href","https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset"),h(St,"rel","nofollow"),h(It,"href","https://github.com/huggingface/transformers/tree/main/examples"),h(It,"rel","nofollow"),h(Ht,"href","https://huggingface.co/docs/transformers/notebooks"),h(Ht,"rel","nofollow"),h(ct,"id","when-to-use-totfdataset"),h(ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ct,"href","#when-to-use-totfdataset"),h(V,"class","relative group"),h(ft,"id","caveats-and-limitations"),h(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ft,"href","#caveats-and-limitations"),h(X,"class","relative group")},m(t,i){a(document.head,_),p(t,tt,i),p(t,E,i),a(E,P),a(P,W),g(q,W,null),a(E,S),a(E,at),a(at,is),p(t,he,i),p(t,M,i),a(M,et),a(et,ea),g(yt,ea,null),a(M,ps),a(M,sa),a(sa,hs),p(t,de,i),p(t,z,i),a(z,st),a(st,na),g(vt,na,null),a(z,ds),a(z,oa),a(oa,cs),p(t,ce,i),p(t,Wt,i),a(Wt,fs),p(t,fe,i),p(t,Mt,i),a(Mt,us),p(t,ue,i),g(wt,t,i),p(t,me,i),g(nt,t,i),p(t,ge,i),p(t,I,i),a(I,ms),a(I,la),a(la,gs),a(I,ys),a(I,ra),a(ra,vs),a(I,ws),p(t,ye,i),g(_t,t,i),p(t,ve,i),p(t,B,i),a(B,ot),a(ot,ia),g(bt,ia,null),a(B,_s),a(B,pa),a(pa,bs),p(t,we,i),p(t,lt,i),a(lt,js),a(lt,ha),a(ha,$s),a(lt,Es),p(t,_e,i),g(jt,t,i),p(t,be,i),p(t,zt,i),a(zt,Ds),p(t,je,i),g($t,t,i),p(t,$e,i),p(t,K,i),a(K,rt),a(rt,da),g(Et,da,null),a(K,ks),a(K,ca),a(ca,Ts),p(t,Ee,i),p(t,Dt,i),a(Dt,Bt),a(Bt,xs),a(Dt,As),p(t,De,i),g(kt,t,i),p(t,ke,i),p(t,Kt,i),a(Kt,qs),p(t,Te,i),g(Tt,t,i),p(t,xe,i),p(t,Gt,i),a(Gt,Cs),p(t,Ae,i),g(xt,t,i),p(t,qe,i),p(t,H,i),a(H,Ps),a(H,Jt),a(Jt,Os),a(H,Fs),a(H,Qt),a(Qt,Ns),a(H,Ss),p(t,Ce,i),p(t,G,i),a(G,it),a(it,fa),g(At,fa,null),a(G,Is),a(G,ua),a(ua,Hs),p(t,Pe,i),p(t,J,i),a(J,pt),a(pt,ma),g(qt,ma,null),a(J,Ls),a(J,ga),a(ga,Us),p(t,Oe,i),p(t,D,i),a(D,Rs),a(D,ya),a(ya,Ys),a(D,Ws),a(D,va),a(va,Ms),a(D,zs),a(D,wa),a(wa,Bs),a(D,Ks),a(D,_a),a(_a,Gs),a(D,Js),a(D,ba),a(ba,Qs),a(D,Vs),p(t,Fe,i),p(t,b,i),a(b,Xs),a(b,ja),a(ja,Zs),a(b,tn),a(b,$a),a($a,an),a(b,en),a(b,Ea),a(Ea,sn),a(b,nn),a(b,Da),a(Da,on),a(b,ln),a(b,ka),a(ka,rn),a(b,pn),a(b,Ta),a(Ta,hn),a(b,dn),p(t,Ne,i),p(t,j,i),a(j,cn),a(j,xa),a(xa,fn),a(j,un),a(j,Aa),a(Aa,mn),a(j,gn),a(j,qa),a(qa,yn),a(j,vn),a(j,Ct),a(Ct,wn),a(j,_n),a(j,Pt),a(Pt,bn),a(j,jn),a(j,Ca),a(Ca,$n),a(j,En),p(t,Se,i),p(t,Q,i),a(Q,ht),a(ht,Pa),g(Ot,Pa,null),a(Q,Dn),a(Q,Vt),a(Vt,kn),a(Vt,Oa),a(Oa,Tn),p(t,Ie,i),p(t,dt,i),a(dt,xn),a(dt,Fa),a(Fa,An),a(dt,qn),p(t,He,i),g(Ft,t,i),p(t,Le,i),p(t,O,i),a(O,Cn),a(O,Na),a(Na,Pn),a(O,On),a(O,Sa),a(Sa,Fn),a(O,Nn),a(O,Ia),a(Ia,Sn),a(O,In),p(t,Ue,i),g(Nt,t,i),p(t,Re,i),p(t,$,i),a($,Hn),a($,Ha),a(Ha,Ln),a($,Un),a($,St),a(St,Rn),a($,Yn),a($,La),a(La,Wn),a($,Mn),a($,Ua),a(Ua,zn),a($,Bn),a($,It),a(It,Kn),a($,Gn),a($,Ht),a(Ht,Jn),a($,Qn),p(t,Ye,i),p(t,V,i),a(V,ct),a(ct,Ra),g(Lt,Ra,null),a(V,Vn),a(V,Ya),a(Ya,Xn),p(t,We,i),p(t,f,i),a(f,Zn),a(f,Wa),a(Wa,to),a(f,ao),a(f,Ma),a(Ma,eo),a(f,so),a(f,za),a(za,no),a(f,oo),a(f,Ba),a(Ba,lo),a(f,ro),a(f,Ka),a(Ka,io),a(f,po),a(f,Ga),a(Ga,ho),a(f,co),a(f,Ja),a(Ja,fo),a(f,uo),p(t,Me,i),p(t,L,i),a(L,mo),a(L,Qa),a(Qa,go),a(L,yo),a(L,Va),a(Va,vo),a(L,wo),p(t,ze,i),p(t,U,i),a(U,Ut),a(Ut,_o),a(Ut,Xa),a(Xa,bo),a(Ut,jo),a(U,$o),a(U,N),a(N,Eo),a(N,Za),a(Za,Do),a(N,ko),a(N,te),a(te,To),a(N,xo),a(N,ae),a(ae,Ao),a(N,qo),a(U,Co),a(U,C),a(C,Po),a(C,ee),a(ee,Oo),a(C,Fo),a(C,se),a(se,No),a(C,So),a(C,ne),a(ne,Io),a(C,Ho),a(C,oe),a(oe,Lo),a(C,Uo),p(t,Be,i),p(t,X,i),a(X,ft),a(ft,le),g(Rt,le,null),a(X,Ro),a(X,re),a(re,Yo),p(t,Ke,i),p(t,ut,i),a(ut,Wo),a(ut,ie),a(ie,Mo),a(ut,zo),Ge=!0},p(t,[i]){const Yt={};i&2&&(Yt.$$scope={dirty:i,ctx:t}),nt.$set(Yt)},i(t){Ge||(y(q.$$.fragment,t),y(yt.$$.fragment,t),y(vt.$$.fragment,t),y(wt.$$.fragment,t),y(nt.$$.fragment,t),y(_t.$$.fragment,t),y(bt.$$.fragment,t),y(jt.$$.fragment,t),y($t.$$.fragment,t),y(Et.$$.fragment,t),y(kt.$$.fragment,t),y(Tt.$$.fragment,t),y(xt.$$.fragment,t),y(At.$$.fragment,t),y(qt.$$.fragment,t),y(Ot.$$.fragment,t),y(Ft.$$.fragment,t),y(Nt.$$.fragment,t),y(Lt.$$.fragment,t),y(Rt.$$.fragment,t),Ge=!0)},o(t){v(q.$$.fragment,t),v(yt.$$.fragment,t),v(vt.$$.fragment,t),v(wt.$$.fragment,t),v(nt.$$.fragment,t),v(_t.$$.fragment,t),v(bt.$$.fragment,t),v(jt.$$.fragment,t),v($t.$$.fragment,t),v(Et.$$.fragment,t),v(kt.$$.fragment,t),v(Tt.$$.fragment,t),v(xt.$$.fragment,t),v(At.$$.fragment,t),v(qt.$$.fragment,t),v(Ot.$$.fragment,t),v(Ft.$$.fragment,t),v(Nt.$$.fragment,t),v(Lt.$$.fragment,t),v(Rt.$$.fragment,t),Ge=!1},d(t){e(_),t&&e(tt),t&&e(E),w(q),t&&e(he),t&&e(M),w(yt),t&&e(de),t&&e(z),w(vt),t&&e(ce),t&&e(Wt),t&&e(fe),t&&e(Mt),t&&e(ue),w(wt,t),t&&e(me),w(nt,t),t&&e(ge),t&&e(I),t&&e(ye),w(_t,t),t&&e(ve),t&&e(B),w(bt),t&&e(we),t&&e(lt),t&&e(_e),w(jt,t),t&&e(be),t&&e(zt),t&&e(je),w($t,t),t&&e($e),t&&e(K),w(Et),t&&e(Ee),t&&e(Dt),t&&e(De),w(kt,t),t&&e(ke),t&&e(Kt),t&&e(Te),w(Tt,t),t&&e(xe),t&&e(Gt),t&&e(Ae),w(xt,t),t&&e(qe),t&&e(H),t&&e(Ce),t&&e(G),w(At),t&&e(Pe),t&&e(J),w(qt),t&&e(Oe),t&&e(D),t&&e(Fe),t&&e(b),t&&e(Ne),t&&e(j),t&&e(Se),t&&e(Q),w(Ot),t&&e(Ie),t&&e(dt),t&&e(He),w(Ft,t),t&&e(Le),t&&e(O),t&&e(Ue),w(Nt,t),t&&e(Re),t&&e($),t&&e(Ye),t&&e(V),w(Lt),t&&e(We),t&&e(f),t&&e(Me),t&&e(L),t&&e(ze),t&&e(U),t&&e(Be),t&&e(X),w(Rt),t&&e(Ke),t&&e(ut)}}}const Fr={local:"using-datasets-with-tensorflow",sections:[{local:"tensors",sections:[{local:"dataset-format",title:"Dataset format"},{local:"ndimensional-arrays",title:"N-dimensional arrays"},{local:"other-feature-types",title:"Other feature types"}],title:"Tensors"},{local:"data-loading",sections:[{local:"interoperability-with-tfdata",title:"Interoperability with tf.data"},{local:"using-totfdataset",title:"Using `to_tf_dataset()`"},{local:"when-to-use-totfdataset",title:"When to use to_tf_dataset"},{local:"caveats-and-limitations",title:"Caveats and limitations"}],title:"Data loading"}],title:"Using Datasets with TensorFlow"};function Nr(pe){return qr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ur extends kr{constructor(_){super();Tr(this,_,Nr,Or,xr,{})}}export{Ur as default,Fr as metadata};
