import{S as lr,i as rr,s as ir,e as o,k as d,w as v,t as e,M as pr,c as l,d as s,m as c,a as r,x as w,h as n,b as h,G as a,g as p,y as _,q as b,o as j,B as E,v as hr}from"../chunks/vendor-hf-doc-builder.js";import{T as dr}from"../chunks/Tip-hf-doc-builder.js";import{I as ft}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as Q}from"../chunks/CodeBlock-hf-doc-builder.js";function cr(ts){let u,V,$,O,R;return{c(){u=o("p"),V=e("A "),$=o("a"),O=e("Dataset"),R=e(" object is a wrapper of an Arrow table, which allows fast reads from arrays in the dataset to TensorFlow tensors."),this.h()},l(C){u=l(C,"P",{});var S=r(u);V=n(S,"A "),$=l(S,"A",{href:!0});var X=r($);O=n(X,"Dataset"),X.forEach(s),R=n(S," object is a wrapper of an Arrow table, which allows fast reads from arrays in the dataset to TensorFlow tensors."),S.forEach(s),this.h()},h(){h($,"href","/docs/datasets/pr_4457/en/package_reference/main_classes#datasets.Dataset")},m(C,S){p(C,u,S),a(u,V),a(u,$),a($,O),a(u,R)},d(C){C&&s(u)}}}function fr(ts){let u,V,$,O,R,C,S,X,Qs,as,W,Z,Vt,ut,Vs,Xt,Xs,ss,Ht,Zs,es,tt,te,Zt,ae,se,ns,mt,os,at,ls,I,ee,ta,ne,oe,aa,le,re,rs,gt,is,M,st,sa,yt,ie,ea,pe,ps,et,he,na,de,ce,hs,vt,ds,Lt,fe,cs,wt,fs,z,nt,oa,_t,ue,la,me,us,bt,Ut,ge,ye,ms,jt,gs,Yt,ve,ys,Et,vs,Rt,we,ws,$t,_s,H,_e,Wt,be,je,Mt,Ee,$e,bs,B,ot,ra,Dt,De,ia,ke,js,D,Te,pa,xe,qe,ha,Ce,Ae,da,Oe,Pe,ca,Fe,Ne,fa,Se,Ie,Es,m,He,ua,Le,Ue,ma,Ye,Re,ga,We,Me,ya,ze,Be,va,Ke,Ge,wa,Je,Qe,$s,g,Ve,_a,Xe,Ze,ba,tn,an,ja,sn,en,kt,nn,on,Tt,ln,rn,Ea,pn,hn,Ds,K,lt,$a,xt,dn,zt,cn,Da,fn,ks,rt,un,ka,mn,gn,Ts,qt,xs,P,yn,Ta,vn,wn,xa,_n,bn,qa,jn,En,qs,Ct,Cs,y,$n,Bt,Dn,kn,Ca,Tn,xn,At,qn,Cn,Aa,An,On,Ot,Pn,Fn,Pt,Nn,Sn,As,G,it,Oa,Ft,In,Pa,Hn,Os,f,Ln,Fa,Un,Yn,Na,Rn,Wn,Sa,Mn,zn,Ia,Bn,Kn,Ha,Gn,Jn,La,Qn,Vn,Ua,Xn,Zn,Ps,L,to,Ya,ao,so,Ra,eo,no,Fs,U,Nt,oo,Wa,lo,ro,io,N,po,Ma,ho,co,za,fo,uo,Ba,mo,go,yo,A,vo,Ka,wo,_o,Ga,bo,jo,Ja,Eo,$o,Qa,Do,ko,Ns,J,pt,Va,St,To,Xa,xo,Ss,ht,qo,Za,Co,Ao,Is;return C=new ft({}),ut=new ft({}),mt=new Q({props:{code:`from datasets import Dataset
data = [[1, 2],[3, 4]]
ds = Dataset.from_dict({"data": [[1, 2],[3, 4]]})
ds = ds.with_format("tf")
ds[0]
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=int64, numpy=array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])&gt;}
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
       [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])&gt;}`}}),at=new dr({props:{$$slots:{default:[cr]},$$scope:{ctx:ts}}}),gt=new Q({props:{code:"ds[:]",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
       [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])&gt;}`}}),yt=new ft({}),vt=new Q({props:{code:`from datasets import Dataset
data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]
ds = Dataset.from_dict({"data": data})
ds = ds.with_format("tf")
ds[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]],[[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>],[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]]
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: data})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.RaggedTensor [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]&gt;}`}}),wt=new Q({props:{code:`from datasets import Dataset, Features, Array2D
data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]
features = Features({"data": Array2D(shape=(2, 2), dtype='int32')})
ds = Dataset.from_dict({"data": data}, features=features)
ds = ds.with_format("tf")
ds[0]
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, Features, Array2D
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]],[[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>],[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]]
<span class="hljs-meta">&gt;&gt;&gt; </span>features = Features({<span class="hljs-string">&quot;data&quot;</span>: Array2D(shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=<span class="hljs-string">&#x27;int32&#x27;</span>)})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: data}, features=features)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
 array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
        [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])&gt;}
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
 array([[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
         [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]],
 
        [[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
         [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]])&gt;}`}}),_t=new ft({}),jt=new Q({props:{code:`from datasets import Dataset, Features, ClassLabel
data = [0, 0, 1]
features = Features({"data": ClassLabel(names=["negative", "positive"])})
ds = Dataset.from_dict({"data": data}, features=features) 
ds = ds.with_format("tf")  
ds[:3]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, Features, ClassLabel
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>features = Features({<span class="hljs-string">&quot;data&quot;</span>: ClassLabel(names=[<span class="hljs-string">&quot;negative&quot;</span>, <span class="hljs-string">&quot;positive&quot;</span>])})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: data}, features=features) 
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)  
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">3</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">3</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])&gt;`}}),Et=new Q({props:{code:`from datasets import Dataset, Features 
text = ["foo", "bar"]
data = [0, 1] 
ds = Dataset.from_dict({"text": text, "data": data})  
ds = ds.with_format("tf") 
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, Features 
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [<span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;bar&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>] 
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;text&quot;</span>: text, <span class="hljs-string">&quot;data&quot;</span>: data})  
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>) 
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;text&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=string, numpy=array([<span class="hljs-string">b&#x27;foo&#x27;</span>, <span class="hljs-string">b&#x27;bar&#x27;</span>], dtype=<span class="hljs-built_in">object</span>)&gt;,
 <span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])&gt;}`}}),$t=new Q({props:{code:`ds = ds.with_format("tf", columns=["data"], output_all_columns=True)
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>, columns=[<span class="hljs-string">&quot;data&quot;</span>], output_all_columns=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])&gt;,
 <span class="hljs-string">&#x27;text&#x27;</span>: [<span class="hljs-string">&#x27;foo&#x27;</span>, <span class="hljs-string">&#x27;bar&#x27;</span>]}`}}),Dt=new ft({}),xt=new ft({}),qt=new Q({props:{code:`from datasets import Dataset
data = {"inputs": [[1, 2],[3, 4]], "labels": [0, 1]}
ds = Dataset.from_dict(data)
tf_ds = ds.to_tf_dataset(`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>data = {<span class="hljs-string">&quot;inputs&quot;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]], <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict(data)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_ds = ds.to_tf_dataset(
            columns=[<span class="hljs-string">&quot;inputs&quot;</span>],
            label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
            batch_size=<span class="hljs-number">2</span>,
            shuffle=<span class="hljs-literal">True</span>
            )`}}),Ct=new Q({props:{code:"model.fit(tf_ds, epochs=2)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(tf_ds, epochs=<span class="hljs-number">2</span>)'}}),Ft=new ft({}),St=new ft({}),{c(){u=o("meta"),V=d(),$=o("h1"),O=o("a"),R=o("span"),v(C.$$.fragment),S=d(),X=o("span"),Qs=e("Using Datasets with TensorFlow"),as=d(),W=o("h3"),Z=o("a"),Vt=o("span"),v(ut.$$.fragment),Vs=d(),Xt=o("span"),Xs=e("Dataset format"),ss=d(),Ht=o("p"),Zs=e("By default, datasets return regular Python objects: integers, floats, strings, lists, etc."),es=d(),tt=o("p"),te=e("To get TensorFlow tensors instead, you can set the format of the dataset to "),Zt=o("code"),ae=e("tf"),se=e(":"),ns=d(),v(mt.$$.fragment),os=d(),v(at.$$.fragment),ls=d(),I=o("p"),ee=e("This can be useful for converting your dataset to a dict of "),ta=o("code"),ne=e("Tensor"),oe=e(` objects, or for writing a generator to load TF
samples from it. If you wish to convert the entire dataset to `),aa=o("code"),le=e("Tensor"),re=e(", simply query the full dataset:"),rs=d(),v(gt.$$.fragment),is=d(),M=o("h3"),st=o("a"),sa=o("span"),v(yt.$$.fragment),ie=d(),ea=o("span"),pe=e("N-dimensional arrays"),ps=d(),et=o("p"),he=e(`If your dataset consists of N-dimensional arrays, you will see that by default they are considered as nested lists.
In particular, a TensorFlow formatted dataset outputs a `),na=o("code"),de=e("RaggedTensor"),ce=e(" instead of a single tensor:"),hs=d(),v(vt.$$.fragment),ds=d(),Lt=o("p"),fe=e("To get a single tensor, you must explicitly use the Array feature type and specify the shape of your tensors:"),cs=d(),v(wt.$$.fragment),fs=d(),z=o("h3"),nt=o("a"),oa=o("span"),v(_t.$$.fragment),ue=d(),la=o("span"),me=e("Other feature types"),us=d(),bt=o("p"),Ut=o("a"),ge=e("ClassLabel"),ye=e(" data are properly converted to tensors:"),ms=d(),v(jt.$$.fragment),gs=d(),Yt=o("p"),ve=e("Strings are also supported:"),ys=d(),v(Et.$$.fragment),vs=d(),Rt=o("p"),we=e("You can also explicitly format certain columns and leave the other columns unformatted:"),ws=d(),v($t.$$.fragment),_s=d(),H=o("p"),_e=e("The "),Wt=o("a"),be=e("Image"),je=e(" and "),Mt=o("a"),Ee=e("Audio"),$e=e(" feature types are not supported yet."),bs=d(),B=o("h2"),ot=o("a"),ra=o("span"),v(Dt.$$.fragment),De=d(),ia=o("span"),ke=e("Data loading"),js=d(),D=o("p"),Te=e(`Although you can load individual samples and batches just by indexing into your dataset, this won\u2019t work if you want
to use Keras methods like `),pa=o("code"),xe=e("fit()"),qe=e(" and "),ha=o("code"),Ce=e("predict()"),Ae=e(`. You could write a generator function that shuffles and loads batches
from your dataset and `),da=o("code"),Oe=e("fit()"),Pe=e(` on that, but that sounds like a lot of unnecessary work. Instead, if you want to stream
data from your dataset on-the-fly, we recommend converting your dataset to a `),ca=o("code"),Fe=e("tf.data.Dataset"),Ne=e(` using the
`),fa=o("code"),Se=e("to_tf_dataset()"),Ie=e(" method."),Es=d(),m=o("p"),He=e("The "),ua=o("code"),Le=e("tf.data.Dataset"),Ue=e(` class covers a wide range of use-cases - it is often created from Tensors in memory, or using a load function to read files on disc
or external storage. The dataset can be transformed arbitrarily with the `),ma=o("code"),Ye=e("map()"),Re=e(" method, or methods like "),ga=o("code"),We=e("batch()"),Me=e(`
and `),ya=o("code"),ze=e("shuffle()"),Be=e(` can be used to create a dataset that\u2019s ready for training. These methods do not modify the stored data
in any way - instead, the methods build a data pipeline graph that will be executed when the dataset is iterated over,
usually during model training or inference. This is different from the `),va=o("code"),Ke=e("map()"),Ge=e(" method of Hugging Face "),wa=o("code"),Je=e("Dataset"),Qe=e(` objects,
which runs the map function immediately and saves the new or changed columns.`),$s=d(),g=o("p"),Ve=e("Since the entire data preprocessing pipeline can be compiled in a "),_a=o("code"),Xe=e("tf.data.Dataset"),Ze=e(`, this approach allows for massively
parallel, asynchronous data loading and training. However, the requirement for graph compilation can be a limitation,
particularly for Hugging Face tokenizers, which are usually not (yet!) compilable as part of a TF graph. As a result,
we usually advise pre-processing the dataset as a Hugging Face dataset, where arbitrary Python functions can be
used, and then converting to `),ba=o("code"),tn=e("tf.data.Dataset"),an=e(" afterwards using "),ja=o("code"),sn=e("to_tf_dataset()"),en=e(` to get a batched dataset ready for
training. To see examples of this approach, please see the `),kt=o("a"),nn=e("examples"),on=e(" or "),Tt=o("a"),ln=e("notebooks"),rn=e(" for "),Ea=o("code"),pn=e("transformers"),hn=e("."),Ds=d(),K=o("h3"),lt=o("a"),$a=o("span"),v(xt.$$.fragment),dn=d(),zt=o("span"),cn=e("Using "),Da=o("code"),fn=e("to_tf_dataset()"),ks=d(),rt=o("p"),un=e("Using "),ka=o("code"),mn=e("to_tf_dataset()"),gn=e(" is straightforward. Once your dataset is preprocessed and ready, simply call it like so:"),Ts=d(),v(qt.$$.fragment),xs=d(),P=o("p"),yn=e("The returned "),Ta=o("code"),vn=e("tf_ds"),wn=e(" object here is now fully ready to train on, and can be passed directly to "),xa=o("code"),_n=e("model.fit()"),bn=e(`! Note
that you set the batch size when creating the dataset, and so you don\u2019t need to specify it when calling `),qa=o("code"),jn=e("fit()"),En=e(":"),qs=d(),v(Ct.$$.fragment),Cs=d(),y=o("p"),$n=e("For a full description of the arguments, please see the "),Bt=o("a"),Dn=e("to_tf_dataset()"),kn=e(`. In many cases,
you will also need to add a `),Ca=o("code"),Tn=e("collate_fn"),xn=e(` to your call. This is a function that takes multiple elements of the dataset
and combines them into a single batch. When all elements have the same length, the built-in default collator will
suffice, but for more complex tasks a custom collator may be necessary. In particular, many tasks have samples
with varying sequence lengths which will require a `),At=o("a"),qn=e("data collator"),Cn=e(` that can pad batches correctly. You can see examples
of this in the `),Aa=o("code"),An=e("transformers"),On=e(" NLP "),Ot=o("a"),Pn=e("examples"),Fn=e(` and
`),Pt=o("a"),Nn=e("notebooks"),Sn=e(", where variable sequence lengths are very common."),As=d(),G=o("h3"),it=o("a"),Oa=o("span"),v(Ft.$$.fragment),In=d(),Pa=o("span"),Hn=e("When to use to_tf_dataset"),Os=d(),f=o("p"),Ln=e(`The astute reader may have noticed at this point that we have offered two approaches to achieve the same goal - if you
want to pass your dataset to a TensorFlow model, you can either convert the dataset to a `),Fa=o("code"),Un=e("Tensor"),Yn=e(" or "),Na=o("code"),Rn=e("dict"),Wn=e(" of "),Sa=o("code"),Mn=e("Tensors"),zn=e(`
using `),Ia=o("code"),Bn=e(".with_format('tf')"),Kn=e(", or you can convert the dataset to a "),Ha=o("code"),Gn=e("tf.data.Dataset"),Jn=e(" with "),La=o("code"),Qn=e("to_tf_dataset()"),Vn=e(`. Either of these
can be passed to `),Ua=o("code"),Xn=e("model.fit()"),Zn=e(", so which should you choose?"),Ps=d(),L=o("p"),to=e("The key thing to recognize is that when you convert the whole dataset to "),Ya=o("code"),ao=e("Tensor"),so=e(`s, it is static and fully loaded into
RAM. This is simple and convenient, but if any of the following apply, you should probably use `),Ra=o("code"),eo=e("to_tf_dataset()"),no=e(`
instead:`),Fs=d(),U=o("ul"),Nt=o("li"),oo=e("Your dataset is too large to fit in RAM. "),Wa=o("code"),lo=e("to_tf_dataset()"),ro=e(` streams only one batch at a time, so even very large
datasets can be handled with this method.`),io=d(),N=o("li"),po=e("You want to apply random transformations using "),Ma=o("code"),ho=e("dataset.with_transform()"),co=e(" or the "),za=o("code"),fo=e("collate_fn"),uo=e(`. This is
common in several modalities, such as image augmentations when training vision models, or random masking when training
masked language models. Using `),Ba=o("code"),mo=e("to_tf_dataset()"),go=e(` will apply those transformations
at the moment when a batch is loaded, which means the same samples will get different augmentations each time
they are loaded. This is usually what you want.`),yo=d(),A=o("li"),vo=e(`Your data has a variable dimension, such as input texts in NLP that consist of varying
numbers of tokens. When you create a batch with samples with a variable dimension, the standard solution is to
pad the shorter samples to the length of the longest one. When you stream samples from a dataset with `),Ka=o("code"),wo=e("to_tf_dataset"),_o=e(`,
you can apply this padding to each batch via your `),Ga=o("code"),bo=e("collate_fn"),jo=e(`. However, if you want to convert
such a dataset to dense `),Ja=o("code"),Eo=e("Tensor"),$o=e("s, then you will have to pad samples to the length of the longest sample in "),Qa=o("em"),Do=e(`the
entire dataset!`),ko=e(" This can result in huge amounts of padding, which wastes memory and reduces your model\u2019s speed."),Ns=d(),J=o("h3"),pt=o("a"),Va=o("span"),v(St.$$.fragment),To=d(),Xa=o("span"),xo=e("Caveats and limitations"),Ss=d(),ht=o("p"),qo=e("Right now, "),Za=o("code"),Co=e("to_tf_dataset()"),Ao=e(" always return a batched dataset - we will add support for unbatched datasets soon!"),this.h()},l(t){const i=pr('[data-svelte="svelte-1phssyn"]',document.head);u=l(i,"META",{name:!0,content:!0}),i.forEach(s),V=c(t),$=l(t,"H1",{class:!0});var It=r($);O=l(It,"A",{id:!0,class:!0,href:!0});var Fo=r(O);R=l(Fo,"SPAN",{});var No=r(R);w(C.$$.fragment,No),No.forEach(s),Fo.forEach(s),S=c(It),X=l(It,"SPAN",{});var So=r(X);Qs=n(So,"Using Datasets with TensorFlow"),So.forEach(s),It.forEach(s),as=c(t),W=l(t,"H3",{class:!0});var Hs=r(W);Z=l(Hs,"A",{id:!0,class:!0,href:!0});var Io=r(Z);Vt=l(Io,"SPAN",{});var Ho=r(Vt);w(ut.$$.fragment,Ho),Ho.forEach(s),Io.forEach(s),Vs=c(Hs),Xt=l(Hs,"SPAN",{});var Lo=r(Xt);Xs=n(Lo,"Dataset format"),Lo.forEach(s),Hs.forEach(s),ss=c(t),Ht=l(t,"P",{});var Uo=r(Ht);Zs=n(Uo,"By default, datasets return regular Python objects: integers, floats, strings, lists, etc."),Uo.forEach(s),es=c(t),tt=l(t,"P",{});var Ls=r(tt);te=n(Ls,"To get TensorFlow tensors instead, you can set the format of the dataset to "),Zt=l(Ls,"CODE",{});var Yo=r(Zt);ae=n(Yo,"tf"),Yo.forEach(s),se=n(Ls,":"),Ls.forEach(s),ns=c(t),w(mt.$$.fragment,t),os=c(t),w(at.$$.fragment,t),ls=c(t),I=l(t,"P",{});var Kt=r(I);ee=n(Kt,"This can be useful for converting your dataset to a dict of "),ta=l(Kt,"CODE",{});var Ro=r(ta);ne=n(Ro,"Tensor"),Ro.forEach(s),oe=n(Kt,` objects, or for writing a generator to load TF
samples from it. If you wish to convert the entire dataset to `),aa=l(Kt,"CODE",{});var Wo=r(aa);le=n(Wo,"Tensor"),Wo.forEach(s),re=n(Kt,", simply query the full dataset:"),Kt.forEach(s),rs=c(t),w(gt.$$.fragment,t),is=c(t),M=l(t,"H3",{class:!0});var Us=r(M);st=l(Us,"A",{id:!0,class:!0,href:!0});var Mo=r(st);sa=l(Mo,"SPAN",{});var zo=r(sa);w(yt.$$.fragment,zo),zo.forEach(s),Mo.forEach(s),ie=c(Us),ea=l(Us,"SPAN",{});var Bo=r(ea);pe=n(Bo,"N-dimensional arrays"),Bo.forEach(s),Us.forEach(s),ps=c(t),et=l(t,"P",{});var Ys=r(et);he=n(Ys,`If your dataset consists of N-dimensional arrays, you will see that by default they are considered as nested lists.
In particular, a TensorFlow formatted dataset outputs a `),na=l(Ys,"CODE",{});var Ko=r(na);de=n(Ko,"RaggedTensor"),Ko.forEach(s),ce=n(Ys," instead of a single tensor:"),Ys.forEach(s),hs=c(t),w(vt.$$.fragment,t),ds=c(t),Lt=l(t,"P",{});var Go=r(Lt);fe=n(Go,"To get a single tensor, you must explicitly use the Array feature type and specify the shape of your tensors:"),Go.forEach(s),cs=c(t),w(wt.$$.fragment,t),fs=c(t),z=l(t,"H3",{class:!0});var Rs=r(z);nt=l(Rs,"A",{id:!0,class:!0,href:!0});var Jo=r(nt);oa=l(Jo,"SPAN",{});var Qo=r(oa);w(_t.$$.fragment,Qo),Qo.forEach(s),Jo.forEach(s),ue=c(Rs),la=l(Rs,"SPAN",{});var Vo=r(la);me=n(Vo,"Other feature types"),Vo.forEach(s),Rs.forEach(s),us=c(t),bt=l(t,"P",{});var Oo=r(bt);Ut=l(Oo,"A",{href:!0});var Xo=r(Ut);ge=n(Xo,"ClassLabel"),Xo.forEach(s),ye=n(Oo," data are properly converted to tensors:"),Oo.forEach(s),ms=c(t),w(jt.$$.fragment,t),gs=c(t),Yt=l(t,"P",{});var Zo=r(Yt);ve=n(Zo,"Strings are also supported:"),Zo.forEach(s),ys=c(t),w(Et.$$.fragment,t),vs=c(t),Rt=l(t,"P",{});var tl=r(Rt);we=n(tl,"You can also explicitly format certain columns and leave the other columns unformatted:"),tl.forEach(s),ws=c(t),w($t.$$.fragment,t),_s=c(t),H=l(t,"P",{});var Gt=r(H);_e=n(Gt,"The "),Wt=l(Gt,"A",{href:!0});var al=r(Wt);be=n(al,"Image"),al.forEach(s),je=n(Gt," and "),Mt=l(Gt,"A",{href:!0});var sl=r(Mt);Ee=n(sl,"Audio"),sl.forEach(s),$e=n(Gt," feature types are not supported yet."),Gt.forEach(s),bs=c(t),B=l(t,"H2",{class:!0});var Ws=r(B);ot=l(Ws,"A",{id:!0,class:!0,href:!0});var el=r(ot);ra=l(el,"SPAN",{});var nl=r(ra);w(Dt.$$.fragment,nl),nl.forEach(s),el.forEach(s),De=c(Ws),ia=l(Ws,"SPAN",{});var ol=r(ia);ke=n(ol,"Data loading"),ol.forEach(s),Ws.forEach(s),js=c(t),D=l(t,"P",{});var F=r(D);Te=n(F,`Although you can load individual samples and batches just by indexing into your dataset, this won\u2019t work if you want
to use Keras methods like `),pa=l(F,"CODE",{});var ll=r(pa);xe=n(ll,"fit()"),ll.forEach(s),qe=n(F," and "),ha=l(F,"CODE",{});var rl=r(ha);Ce=n(rl,"predict()"),rl.forEach(s),Ae=n(F,`. You could write a generator function that shuffles and loads batches
from your dataset and `),da=l(F,"CODE",{});var il=r(da);Oe=n(il,"fit()"),il.forEach(s),Pe=n(F,` on that, but that sounds like a lot of unnecessary work. Instead, if you want to stream
data from your dataset on-the-fly, we recommend converting your dataset to a `),ca=l(F,"CODE",{});var pl=r(ca);Fe=n(pl,"tf.data.Dataset"),pl.forEach(s),Ne=n(F,` using the
`),fa=l(F,"CODE",{});var hl=r(fa);Se=n(hl,"to_tf_dataset()"),hl.forEach(s),Ie=n(F," method."),F.forEach(s),Es=c(t),m=l(t,"P",{});var T=r(m);He=n(T,"The "),ua=l(T,"CODE",{});var dl=r(ua);Le=n(dl,"tf.data.Dataset"),dl.forEach(s),Ue=n(T,` class covers a wide range of use-cases - it is often created from Tensors in memory, or using a load function to read files on disc
or external storage. The dataset can be transformed arbitrarily with the `),ma=l(T,"CODE",{});var cl=r(ma);Ye=n(cl,"map()"),cl.forEach(s),Re=n(T," method, or methods like "),ga=l(T,"CODE",{});var fl=r(ga);We=n(fl,"batch()"),fl.forEach(s),Me=n(T,`
and `),ya=l(T,"CODE",{});var ul=r(ya);ze=n(ul,"shuffle()"),ul.forEach(s),Be=n(T,` can be used to create a dataset that\u2019s ready for training. These methods do not modify the stored data
in any way - instead, the methods build a data pipeline graph that will be executed when the dataset is iterated over,
usually during model training or inference. This is different from the `),va=l(T,"CODE",{});var ml=r(va);Ke=n(ml,"map()"),ml.forEach(s),Ge=n(T," method of Hugging Face "),wa=l(T,"CODE",{});var gl=r(wa);Je=n(gl,"Dataset"),gl.forEach(s),Qe=n(T,` objects,
which runs the map function immediately and saves the new or changed columns.`),T.forEach(s),$s=c(t),g=l(t,"P",{});var x=r(g);Ve=n(x,"Since the entire data preprocessing pipeline can be compiled in a "),_a=l(x,"CODE",{});var yl=r(_a);Xe=n(yl,"tf.data.Dataset"),yl.forEach(s),Ze=n(x,`, this approach allows for massively
parallel, asynchronous data loading and training. However, the requirement for graph compilation can be a limitation,
particularly for Hugging Face tokenizers, which are usually not (yet!) compilable as part of a TF graph. As a result,
we usually advise pre-processing the dataset as a Hugging Face dataset, where arbitrary Python functions can be
used, and then converting to `),ba=l(x,"CODE",{});var vl=r(ba);tn=n(vl,"tf.data.Dataset"),vl.forEach(s),an=n(x," afterwards using "),ja=l(x,"CODE",{});var wl=r(ja);sn=n(wl,"to_tf_dataset()"),wl.forEach(s),en=n(x,` to get a batched dataset ready for
training. To see examples of this approach, please see the `),kt=l(x,"A",{href:!0,rel:!0});var _l=r(kt);nn=n(_l,"examples"),_l.forEach(s),on=n(x," or "),Tt=l(x,"A",{href:!0,rel:!0});var bl=r(Tt);ln=n(bl,"notebooks"),bl.forEach(s),rn=n(x," for "),Ea=l(x,"CODE",{});var jl=r(Ea);pn=n(jl,"transformers"),jl.forEach(s),hn=n(x,"."),x.forEach(s),Ds=c(t),K=l(t,"H3",{class:!0});var Ms=r(K);lt=l(Ms,"A",{id:!0,class:!0,href:!0});var El=r(lt);$a=l(El,"SPAN",{});var $l=r($a);w(xt.$$.fragment,$l),$l.forEach(s),El.forEach(s),dn=c(Ms),zt=l(Ms,"SPAN",{});var Po=r(zt);cn=n(Po,"Using "),Da=l(Po,"CODE",{});var Dl=r(Da);fn=n(Dl,"to_tf_dataset()"),Dl.forEach(s),Po.forEach(s),Ms.forEach(s),ks=c(t),rt=l(t,"P",{});var zs=r(rt);un=n(zs,"Using "),ka=l(zs,"CODE",{});var kl=r(ka);mn=n(kl,"to_tf_dataset()"),kl.forEach(s),gn=n(zs," is straightforward. Once your dataset is preprocessed and ready, simply call it like so:"),zs.forEach(s),Ts=c(t),w(qt.$$.fragment,t),xs=c(t),P=l(t,"P",{});var dt=r(P);yn=n(dt,"The returned "),Ta=l(dt,"CODE",{});var Tl=r(Ta);vn=n(Tl,"tf_ds"),Tl.forEach(s),wn=n(dt," object here is now fully ready to train on, and can be passed directly to "),xa=l(dt,"CODE",{});var xl=r(xa);_n=n(xl,"model.fit()"),xl.forEach(s),bn=n(dt,`! Note
that you set the batch size when creating the dataset, and so you don\u2019t need to specify it when calling `),qa=l(dt,"CODE",{});var ql=r(qa);jn=n(ql,"fit()"),ql.forEach(s),En=n(dt,":"),dt.forEach(s),qs=c(t),w(Ct.$$.fragment,t),Cs=c(t),y=l(t,"P",{});var q=r(y);$n=n(q,"For a full description of the arguments, please see the "),Bt=l(q,"A",{href:!0});var Cl=r(Bt);Dn=n(Cl,"to_tf_dataset()"),Cl.forEach(s),kn=n(q,`. In many cases,
you will also need to add a `),Ca=l(q,"CODE",{});var Al=r(Ca);Tn=n(Al,"collate_fn"),Al.forEach(s),xn=n(q,` to your call. This is a function that takes multiple elements of the dataset
and combines them into a single batch. When all elements have the same length, the built-in default collator will
suffice, but for more complex tasks a custom collator may be necessary. In particular, many tasks have samples
with varying sequence lengths which will require a `),At=l(q,"A",{href:!0,rel:!0});var Ol=r(At);qn=n(Ol,"data collator"),Ol.forEach(s),Cn=n(q,` that can pad batches correctly. You can see examples
of this in the `),Aa=l(q,"CODE",{});var Pl=r(Aa);An=n(Pl,"transformers"),Pl.forEach(s),On=n(q," NLP "),Ot=l(q,"A",{href:!0,rel:!0});var Fl=r(Ot);Pn=n(Fl,"examples"),Fl.forEach(s),Fn=n(q,` and
`),Pt=l(q,"A",{href:!0,rel:!0});var Nl=r(Pt);Nn=n(Nl,"notebooks"),Nl.forEach(s),Sn=n(q,", where variable sequence lengths are very common."),q.forEach(s),As=c(t),G=l(t,"H3",{class:!0});var Bs=r(G);it=l(Bs,"A",{id:!0,class:!0,href:!0});var Sl=r(it);Oa=l(Sl,"SPAN",{});var Il=r(Oa);w(Ft.$$.fragment,Il),Il.forEach(s),Sl.forEach(s),In=c(Bs),Pa=l(Bs,"SPAN",{});var Hl=r(Pa);Hn=n(Hl,"When to use to_tf_dataset"),Hl.forEach(s),Bs.forEach(s),Os=c(t),f=l(t,"P",{});var k=r(f);Ln=n(k,`The astute reader may have noticed at this point that we have offered two approaches to achieve the same goal - if you
want to pass your dataset to a TensorFlow model, you can either convert the dataset to a `),Fa=l(k,"CODE",{});var Ll=r(Fa);Un=n(Ll,"Tensor"),Ll.forEach(s),Yn=n(k," or "),Na=l(k,"CODE",{});var Ul=r(Na);Rn=n(Ul,"dict"),Ul.forEach(s),Wn=n(k," of "),Sa=l(k,"CODE",{});var Yl=r(Sa);Mn=n(Yl,"Tensors"),Yl.forEach(s),zn=n(k,`
using `),Ia=l(k,"CODE",{});var Rl=r(Ia);Bn=n(Rl,".with_format('tf')"),Rl.forEach(s),Kn=n(k,", or you can convert the dataset to a "),Ha=l(k,"CODE",{});var Wl=r(Ha);Gn=n(Wl,"tf.data.Dataset"),Wl.forEach(s),Jn=n(k," with "),La=l(k,"CODE",{});var Ml=r(La);Qn=n(Ml,"to_tf_dataset()"),Ml.forEach(s),Vn=n(k,`. Either of these
can be passed to `),Ua=l(k,"CODE",{});var zl=r(Ua);Xn=n(zl,"model.fit()"),zl.forEach(s),Zn=n(k,", so which should you choose?"),k.forEach(s),Ps=c(t),L=l(t,"P",{});var Jt=r(L);to=n(Jt,"The key thing to recognize is that when you convert the whole dataset to "),Ya=l(Jt,"CODE",{});var Bl=r(Ya);ao=n(Bl,"Tensor"),Bl.forEach(s),so=n(Jt,`s, it is static and fully loaded into
RAM. This is simple and convenient, but if any of the following apply, you should probably use `),Ra=l(Jt,"CODE",{});var Kl=r(Ra);eo=n(Kl,"to_tf_dataset()"),Kl.forEach(s),no=n(Jt,`
instead:`),Jt.forEach(s),Fs=c(t),U=l(t,"UL",{});var Qt=r(U);Nt=l(Qt,"LI",{});var Ks=r(Nt);oo=n(Ks,"Your dataset is too large to fit in RAM. "),Wa=l(Ks,"CODE",{});var Gl=r(Wa);lo=n(Gl,"to_tf_dataset()"),Gl.forEach(s),ro=n(Ks,` streams only one batch at a time, so even very large
datasets can be handled with this method.`),Ks.forEach(s),io=c(Qt),N=l(Qt,"LI",{});var ct=r(N);po=n(ct,"You want to apply random transformations using "),Ma=l(ct,"CODE",{});var Jl=r(Ma);ho=n(Jl,"dataset.with_transform()"),Jl.forEach(s),co=n(ct," or the "),za=l(ct,"CODE",{});var Ql=r(za);fo=n(Ql,"collate_fn"),Ql.forEach(s),uo=n(ct,`. This is
common in several modalities, such as image augmentations when training vision models, or random masking when training
masked language models. Using `),Ba=l(ct,"CODE",{});var Vl=r(Ba);mo=n(Vl,"to_tf_dataset()"),Vl.forEach(s),go=n(ct,` will apply those transformations
at the moment when a batch is loaded, which means the same samples will get different augmentations each time
they are loaded. This is usually what you want.`),ct.forEach(s),yo=c(Qt),A=l(Qt,"LI",{});var Y=r(A);vo=n(Y,`Your data has a variable dimension, such as input texts in NLP that consist of varying
numbers of tokens. When you create a batch with samples with a variable dimension, the standard solution is to
pad the shorter samples to the length of the longest one. When you stream samples from a dataset with `),Ka=l(Y,"CODE",{});var Xl=r(Ka);wo=n(Xl,"to_tf_dataset"),Xl.forEach(s),_o=n(Y,`,
you can apply this padding to each batch via your `),Ga=l(Y,"CODE",{});var Zl=r(Ga);bo=n(Zl,"collate_fn"),Zl.forEach(s),jo=n(Y,`. However, if you want to convert
such a dataset to dense `),Ja=l(Y,"CODE",{});var tr=r(Ja);Eo=n(tr,"Tensor"),tr.forEach(s),$o=n(Y,"s, then you will have to pad samples to the length of the longest sample in "),Qa=l(Y,"EM",{});var ar=r(Qa);Do=n(ar,`the
entire dataset!`),ar.forEach(s),ko=n(Y," This can result in huge amounts of padding, which wastes memory and reduces your model\u2019s speed."),Y.forEach(s),Qt.forEach(s),Ns=c(t),J=l(t,"H3",{class:!0});var Gs=r(J);pt=l(Gs,"A",{id:!0,class:!0,href:!0});var sr=r(pt);Va=l(sr,"SPAN",{});var er=r(Va);w(St.$$.fragment,er),er.forEach(s),sr.forEach(s),To=c(Gs),Xa=l(Gs,"SPAN",{});var nr=r(Xa);xo=n(nr,"Caveats and limitations"),nr.forEach(s),Gs.forEach(s),Ss=c(t),ht=l(t,"P",{});var Js=r(ht);qo=n(Js,"Right now, "),Za=l(Js,"CODE",{});var or=r(Za);Co=n(or,"to_tf_dataset()"),or.forEach(s),Ao=n(Js," always return a batched dataset - we will add support for unbatched datasets soon!"),Js.forEach(s),this.h()},h(){h(u,"name","hf:doc:metadata"),h(u,"content",JSON.stringify(ur)),h(O,"id","using-datasets-with-tensorflow"),h(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(O,"href","#using-datasets-with-tensorflow"),h($,"class","relative group"),h(Z,"id","dataset-format"),h(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Z,"href","#dataset-format"),h(W,"class","relative group"),h(st,"id","ndimensional-arrays"),h(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(st,"href","#ndimensional-arrays"),h(M,"class","relative group"),h(nt,"id","other-feature-types"),h(nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(nt,"href","#other-feature-types"),h(z,"class","relative group"),h(Ut,"href","/docs/datasets/pr_4457/en/package_reference/main_classes#datasets.ClassLabel"),h(Wt,"href","/docs/datasets/pr_4457/en/package_reference/main_classes#datasets.Image"),h(Mt,"href","/docs/datasets/pr_4457/en/package_reference/main_classes#datasets.Audio"),h(ot,"id","data-loading"),h(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ot,"href","#data-loading"),h(B,"class","relative group"),h(kt,"href","https://github.com/huggingface/transformers/tree/main/examples"),h(kt,"rel","nofollow"),h(Tt,"href","https://huggingface.co/docs/transformers/notebooks"),h(Tt,"rel","nofollow"),h(lt,"id","using-totfdataset"),h(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(lt,"href","#using-totfdataset"),h(K,"class","relative group"),h(Bt,"href","/docs/datasets/pr_4457/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset"),h(At,"href","https://huggingface.co/docs/transformers/main/en/main_classes/data_collator"),h(At,"rel","nofollow"),h(Ot,"href","https://github.com/huggingface/transformers/tree/main/examples"),h(Ot,"rel","nofollow"),h(Pt,"href","https://huggingface.co/docs/transformers/notebooks"),h(Pt,"rel","nofollow"),h(it,"id","when-to-use-totfdataset"),h(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(it,"href","#when-to-use-totfdataset"),h(G,"class","relative group"),h(pt,"id","caveats-and-limitations"),h(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(pt,"href","#caveats-and-limitations"),h(J,"class","relative group")},m(t,i){a(document.head,u),p(t,V,i),p(t,$,i),a($,O),a(O,R),_(C,R,null),a($,S),a($,X),a(X,Qs),p(t,as,i),p(t,W,i),a(W,Z),a(Z,Vt),_(ut,Vt,null),a(W,Vs),a(W,Xt),a(Xt,Xs),p(t,ss,i),p(t,Ht,i),a(Ht,Zs),p(t,es,i),p(t,tt,i),a(tt,te),a(tt,Zt),a(Zt,ae),a(tt,se),p(t,ns,i),_(mt,t,i),p(t,os,i),_(at,t,i),p(t,ls,i),p(t,I,i),a(I,ee),a(I,ta),a(ta,ne),a(I,oe),a(I,aa),a(aa,le),a(I,re),p(t,rs,i),_(gt,t,i),p(t,is,i),p(t,M,i),a(M,st),a(st,sa),_(yt,sa,null),a(M,ie),a(M,ea),a(ea,pe),p(t,ps,i),p(t,et,i),a(et,he),a(et,na),a(na,de),a(et,ce),p(t,hs,i),_(vt,t,i),p(t,ds,i),p(t,Lt,i),a(Lt,fe),p(t,cs,i),_(wt,t,i),p(t,fs,i),p(t,z,i),a(z,nt),a(nt,oa),_(_t,oa,null),a(z,ue),a(z,la),a(la,me),p(t,us,i),p(t,bt,i),a(bt,Ut),a(Ut,ge),a(bt,ye),p(t,ms,i),_(jt,t,i),p(t,gs,i),p(t,Yt,i),a(Yt,ve),p(t,ys,i),_(Et,t,i),p(t,vs,i),p(t,Rt,i),a(Rt,we),p(t,ws,i),_($t,t,i),p(t,_s,i),p(t,H,i),a(H,_e),a(H,Wt),a(Wt,be),a(H,je),a(H,Mt),a(Mt,Ee),a(H,$e),p(t,bs,i),p(t,B,i),a(B,ot),a(ot,ra),_(Dt,ra,null),a(B,De),a(B,ia),a(ia,ke),p(t,js,i),p(t,D,i),a(D,Te),a(D,pa),a(pa,xe),a(D,qe),a(D,ha),a(ha,Ce),a(D,Ae),a(D,da),a(da,Oe),a(D,Pe),a(D,ca),a(ca,Fe),a(D,Ne),a(D,fa),a(fa,Se),a(D,Ie),p(t,Es,i),p(t,m,i),a(m,He),a(m,ua),a(ua,Le),a(m,Ue),a(m,ma),a(ma,Ye),a(m,Re),a(m,ga),a(ga,We),a(m,Me),a(m,ya),a(ya,ze),a(m,Be),a(m,va),a(va,Ke),a(m,Ge),a(m,wa),a(wa,Je),a(m,Qe),p(t,$s,i),p(t,g,i),a(g,Ve),a(g,_a),a(_a,Xe),a(g,Ze),a(g,ba),a(ba,tn),a(g,an),a(g,ja),a(ja,sn),a(g,en),a(g,kt),a(kt,nn),a(g,on),a(g,Tt),a(Tt,ln),a(g,rn),a(g,Ea),a(Ea,pn),a(g,hn),p(t,Ds,i),p(t,K,i),a(K,lt),a(lt,$a),_(xt,$a,null),a(K,dn),a(K,zt),a(zt,cn),a(zt,Da),a(Da,fn),p(t,ks,i),p(t,rt,i),a(rt,un),a(rt,ka),a(ka,mn),a(rt,gn),p(t,Ts,i),_(qt,t,i),p(t,xs,i),p(t,P,i),a(P,yn),a(P,Ta),a(Ta,vn),a(P,wn),a(P,xa),a(xa,_n),a(P,bn),a(P,qa),a(qa,jn),a(P,En),p(t,qs,i),_(Ct,t,i),p(t,Cs,i),p(t,y,i),a(y,$n),a(y,Bt),a(Bt,Dn),a(y,kn),a(y,Ca),a(Ca,Tn),a(y,xn),a(y,At),a(At,qn),a(y,Cn),a(y,Aa),a(Aa,An),a(y,On),a(y,Ot),a(Ot,Pn),a(y,Fn),a(y,Pt),a(Pt,Nn),a(y,Sn),p(t,As,i),p(t,G,i),a(G,it),a(it,Oa),_(Ft,Oa,null),a(G,In),a(G,Pa),a(Pa,Hn),p(t,Os,i),p(t,f,i),a(f,Ln),a(f,Fa),a(Fa,Un),a(f,Yn),a(f,Na),a(Na,Rn),a(f,Wn),a(f,Sa),a(Sa,Mn),a(f,zn),a(f,Ia),a(Ia,Bn),a(f,Kn),a(f,Ha),a(Ha,Gn),a(f,Jn),a(f,La),a(La,Qn),a(f,Vn),a(f,Ua),a(Ua,Xn),a(f,Zn),p(t,Ps,i),p(t,L,i),a(L,to),a(L,Ya),a(Ya,ao),a(L,so),a(L,Ra),a(Ra,eo),a(L,no),p(t,Fs,i),p(t,U,i),a(U,Nt),a(Nt,oo),a(Nt,Wa),a(Wa,lo),a(Nt,ro),a(U,io),a(U,N),a(N,po),a(N,Ma),a(Ma,ho),a(N,co),a(N,za),a(za,fo),a(N,uo),a(N,Ba),a(Ba,mo),a(N,go),a(U,yo),a(U,A),a(A,vo),a(A,Ka),a(Ka,wo),a(A,_o),a(A,Ga),a(Ga,bo),a(A,jo),a(A,Ja),a(Ja,Eo),a(A,$o),a(A,Qa),a(Qa,Do),a(A,ko),p(t,Ns,i),p(t,J,i),a(J,pt),a(pt,Va),_(St,Va,null),a(J,To),a(J,Xa),a(Xa,xo),p(t,Ss,i),p(t,ht,i),a(ht,qo),a(ht,Za),a(Za,Co),a(ht,Ao),Is=!0},p(t,[i]){const It={};i&2&&(It.$$scope={dirty:i,ctx:t}),at.$set(It)},i(t){Is||(b(C.$$.fragment,t),b(ut.$$.fragment,t),b(mt.$$.fragment,t),b(at.$$.fragment,t),b(gt.$$.fragment,t),b(yt.$$.fragment,t),b(vt.$$.fragment,t),b(wt.$$.fragment,t),b(_t.$$.fragment,t),b(jt.$$.fragment,t),b(Et.$$.fragment,t),b($t.$$.fragment,t),b(Dt.$$.fragment,t),b(xt.$$.fragment,t),b(qt.$$.fragment,t),b(Ct.$$.fragment,t),b(Ft.$$.fragment,t),b(St.$$.fragment,t),Is=!0)},o(t){j(C.$$.fragment,t),j(ut.$$.fragment,t),j(mt.$$.fragment,t),j(at.$$.fragment,t),j(gt.$$.fragment,t),j(yt.$$.fragment,t),j(vt.$$.fragment,t),j(wt.$$.fragment,t),j(_t.$$.fragment,t),j(jt.$$.fragment,t),j(Et.$$.fragment,t),j($t.$$.fragment,t),j(Dt.$$.fragment,t),j(xt.$$.fragment,t),j(qt.$$.fragment,t),j(Ct.$$.fragment,t),j(Ft.$$.fragment,t),j(St.$$.fragment,t),Is=!1},d(t){s(u),t&&s(V),t&&s($),E(C),t&&s(as),t&&s(W),E(ut),t&&s(ss),t&&s(Ht),t&&s(es),t&&s(tt),t&&s(ns),E(mt,t),t&&s(os),E(at,t),t&&s(ls),t&&s(I),t&&s(rs),E(gt,t),t&&s(is),t&&s(M),E(yt),t&&s(ps),t&&s(et),t&&s(hs),E(vt,t),t&&s(ds),t&&s(Lt),t&&s(cs),E(wt,t),t&&s(fs),t&&s(z),E(_t),t&&s(us),t&&s(bt),t&&s(ms),E(jt,t),t&&s(gs),t&&s(Yt),t&&s(ys),E(Et,t),t&&s(vs),t&&s(Rt),t&&s(ws),E($t,t),t&&s(_s),t&&s(H),t&&s(bs),t&&s(B),E(Dt),t&&s(js),t&&s(D),t&&s(Es),t&&s(m),t&&s($s),t&&s(g),t&&s(Ds),t&&s(K),E(xt),t&&s(ks),t&&s(rt),t&&s(Ts),E(qt,t),t&&s(xs),t&&s(P),t&&s(qs),E(Ct,t),t&&s(Cs),t&&s(y),t&&s(As),t&&s(G),E(Ft),t&&s(Os),t&&s(f),t&&s(Ps),t&&s(L),t&&s(Fs),t&&s(U),t&&s(Ns),t&&s(J),E(St),t&&s(Ss),t&&s(ht)}}}const ur={local:"using-datasets-with-tensorflow",sections:[{local:null,sections:[{local:"dataset-format",title:"Dataset format"},{local:"ndimensional-arrays",title:"N-dimensional arrays"},{local:"other-feature-types",title:"Other feature types"}],title:null},{local:"data-loading",sections:[{local:"using-totfdataset",title:"Using `to_tf_dataset()`"},{local:"when-to-use-totfdataset",title:"When to use to_tf_dataset"},{local:"caveats-and-limitations",title:"Caveats and limitations"}],title:"Data loading"}],title:"Using Datasets with TensorFlow"};function mr(ts){return hr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _r extends lr{constructor(u){super();rr(this,u,mr,fr,ir,{})}}export{_r as default,ur as metadata};
