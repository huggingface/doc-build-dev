import{S as sr,i as er,s as nr,e as o,k as h,w as u,t as e,M as or,c as l,d as s,m as d,a as r,x as m,h as n,b as c,G as a,g as p,y as g,q as y,o as v,B as w,v as lr}from"../chunks/vendor-hf-doc-builder.js";import{T as rr}from"../chunks/Tip-hf-doc-builder.js";import{I as R}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as X}from"../chunks/CodeBlock-hf-doc-builder.js";function ir(es){let _,Z,j,q,z;return{c(){_=o("p"),Z=e("A "),j=o("a"),q=e("Dataset"),z=e(" object is a wrapper of an Arrow table, which allows fast zero-copy reads from arrays in the dataset to TensorFlow tensors."),this.h()},l(T){_=l(T,"P",{});var P=r(_);Z=n(P,"A "),j=l(P,"A",{href:!0});var tt=r(j);q=n(tt,"Dataset"),tt.forEach(s),z=n(P," object is a wrapper of an Arrow table, which allows fast zero-copy reads from arrays in the dataset to TensorFlow tensors."),P.forEach(s),this.h()},h(){c(j,"href","/docs/datasets/pr_4457/en/package_reference/main_classes#datasets.Dataset")},m(T,P){p(T,_,P),a(_,Z),a(_,j),a(j,q),a(_,z)},d(T){T&&s(_)}}}function pr(es){let _,Z,j,q,z,T,P,tt,se,ns,W,at,Vt,gt,ee,Xt,ne,os,M,st,Zt,yt,oe,ta,le,ls,It,re,rs,Ht,ie,is,vt,ps,et,hs,F,pe,aa,he,de,sa,ce,fe,ds,wt,cs,Y,nt,ea,_t,ue,na,me,fs,ot,ge,oa,ye,ve,us,bt,ms,Lt,we,gs,jt,ys,B,lt,la,$t,_e,ra,be,vs,Et,Ut,je,$e,ws,kt,_s,Rt,Ee,bs,Dt,js,zt,ke,$s,Tt,Es,N,De,Wt,Te,xe,Mt,qe,Ae,ks,K,rt,ia,xt,Ce,pa,Oe,Ds,G,it,ha,qt,Pe,da,Fe,Ts,$,Ne,ca,Se,Ie,fa,He,Le,ua,Ue,Re,ma,ze,We,ga,Me,Ye,xs,b,Be,ya,Ke,Ge,va,Je,Qe,wa,Ve,Xe,_a,Ze,tn,ba,an,sn,ja,en,nn,qs,k,on,$a,ln,rn,Ea,pn,hn,ka,dn,cn,Da,fn,un,As,J,pt,Ta,At,mn,Yt,gn,xa,yn,Cs,ht,vn,qa,wn,_n,Os,Ct,Ps,A,bn,Aa,jn,$n,Ca,En,kn,Oa,Dn,Tn,Fs,Ot,Ns,S,xn,Pa,qn,An,Fa,Cn,On,Ss,Q,dt,Na,Pt,Pn,Sa,Fn,Is,f,Nn,Ia,Sn,In,Ha,Hn,Ln,La,Un,Rn,Ua,zn,Wn,Ra,Mn,Yn,za,Bn,Kn,Wa,Gn,Jn,Hs,I,Qn,Ma,Vn,Xn,Ya,Zn,to,Ls,H,Ft,ao,Ba,so,eo,no,O,oo,Ka,lo,ro,Ga,io,po,Ja,ho,co,fo,x,uo,Qa,mo,go,Va,yo,vo,Xa,wo,_o,Za,bo,jo,Us,V,ct,ts,Nt,$o,as,Eo,Rs,ft,ko,ss,Do,To,zs;return T=new R({}),gt=new R({}),yt=new R({}),vt=new X({props:{code:`from datasets import Dataset
data = [[1, 2],[3, 4]]
ds = Dataset.from_dict({"data": [[1, 2],[3, 4]]})
ds = ds.with_format("tf")
ds[0]
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=int64, numpy=array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])&gt;}
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
       [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])&gt;}`}}),et=new rr({props:{$$slots:{default:[ir]},$$scope:{ctx:es}}}),wt=new X({props:{code:"ds[:]",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
       [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])&gt;}`}}),_t=new R({}),bt=new X({props:{code:`from datasets import Dataset
data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]
ds = Dataset.from_dict({"data": data})
ds = ds.with_format("tf")
ds[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]],[[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>],[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]]
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: data})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.RaggedTensor [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]&gt;}`}}),jt=new X({props:{code:`from datasets import Dataset, Features, Array2D
data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]
features = Features({"data": Array2D(shape=(2, 2), dtype='int32')})
ds = Dataset.from_dict({"data": data}, features=features)
ds = ds.with_format("tf")
ds[0]
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, Features, Array2D
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]],[[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>],[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]]
<span class="hljs-meta">&gt;&gt;&gt; </span>features = Features({<span class="hljs-string">&quot;data&quot;</span>: Array2D(shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=<span class="hljs-string">&#x27;int32&#x27;</span>)})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: data}, features=features)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
 array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
        [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])&gt;}
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
 array([[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
         [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]],
 
        [[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
         [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]])&gt;}`}}),$t=new R({}),kt=new X({props:{code:`from datasets import Dataset, Features, ClassLabel
data = [0, 0, 1]
features = Features({"data": ClassLabel(names=["negative", "positive"])})
ds = Dataset.from_dict({"data": data}, features=features) 
ds = ds.with_format("tf")  
ds[:3]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, Features, ClassLabel
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>features = Features({<span class="hljs-string">&quot;data&quot;</span>: ClassLabel(names=[<span class="hljs-string">&quot;negative&quot;</span>, <span class="hljs-string">&quot;positive&quot;</span>])})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: data}, features=features) 
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)  
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">3</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">3</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])&gt;`}}),Dt=new X({props:{code:`from datasets import Dataset, Features 
text = ["foo", "bar"]
data = [0, 1] 
ds = Dataset.from_dict({"text": text, "data": data})  
ds = ds.with_format("tf") 
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, Features 
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [<span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;bar&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>] 
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;text&quot;</span>: text, <span class="hljs-string">&quot;data&quot;</span>: data})  
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>) 
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;text&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=string, numpy=array([<span class="hljs-string">b&#x27;foo&#x27;</span>, <span class="hljs-string">b&#x27;bar&#x27;</span>], dtype=<span class="hljs-built_in">object</span>)&gt;,
 <span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])&gt;}`}}),Tt=new X({props:{code:`ds = ds.with_format("tf", columns=["data"], output_all_columns=True)
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>, columns=[<span class="hljs-string">&quot;data&quot;</span>], output_all_columns=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])&gt;,
 <span class="hljs-string">&#x27;text&#x27;</span>: [<span class="hljs-string">&#x27;foo&#x27;</span>, <span class="hljs-string">&#x27;bar&#x27;</span>]}`}}),xt=new R({}),qt=new R({}),At=new R({}),Ct=new X({props:{code:`from datasets import Dataset
data = {"inputs": [[1, 2],[3, 4]], "labels": [0, 1]}
ds = Dataset.from_dict(data)
tf_ds = ds.to_tf_dataset(`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>data = {<span class="hljs-string">&quot;inputs&quot;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]], <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict(data)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_ds = ds.to_tf_dataset(
            columns=[<span class="hljs-string">&quot;inputs&quot;</span>],
            label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
            batch_size=<span class="hljs-number">2</span>,
            shuffle=<span class="hljs-literal">True</span>
            )`}}),Ot=new X({props:{code:"model.fit(tf_ds, epochs=2)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(tf_ds, epochs=<span class="hljs-number">2</span>)'}}),Pt=new R({}),Nt=new R({}),{c(){_=o("meta"),Z=h(),j=o("h1"),q=o("a"),z=o("span"),u(T.$$.fragment),P=h(),tt=o("span"),se=e("Using Datasets with TensorFlow"),ns=h(),W=o("h2"),at=o("a"),Vt=o("span"),u(gt.$$.fragment),ee=h(),Xt=o("span"),ne=e("Tensors"),os=h(),M=o("h3"),st=o("a"),Zt=o("span"),u(yt.$$.fragment),oe=h(),ta=o("span"),le=e("Dataset format"),ls=h(),It=o("p"),re=e("By default, datasets return regular Python objects: integers, floats, strings, lists, etc."),rs=h(),Ht=o("p"),ie=e("To get TensorFlow tensors instead, you can set the format of the dataset to \u201Ctf\u201D:"),is=h(),u(vt.$$.fragment),ps=h(),u(et.$$.fragment),hs=h(),F=o("p"),pe=e("This can be useful for converting your dataset to a dict of "),aa=o("code"),he=e("Tensor"),de=e(` objects, or for writing a generator to load TF
samples from it. If you wish to convert the entire dataset to `),sa=o("code"),ce=e("Tensor"),fe=e(", simply query the full dataset:"),ds=h(),u(wt.$$.fragment),cs=h(),Y=o("h3"),nt=o("a"),ea=o("span"),u(_t.$$.fragment),ue=h(),na=o("span"),me=e("N-dimensional arrays"),fs=h(),ot=o("p"),ge=e(`If your dataset consists of N-dimensional arrays, you will see that by default they are considered as nested lists.
In particular, a TensorFlow formatted dataset outputs a `),oa=o("code"),ye=e("RaggedTensor"),ve=e(" instead of one single tensor:"),us=h(),u(bt.$$.fragment),ms=h(),Lt=o("p"),we=e("To get one single tensor, you must explicitly use the Array feature type and specify the shape of your tensors:"),gs=h(),u(jt.$$.fragment),ys=h(),B=o("h3"),lt=o("a"),la=o("span"),u($t.$$.fragment),_e=h(),ra=o("span"),be=e("Other feature types"),vs=h(),Et=o("p"),Ut=o("a"),je=e("ClassLabel"),$e=e(" data are properly converted to tensors:"),ws=h(),u(kt.$$.fragment),_s=h(),Rt=o("p"),Ee=e("Strings are also supported:"),bs=h(),u(Dt.$$.fragment),js=h(),zt=o("p"),ke=e("But if you want, you can explicitly format certain columns and leave the other columns unformatted:"),$s=h(),u(Tt.$$.fragment),Es=h(),N=o("p"),De=e("The "),Wt=o("a"),Te=e("Image"),xe=e(" and "),Mt=o("a"),qe=e("Audio"),Ae=e(" feature types are not supported yet."),ks=h(),K=o("h2"),rt=o("a"),ia=o("span"),u(xt.$$.fragment),Ce=h(),pa=o("span"),Oe=e("Data loading"),Ds=h(),G=o("h3"),it=o("a"),ha=o("span"),u(qt.$$.fragment),Pe=h(),da=o("span"),Fe=e("Interoperability with tf.data"),Ts=h(),$=o("p"),Ne=e(`Although you can load individual samples and batches just by indexing into your dataset, this won\u2019t work if you want
to use Keras methods like `),ca=o("code"),Se=e("fit()"),Ie=e(" and "),fa=o("code"),He=e("predict()"),Le=e(`. You could write a generator function that shuffles and loads batches
from your dataset and `),ua=o("code"),Ue=e("fit()"),Re=e(` on that, but that sounds like a lot of unnecessary work. Instead, if you want to stream
data from your dataset on-the-fly, we recommend converting your dataset to a `),ma=o("code"),ze=e("tf.data.Dataset"),We=e(` using the
`),ga=o("code"),Me=e("to_tf_dataset()"),Ye=e(" method."),xs=h(),b=o("p"),Be=e("The "),ya=o("code"),Ke=e("tf.data.Dataset"),Ge=e(` class covers a wide range of use-cases - it is often created from Tensors in memory, or using a load function to read files on disc
or external storage. The dataset can be transformed arbitrarily with the `),va=o("code"),Je=e("map()"),Qe=e(" method, or methods like "),wa=o("code"),Ve=e("batch()"),Xe=e(`
and `),_a=o("code"),Ze=e("shuffle()"),tn=e(` can be used to create a dataset that\u2019s ready for training. These methods do not modify the stored data
in any way - instead, the methods build a data pipeline graph that will be executed when the dataset is iterated over,
usually during model training or inference. This is different from the `),ba=o("code"),an=e("map()"),sn=e(" method of Hugging Face "),ja=o("code"),en=e("Dataset"),nn=e(` objects,
which runs the map function immediately and saves the new or changed columns.`),qs=h(),k=o("p"),on=e("Since the entire data preprocessing pipeline can be compiled in a "),$a=o("code"),ln=e("tf.data.Dataset"),rn=e(`, this approach allows for massively
parallel, asynchronous data loading and training. However, the requirement for graph compilation can be a limitation,
particularly for HuggingFace tokenizers, which are usually not (yet!) compilable as part of a TF graph. As a result,
we usually advise pre-processing the dataset as a Hugging Face dataset, where arbitrary Python functions can be
used, and then converting to `),Ea=o("code"),pn=e("tf.data.Dataset"),hn=e(" afterwards using "),ka=o("code"),dn=e("to_tf_dataset()"),cn=e(` to get a batched dataset ready for
training. To see examples of this approach, please see the example scripts(link) or notebooks(link) for `),Da=o("code"),fn=e("transformers"),un=e("."),As=h(),J=o("h3"),pt=o("a"),Ta=o("span"),u(At.$$.fragment),mn=h(),Yt=o("span"),gn=e("Using "),xa=o("code"),yn=e("to_tf_dataset()"),Cs=h(),ht=o("p"),vn=e("Using "),qa=o("code"),wn=e("to_tf_dataset()"),_n=e(" is straightforward. Once your dataset is preprocessed and ready, simply call it like so:"),Os=h(),u(Ct.$$.fragment),Ps=h(),A=o("p"),bn=e("The returned "),Aa=o("code"),jn=e("tf_ds"),$n=e(" object here is now fully ready to train on, and can be passed directly to "),Ca=o("code"),En=e("model.fit()"),kn=e(`! Note
that you set the batch size when creating the dataset, and so you don\u2019t need to specify it when calling `),Oa=o("code"),Dn=e("fit()"),Tn=e(":"),Fs=h(),u(Ot.$$.fragment),Ns=h(),S=o("p"),xn=e("For a full description of the arguments, please see the "),Pa=o("code"),qn=e("to_tf_dataset()"),An=e(` documentation(link). In many cases,
you will also need to add a `),Fa=o("code"),Cn=e("collate_fn"),On=e(` to your call. This is a function that takes multiple elements of the dataset
and combines them into a single batch. When all elements have the same length, the built-in default collator will
suffice, but for more complex tasks a custom collator may be necessary. In particular, many tasks have samples
with varying sequence lengths which will require a collator that can pad batches correctly. (Link to transformers
collators or examples here?)`),Ss=h(),Q=o("h3"),dt=o("a"),Na=o("span"),u(Pt.$$.fragment),Pn=h(),Sa=o("span"),Fn=e("When to use to_tf_dataset"),Is=h(),f=o("p"),Nn=e(`The astute reader may have noticed at this point that we have offered two approaches to achieve the same goal - if you
want to pass your dataset to a TensorFlow model, you can either convert the dataset to a `),Ia=o("code"),Sn=e("Tensor"),In=e(" or "),Ha=o("code"),Hn=e("dict"),Ln=e(" of "),La=o("code"),Un=e("Tensors"),Rn=e(`
using `),Ua=o("code"),zn=e(".with_format('tf')"),Wn=e(", or you can convert the dataset to a "),Ra=o("code"),Mn=e("tf.data.Dataset"),Yn=e(" with "),za=o("code"),Bn=e("to_tf_dataset()"),Kn=e(`. Either of these
can be passed to `),Wa=o("code"),Gn=e("model.fit()"),Jn=e(", so which should you choose?"),Hs=h(),I=o("p"),Qn=e("The key thing to recognize is that when you convert the whole dataset to "),Ma=o("code"),Vn=e("Tensor"),Xn=e(`s, it is static and fully loaded into
RAM. This is simple and convenient, but if any of the following apply, you should probably use `),Ya=o("code"),Zn=e("to_tf_dataset()"),to=e(`
instead:`),Ls=h(),H=o("ul"),Ft=o("li"),ao=e("Your dataset is too large to fit in RAM. "),Ba=o("code"),so=e("to_tf_dataset()"),eo=e(` streams only one batch at a time, so even very large
datasets can be handled with this method.`),no=h(),O=o("li"),oo=e("You want to apply random transformations using "),Ka=o("code"),lo=e("dataset.with_transform()"),ro=e(" or the "),Ga=o("code"),io=e("collate_fn"),po=e(`. This is
common in several modalities, such as image augmentations when training vision models, or random masking when training
masked language models. Using `),Ja=o("code"),ho=e("to_tf_dataset()"),co=e(` will apply those transformations
at the moment when a batch is loaded, which means the same samples will get different augmentations each time
they are loaded. This is usually what you want.`),fo=h(),x=o("li"),uo=e(`Your data has a variable dimension, such as input texts in NLP that consist of varying
numbers of tokens. When you create a batch with samples with a variable dimension, the standard solution is to
pad the shorter samples to the length of the longest one. When you stream samples from a dataset with `),Qa=o("code"),mo=e("to_tf_dataset"),go=e(`,
you can apply this padding to each batch via your `),Va=o("code"),yo=e("collate_fn"),vo=e(`. (link examples here?) However, if you want to convert
such a dataset to dense `),Xa=o("code"),wo=e("Tensor"),_o=e("s, then you will have to pad samples to the length of the longest sample in "),Za=o("em"),bo=e(`the
entire dataset!`),jo=e(" This can result in huge amounts of padding, which wastes memory and reduces your model\u2019s speed."),Us=h(),V=o("h3"),ct=o("a"),ts=o("span"),u(Nt.$$.fragment),$o=h(),as=o("span"),Eo=e("Caveats and limitations"),Rs=h(),ft=o("p"),ko=e("Right now, "),ss=o("code"),Do=e("to_tf_dataset()"),To=e(" always return a batched dataset - we will add support for unbatched datasets soon!"),this.h()},l(t){const i=or('[data-svelte="svelte-1phssyn"]',document.head);_=l(i,"META",{name:!0,content:!0}),i.forEach(s),Z=d(t),j=l(t,"H1",{class:!0});var St=r(j);q=l(St,"A",{id:!0,class:!0,href:!0});var Ao=r(q);z=l(Ao,"SPAN",{});var Co=r(z);m(T.$$.fragment,Co),Co.forEach(s),Ao.forEach(s),P=d(St),tt=l(St,"SPAN",{});var Oo=r(tt);se=n(Oo,"Using Datasets with TensorFlow"),Oo.forEach(s),St.forEach(s),ns=d(t),W=l(t,"H2",{class:!0});var Ws=r(W);at=l(Ws,"A",{id:!0,class:!0,href:!0});var Po=r(at);Vt=l(Po,"SPAN",{});var Fo=r(Vt);m(gt.$$.fragment,Fo),Fo.forEach(s),Po.forEach(s),ee=d(Ws),Xt=l(Ws,"SPAN",{});var No=r(Xt);ne=n(No,"Tensors"),No.forEach(s),Ws.forEach(s),os=d(t),M=l(t,"H3",{class:!0});var Ms=r(M);st=l(Ms,"A",{id:!0,class:!0,href:!0});var So=r(st);Zt=l(So,"SPAN",{});var Io=r(Zt);m(yt.$$.fragment,Io),Io.forEach(s),So.forEach(s),oe=d(Ms),ta=l(Ms,"SPAN",{});var Ho=r(ta);le=n(Ho,"Dataset format"),Ho.forEach(s),Ms.forEach(s),ls=d(t),It=l(t,"P",{});var Lo=r(It);re=n(Lo,"By default, datasets return regular Python objects: integers, floats, strings, lists, etc."),Lo.forEach(s),rs=d(t),Ht=l(t,"P",{});var Uo=r(Ht);ie=n(Uo,"To get TensorFlow tensors instead, you can set the format of the dataset to \u201Ctf\u201D:"),Uo.forEach(s),is=d(t),m(vt.$$.fragment,t),ps=d(t),m(et.$$.fragment,t),hs=d(t),F=l(t,"P",{});var Bt=r(F);pe=n(Bt,"This can be useful for converting your dataset to a dict of "),aa=l(Bt,"CODE",{});var Ro=r(aa);he=n(Ro,"Tensor"),Ro.forEach(s),de=n(Bt,` objects, or for writing a generator to load TF
samples from it. If you wish to convert the entire dataset to `),sa=l(Bt,"CODE",{});var zo=r(sa);ce=n(zo,"Tensor"),zo.forEach(s),fe=n(Bt,", simply query the full dataset:"),Bt.forEach(s),ds=d(t),m(wt.$$.fragment,t),cs=d(t),Y=l(t,"H3",{class:!0});var Ys=r(Y);nt=l(Ys,"A",{id:!0,class:!0,href:!0});var Wo=r(nt);ea=l(Wo,"SPAN",{});var Mo=r(ea);m(_t.$$.fragment,Mo),Mo.forEach(s),Wo.forEach(s),ue=d(Ys),na=l(Ys,"SPAN",{});var Yo=r(na);me=n(Yo,"N-dimensional arrays"),Yo.forEach(s),Ys.forEach(s),fs=d(t),ot=l(t,"P",{});var Bs=r(ot);ge=n(Bs,`If your dataset consists of N-dimensional arrays, you will see that by default they are considered as nested lists.
In particular, a TensorFlow formatted dataset outputs a `),oa=l(Bs,"CODE",{});var Bo=r(oa);ye=n(Bo,"RaggedTensor"),Bo.forEach(s),ve=n(Bs," instead of one single tensor:"),Bs.forEach(s),us=d(t),m(bt.$$.fragment,t),ms=d(t),Lt=l(t,"P",{});var Ko=r(Lt);we=n(Ko,"To get one single tensor, you must explicitly use the Array feature type and specify the shape of your tensors:"),Ko.forEach(s),gs=d(t),m(jt.$$.fragment,t),ys=d(t),B=l(t,"H3",{class:!0});var Ks=r(B);lt=l(Ks,"A",{id:!0,class:!0,href:!0});var Go=r(lt);la=l(Go,"SPAN",{});var Jo=r(la);m($t.$$.fragment,Jo),Jo.forEach(s),Go.forEach(s),_e=d(Ks),ra=l(Ks,"SPAN",{});var Qo=r(ra);be=n(Qo,"Other feature types"),Qo.forEach(s),Ks.forEach(s),vs=d(t),Et=l(t,"P",{});var xo=r(Et);Ut=l(xo,"A",{href:!0});var Vo=r(Ut);je=n(Vo,"ClassLabel"),Vo.forEach(s),$e=n(xo," data are properly converted to tensors:"),xo.forEach(s),ws=d(t),m(kt.$$.fragment,t),_s=d(t),Rt=l(t,"P",{});var Xo=r(Rt);Ee=n(Xo,"Strings are also supported:"),Xo.forEach(s),bs=d(t),m(Dt.$$.fragment,t),js=d(t),zt=l(t,"P",{});var Zo=r(zt);ke=n(Zo,"But if you want, you can explicitly format certain columns and leave the other columns unformatted:"),Zo.forEach(s),$s=d(t),m(Tt.$$.fragment,t),Es=d(t),N=l(t,"P",{});var Kt=r(N);De=n(Kt,"The "),Wt=l(Kt,"A",{href:!0});var tl=r(Wt);Te=n(tl,"Image"),tl.forEach(s),xe=n(Kt," and "),Mt=l(Kt,"A",{href:!0});var al=r(Mt);qe=n(al,"Audio"),al.forEach(s),Ae=n(Kt," feature types are not supported yet."),Kt.forEach(s),ks=d(t),K=l(t,"H2",{class:!0});var Gs=r(K);rt=l(Gs,"A",{id:!0,class:!0,href:!0});var sl=r(rt);ia=l(sl,"SPAN",{});var el=r(ia);m(xt.$$.fragment,el),el.forEach(s),sl.forEach(s),Ce=d(Gs),pa=l(Gs,"SPAN",{});var nl=r(pa);Oe=n(nl,"Data loading"),nl.forEach(s),Gs.forEach(s),Ds=d(t),G=l(t,"H3",{class:!0});var Js=r(G);it=l(Js,"A",{id:!0,class:!0,href:!0});var ol=r(it);ha=l(ol,"SPAN",{});var ll=r(ha);m(qt.$$.fragment,ll),ll.forEach(s),ol.forEach(s),Pe=d(Js),da=l(Js,"SPAN",{});var rl=r(da);Fe=n(rl,"Interoperability with tf.data"),rl.forEach(s),Js.forEach(s),Ts=d(t),$=l(t,"P",{});var C=r($);Ne=n(C,`Although you can load individual samples and batches just by indexing into your dataset, this won\u2019t work if you want
to use Keras methods like `),ca=l(C,"CODE",{});var il=r(ca);Se=n(il,"fit()"),il.forEach(s),Ie=n(C," and "),fa=l(C,"CODE",{});var pl=r(fa);He=n(pl,"predict()"),pl.forEach(s),Le=n(C,`. You could write a generator function that shuffles and loads batches
from your dataset and `),ua=l(C,"CODE",{});var hl=r(ua);Ue=n(hl,"fit()"),hl.forEach(s),Re=n(C,` on that, but that sounds like a lot of unnecessary work. Instead, if you want to stream
data from your dataset on-the-fly, we recommend converting your dataset to a `),ma=l(C,"CODE",{});var dl=r(ma);ze=n(dl,"tf.data.Dataset"),dl.forEach(s),We=n(C,` using the
`),ga=l(C,"CODE",{});var cl=r(ga);Me=n(cl,"to_tf_dataset()"),cl.forEach(s),Ye=n(C," method."),C.forEach(s),xs=d(t),b=l(t,"P",{});var D=r(b);Be=n(D,"The "),ya=l(D,"CODE",{});var fl=r(ya);Ke=n(fl,"tf.data.Dataset"),fl.forEach(s),Ge=n(D,` class covers a wide range of use-cases - it is often created from Tensors in memory, or using a load function to read files on disc
or external storage. The dataset can be transformed arbitrarily with the `),va=l(D,"CODE",{});var ul=r(va);Je=n(ul,"map()"),ul.forEach(s),Qe=n(D," method, or methods like "),wa=l(D,"CODE",{});var ml=r(wa);Ve=n(ml,"batch()"),ml.forEach(s),Xe=n(D,`
and `),_a=l(D,"CODE",{});var gl=r(_a);Ze=n(gl,"shuffle()"),gl.forEach(s),tn=n(D,` can be used to create a dataset that\u2019s ready for training. These methods do not modify the stored data
in any way - instead, the methods build a data pipeline graph that will be executed when the dataset is iterated over,
usually during model training or inference. This is different from the `),ba=l(D,"CODE",{});var yl=r(ba);an=n(yl,"map()"),yl.forEach(s),sn=n(D," method of Hugging Face "),ja=l(D,"CODE",{});var vl=r(ja);en=n(vl,"Dataset"),vl.forEach(s),nn=n(D,` objects,
which runs the map function immediately and saves the new or changed columns.`),D.forEach(s),qs=d(t),k=l(t,"P",{});var L=r(k);on=n(L,"Since the entire data preprocessing pipeline can be compiled in a "),$a=l(L,"CODE",{});var wl=r($a);ln=n(wl,"tf.data.Dataset"),wl.forEach(s),rn=n(L,`, this approach allows for massively
parallel, asynchronous data loading and training. However, the requirement for graph compilation can be a limitation,
particularly for HuggingFace tokenizers, which are usually not (yet!) compilable as part of a TF graph. As a result,
we usually advise pre-processing the dataset as a Hugging Face dataset, where arbitrary Python functions can be
used, and then converting to `),Ea=l(L,"CODE",{});var _l=r(Ea);pn=n(_l,"tf.data.Dataset"),_l.forEach(s),hn=n(L," afterwards using "),ka=l(L,"CODE",{});var bl=r(ka);dn=n(bl,"to_tf_dataset()"),bl.forEach(s),cn=n(L,` to get a batched dataset ready for
training. To see examples of this approach, please see the example scripts(link) or notebooks(link) for `),Da=l(L,"CODE",{});var jl=r(Da);fn=n(jl,"transformers"),jl.forEach(s),un=n(L,"."),L.forEach(s),As=d(t),J=l(t,"H3",{class:!0});var Qs=r(J);pt=l(Qs,"A",{id:!0,class:!0,href:!0});var $l=r(pt);Ta=l($l,"SPAN",{});var El=r(Ta);m(At.$$.fragment,El),El.forEach(s),$l.forEach(s),mn=d(Qs),Yt=l(Qs,"SPAN",{});var qo=r(Yt);gn=n(qo,"Using "),xa=l(qo,"CODE",{});var kl=r(xa);yn=n(kl,"to_tf_dataset()"),kl.forEach(s),qo.forEach(s),Qs.forEach(s),Cs=d(t),ht=l(t,"P",{});var Vs=r(ht);vn=n(Vs,"Using "),qa=l(Vs,"CODE",{});var Dl=r(qa);wn=n(Dl,"to_tf_dataset()"),Dl.forEach(s),_n=n(Vs," is straightforward. Once your dataset is preprocessed and ready, simply call it like so:"),Vs.forEach(s),Os=d(t),m(Ct.$$.fragment,t),Ps=d(t),A=l(t,"P",{});var ut=r(A);bn=n(ut,"The returned "),Aa=l(ut,"CODE",{});var Tl=r(Aa);jn=n(Tl,"tf_ds"),Tl.forEach(s),$n=n(ut," object here is now fully ready to train on, and can be passed directly to "),Ca=l(ut,"CODE",{});var xl=r(Ca);En=n(xl,"model.fit()"),xl.forEach(s),kn=n(ut,`! Note
that you set the batch size when creating the dataset, and so you don\u2019t need to specify it when calling `),Oa=l(ut,"CODE",{});var ql=r(Oa);Dn=n(ql,"fit()"),ql.forEach(s),Tn=n(ut,":"),ut.forEach(s),Fs=d(t),m(Ot.$$.fragment,t),Ns=d(t),S=l(t,"P",{});var Gt=r(S);xn=n(Gt,"For a full description of the arguments, please see the "),Pa=l(Gt,"CODE",{});var Al=r(Pa);qn=n(Al,"to_tf_dataset()"),Al.forEach(s),An=n(Gt,` documentation(link). In many cases,
you will also need to add a `),Fa=l(Gt,"CODE",{});var Cl=r(Fa);Cn=n(Cl,"collate_fn"),Cl.forEach(s),On=n(Gt,` to your call. This is a function that takes multiple elements of the dataset
and combines them into a single batch. When all elements have the same length, the built-in default collator will
suffice, but for more complex tasks a custom collator may be necessary. In particular, many tasks have samples
with varying sequence lengths which will require a collator that can pad batches correctly. (Link to transformers
collators or examples here?)`),Gt.forEach(s),Ss=d(t),Q=l(t,"H3",{class:!0});var Xs=r(Q);dt=l(Xs,"A",{id:!0,class:!0,href:!0});var Ol=r(dt);Na=l(Ol,"SPAN",{});var Pl=r(Na);m(Pt.$$.fragment,Pl),Pl.forEach(s),Ol.forEach(s),Pn=d(Xs),Sa=l(Xs,"SPAN",{});var Fl=r(Sa);Fn=n(Fl,"When to use to_tf_dataset"),Fl.forEach(s),Xs.forEach(s),Is=d(t),f=l(t,"P",{});var E=r(f);Nn=n(E,`The astute reader may have noticed at this point that we have offered two approaches to achieve the same goal - if you
want to pass your dataset to a TensorFlow model, you can either convert the dataset to a `),Ia=l(E,"CODE",{});var Nl=r(Ia);Sn=n(Nl,"Tensor"),Nl.forEach(s),In=n(E," or "),Ha=l(E,"CODE",{});var Sl=r(Ha);Hn=n(Sl,"dict"),Sl.forEach(s),Ln=n(E," of "),La=l(E,"CODE",{});var Il=r(La);Un=n(Il,"Tensors"),Il.forEach(s),Rn=n(E,`
using `),Ua=l(E,"CODE",{});var Hl=r(Ua);zn=n(Hl,".with_format('tf')"),Hl.forEach(s),Wn=n(E,", or you can convert the dataset to a "),Ra=l(E,"CODE",{});var Ll=r(Ra);Mn=n(Ll,"tf.data.Dataset"),Ll.forEach(s),Yn=n(E," with "),za=l(E,"CODE",{});var Ul=r(za);Bn=n(Ul,"to_tf_dataset()"),Ul.forEach(s),Kn=n(E,`. Either of these
can be passed to `),Wa=l(E,"CODE",{});var Rl=r(Wa);Gn=n(Rl,"model.fit()"),Rl.forEach(s),Jn=n(E,", so which should you choose?"),E.forEach(s),Hs=d(t),I=l(t,"P",{});var Jt=r(I);Qn=n(Jt,"The key thing to recognize is that when you convert the whole dataset to "),Ma=l(Jt,"CODE",{});var zl=r(Ma);Vn=n(zl,"Tensor"),zl.forEach(s),Xn=n(Jt,`s, it is static and fully loaded into
RAM. This is simple and convenient, but if any of the following apply, you should probably use `),Ya=l(Jt,"CODE",{});var Wl=r(Ya);Zn=n(Wl,"to_tf_dataset()"),Wl.forEach(s),to=n(Jt,`
instead:`),Jt.forEach(s),Ls=d(t),H=l(t,"UL",{});var Qt=r(H);Ft=l(Qt,"LI",{});var Zs=r(Ft);ao=n(Zs,"Your dataset is too large to fit in RAM. "),Ba=l(Zs,"CODE",{});var Ml=r(Ba);so=n(Ml,"to_tf_dataset()"),Ml.forEach(s),eo=n(Zs,` streams only one batch at a time, so even very large
datasets can be handled with this method.`),Zs.forEach(s),no=d(Qt),O=l(Qt,"LI",{});var mt=r(O);oo=n(mt,"You want to apply random transformations using "),Ka=l(mt,"CODE",{});var Yl=r(Ka);lo=n(Yl,"dataset.with_transform()"),Yl.forEach(s),ro=n(mt," or the "),Ga=l(mt,"CODE",{});var Bl=r(Ga);io=n(Bl,"collate_fn"),Bl.forEach(s),po=n(mt,`. This is
common in several modalities, such as image augmentations when training vision models, or random masking when training
masked language models. Using `),Ja=l(mt,"CODE",{});var Kl=r(Ja);ho=n(Kl,"to_tf_dataset()"),Kl.forEach(s),co=n(mt,` will apply those transformations
at the moment when a batch is loaded, which means the same samples will get different augmentations each time
they are loaded. This is usually what you want.`),mt.forEach(s),fo=d(Qt),x=l(Qt,"LI",{});var U=r(x);uo=n(U,`Your data has a variable dimension, such as input texts in NLP that consist of varying
numbers of tokens. When you create a batch with samples with a variable dimension, the standard solution is to
pad the shorter samples to the length of the longest one. When you stream samples from a dataset with `),Qa=l(U,"CODE",{});var Gl=r(Qa);mo=n(Gl,"to_tf_dataset"),Gl.forEach(s),go=n(U,`,
you can apply this padding to each batch via your `),Va=l(U,"CODE",{});var Jl=r(Va);yo=n(Jl,"collate_fn"),Jl.forEach(s),vo=n(U,`. (link examples here?) However, if you want to convert
such a dataset to dense `),Xa=l(U,"CODE",{});var Ql=r(Xa);wo=n(Ql,"Tensor"),Ql.forEach(s),_o=n(U,"s, then you will have to pad samples to the length of the longest sample in "),Za=l(U,"EM",{});var Vl=r(Za);bo=n(Vl,`the
entire dataset!`),Vl.forEach(s),jo=n(U," This can result in huge amounts of padding, which wastes memory and reduces your model\u2019s speed."),U.forEach(s),Qt.forEach(s),Us=d(t),V=l(t,"H3",{class:!0});var te=r(V);ct=l(te,"A",{id:!0,class:!0,href:!0});var Xl=r(ct);ts=l(Xl,"SPAN",{});var Zl=r(ts);m(Nt.$$.fragment,Zl),Zl.forEach(s),Xl.forEach(s),$o=d(te),as=l(te,"SPAN",{});var tr=r(as);Eo=n(tr,"Caveats and limitations"),tr.forEach(s),te.forEach(s),Rs=d(t),ft=l(t,"P",{});var ae=r(ft);ko=n(ae,"Right now, "),ss=l(ae,"CODE",{});var ar=r(ss);Do=n(ar,"to_tf_dataset()"),ar.forEach(s),To=n(ae," always return a batched dataset - we will add support for unbatched datasets soon!"),ae.forEach(s),this.h()},h(){c(_,"name","hf:doc:metadata"),c(_,"content",JSON.stringify(hr)),c(q,"id","using-datasets-with-tensorflow"),c(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(q,"href","#using-datasets-with-tensorflow"),c(j,"class","relative group"),c(at,"id","tensors"),c(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(at,"href","#tensors"),c(W,"class","relative group"),c(st,"id","dataset-format"),c(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(st,"href","#dataset-format"),c(M,"class","relative group"),c(nt,"id","ndimensional-arrays"),c(nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(nt,"href","#ndimensional-arrays"),c(Y,"class","relative group"),c(lt,"id","other-feature-types"),c(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lt,"href","#other-feature-types"),c(B,"class","relative group"),c(Ut,"href","/docs/datasets/pr_4457/en/package_reference/main_classes#datasets.ClassLabel"),c(Wt,"href","/docs/datasets/pr_4457/en/package_reference/main_classes#datasets.Image"),c(Mt,"href","/docs/datasets/pr_4457/en/package_reference/main_classes#datasets.Audio"),c(rt,"id","data-loading"),c(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(rt,"href","#data-loading"),c(K,"class","relative group"),c(it,"id","interoperability-with-tfdata"),c(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(it,"href","#interoperability-with-tfdata"),c(G,"class","relative group"),c(pt,"id","using-totfdataset"),c(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pt,"href","#using-totfdataset"),c(J,"class","relative group"),c(dt,"id","when-to-use-totfdataset"),c(dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(dt,"href","#when-to-use-totfdataset"),c(Q,"class","relative group"),c(ct,"id","caveats-and-limitations"),c(ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ct,"href","#caveats-and-limitations"),c(V,"class","relative group")},m(t,i){a(document.head,_),p(t,Z,i),p(t,j,i),a(j,q),a(q,z),g(T,z,null),a(j,P),a(j,tt),a(tt,se),p(t,ns,i),p(t,W,i),a(W,at),a(at,Vt),g(gt,Vt,null),a(W,ee),a(W,Xt),a(Xt,ne),p(t,os,i),p(t,M,i),a(M,st),a(st,Zt),g(yt,Zt,null),a(M,oe),a(M,ta),a(ta,le),p(t,ls,i),p(t,It,i),a(It,re),p(t,rs,i),p(t,Ht,i),a(Ht,ie),p(t,is,i),g(vt,t,i),p(t,ps,i),g(et,t,i),p(t,hs,i),p(t,F,i),a(F,pe),a(F,aa),a(aa,he),a(F,de),a(F,sa),a(sa,ce),a(F,fe),p(t,ds,i),g(wt,t,i),p(t,cs,i),p(t,Y,i),a(Y,nt),a(nt,ea),g(_t,ea,null),a(Y,ue),a(Y,na),a(na,me),p(t,fs,i),p(t,ot,i),a(ot,ge),a(ot,oa),a(oa,ye),a(ot,ve),p(t,us,i),g(bt,t,i),p(t,ms,i),p(t,Lt,i),a(Lt,we),p(t,gs,i),g(jt,t,i),p(t,ys,i),p(t,B,i),a(B,lt),a(lt,la),g($t,la,null),a(B,_e),a(B,ra),a(ra,be),p(t,vs,i),p(t,Et,i),a(Et,Ut),a(Ut,je),a(Et,$e),p(t,ws,i),g(kt,t,i),p(t,_s,i),p(t,Rt,i),a(Rt,Ee),p(t,bs,i),g(Dt,t,i),p(t,js,i),p(t,zt,i),a(zt,ke),p(t,$s,i),g(Tt,t,i),p(t,Es,i),p(t,N,i),a(N,De),a(N,Wt),a(Wt,Te),a(N,xe),a(N,Mt),a(Mt,qe),a(N,Ae),p(t,ks,i),p(t,K,i),a(K,rt),a(rt,ia),g(xt,ia,null),a(K,Ce),a(K,pa),a(pa,Oe),p(t,Ds,i),p(t,G,i),a(G,it),a(it,ha),g(qt,ha,null),a(G,Pe),a(G,da),a(da,Fe),p(t,Ts,i),p(t,$,i),a($,Ne),a($,ca),a(ca,Se),a($,Ie),a($,fa),a(fa,He),a($,Le),a($,ua),a(ua,Ue),a($,Re),a($,ma),a(ma,ze),a($,We),a($,ga),a(ga,Me),a($,Ye),p(t,xs,i),p(t,b,i),a(b,Be),a(b,ya),a(ya,Ke),a(b,Ge),a(b,va),a(va,Je),a(b,Qe),a(b,wa),a(wa,Ve),a(b,Xe),a(b,_a),a(_a,Ze),a(b,tn),a(b,ba),a(ba,an),a(b,sn),a(b,ja),a(ja,en),a(b,nn),p(t,qs,i),p(t,k,i),a(k,on),a(k,$a),a($a,ln),a(k,rn),a(k,Ea),a(Ea,pn),a(k,hn),a(k,ka),a(ka,dn),a(k,cn),a(k,Da),a(Da,fn),a(k,un),p(t,As,i),p(t,J,i),a(J,pt),a(pt,Ta),g(At,Ta,null),a(J,mn),a(J,Yt),a(Yt,gn),a(Yt,xa),a(xa,yn),p(t,Cs,i),p(t,ht,i),a(ht,vn),a(ht,qa),a(qa,wn),a(ht,_n),p(t,Os,i),g(Ct,t,i),p(t,Ps,i),p(t,A,i),a(A,bn),a(A,Aa),a(Aa,jn),a(A,$n),a(A,Ca),a(Ca,En),a(A,kn),a(A,Oa),a(Oa,Dn),a(A,Tn),p(t,Fs,i),g(Ot,t,i),p(t,Ns,i),p(t,S,i),a(S,xn),a(S,Pa),a(Pa,qn),a(S,An),a(S,Fa),a(Fa,Cn),a(S,On),p(t,Ss,i),p(t,Q,i),a(Q,dt),a(dt,Na),g(Pt,Na,null),a(Q,Pn),a(Q,Sa),a(Sa,Fn),p(t,Is,i),p(t,f,i),a(f,Nn),a(f,Ia),a(Ia,Sn),a(f,In),a(f,Ha),a(Ha,Hn),a(f,Ln),a(f,La),a(La,Un),a(f,Rn),a(f,Ua),a(Ua,zn),a(f,Wn),a(f,Ra),a(Ra,Mn),a(f,Yn),a(f,za),a(za,Bn),a(f,Kn),a(f,Wa),a(Wa,Gn),a(f,Jn),p(t,Hs,i),p(t,I,i),a(I,Qn),a(I,Ma),a(Ma,Vn),a(I,Xn),a(I,Ya),a(Ya,Zn),a(I,to),p(t,Ls,i),p(t,H,i),a(H,Ft),a(Ft,ao),a(Ft,Ba),a(Ba,so),a(Ft,eo),a(H,no),a(H,O),a(O,oo),a(O,Ka),a(Ka,lo),a(O,ro),a(O,Ga),a(Ga,io),a(O,po),a(O,Ja),a(Ja,ho),a(O,co),a(H,fo),a(H,x),a(x,uo),a(x,Qa),a(Qa,mo),a(x,go),a(x,Va),a(Va,yo),a(x,vo),a(x,Xa),a(Xa,wo),a(x,_o),a(x,Za),a(Za,bo),a(x,jo),p(t,Us,i),p(t,V,i),a(V,ct),a(ct,ts),g(Nt,ts,null),a(V,$o),a(V,as),a(as,Eo),p(t,Rs,i),p(t,ft,i),a(ft,ko),a(ft,ss),a(ss,Do),a(ft,To),zs=!0},p(t,[i]){const St={};i&2&&(St.$$scope={dirty:i,ctx:t}),et.$set(St)},i(t){zs||(y(T.$$.fragment,t),y(gt.$$.fragment,t),y(yt.$$.fragment,t),y(vt.$$.fragment,t),y(et.$$.fragment,t),y(wt.$$.fragment,t),y(_t.$$.fragment,t),y(bt.$$.fragment,t),y(jt.$$.fragment,t),y($t.$$.fragment,t),y(kt.$$.fragment,t),y(Dt.$$.fragment,t),y(Tt.$$.fragment,t),y(xt.$$.fragment,t),y(qt.$$.fragment,t),y(At.$$.fragment,t),y(Ct.$$.fragment,t),y(Ot.$$.fragment,t),y(Pt.$$.fragment,t),y(Nt.$$.fragment,t),zs=!0)},o(t){v(T.$$.fragment,t),v(gt.$$.fragment,t),v(yt.$$.fragment,t),v(vt.$$.fragment,t),v(et.$$.fragment,t),v(wt.$$.fragment,t),v(_t.$$.fragment,t),v(bt.$$.fragment,t),v(jt.$$.fragment,t),v($t.$$.fragment,t),v(kt.$$.fragment,t),v(Dt.$$.fragment,t),v(Tt.$$.fragment,t),v(xt.$$.fragment,t),v(qt.$$.fragment,t),v(At.$$.fragment,t),v(Ct.$$.fragment,t),v(Ot.$$.fragment,t),v(Pt.$$.fragment,t),v(Nt.$$.fragment,t),zs=!1},d(t){s(_),t&&s(Z),t&&s(j),w(T),t&&s(ns),t&&s(W),w(gt),t&&s(os),t&&s(M),w(yt),t&&s(ls),t&&s(It),t&&s(rs),t&&s(Ht),t&&s(is),w(vt,t),t&&s(ps),w(et,t),t&&s(hs),t&&s(F),t&&s(ds),w(wt,t),t&&s(cs),t&&s(Y),w(_t),t&&s(fs),t&&s(ot),t&&s(us),w(bt,t),t&&s(ms),t&&s(Lt),t&&s(gs),w(jt,t),t&&s(ys),t&&s(B),w($t),t&&s(vs),t&&s(Et),t&&s(ws),w(kt,t),t&&s(_s),t&&s(Rt),t&&s(bs),w(Dt,t),t&&s(js),t&&s(zt),t&&s($s),w(Tt,t),t&&s(Es),t&&s(N),t&&s(ks),t&&s(K),w(xt),t&&s(Ds),t&&s(G),w(qt),t&&s(Ts),t&&s($),t&&s(xs),t&&s(b),t&&s(qs),t&&s(k),t&&s(As),t&&s(J),w(At),t&&s(Cs),t&&s(ht),t&&s(Os),w(Ct,t),t&&s(Ps),t&&s(A),t&&s(Fs),w(Ot,t),t&&s(Ns),t&&s(S),t&&s(Ss),t&&s(Q),w(Pt),t&&s(Is),t&&s(f),t&&s(Hs),t&&s(I),t&&s(Ls),t&&s(H),t&&s(Us),t&&s(V),w(Nt),t&&s(Rs),t&&s(ft)}}}const hr={local:"using-datasets-with-tensorflow",sections:[{local:"tensors",sections:[{local:"dataset-format",title:"Dataset format"},{local:"ndimensional-arrays",title:"N-dimensional arrays"},{local:"other-feature-types",title:"Other feature types"}],title:"Tensors"},{local:"data-loading",sections:[{local:"interoperability-with-tfdata",title:"Interoperability with tf.data"},{local:"using-totfdataset",title:"Using `to_tf_dataset()`"},{local:"when-to-use-totfdataset",title:"When to use to_tf_dataset"},{local:"caveats-and-limitations",title:"Caveats and limitations"}],title:"Data loading"}],title:"Using Datasets with TensorFlow"};function dr(es){return lr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class gr extends sr{constructor(_){super();er(this,_,dr,pr,nr,{})}}export{gr as default,hr as metadata};
