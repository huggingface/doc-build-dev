import{S as kr,i as Tr,s as xr,e as o,k as h,w,t as s,M as qr,c as l,d as e,m as c,a as r,x as v,h as n,b as d,G as a,g as p,y as _,q as b,o as j,B as E,v as Cr}from"../chunks/vendor-hf-doc-builder.js";import{T as Ar}from"../chunks/Tip-hf-doc-builder.js";import{I as mt}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as X}from"../chunks/CodeBlock-hf-doc-builder.js";function Or(le){let u,Z,$,P,M;return{c(){u=o("p"),Z=s("A "),$=o("a"),P=s("Dataset"),M=s(" object is a wrapper of an Arrow table, which allows fast reads from arrays in the dataset to TensorFlow tensors."),this.h()},l(A){u=l(A,"P",{});var H=r(u);Z=n(H,"A "),$=l(H,"A",{href:!0});var tt=r($);P=n(tt,"Dataset"),tt.forEach(e),M=n(H," object is a wrapper of an Arrow table, which allows fast reads from arrays in the dataset to TensorFlow tensors."),H.forEach(e),this.h()},h(){d($,"href","/docs/datasets/pr_4613/en/package_reference/main_classes#datasets.Dataset")},m(A,H){p(A,u,H),a(u,Z),a(u,$),a($,P),a(u,M)},d(A){A&&e(u)}}}function Pr(le){let u,Z,$,P,M,A,H,tt,ss,re,T,ns,Zt,os,ls,ta,rs,is,aa,ps,ds,ea,hs,cs,ie,z,at,sa,gt,fs,na,us,pe,Ut,ms,de,et,gs,oa,ys,ws,he,yt,ce,st,fe,I,vs,la,_s,bs,ra,js,Es,ue,wt,me,B,nt,ia,vt,$s,pa,Ds,ge,ot,ks,da,Ts,xs,ye,_t,we,Yt,qs,ve,bt,_e,K,lt,ha,jt,Cs,ca,As,be,Et,Rt,Os,Ps,je,$t,Ee,Wt,Fs,$e,Dt,De,Mt,Ns,ke,kt,Te,L,Ss,zt,Hs,Is,Bt,Ls,Us,xe,G,rt,fa,Tt,Ys,ua,Rs,qe,D,Ws,ma,Ms,zs,ga,Bs,Ks,ya,Gs,Js,wa,Qs,Vs,va,Xs,Zs,Ce,m,tn,_a,an,en,ba,sn,nn,ja,on,ln,Ea,rn,pn,$a,dn,hn,Da,cn,fn,Ae,g,un,ka,mn,gn,Ta,yn,wn,xa,vn,_n,xt,bn,jn,qt,En,$n,qa,Dn,kn,Oe,J,it,Ca,Ct,Tn,Kt,xn,Aa,qn,Pe,pt,Cn,Oa,An,On,Fe,At,Ne,F,Pn,Pa,Fn,Nn,Fa,Sn,Hn,Na,In,Ln,Se,Ot,He,y,Un,Gt,Yn,Rn,Sa,Wn,Mn,Pt,zn,Bn,Ha,Kn,Gn,Ft,Jn,Qn,Nt,Vn,Xn,Ie,Q,dt,Ia,St,Zn,La,to,Le,f,ao,Ua,eo,so,Ya,no,oo,Ra,lo,ro,Wa,io,po,Ma,ho,co,za,fo,uo,Ba,mo,go,Ue,U,yo,Ka,wo,vo,Ga,_o,bo,Ye,Y,Ht,jo,Ja,Eo,$o,Do,S,ko,Qa,To,xo,Va,qo,Co,Xa,Ao,Oo,Po,O,Fo,Za,No,So,te,Ho,Io,ae,Lo,Uo,ee,Yo,Ro,Re,V,ht,se,It,Wo,ne,Mo,We,ct,zo,oe,Bo,Ko,Me;return A=new mt({}),gt=new mt({}),yt=new X({props:{code:`from datasets import Dataset
data = [[1, 2],[3, 4]]
ds = Dataset.from_dict({"data": [[1, 2],[3, 4]]})
ds = ds.with_format("tf")
ds[0]
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=int64, numpy=array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])&gt;}
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
       [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])&gt;}`}}),st=new Ar({props:{$$slots:{default:[Or]},$$scope:{ctx:le}}}),wt=new X({props:{code:"ds[:]",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
       [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])&gt;}`}}),vt=new mt({}),_t=new X({props:{code:`from datasets import Dataset
data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]
ds = Dataset.from_dict({"data": data})
ds = ds.with_format("tf")
ds[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]],[[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>],[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]]
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: data})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.RaggedTensor [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]&gt;}`}}),bt=new X({props:{code:`from datasets import Dataset, Features, Array2D
data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]
features = Features({"data": Array2D(shape=(2, 2), dtype='int32')})
ds = Dataset.from_dict({"data": data}, features=features)
ds = ds.with_format("tf")
ds[0]
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, Features, Array2D
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]],[[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>],[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]]
<span class="hljs-meta">&gt;&gt;&gt; </span>features = Features({<span class="hljs-string">&quot;data&quot;</span>: Array2D(shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=<span class="hljs-string">&#x27;int32&#x27;</span>)})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: data}, features=features)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
 array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
        [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])&gt;}
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=int64, numpy=
 array([[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
         [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]],
 
        [[<span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
         [<span class="hljs-number">7</span>, <span class="hljs-number">8</span>]]])&gt;}`}}),jt=new mt({}),$t=new X({props:{code:`from datasets import Dataset, Features, ClassLabel
data = [0, 0, 1]
features = Features({"data": ClassLabel(names=["negative", "positive"])})
ds = Dataset.from_dict({"data": data}, features=features) 
ds = ds.with_format("tf")  
ds[:3]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, Features, ClassLabel
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>features = Features({<span class="hljs-string">&quot;data&quot;</span>: ClassLabel(names=[<span class="hljs-string">&quot;negative&quot;</span>, <span class="hljs-string">&quot;positive&quot;</span>])})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;data&quot;</span>: data}, features=features) 
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>)  
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">3</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">3</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])&gt;`}}),Dt=new X({props:{code:`from datasets import Dataset, Features 
text = ["foo", "bar"]
data = [0, 1] 
ds = Dataset.from_dict({"text": text, "data": data})  
ds = ds.with_format("tf") 
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, Features 
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [<span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;bar&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>data = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>] 
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict({<span class="hljs-string">&quot;text&quot;</span>: text, <span class="hljs-string">&quot;data&quot;</span>: data})  
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>) 
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;text&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=string, numpy=array([<span class="hljs-string">b&#x27;foo&#x27;</span>, <span class="hljs-string">b&#x27;bar&#x27;</span>], dtype=<span class="hljs-built_in">object</span>)&gt;,
 <span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])&gt;}`}}),kt=new X({props:{code:`ds = ds.with_format("tf", columns=["data"], output_all_columns=True)
ds[:2]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.with_format(<span class="hljs-string">&quot;tf&quot;</span>, columns=[<span class="hljs-string">&quot;data&quot;</span>], output_all_columns=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds[:<span class="hljs-number">2</span>]
{<span class="hljs-string">&#x27;data&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=int64, numpy=array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])&gt;,
 <span class="hljs-string">&#x27;text&#x27;</span>: [<span class="hljs-string">&#x27;foo&#x27;</span>, <span class="hljs-string">&#x27;bar&#x27;</span>]}`}}),Tt=new mt({}),Ct=new mt({}),At=new X({props:{code:`from datasets import Dataset
data = {"inputs": [[1, 2],[3, 4]], "labels": [0, 1]}
ds = Dataset.from_dict(data)
tf_ds = ds.to_tf_dataset(`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>data = {<span class="hljs-string">&quot;inputs&quot;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]], <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_dict(data)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_ds = ds.to_tf_dataset(
            columns=[<span class="hljs-string">&quot;inputs&quot;</span>],
            label_cols=[<span class="hljs-string">&quot;labels&quot;</span>],
            batch_size=<span class="hljs-number">2</span>,
            shuffle=<span class="hljs-literal">True</span>
            )`}}),Ot=new X({props:{code:"model.fit(tf_ds, epochs=2)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(tf_ds, epochs=<span class="hljs-number">2</span>)'}}),St=new mt({}),It=new mt({}),{c(){u=o("meta"),Z=h(),$=o("h1"),P=o("a"),M=o("span"),w(A.$$.fragment),H=h(),tt=o("span"),ss=s("Using Datasets with TensorFlow"),re=h(),T=o("p"),ns=s("This document is a quick introduction to using "),Zt=o("code"),os=s("datasets"),ls=s(` with TensorFlow, with a particular focus on how to get
`),ta=o("code"),rs=s("tf.Tensor"),is=s(" objects out of our datasets, and how to stream data from Hugging Face "),aa=o("code"),ps=s("Dataset"),ds=s(` objects to Keras methods
like `),ea=o("code"),hs=s("model.fit()"),cs=s("."),ie=h(),z=o("h2"),at=o("a"),sa=o("span"),w(gt.$$.fragment),fs=h(),na=o("span"),us=s("Dataset format"),pe=h(),Ut=o("p"),ms=s("By default, datasets return regular Python objects: integers, floats, strings, lists, etc."),de=h(),et=o("p"),gs=s("To get TensorFlow tensors instead, you can set the format of the dataset to "),oa=o("code"),ys=s("tf"),ws=s(":"),he=h(),w(yt.$$.fragment),ce=h(),w(st.$$.fragment),fe=h(),I=o("p"),vs=s("This can be useful for converting your dataset to a dict of "),la=o("code"),_s=s("Tensor"),bs=s(` objects, or for writing a generator to load TF
samples from it. If you wish to convert the entire dataset to `),ra=o("code"),js=s("Tensor"),Es=s(", simply query the full dataset:"),ue=h(),w(wt.$$.fragment),me=h(),B=o("h2"),nt=o("a"),ia=o("span"),w(vt.$$.fragment),$s=h(),pa=o("span"),Ds=s("N-dimensional arrays"),ge=h(),ot=o("p"),ks=s(`If your dataset consists of N-dimensional arrays, you will see that by default they are considered as nested lists.
In particular, a TensorFlow formatted dataset outputs a `),da=o("code"),Ts=s("RaggedTensor"),xs=s(" instead of a single tensor:"),ye=h(),w(_t.$$.fragment),we=h(),Yt=o("p"),qs=s("To get a single tensor, you must explicitly use the Array feature type and specify the shape of your tensors:"),ve=h(),w(bt.$$.fragment),_e=h(),K=o("h2"),lt=o("a"),ha=o("span"),w(jt.$$.fragment),Cs=h(),ca=o("span"),As=s("Other feature types"),be=h(),Et=o("p"),Rt=o("a"),Os=s("ClassLabel"),Ps=s(" data are properly converted to tensors:"),je=h(),w($t.$$.fragment),Ee=h(),Wt=o("p"),Fs=s("Strings are also supported:"),$e=h(),w(Dt.$$.fragment),De=h(),Mt=o("p"),Ns=s("You can also explicitly format certain columns and leave the other columns unformatted:"),ke=h(),w(kt.$$.fragment),Te=h(),L=o("p"),Ss=s("The "),zt=o("a"),Hs=s("Image"),Is=s(" and "),Bt=o("a"),Ls=s("Audio"),Us=s(" feature types are not supported yet."),xe=h(),G=o("h2"),rt=o("a"),fa=o("span"),w(Tt.$$.fragment),Ys=h(),ua=o("span"),Rs=s("Data loading"),qe=h(),D=o("p"),Ws=s(`Although you can load individual samples and batches just by indexing into your dataset, this won\u2019t work if you want
to use Keras methods like `),ma=o("code"),Ms=s("fit()"),zs=s(" and "),ga=o("code"),Bs=s("predict()"),Ks=s(`. You could write a generator function that shuffles and loads batches
from your dataset and `),ya=o("code"),Gs=s("fit()"),Js=s(` on that, but that sounds like a lot of unnecessary work. Instead, if you want to stream
data from your dataset on-the-fly, we recommend converting your dataset to a `),wa=o("code"),Qs=s("tf.data.Dataset"),Vs=s(` using the
`),va=o("code"),Xs=s("to_tf_dataset()"),Zs=s(" method."),Ce=h(),m=o("p"),tn=s("The "),_a=o("code"),an=s("tf.data.Dataset"),en=s(` class covers a wide range of use-cases - it is often created from Tensors in memory, or using a load function to read files on disc
or external storage. The dataset can be transformed arbitrarily with the `),ba=o("code"),sn=s("map()"),nn=s(" method, or methods like "),ja=o("code"),on=s("batch()"),ln=s(`
and `),Ea=o("code"),rn=s("shuffle()"),pn=s(` can be used to create a dataset that\u2019s ready for training. These methods do not modify the stored data
in any way - instead, the methods build a data pipeline graph that will be executed when the dataset is iterated over,
usually during model training or inference. This is different from the `),$a=o("code"),dn=s("map()"),hn=s(" method of Hugging Face "),Da=o("code"),cn=s("Dataset"),fn=s(` objects,
which runs the map function immediately and saves the new or changed columns.`),Ae=h(),g=o("p"),un=s("Since the entire data preprocessing pipeline can be compiled in a "),ka=o("code"),mn=s("tf.data.Dataset"),gn=s(`, this approach allows for massively
parallel, asynchronous data loading and training. However, the requirement for graph compilation can be a limitation,
particularly for Hugging Face tokenizers, which are usually not (yet!) compilable as part of a TF graph. As a result,
we usually advise pre-processing the dataset as a Hugging Face dataset, where arbitrary Python functions can be
used, and then converting to `),Ta=o("code"),yn=s("tf.data.Dataset"),wn=s(" afterwards using "),xa=o("code"),vn=s("to_tf_dataset()"),_n=s(` to get a batched dataset ready for
training. To see examples of this approach, please see the `),xt=o("a"),bn=s("examples"),jn=s(" or "),qt=o("a"),En=s("notebooks"),$n=s(" for "),qa=o("code"),Dn=s("transformers"),kn=s("."),Oe=h(),J=o("h3"),it=o("a"),Ca=o("span"),w(Ct.$$.fragment),Tn=h(),Kt=o("span"),xn=s("Using "),Aa=o("code"),qn=s("to_tf_dataset()"),Pe=h(),pt=o("p"),Cn=s("Using "),Oa=o("code"),An=s("to_tf_dataset()"),On=s(" is straightforward. Once your dataset is preprocessed and ready, simply call it like so:"),Fe=h(),w(At.$$.fragment),Ne=h(),F=o("p"),Pn=s("The returned "),Pa=o("code"),Fn=s("tf_ds"),Nn=s(" object here is now fully ready to train on, and can be passed directly to "),Fa=o("code"),Sn=s("model.fit()"),Hn=s(`! Note
that you set the batch size when creating the dataset, and so you don\u2019t need to specify it when calling `),Na=o("code"),In=s("fit()"),Ln=s(":"),Se=h(),w(Ot.$$.fragment),He=h(),y=o("p"),Un=s("For a full description of the arguments, please see the "),Gt=o("a"),Yn=s("to_tf_dataset()"),Rn=s(` documentation. In many cases,
you will also need to add a `),Sa=o("code"),Wn=s("collate_fn"),Mn=s(` to your call. This is a function that takes multiple elements of the dataset
and combines them into a single batch. When all elements have the same length, the built-in default collator will
suffice, but for more complex tasks a custom collator may be necessary. In particular, many tasks have samples
with varying sequence lengths which will require a `),Pt=o("a"),zn=s("data collator"),Bn=s(` that can pad batches correctly. You can see examples
of this in the `),Ha=o("code"),Kn=s("transformers"),Gn=s(" NLP "),Ft=o("a"),Jn=s("examples"),Qn=s(` and
`),Nt=o("a"),Vn=s("notebooks"),Xn=s(", where variable sequence lengths are very common."),Ie=h(),Q=o("h3"),dt=o("a"),Ia=o("span"),w(St.$$.fragment),Zn=h(),La=o("span"),to=s("When to use to_tf_dataset"),Le=h(),f=o("p"),ao=s(`The astute reader may have noticed at this point that we have offered two approaches to achieve the same goal - if you
want to pass your dataset to a TensorFlow model, you can either convert the dataset to a `),Ua=o("code"),eo=s("Tensor"),so=s(" or "),Ya=o("code"),no=s("dict"),oo=s(" of "),Ra=o("code"),lo=s("Tensors"),ro=s(`
using `),Wa=o("code"),io=s(".with_format('tf')"),po=s(", or you can convert the dataset to a "),Ma=o("code"),ho=s("tf.data.Dataset"),co=s(" with "),za=o("code"),fo=s("to_tf_dataset()"),uo=s(`. Either of these
can be passed to `),Ba=o("code"),mo=s("model.fit()"),go=s(", so which should you choose?"),Ue=h(),U=o("p"),yo=s("The key thing to recognize is that when you convert the whole dataset to "),Ka=o("code"),wo=s("Tensor"),vo=s(`s, it is static and fully loaded into
RAM. This is simple and convenient, but if any of the following apply, you should probably use `),Ga=o("code"),_o=s("to_tf_dataset()"),bo=s(`
instead:`),Ye=h(),Y=o("ul"),Ht=o("li"),jo=s("Your dataset is too large to fit in RAM. "),Ja=o("code"),Eo=s("to_tf_dataset()"),$o=s(` streams only one batch at a time, so even very large
datasets can be handled with this method.`),Do=h(),S=o("li"),ko=s("You want to apply random transformations using "),Qa=o("code"),To=s("dataset.with_transform()"),xo=s(" or the "),Va=o("code"),qo=s("collate_fn"),Co=s(`. This is
common in several modalities, such as image augmentations when training vision models, or random masking when training
masked language models. Using `),Xa=o("code"),Ao=s("to_tf_dataset()"),Oo=s(` will apply those transformations
at the moment when a batch is loaded, which means the same samples will get different augmentations each time
they are loaded. This is usually what you want.`),Po=h(),O=o("li"),Fo=s(`Your data has a variable dimension, such as input texts in NLP that consist of varying
numbers of tokens. When you create a batch with samples with a variable dimension, the standard solution is to
pad the shorter samples to the length of the longest one. When you stream samples from a dataset with `),Za=o("code"),No=s("to_tf_dataset"),So=s(`,
you can apply this padding to each batch via your `),te=o("code"),Ho=s("collate_fn"),Io=s(`. However, if you want to convert
such a dataset to dense `),ae=o("code"),Lo=s("Tensor"),Uo=s("s, then you will have to pad samples to the length of the longest sample in "),ee=o("em"),Yo=s(`the
entire dataset!`),Ro=s(" This can result in huge amounts of padding, which wastes memory and reduces your model\u2019s speed."),Re=h(),V=o("h3"),ht=o("a"),se=o("span"),w(It.$$.fragment),Wo=h(),ne=o("span"),Mo=s("Caveats and limitations"),We=h(),ct=o("p"),zo=s("Right now, "),oe=o("code"),Bo=s("to_tf_dataset()"),Ko=s(" always return a batched dataset - we will add support for unbatched datasets soon!"),this.h()},l(t){const i=qr('[data-svelte="svelte-1phssyn"]',document.head);u=l(i,"META",{name:!0,content:!0}),i.forEach(e),Z=c(t),$=l(t,"H1",{class:!0});var Lt=r($);P=l(Lt,"A",{id:!0,class:!0,href:!0});var Qo=r(P);M=l(Qo,"SPAN",{});var Vo=r(M);v(A.$$.fragment,Vo),Vo.forEach(e),Qo.forEach(e),H=c(Lt),tt=l(Lt,"SPAN",{});var Xo=r(tt);ss=n(Xo,"Using Datasets with TensorFlow"),Xo.forEach(e),Lt.forEach(e),re=c(t),T=l(t,"P",{});var R=r(T);ns=n(R,"This document is a quick introduction to using "),Zt=l(R,"CODE",{});var Zo=r(Zt);os=n(Zo,"datasets"),Zo.forEach(e),ls=n(R,` with TensorFlow, with a particular focus on how to get
`),ta=l(R,"CODE",{});var tl=r(ta);rs=n(tl,"tf.Tensor"),tl.forEach(e),is=n(R," objects out of our datasets, and how to stream data from Hugging Face "),aa=l(R,"CODE",{});var al=r(aa);ps=n(al,"Dataset"),al.forEach(e),ds=n(R,` objects to Keras methods
like `),ea=l(R,"CODE",{});var el=r(ea);hs=n(el,"model.fit()"),el.forEach(e),cs=n(R,"."),R.forEach(e),ie=c(t),z=l(t,"H2",{class:!0});var ze=r(z);at=l(ze,"A",{id:!0,class:!0,href:!0});var sl=r(at);sa=l(sl,"SPAN",{});var nl=r(sa);v(gt.$$.fragment,nl),nl.forEach(e),sl.forEach(e),fs=c(ze),na=l(ze,"SPAN",{});var ol=r(na);us=n(ol,"Dataset format"),ol.forEach(e),ze.forEach(e),pe=c(t),Ut=l(t,"P",{});var ll=r(Ut);ms=n(ll,"By default, datasets return regular Python objects: integers, floats, strings, lists, etc."),ll.forEach(e),de=c(t),et=l(t,"P",{});var Be=r(et);gs=n(Be,"To get TensorFlow tensors instead, you can set the format of the dataset to "),oa=l(Be,"CODE",{});var rl=r(oa);ys=n(rl,"tf"),rl.forEach(e),ws=n(Be,":"),Be.forEach(e),he=c(t),v(yt.$$.fragment,t),ce=c(t),v(st.$$.fragment,t),fe=c(t),I=l(t,"P",{});var Jt=r(I);vs=n(Jt,"This can be useful for converting your dataset to a dict of "),la=l(Jt,"CODE",{});var il=r(la);_s=n(il,"Tensor"),il.forEach(e),bs=n(Jt,` objects, or for writing a generator to load TF
samples from it. If you wish to convert the entire dataset to `),ra=l(Jt,"CODE",{});var pl=r(ra);js=n(pl,"Tensor"),pl.forEach(e),Es=n(Jt,", simply query the full dataset:"),Jt.forEach(e),ue=c(t),v(wt.$$.fragment,t),me=c(t),B=l(t,"H2",{class:!0});var Ke=r(B);nt=l(Ke,"A",{id:!0,class:!0,href:!0});var dl=r(nt);ia=l(dl,"SPAN",{});var hl=r(ia);v(vt.$$.fragment,hl),hl.forEach(e),dl.forEach(e),$s=c(Ke),pa=l(Ke,"SPAN",{});var cl=r(pa);Ds=n(cl,"N-dimensional arrays"),cl.forEach(e),Ke.forEach(e),ge=c(t),ot=l(t,"P",{});var Ge=r(ot);ks=n(Ge,`If your dataset consists of N-dimensional arrays, you will see that by default they are considered as nested lists.
In particular, a TensorFlow formatted dataset outputs a `),da=l(Ge,"CODE",{});var fl=r(da);Ts=n(fl,"RaggedTensor"),fl.forEach(e),xs=n(Ge," instead of a single tensor:"),Ge.forEach(e),ye=c(t),v(_t.$$.fragment,t),we=c(t),Yt=l(t,"P",{});var ul=r(Yt);qs=n(ul,"To get a single tensor, you must explicitly use the Array feature type and specify the shape of your tensors:"),ul.forEach(e),ve=c(t),v(bt.$$.fragment,t),_e=c(t),K=l(t,"H2",{class:!0});var Je=r(K);lt=l(Je,"A",{id:!0,class:!0,href:!0});var ml=r(lt);ha=l(ml,"SPAN",{});var gl=r(ha);v(jt.$$.fragment,gl),gl.forEach(e),ml.forEach(e),Cs=c(Je),ca=l(Je,"SPAN",{});var yl=r(ca);As=n(yl,"Other feature types"),yl.forEach(e),Je.forEach(e),be=c(t),Et=l(t,"P",{});var Go=r(Et);Rt=l(Go,"A",{href:!0});var wl=r(Rt);Os=n(wl,"ClassLabel"),wl.forEach(e),Ps=n(Go," data are properly converted to tensors:"),Go.forEach(e),je=c(t),v($t.$$.fragment,t),Ee=c(t),Wt=l(t,"P",{});var vl=r(Wt);Fs=n(vl,"Strings are also supported:"),vl.forEach(e),$e=c(t),v(Dt.$$.fragment,t),De=c(t),Mt=l(t,"P",{});var _l=r(Mt);Ns=n(_l,"You can also explicitly format certain columns and leave the other columns unformatted:"),_l.forEach(e),ke=c(t),v(kt.$$.fragment,t),Te=c(t),L=l(t,"P",{});var Qt=r(L);Ss=n(Qt,"The "),zt=l(Qt,"A",{href:!0});var bl=r(zt);Hs=n(bl,"Image"),bl.forEach(e),Is=n(Qt," and "),Bt=l(Qt,"A",{href:!0});var jl=r(Bt);Ls=n(jl,"Audio"),jl.forEach(e),Us=n(Qt," feature types are not supported yet."),Qt.forEach(e),xe=c(t),G=l(t,"H2",{class:!0});var Qe=r(G);rt=l(Qe,"A",{id:!0,class:!0,href:!0});var El=r(rt);fa=l(El,"SPAN",{});var $l=r(fa);v(Tt.$$.fragment,$l),$l.forEach(e),El.forEach(e),Ys=c(Qe),ua=l(Qe,"SPAN",{});var Dl=r(ua);Rs=n(Dl,"Data loading"),Dl.forEach(e),Qe.forEach(e),qe=c(t),D=l(t,"P",{});var N=r(D);Ws=n(N,`Although you can load individual samples and batches just by indexing into your dataset, this won\u2019t work if you want
to use Keras methods like `),ma=l(N,"CODE",{});var kl=r(ma);Ms=n(kl,"fit()"),kl.forEach(e),zs=n(N," and "),ga=l(N,"CODE",{});var Tl=r(ga);Bs=n(Tl,"predict()"),Tl.forEach(e),Ks=n(N,`. You could write a generator function that shuffles and loads batches
from your dataset and `),ya=l(N,"CODE",{});var xl=r(ya);Gs=n(xl,"fit()"),xl.forEach(e),Js=n(N,` on that, but that sounds like a lot of unnecessary work. Instead, if you want to stream
data from your dataset on-the-fly, we recommend converting your dataset to a `),wa=l(N,"CODE",{});var ql=r(wa);Qs=n(ql,"tf.data.Dataset"),ql.forEach(e),Vs=n(N,` using the
`),va=l(N,"CODE",{});var Cl=r(va);Xs=n(Cl,"to_tf_dataset()"),Cl.forEach(e),Zs=n(N," method."),N.forEach(e),Ce=c(t),m=l(t,"P",{});var x=r(m);tn=n(x,"The "),_a=l(x,"CODE",{});var Al=r(_a);an=n(Al,"tf.data.Dataset"),Al.forEach(e),en=n(x,` class covers a wide range of use-cases - it is often created from Tensors in memory, or using a load function to read files on disc
or external storage. The dataset can be transformed arbitrarily with the `),ba=l(x,"CODE",{});var Ol=r(ba);sn=n(Ol,"map()"),Ol.forEach(e),nn=n(x," method, or methods like "),ja=l(x,"CODE",{});var Pl=r(ja);on=n(Pl,"batch()"),Pl.forEach(e),ln=n(x,`
and `),Ea=l(x,"CODE",{});var Fl=r(Ea);rn=n(Fl,"shuffle()"),Fl.forEach(e),pn=n(x,` can be used to create a dataset that\u2019s ready for training. These methods do not modify the stored data
in any way - instead, the methods build a data pipeline graph that will be executed when the dataset is iterated over,
usually during model training or inference. This is different from the `),$a=l(x,"CODE",{});var Nl=r($a);dn=n(Nl,"map()"),Nl.forEach(e),hn=n(x," method of Hugging Face "),Da=l(x,"CODE",{});var Sl=r(Da);cn=n(Sl,"Dataset"),Sl.forEach(e),fn=n(x,` objects,
which runs the map function immediately and saves the new or changed columns.`),x.forEach(e),Ae=c(t),g=l(t,"P",{});var q=r(g);un=n(q,"Since the entire data preprocessing pipeline can be compiled in a "),ka=l(q,"CODE",{});var Hl=r(ka);mn=n(Hl,"tf.data.Dataset"),Hl.forEach(e),gn=n(q,`, this approach allows for massively
parallel, asynchronous data loading and training. However, the requirement for graph compilation can be a limitation,
particularly for Hugging Face tokenizers, which are usually not (yet!) compilable as part of a TF graph. As a result,
we usually advise pre-processing the dataset as a Hugging Face dataset, where arbitrary Python functions can be
used, and then converting to `),Ta=l(q,"CODE",{});var Il=r(Ta);yn=n(Il,"tf.data.Dataset"),Il.forEach(e),wn=n(q," afterwards using "),xa=l(q,"CODE",{});var Ll=r(xa);vn=n(Ll,"to_tf_dataset()"),Ll.forEach(e),_n=n(q,` to get a batched dataset ready for
training. To see examples of this approach, please see the `),xt=l(q,"A",{href:!0,rel:!0});var Ul=r(xt);bn=n(Ul,"examples"),Ul.forEach(e),jn=n(q," or "),qt=l(q,"A",{href:!0,rel:!0});var Yl=r(qt);En=n(Yl,"notebooks"),Yl.forEach(e),$n=n(q," for "),qa=l(q,"CODE",{});var Rl=r(qa);Dn=n(Rl,"transformers"),Rl.forEach(e),kn=n(q,"."),q.forEach(e),Oe=c(t),J=l(t,"H3",{class:!0});var Ve=r(J);it=l(Ve,"A",{id:!0,class:!0,href:!0});var Wl=r(it);Ca=l(Wl,"SPAN",{});var Ml=r(Ca);v(Ct.$$.fragment,Ml),Ml.forEach(e),Wl.forEach(e),Tn=c(Ve),Kt=l(Ve,"SPAN",{});var Jo=r(Kt);xn=n(Jo,"Using "),Aa=l(Jo,"CODE",{});var zl=r(Aa);qn=n(zl,"to_tf_dataset()"),zl.forEach(e),Jo.forEach(e),Ve.forEach(e),Pe=c(t),pt=l(t,"P",{});var Xe=r(pt);Cn=n(Xe,"Using "),Oa=l(Xe,"CODE",{});var Bl=r(Oa);An=n(Bl,"to_tf_dataset()"),Bl.forEach(e),On=n(Xe," is straightforward. Once your dataset is preprocessed and ready, simply call it like so:"),Xe.forEach(e),Fe=c(t),v(At.$$.fragment,t),Ne=c(t),F=l(t,"P",{});var ft=r(F);Pn=n(ft,"The returned "),Pa=l(ft,"CODE",{});var Kl=r(Pa);Fn=n(Kl,"tf_ds"),Kl.forEach(e),Nn=n(ft," object here is now fully ready to train on, and can be passed directly to "),Fa=l(ft,"CODE",{});var Gl=r(Fa);Sn=n(Gl,"model.fit()"),Gl.forEach(e),Hn=n(ft,`! Note
that you set the batch size when creating the dataset, and so you don\u2019t need to specify it when calling `),Na=l(ft,"CODE",{});var Jl=r(Na);In=n(Jl,"fit()"),Jl.forEach(e),Ln=n(ft,":"),ft.forEach(e),Se=c(t),v(Ot.$$.fragment,t),He=c(t),y=l(t,"P",{});var C=r(y);Un=n(C,"For a full description of the arguments, please see the "),Gt=l(C,"A",{href:!0});var Ql=r(Gt);Yn=n(Ql,"to_tf_dataset()"),Ql.forEach(e),Rn=n(C,` documentation. In many cases,
you will also need to add a `),Sa=l(C,"CODE",{});var Vl=r(Sa);Wn=n(Vl,"collate_fn"),Vl.forEach(e),Mn=n(C,` to your call. This is a function that takes multiple elements of the dataset
and combines them into a single batch. When all elements have the same length, the built-in default collator will
suffice, but for more complex tasks a custom collator may be necessary. In particular, many tasks have samples
with varying sequence lengths which will require a `),Pt=l(C,"A",{href:!0,rel:!0});var Xl=r(Pt);zn=n(Xl,"data collator"),Xl.forEach(e),Bn=n(C,` that can pad batches correctly. You can see examples
of this in the `),Ha=l(C,"CODE",{});var Zl=r(Ha);Kn=n(Zl,"transformers"),Zl.forEach(e),Gn=n(C," NLP "),Ft=l(C,"A",{href:!0,rel:!0});var tr=r(Ft);Jn=n(tr,"examples"),tr.forEach(e),Qn=n(C,` and
`),Nt=l(C,"A",{href:!0,rel:!0});var ar=r(Nt);Vn=n(ar,"notebooks"),ar.forEach(e),Xn=n(C,", where variable sequence lengths are very common."),C.forEach(e),Ie=c(t),Q=l(t,"H3",{class:!0});var Ze=r(Q);dt=l(Ze,"A",{id:!0,class:!0,href:!0});var er=r(dt);Ia=l(er,"SPAN",{});var sr=r(Ia);v(St.$$.fragment,sr),sr.forEach(e),er.forEach(e),Zn=c(Ze),La=l(Ze,"SPAN",{});var nr=r(La);to=n(nr,"When to use to_tf_dataset"),nr.forEach(e),Ze.forEach(e),Le=c(t),f=l(t,"P",{});var k=r(f);ao=n(k,`The astute reader may have noticed at this point that we have offered two approaches to achieve the same goal - if you
want to pass your dataset to a TensorFlow model, you can either convert the dataset to a `),Ua=l(k,"CODE",{});var or=r(Ua);eo=n(or,"Tensor"),or.forEach(e),so=n(k," or "),Ya=l(k,"CODE",{});var lr=r(Ya);no=n(lr,"dict"),lr.forEach(e),oo=n(k," of "),Ra=l(k,"CODE",{});var rr=r(Ra);lo=n(rr,"Tensors"),rr.forEach(e),ro=n(k,`
using `),Wa=l(k,"CODE",{});var ir=r(Wa);io=n(ir,".with_format('tf')"),ir.forEach(e),po=n(k,", or you can convert the dataset to a "),Ma=l(k,"CODE",{});var pr=r(Ma);ho=n(pr,"tf.data.Dataset"),pr.forEach(e),co=n(k," with "),za=l(k,"CODE",{});var dr=r(za);fo=n(dr,"to_tf_dataset()"),dr.forEach(e),uo=n(k,`. Either of these
can be passed to `),Ba=l(k,"CODE",{});var hr=r(Ba);mo=n(hr,"model.fit()"),hr.forEach(e),go=n(k,", so which should you choose?"),k.forEach(e),Ue=c(t),U=l(t,"P",{});var Vt=r(U);yo=n(Vt,"The key thing to recognize is that when you convert the whole dataset to "),Ka=l(Vt,"CODE",{});var cr=r(Ka);wo=n(cr,"Tensor"),cr.forEach(e),vo=n(Vt,`s, it is static and fully loaded into
RAM. This is simple and convenient, but if any of the following apply, you should probably use `),Ga=l(Vt,"CODE",{});var fr=r(Ga);_o=n(fr,"to_tf_dataset()"),fr.forEach(e),bo=n(Vt,`
instead:`),Vt.forEach(e),Ye=c(t),Y=l(t,"UL",{});var Xt=r(Y);Ht=l(Xt,"LI",{});var ts=r(Ht);jo=n(ts,"Your dataset is too large to fit in RAM. "),Ja=l(ts,"CODE",{});var ur=r(Ja);Eo=n(ur,"to_tf_dataset()"),ur.forEach(e),$o=n(ts,` streams only one batch at a time, so even very large
datasets can be handled with this method.`),ts.forEach(e),Do=c(Xt),S=l(Xt,"LI",{});var ut=r(S);ko=n(ut,"You want to apply random transformations using "),Qa=l(ut,"CODE",{});var mr=r(Qa);To=n(mr,"dataset.with_transform()"),mr.forEach(e),xo=n(ut," or the "),Va=l(ut,"CODE",{});var gr=r(Va);qo=n(gr,"collate_fn"),gr.forEach(e),Co=n(ut,`. This is
common in several modalities, such as image augmentations when training vision models, or random masking when training
masked language models. Using `),Xa=l(ut,"CODE",{});var yr=r(Xa);Ao=n(yr,"to_tf_dataset()"),yr.forEach(e),Oo=n(ut,` will apply those transformations
at the moment when a batch is loaded, which means the same samples will get different augmentations each time
they are loaded. This is usually what you want.`),ut.forEach(e),Po=c(Xt),O=l(Xt,"LI",{});var W=r(O);Fo=n(W,`Your data has a variable dimension, such as input texts in NLP that consist of varying
numbers of tokens. When you create a batch with samples with a variable dimension, the standard solution is to
pad the shorter samples to the length of the longest one. When you stream samples from a dataset with `),Za=l(W,"CODE",{});var wr=r(Za);No=n(wr,"to_tf_dataset"),wr.forEach(e),So=n(W,`,
you can apply this padding to each batch via your `),te=l(W,"CODE",{});var vr=r(te);Ho=n(vr,"collate_fn"),vr.forEach(e),Io=n(W,`. However, if you want to convert
such a dataset to dense `),ae=l(W,"CODE",{});var _r=r(ae);Lo=n(_r,"Tensor"),_r.forEach(e),Uo=n(W,"s, then you will have to pad samples to the length of the longest sample in "),ee=l(W,"EM",{});var br=r(ee);Yo=n(br,`the
entire dataset!`),br.forEach(e),Ro=n(W," This can result in huge amounts of padding, which wastes memory and reduces your model\u2019s speed."),W.forEach(e),Xt.forEach(e),Re=c(t),V=l(t,"H3",{class:!0});var as=r(V);ht=l(as,"A",{id:!0,class:!0,href:!0});var jr=r(ht);se=l(jr,"SPAN",{});var Er=r(se);v(It.$$.fragment,Er),Er.forEach(e),jr.forEach(e),Wo=c(as),ne=l(as,"SPAN",{});var $r=r(ne);Mo=n($r,"Caveats and limitations"),$r.forEach(e),as.forEach(e),We=c(t),ct=l(t,"P",{});var es=r(ct);zo=n(es,"Right now, "),oe=l(es,"CODE",{});var Dr=r(oe);Bo=n(Dr,"to_tf_dataset()"),Dr.forEach(e),Ko=n(es," always return a batched dataset - we will add support for unbatched datasets soon!"),es.forEach(e),this.h()},h(){d(u,"name","hf:doc:metadata"),d(u,"content",JSON.stringify(Fr)),d(P,"id","using-datasets-with-tensorflow"),d(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(P,"href","#using-datasets-with-tensorflow"),d($,"class","relative group"),d(at,"id","dataset-format"),d(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(at,"href","#dataset-format"),d(z,"class","relative group"),d(nt,"id","ndimensional-arrays"),d(nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(nt,"href","#ndimensional-arrays"),d(B,"class","relative group"),d(lt,"id","other-feature-types"),d(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(lt,"href","#other-feature-types"),d(K,"class","relative group"),d(Rt,"href","/docs/datasets/pr_4613/en/package_reference/main_classes#datasets.ClassLabel"),d(zt,"href","/docs/datasets/pr_4613/en/package_reference/main_classes#datasets.Image"),d(Bt,"href","/docs/datasets/pr_4613/en/package_reference/main_classes#datasets.Audio"),d(rt,"id","data-loading"),d(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(rt,"href","#data-loading"),d(G,"class","relative group"),d(xt,"href","https://github.com/huggingface/transformers/tree/main/examples"),d(xt,"rel","nofollow"),d(qt,"href","https://huggingface.co/docs/transformers/notebooks"),d(qt,"rel","nofollow"),d(it,"id","using-totfdataset"),d(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(it,"href","#using-totfdataset"),d(J,"class","relative group"),d(Gt,"href","/docs/datasets/pr_4613/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset"),d(Pt,"href","https://huggingface.co/docs/transformers/main/en/main_classes/data_collator"),d(Pt,"rel","nofollow"),d(Ft,"href","https://github.com/huggingface/transformers/tree/main/examples"),d(Ft,"rel","nofollow"),d(Nt,"href","https://huggingface.co/docs/transformers/notebooks"),d(Nt,"rel","nofollow"),d(dt,"id","when-to-use-totfdataset"),d(dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(dt,"href","#when-to-use-totfdataset"),d(Q,"class","relative group"),d(ht,"id","caveats-and-limitations"),d(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ht,"href","#caveats-and-limitations"),d(V,"class","relative group")},m(t,i){a(document.head,u),p(t,Z,i),p(t,$,i),a($,P),a(P,M),_(A,M,null),a($,H),a($,tt),a(tt,ss),p(t,re,i),p(t,T,i),a(T,ns),a(T,Zt),a(Zt,os),a(T,ls),a(T,ta),a(ta,rs),a(T,is),a(T,aa),a(aa,ps),a(T,ds),a(T,ea),a(ea,hs),a(T,cs),p(t,ie,i),p(t,z,i),a(z,at),a(at,sa),_(gt,sa,null),a(z,fs),a(z,na),a(na,us),p(t,pe,i),p(t,Ut,i),a(Ut,ms),p(t,de,i),p(t,et,i),a(et,gs),a(et,oa),a(oa,ys),a(et,ws),p(t,he,i),_(yt,t,i),p(t,ce,i),_(st,t,i),p(t,fe,i),p(t,I,i),a(I,vs),a(I,la),a(la,_s),a(I,bs),a(I,ra),a(ra,js),a(I,Es),p(t,ue,i),_(wt,t,i),p(t,me,i),p(t,B,i),a(B,nt),a(nt,ia),_(vt,ia,null),a(B,$s),a(B,pa),a(pa,Ds),p(t,ge,i),p(t,ot,i),a(ot,ks),a(ot,da),a(da,Ts),a(ot,xs),p(t,ye,i),_(_t,t,i),p(t,we,i),p(t,Yt,i),a(Yt,qs),p(t,ve,i),_(bt,t,i),p(t,_e,i),p(t,K,i),a(K,lt),a(lt,ha),_(jt,ha,null),a(K,Cs),a(K,ca),a(ca,As),p(t,be,i),p(t,Et,i),a(Et,Rt),a(Rt,Os),a(Et,Ps),p(t,je,i),_($t,t,i),p(t,Ee,i),p(t,Wt,i),a(Wt,Fs),p(t,$e,i),_(Dt,t,i),p(t,De,i),p(t,Mt,i),a(Mt,Ns),p(t,ke,i),_(kt,t,i),p(t,Te,i),p(t,L,i),a(L,Ss),a(L,zt),a(zt,Hs),a(L,Is),a(L,Bt),a(Bt,Ls),a(L,Us),p(t,xe,i),p(t,G,i),a(G,rt),a(rt,fa),_(Tt,fa,null),a(G,Ys),a(G,ua),a(ua,Rs),p(t,qe,i),p(t,D,i),a(D,Ws),a(D,ma),a(ma,Ms),a(D,zs),a(D,ga),a(ga,Bs),a(D,Ks),a(D,ya),a(ya,Gs),a(D,Js),a(D,wa),a(wa,Qs),a(D,Vs),a(D,va),a(va,Xs),a(D,Zs),p(t,Ce,i),p(t,m,i),a(m,tn),a(m,_a),a(_a,an),a(m,en),a(m,ba),a(ba,sn),a(m,nn),a(m,ja),a(ja,on),a(m,ln),a(m,Ea),a(Ea,rn),a(m,pn),a(m,$a),a($a,dn),a(m,hn),a(m,Da),a(Da,cn),a(m,fn),p(t,Ae,i),p(t,g,i),a(g,un),a(g,ka),a(ka,mn),a(g,gn),a(g,Ta),a(Ta,yn),a(g,wn),a(g,xa),a(xa,vn),a(g,_n),a(g,xt),a(xt,bn),a(g,jn),a(g,qt),a(qt,En),a(g,$n),a(g,qa),a(qa,Dn),a(g,kn),p(t,Oe,i),p(t,J,i),a(J,it),a(it,Ca),_(Ct,Ca,null),a(J,Tn),a(J,Kt),a(Kt,xn),a(Kt,Aa),a(Aa,qn),p(t,Pe,i),p(t,pt,i),a(pt,Cn),a(pt,Oa),a(Oa,An),a(pt,On),p(t,Fe,i),_(At,t,i),p(t,Ne,i),p(t,F,i),a(F,Pn),a(F,Pa),a(Pa,Fn),a(F,Nn),a(F,Fa),a(Fa,Sn),a(F,Hn),a(F,Na),a(Na,In),a(F,Ln),p(t,Se,i),_(Ot,t,i),p(t,He,i),p(t,y,i),a(y,Un),a(y,Gt),a(Gt,Yn),a(y,Rn),a(y,Sa),a(Sa,Wn),a(y,Mn),a(y,Pt),a(Pt,zn),a(y,Bn),a(y,Ha),a(Ha,Kn),a(y,Gn),a(y,Ft),a(Ft,Jn),a(y,Qn),a(y,Nt),a(Nt,Vn),a(y,Xn),p(t,Ie,i),p(t,Q,i),a(Q,dt),a(dt,Ia),_(St,Ia,null),a(Q,Zn),a(Q,La),a(La,to),p(t,Le,i),p(t,f,i),a(f,ao),a(f,Ua),a(Ua,eo),a(f,so),a(f,Ya),a(Ya,no),a(f,oo),a(f,Ra),a(Ra,lo),a(f,ro),a(f,Wa),a(Wa,io),a(f,po),a(f,Ma),a(Ma,ho),a(f,co),a(f,za),a(za,fo),a(f,uo),a(f,Ba),a(Ba,mo),a(f,go),p(t,Ue,i),p(t,U,i),a(U,yo),a(U,Ka),a(Ka,wo),a(U,vo),a(U,Ga),a(Ga,_o),a(U,bo),p(t,Ye,i),p(t,Y,i),a(Y,Ht),a(Ht,jo),a(Ht,Ja),a(Ja,Eo),a(Ht,$o),a(Y,Do),a(Y,S),a(S,ko),a(S,Qa),a(Qa,To),a(S,xo),a(S,Va),a(Va,qo),a(S,Co),a(S,Xa),a(Xa,Ao),a(S,Oo),a(Y,Po),a(Y,O),a(O,Fo),a(O,Za),a(Za,No),a(O,So),a(O,te),a(te,Ho),a(O,Io),a(O,ae),a(ae,Lo),a(O,Uo),a(O,ee),a(ee,Yo),a(O,Ro),p(t,Re,i),p(t,V,i),a(V,ht),a(ht,se),_(It,se,null),a(V,Wo),a(V,ne),a(ne,Mo),p(t,We,i),p(t,ct,i),a(ct,zo),a(ct,oe),a(oe,Bo),a(ct,Ko),Me=!0},p(t,[i]){const Lt={};i&2&&(Lt.$$scope={dirty:i,ctx:t}),st.$set(Lt)},i(t){Me||(b(A.$$.fragment,t),b(gt.$$.fragment,t),b(yt.$$.fragment,t),b(st.$$.fragment,t),b(wt.$$.fragment,t),b(vt.$$.fragment,t),b(_t.$$.fragment,t),b(bt.$$.fragment,t),b(jt.$$.fragment,t),b($t.$$.fragment,t),b(Dt.$$.fragment,t),b(kt.$$.fragment,t),b(Tt.$$.fragment,t),b(Ct.$$.fragment,t),b(At.$$.fragment,t),b(Ot.$$.fragment,t),b(St.$$.fragment,t),b(It.$$.fragment,t),Me=!0)},o(t){j(A.$$.fragment,t),j(gt.$$.fragment,t),j(yt.$$.fragment,t),j(st.$$.fragment,t),j(wt.$$.fragment,t),j(vt.$$.fragment,t),j(_t.$$.fragment,t),j(bt.$$.fragment,t),j(jt.$$.fragment,t),j($t.$$.fragment,t),j(Dt.$$.fragment,t),j(kt.$$.fragment,t),j(Tt.$$.fragment,t),j(Ct.$$.fragment,t),j(At.$$.fragment,t),j(Ot.$$.fragment,t),j(St.$$.fragment,t),j(It.$$.fragment,t),Me=!1},d(t){e(u),t&&e(Z),t&&e($),E(A),t&&e(re),t&&e(T),t&&e(ie),t&&e(z),E(gt),t&&e(pe),t&&e(Ut),t&&e(de),t&&e(et),t&&e(he),E(yt,t),t&&e(ce),E(st,t),t&&e(fe),t&&e(I),t&&e(ue),E(wt,t),t&&e(me),t&&e(B),E(vt),t&&e(ge),t&&e(ot),t&&e(ye),E(_t,t),t&&e(we),t&&e(Yt),t&&e(ve),E(bt,t),t&&e(_e),t&&e(K),E(jt),t&&e(be),t&&e(Et),t&&e(je),E($t,t),t&&e(Ee),t&&e(Wt),t&&e($e),E(Dt,t),t&&e(De),t&&e(Mt),t&&e(ke),E(kt,t),t&&e(Te),t&&e(L),t&&e(xe),t&&e(G),E(Tt),t&&e(qe),t&&e(D),t&&e(Ce),t&&e(m),t&&e(Ae),t&&e(g),t&&e(Oe),t&&e(J),E(Ct),t&&e(Pe),t&&e(pt),t&&e(Fe),E(At,t),t&&e(Ne),t&&e(F),t&&e(Se),E(Ot,t),t&&e(He),t&&e(y),t&&e(Ie),t&&e(Q),E(St),t&&e(Le),t&&e(f),t&&e(Ue),t&&e(U),t&&e(Ye),t&&e(Y),t&&e(Re),t&&e(V),E(It),t&&e(We),t&&e(ct)}}}const Fr={local:"using-datasets-with-tensorflow",sections:[{local:"dataset-format",title:"Dataset format"},{local:"ndimensional-arrays",title:"N-dimensional arrays"},{local:"other-feature-types",title:"Other feature types"},{local:"data-loading",sections:[{local:"using-totfdataset",title:"Using `to_tf_dataset()`"},{local:"when-to-use-totfdataset",title:"When to use to_tf_dataset"},{local:"caveats-and-limitations",title:"Caveats and limitations"}],title:"Data loading"}],title:"Using Datasets with TensorFlow"};function Nr(le){return Cr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ur extends kr{constructor(u){super();Tr(this,u,Nr,Pr,xr,{})}}export{Ur as default,Fr as metadata};
