import{S as tu,i as au,s as eu,e as l,k as d,w as u,t as r,M as lu,c as o,d as t,m as f,a as n,x as m,h as i,b as c,G as a,g as p,y as g,q as _,o as v,B as $,v as ou}from"../chunks/vendor-hf-doc-builder.js";import{T as ya}from"../chunks/Tip-hf-doc-builder.js";import{I as A}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as E}from"../chunks/CodeBlock-hf-doc-builder.js";import{C as Wl}from"../chunks/CodeBlockFw-hf-doc-builder.js";import"../chunks/IconTensorflow-hf-doc-builder.js";function nu(T){let h,k,y,b,x;return{c(){h=l("p"),k=r("Refer to the "),y=l("a"),b=r("Upload"),x=r(" guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"Refer to the "),y=o(j,"A",{href:!0});var q=n(y);b=i(q,"Upload"),q.forEach(t),x=i(j," guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),j.forEach(t),this.h()},h(){c(y,"href","./upload_dataset")},m(w,j){p(w,h,j),a(h,k),a(h,y),a(y,b),a(h,x)},d(w){w&&t(h)}}}function ru(T){let h,k,y,b,x;return{c(){h=l("p"),k=r("If you don\u2019t specify which data files to use, "),y=l("code"),b=r("load_dataset"),x=r(" will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data.")},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"If you don\u2019t specify which data files to use, "),y=o(j,"CODE",{});var q=n(y);b=i(q,"load_dataset"),q.forEach(t),x=i(j," will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data."),j.forEach(t)},m(w,j){p(w,h,j),a(h,k),a(h,y),a(y,b),a(h,x)},d(w){w&&t(h)}}}function iu(T){let h,k,y,b,x;return{c(){h=l("p"),k=r("Curious about how to load datasets for vision? Check out the image loading guide "),y=l("a"),b=r("here"),x=r("!"),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"Curious about how to load datasets for vision? Check out the image loading guide "),y=o(j,"A",{href:!0});var q=n(y);b=i(q,"here"),q.forEach(t),x=i(j,"!"),j.forEach(t),this.h()},h(){c(y,"href","./image_process")},m(w,j){p(w,h,j),a(h,k),a(h,y),a(y,b),a(h,x)},d(w){w&&t(h)}}}function pu(T){let h,k,y,b,x,w,j,q,K,ys,F,Z,ws,R,M,js,P;return{c(){h=l("p"),k=r("An object data type in "),y=l("a"),b=r("pandas.Series"),x=r(" doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),w=l("a"),j=r("Features"),q=r(" using the "),K=l("code"),ys=r("from_dict"),F=r(" or "),Z=l("code"),ws=r("from_pandas"),R=r(" methods. See the "),M=l("a"),js=r("troubleshoot"),P=r(" for more details on how to explicitly specify your own features."),this.h()},l(L){h=o(L,"P",{});var S=n(h);k=i(S,"An object data type in "),y=o(S,"A",{href:!0,rel:!0});var wa=n(y);b=i(wa,"pandas.Series"),wa.forEach(t),x=i(S," doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),w=o(S,"A",{href:!0});var bs=n(w);j=i(bs,"Features"),bs.forEach(t),q=i(S," using the "),K=o(S,"CODE",{});var ja=n(K);ys=i(ja,"from_dict"),ja.forEach(t),F=i(S," or "),Z=o(S,"CODE",{});var ba=n(Z);ws=i(ba,"from_pandas"),ba.forEach(t),R=i(S," methods. See the "),M=o(S,"A",{href:!0});var xs=n(M);js=i(xs,"troubleshoot"),xs.forEach(t),P=i(S," for more details on how to explicitly specify your own features."),S.forEach(t),this.h()},h(){c(y,"href","https://pandas.pydata.org/docs/reference/api/pandas.Series.html"),c(y,"rel","nofollow"),c(w,"href","/docs/datasets/pr_4598/en/package_reference/main_classes#datasets.Features"),c(M,"href","./loading#specify-features")},m(L,S){p(L,h,S),a(h,k),a(h,y),a(y,b),a(h,x),a(h,w),a(w,j),a(h,q),a(h,K),a(K,ys),a(h,F),a(h,Z),a(Z,ws),a(h,R),a(h,M),a(M,js),a(h,P)},d(L){L&&t(h)}}}function du(T){let h,k,y,b,x;return{c(){h=l("p"),k=r("Using "),y=l("code"),b=r("pct1_dropremainder"),x=r(" rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100.")},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"Using "),y=o(j,"CODE",{});var q=n(y);b=i(q,"pct1_dropremainder"),q.forEach(t),x=i(j," rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100."),j.forEach(t)},m(w,j){p(w,h,j),a(h,k),a(h,y),a(y,b),a(h,x)},d(w){w&&t(h)}}}function fu(T){let h,k,y,b,x;return{c(){h=l("p"),k=r("See the "),y=l("a"),b=r("Metrics"),x=r(" guide for more details on how to write your own metric loading script."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"See the "),y=o(j,"A",{href:!0});var q=n(y);b=i(q,"Metrics"),q.forEach(t),x=i(j," guide for more details on how to write your own metric loading script."),j.forEach(t),this.h()},h(){c(y,"href","./how_to_metrics#custom-metric-loading-script")},m(w,j){p(w,h,j),a(h,k),a(h,y),a(y,b),a(h,x)},d(w){w&&t(h)}}}function cu(T){let h,k,y,b,x;return{c(){h=l("p"),k=r("Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),y=l("a"),b=r("Metric.compute()"),x=r(" gathers all the predictions and references from the nodes, and computes the final metric."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),y=o(j,"A",{href:!0});var q=n(y);b=i(q,"Metric.compute()"),q.forEach(t),x=i(j," gathers all the predictions and references from the nodes, and computes the final metric."),j.forEach(t),this.h()},h(){c(y,"href","/docs/datasets/pr_4598/en/package_reference/main_classes#datasets.Metric.compute")},m(w,j){p(w,h,j),a(h,k),a(h,y),a(y,b),a(h,x)},d(w){w&&t(h)}}}function hu(T){let h,k,y,b,x,w,j,q,K,ys,F,Z,ws,R,M,js,P,L,S,wa,bs,ja,ba,xs,Rr,Mr,be,Vr,zr,xe,Ur,Jr,ke,Yr,Gl,xa,Br,Ql,ka,Xl,ss,ks,Ee,ct,Wr,qe,Gr,Kl,Es,Qr,Pe,Xr,Kr,Zl,V,Zr,Ea,si,ti,ht,ai,ei,so,ut,to,qa,li,ao,qs,oi,Ae,ni,ri,eo,mt,lo,Ps,oo,D,ii,Se,pi,di,Te,fi,ci,De,hi,ui,Ce,mi,gi,Ie,_i,vi,no,gt,ro,As,io,z,$i,Ne,yi,wi,_t,ji,bi,po,vt,fo,Ss,xi,Oe,ki,Ei,co,$t,ho,ts,Ts,He,yt,qi,Le,Pi,uo,Ds,Ai,Pa,Si,Ti,mo,Cs,Fe,Di,Ci,Re,Ii,go,wt,_o,as,Is,Me,jt,Ni,Ve,Oi,vo,C,Hi,ze,Li,Fi,Ue,Ri,Mi,Je,Vi,zi,Ye,Ui,Ji,Aa,Yi,Bi,$o,Ns,yo,es,Os,Be,bt,Wi,We,Gi,wo,Sa,Qi,jo,xt,bo,Ta,Xi,xo,kt,ko,Da,Ki,Eo,Et,qo,Ca,Zi,Po,qt,Ao,Ia,sp,So,Pt,To,ls,Hs,Ge,At,tp,Qe,ap,Do,Ls,ep,Na,lp,op,Co,St,Io,Oa,np,No,Tt,Oo,Fs,rp,Xe,ip,pp,Ho,Dt,Lo,Ha,dp,Fo,Ct,Ro,La,fp,Mo,os,Rs,Ke,It,cp,Ze,hp,Vo,Fa,up,zo,Nt,Uo,Ra,mp,Jo,Ot,Yo,ns,Ms,sl,Ht,gp,tl,_p,Bo,Ma,vp,Wo,Lt,Go,Va,$p,Qo,Ft,Xo,rs,Vs,al,Rt,yp,el,wp,Ko,zs,jp,za,bp,xp,Zo,is,Us,ll,Mt,kp,ol,Ep,sn,Js,qp,Ua,Pp,Ap,tn,Vt,an,ps,Ys,nl,zt,Sp,rl,Tp,en,Bs,Dp,Ja,Cp,Ip,ln,Ut,on,Ws,nn,ds,Gs,il,Jt,Np,pl,Op,rn,Ya,Hp,pn,U,Lp,dl,Fp,Rp,fl,Mp,Vp,dn,fs,Qs,cl,Yt,zp,hl,Up,fn,J,Jp,Ba,Yp,Bp,Wa,Wp,Gp,cn,Y,Qp,ul,Xp,Kp,ml,Zp,sd,hn,Bt,un,Xs,td,gl,ad,ed,mn,Wt,gn,Ga,ld,_n,Gt,vn,Qa,od,$n,Qt,yn,Xa,nd,wn,Xt,jn,cs,Ks,_l,Kt,rd,vl,id,bn,Ka,pd,xn,Zt,kn,Zs,dd,$l,fd,cd,En,sa,qn,st,Pn,Za,An,hs,tt,yl,ta,hd,wl,ud,Sn,se,md,Tn,us,at,jl,aa,gd,bl,_d,Dn,N,vd,te,$d,yd,xl,wd,jd,kl,bd,xd,Cn,et,kd,ea,Ed,qd,In,la,Nn,ms,lt,El,oa,Pd,ql,Ad,On,B,Sd,ae,Td,Dd,na,Cd,Id,Hn,W,Nd,ee,Od,Hd,le,Ld,Fd,Ln,ra,Fn,G,Rd,Pl,Md,Vd,oe,zd,Ud,Rn,ia,Mn,ne,Jd,Vn,pa,zn,gs,ot,Al,da,Yd,Sl,Bd,Un,re,Wd,Jn,fa,Yn,nt,Bn,_s,rt,Tl,ca,Gd,Dl,Qd,Wn,Q,Xd,Cl,Kd,Zd,ie,sf,tf,Gn,ha,Qn,vs,it,Il,ua,af,Nl,ef,Xn,pe,lf,Kn,de,of,Zn,X,Ol,ma,nf,Hl,rf,pf,df,Ll,$s,ff,Fl,cf,hf,Rl,uf,mf,gf,Ml,ga,_f,fe,vf,$f,sr,_a,tr,pt,ar,dt,yf,Vl,wf,jf,er,va,lr;return w=new A({}),ct=new A({}),ut=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset('lhoestq/demo1')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;lhoestq/demo1&#x27;</span>)`}}),mt=new E({props:{code:`dataset = load_dataset(
  "lhoestq/custom_squad",
  revision="main"  # tag name, or branch name, or commit hash
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(
<span class="hljs-meta">&gt;&gt;&gt; </span>  <span class="hljs-string">&quot;lhoestq/custom_squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>  revision=<span class="hljs-string">&quot;main&quot;</span>  <span class="hljs-comment"># tag name, or branch name, or commit hash</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),Ps=new ya({props:{$$slots:{default:[nu]},$$scope:{ctx:T}}}),gt=new E({props:{code:`data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;train.csv&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;test.csv&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;namespace/your_dataset_name&quot;</span>, data_files=data_files)`}}),As=new ya({props:{warning:!0,$$slots:{default:[ru]},$$scope:{ctx:T}}}),vt=new E({props:{code:`from datasets import load_dataset
c4_subset = load_dataset('allenai/c4', data_files='en/c4-train.0000*-of-01024.json.gz')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_subset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, data_files=<span class="hljs-string">&#x27;en/c4-train.0000*-of-01024.json.gz&#x27;</span>)`}}),$t=new E({props:{code:`data_files = {"validation": "en/c4-validation.*.json.gz"}
c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;validation&quot;</span>: <span class="hljs-string">&quot;en/c4-validation.*.json.gz&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_validation = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;validation&quot;</span>)`}}),yt=new A({}),wt=new E({props:{code:`dataset = load_dataset("path/to/local/loading_script/loading_script.py", split="train")
dataset = load_dataset("path/to/local/loading_script", split="train")  # equivalent because the file has the same name as the directory`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;path/to/local/loading_script/loading_script.py&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;path/to/local/loading_script&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)  <span class="hljs-comment"># equivalent because the file has the same name as the directory</span>`}}),jt=new A({}),Ns=new ya({props:{$$slots:{default:[iu]},$$scope:{ctx:T}}}),bt=new A({}),xt=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset('csv', data_files='my_file.csv')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.csv&#x27;</span>)`}}),kt=new E({props:{code:"dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv'])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=[<span class="hljs-string">&#x27;my_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_2.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_3.csv&#x27;</span>])'}}),Et=new E({props:{code:"dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'], 'test': 'my_test_file.csv'})",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_train_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_train_file_2.csv&#x27;</span>], <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.csv&#x27;</span>})'}}),qt=new E({props:{code:`base_url = "https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/"
dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train.csv&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: base_url + <span class="hljs-string">&#x27;test.csv&#x27;</span>})`}}),Pt=new E({props:{code:`url = "https://domain.org/train_data.zip"
data_files = {"train": url}
dataset = load_dataset("csv", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://domain.org/train_data.zip&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: url}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files)`}}),At=new A({}),St=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset('json', data_files='my_file.json')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>)`}}),Tt=new E({props:{code:`{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true}`,highlighted:`{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false}
{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}`}}),Dt=new E({props:{code:`
from datasets import load_dataset
dataset = load_dataset('json', data_files='my_file.json', field='data')`,highlighted:`{<span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;0.1.0&quot;</span>,
    <span class="hljs-string">&quot;data&quot;</span>: [{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false},
            {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}]
}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>, field=<span class="hljs-string">&#x27;data&#x27;</span>)`}}),Ct=new E({props:{code:`base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
dataset = load_dataset('json', data_files={'train': base_url + 'train-v1.1.json', 'validation': base_url + 'dev-v1.1.json'}, field="data")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train-v1.1.json&#x27;</span>, <span class="hljs-string">&#x27;validation&#x27;</span>: base_url + <span class="hljs-string">&#x27;dev-v1.1.json&#x27;</span>}, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),It=new A({}),Nt=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset('text', data_files={'train': ['my_text_1.txt', 'my_text_2.txt'], 'test': 'my_test_file.txt'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_text_1.txt&#x27;</span>, <span class="hljs-string">&#x27;my_text_2.txt&#x27;</span>], <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.txt&#x27;</span>})`}}),Ot=new E({props:{code:"dataset = load_dataset('text', data_files='https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files=<span class="hljs-string">&#x27;https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt&#x27;</span>)'}}),Ht=new A({}),Lt=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;train.parquet&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;test.parquet&#x27;</span>})`}}),Ft=new E({props:{code:`base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
data_files = {"train": base_url + "wikipedia-train.parquet"}
wiki = load_dataset("parquet", data_files=data_files, split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;wikipedia-train.parquet&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Rt=new A({}),Mt=new A({}),Vt=new E({props:{code:`from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>my_dict = {<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_dict(my_dict)`}}),zt=new A({}),Ut=new E({props:{code:`from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-meta">&gt;&gt;&gt; </span>df = pd.DataFrame({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_pandas(df)`}}),Ws=new ya({props:{warning:!0,$$slots:{default:[pu]},$$scope:{ctx:T}}}),Jt=new A({}),Yt=new A({}),Bt=new Wl({props:{group1:{id:"stringapi",code:"train_test_ds = datasets.load_dataset('bookcorpus', split='train+test')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train+test&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = datasets.ReadInstruction('train') + datasets.ReadInstruction('test')
train_test_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Wt=new Wl({props:{group1:{id:"stringapi",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split='train[10:20]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[10:20]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', from_=10, to=20, unit='abs'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">10</span>, to=<span class="hljs-number">20</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>))'}}}),Gt=new Wl({props:{group1:{id:"stringapi",code:"train_10pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', to=10, unit='%'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))'}}}),Qt=new Wl({props:{group1:{id:"stringapi",code:"train_10_80pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]+train[-80%:]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]+train[-80%:]&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = (datasets.ReadInstruction('train', to=10, unit='%') + datasets.ReadInstruction('train', from_=-80, unit='%'))
train_10_80pct_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = (datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=-<span class="hljs-number">80</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Xt=new Wl({props:{group1:{id:"stringapi",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[<span class="hljs-subst">{k}</span>%:<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[:<span class="hljs-subst">{k}</span>%]+train[<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%:]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`},group2:{id:"readinstruction",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', [datasets.ReadInstruction('train', from_=k, to=k+10, unit='%') for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', [(datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')) for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [(datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}}),Kt=new A({}),Zt=new E({props:{code:`train_50_52_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%]')
train_52_54_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%]')`,highlighted:`<span class="hljs-comment"># Assuming *train* split contains 999 records.</span>
<span class="hljs-comment"># 19 records, from 500 (included) to 519 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%]&#x27;</span>)
<span class="hljs-comment"># 20 records, from 519 (included) to 539 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%]&#x27;</span>)`}}),sa=new E({props:{code:`train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction( 'train', from_=50, to=52, unit='%', rounding='pct1_dropremainder'))
train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train',from_=52, to=54, unit='%', rounding='pct1_dropremainder'))
train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%](pct1_dropremainder)')
train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%](pct1_dropremainder)')`,highlighted:`<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction( <span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># Or equivalently:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%](pct1_dropremainder)&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%](pct1_dropremainder)&#x27;</span>)`}}),st=new ya({props:{warning:!0,$$slots:{default:[du]},$$scope:{ctx:T}}}),ta=new A({}),aa=new A({}),la=new E({props:{code:'dataset = load_dataset("matinf", "summarization")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;matinf&quot;</span>, <span class="hljs-string">&quot;summarization&quot;</span>)
Downloading <span class="hljs-keyword">and</span> preparing dataset matinf/summarization (download: Unknown size, generated: <span class="hljs-number">246.89</span> MiB, post-processed: Unknown size, total: <span class="hljs-number">246.89</span> MiB) to /root/.cache/huggingface/datasets/matinf/summarization/<span class="hljs-number">1.0</span><span class="hljs-number">.0</span>/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
AssertionError: The dataset matinf <span class="hljs-keyword">with</span> config summarization requires manual data. 
Please follow the manual download instructions: To use MATINF you have to download it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9). You will receive a download link <span class="hljs-keyword">and</span> a password once you complete the form. Please extract <span class="hljs-built_in">all</span> files <span class="hljs-keyword">in</span> one folder <span class="hljs-keyword">and</span> load the dataset <span class="hljs-keyword">with</span>: *datasets.load_dataset(<span class="hljs-string">&#x27;matinf&#x27;</span>, data_dir=<span class="hljs-string">&#x27;path/to/folder/folder_name&#x27;</span>)*. 
Manual data can be loaded <span class="hljs-keyword">with</span> \`datasets.load_dataset(matinf, data_dir=<span class="hljs-string">&#x27;&lt;path/to/manual/data&gt;&#x27;</span>) `}}),oa=new A({}),ra=new E({props:{code:`class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]
emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>class_names = [<span class="hljs-string">&quot;sadness&quot;</span>, <span class="hljs-string">&quot;joy&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;anger&quot;</span>, <span class="hljs-string">&quot;fear&quot;</span>, <span class="hljs-string">&quot;surprise&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>emotion_features = Features({<span class="hljs-string">&#x27;text&#x27;</span>: Value(<span class="hljs-string">&#x27;string&#x27;</span>), <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=class_names)})`}}),ia=new E({props:{code:"dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=file_dict, delimiter=<span class="hljs-string">&#x27;;&#x27;</span>, column_names=[<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>], features=emotion_features)'}}),pa=new E({props:{code:"dataset['train'].features",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&#x27;train&#x27;</span>].features
{<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">6</span>, names=[<span class="hljs-string">&#x27;sadness&#x27;</span>, <span class="hljs-string">&#x27;joy&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;anger&#x27;</span>, <span class="hljs-string">&#x27;fear&#x27;</span>, <span class="hljs-string">&#x27;surprise&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),da=new A({}),fa=new E({props:{code:`from datasets import load_metric
metric = load_metric('PATH/TO/MY/METRIC/SCRIPT')

# Example of typical usage
for batch in dataset:
    inputs, references = batch
    predictions = model(inputs)
    metric.add_batch(predictions=predictions, references=references)
score = metric.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;PATH/TO/MY/METRIC/SCRIPT&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example of typical usage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataset:
<span class="hljs-meta">... </span>    inputs, references = batch
<span class="hljs-meta">... </span>    predictions = model(inputs)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=references)
<span class="hljs-meta">&gt;&gt;&gt; </span>score = metric.compute()`}}),nt=new ya({props:{$$slots:{default:[fu]},$$scope:{ctx:T}}}),ca=new A({}),ha=new E({props:{code:`from datasets import load_metric
metric = load_metric('bleurt', name='bleurt-base-128')
metric = load_metric('bleurt', name='bleurt-base-512')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-128&#x27;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-512&#x27;</span>)</span>`}}),ua=new A({}),_a=new E({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=rank)`}}),pt=new ya({props:{$$slots:{default:[cu]},$$scope:{ctx:T}}}),va=new E({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=process_id, experiment_id=<span class="hljs-string">&quot;My_experiment_10&quot;</span>)`}}),{c(){h=l("meta"),k=d(),y=l("h1"),b=l("a"),x=l("span"),u(w.$$.fragment),j=d(),q=l("span"),K=r("Load"),ys=d(),F=l("p"),Z=r("You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),ws=d(),R=l("p"),M=r("This guide will show you how to load a dataset from:"),js=d(),P=l("ul"),L=l("li"),S=r("The Hub without a dataset loading script"),wa=d(),bs=l("li"),ja=r("Local loading script"),ba=d(),xs=l("li"),Rr=r("Local files"),Mr=d(),be=l("li"),Vr=r("In-memory data"),zr=d(),xe=l("li"),Ur=r("Offline"),Jr=d(),ke=l("li"),Yr=r("A specific slice of a split"),Gl=d(),xa=l("p"),Br=r("You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Ql=d(),ka=l("a"),Xl=d(),ss=l("h2"),ks=l("a"),Ee=l("span"),u(ct.$$.fragment),Wr=d(),qe=l("span"),Gr=r("Hugging Face Hub"),Kl=d(),Es=l("p"),Qr=r("In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),Pe=l("strong"),Xr=r("without"),Kr=r(" a loading script!"),Zl=d(),V=l("p"),Zr=r("First, create a dataset repository and upload your data files. Then you can use "),Ea=l("a"),si=r("load_dataset()"),ti=r(" like you learned in the tutorial. For example, load the files from this "),ht=l("a"),ai=r("demo repository"),ei=r(" by providing the repository namespace and dataset name:"),so=d(),u(ut.$$.fragment),to=d(),qa=l("p"),li=r("This dataset repository contains CSV files, and this code loads all the data from the CSV files."),ao=d(),qs=l("p"),oi=r("Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),Ae=l("code"),ni=r("revision"),ri=r(" flag to specify which dataset version you want to load:"),eo=d(),u(mt.$$.fragment),lo=d(),u(Ps.$$.fragment),oo=d(),D=l("p"),ii=r("If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),Se=l("code"),pi=r("train"),di=r(" split. Use the "),Te=l("code"),fi=r("data_files"),ci=r(" parameter to map data files to splits like "),De=l("code"),hi=r("train"),ui=r(", "),Ce=l("code"),mi=r("validation"),gi=r(" and "),Ie=l("code"),_i=r("test"),vi=r(":"),no=d(),u(gt.$$.fragment),ro=d(),u(As.$$.fragment),io=d(),z=l("p"),$i=r("You can also load a specific subset of the files with the "),Ne=l("code"),yi=r("data_files"),wi=r(" parameter. The example below loads files from the "),_t=l("a"),ji=r("C4 dataset"),bi=r(":"),po=d(),u(vt.$$.fragment),fo=d(),Ss=l("p"),xi=r("Specify a custom split with the "),Oe=l("code"),ki=r("split"),Ei=r(" parameter:"),co=d(),u($t.$$.fragment),ho=d(),ts=l("h2"),Ts=l("a"),He=l("span"),u(yt.$$.fragment),qi=d(),Le=l("span"),Pi=r("Local loading script"),uo=d(),Ds=l("p"),Ai=r("You may have a \u{1F917} Datasets loading script locally on your computer. You can load this dataset by passing to "),Pa=l("a"),Si=r("load_dataset()"),Ti=r(" one of the following paths:"),mo=d(),Cs=l("ul"),Fe=l("li"),Di=r("The local path to the loading script file."),Ci=d(),Re=l("li"),Ii=r("The local path to the directory containing the loading script file (only if the script file has the same name as the directory)."),go=d(),u(wt.$$.fragment),_o=d(),as=l("h2"),Is=l("a"),Me=l("span"),u(jt.$$.fragment),Ni=d(),Ve=l("span"),Oi=r("Local and remote files"),vo=d(),C=l("p"),Hi=r("\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),ze=l("code"),Li=r("csv"),Fi=r(", "),Ue=l("code"),Ri=r("json"),Mi=r(", "),Je=l("code"),Vi=r("txt"),zi=r(" or "),Ye=l("code"),Ui=r("parquet"),Ji=r(" file. The "),Aa=l("a"),Yi=r("load_dataset()"),Bi=r(" method is able to load each of these file types."),$o=d(),u(Ns.$$.fragment),yo=d(),es=l("h3"),Os=l("a"),Be=l("span"),u(bt.$$.fragment),Wi=d(),We=l("span"),Gi=r("CSV"),wo=d(),Sa=l("p"),Qi=r("\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),jo=d(),u(xt.$$.fragment),bo=d(),Ta=l("p"),Xi=r("If you have more than one CSV file:"),xo=d(),u(kt.$$.fragment),ko=d(),Da=l("p"),Ki=r("You can also map the training and test splits to specific CSV files:"),Eo=d(),u(Et.$$.fragment),qo=d(),Ca=l("p"),Zi=r("To load remote CSV files via HTTP, you can pass the URLs:"),Po=d(),u(qt.$$.fragment),Ao=d(),Ia=l("p"),sp=r("To load zipped CSV files:"),So=d(),u(Pt.$$.fragment),To=d(),ls=l("h3"),Hs=l("a"),Ge=l("span"),u(At.$$.fragment),tp=d(),Qe=l("span"),ap=r("JSON"),Do=d(),Ls=l("p"),ep=r("JSON files are loaded directly with "),Na=l("a"),lp=r("load_dataset()"),op=r(" as shown below:"),Co=d(),u(St.$$.fragment),Io=d(),Oa=l("p"),np=r("JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),No=d(),u(Tt.$$.fragment),Oo=d(),Fs=l("p"),rp=r("Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Xe=l("code"),ip=r("field"),pp=r(" argument as shown in the following:"),Ho=d(),u(Dt.$$.fragment),Lo=d(),Ha=l("p"),dp=r("To load remote JSON files via HTTP, you can pass the URLs:"),Fo=d(),u(Ct.$$.fragment),Ro=d(),La=l("p"),fp=r("While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Mo=d(),os=l("h3"),Rs=l("a"),Ke=l("span"),u(It.$$.fragment),cp=d(),Ze=l("span"),hp=r("Text files"),Vo=d(),Fa=l("p"),up=r("Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),zo=d(),u(Nt.$$.fragment),Uo=d(),Ra=l("p"),mp=r("To load remote TXT files via HTTP, you can pass the URLs:"),Jo=d(),u(Ot.$$.fragment),Yo=d(),ns=l("h3"),Ms=l("a"),sl=l("span"),u(Ht.$$.fragment),gp=d(),tl=l("span"),_p=r("Parquet"),Bo=d(),Ma=l("p"),vp=r("Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Wo=d(),u(Lt.$$.fragment),Go=d(),Va=l("p"),$p=r("To load remote parquet files via HTTP, you can pass the URLs:"),Qo=d(),u(Ft.$$.fragment),Xo=d(),rs=l("h2"),Vs=l("a"),al=l("span"),u(Rt.$$.fragment),yp=d(),el=l("span"),wp=r("In-memory data"),Ko=d(),zs=l("p"),jp=r("\u{1F917} Datasets will also allow you to create a "),za=l("a"),bp=r("Dataset"),xp=r(" directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),Zo=d(),is=l("h3"),Us=l("a"),ll=l("span"),u(Mt.$$.fragment),kp=d(),ol=l("span"),Ep=r("Python dictionary"),sn=d(),Js=l("p"),qp=r("Load Python dictionaries with "),Ua=l("a"),Pp=r("Dataset.from_dict()"),Ap=r(":"),tn=d(),u(Vt.$$.fragment),an=d(),ps=l("h3"),Ys=l("a"),nl=l("span"),u(zt.$$.fragment),Sp=d(),rl=l("span"),Tp=r("Pandas DataFrame"),en=d(),Bs=l("p"),Dp=r("Load Pandas DataFrames with "),Ja=l("a"),Cp=r("Dataset.from_pandas()"),Ip=r(":"),ln=d(),u(Ut.$$.fragment),on=d(),u(Ws.$$.fragment),nn=d(),ds=l("h2"),Gs=l("a"),il=l("span"),u(Jt.$$.fragment),Np=d(),pl=l("span"),Op=r("Offline"),rn=d(),Ya=l("p"),Hp=r("Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),pn=d(),U=l("p"),Lp=r("If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),dl=l("code"),Fp=r("HF_DATASETS_OFFLINE"),Rp=r(" to "),fl=l("code"),Mp=r("1"),Vp=r(" to enable full offline mode."),dn=d(),fs=l("h2"),Qs=l("a"),cl=l("span"),u(Yt.$$.fragment),zp=d(),hl=l("span"),Up=r("Slice splits"),fn=d(),J=l("p"),Jp=r("For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Ba=l("a"),Yp=r("ReadInstruction"),Bp=r(". Strings are more compact and readable for simple cases, while "),Wa=l("a"),Wp=r("ReadInstruction"),Gp=r(" is easier to use with variable slicing parameters."),cn=d(),Y=l("p"),Qp=r("Concatenate the "),ul=l("code"),Xp=r("train"),Kp=r(" and "),ml=l("code"),Zp=r("test"),sd=r(" split by:"),hn=d(),u(Bt.$$.fragment),un=d(),Xs=l("p"),td=r("Select specific rows of the "),gl=l("code"),ad=r("train"),ed=r(" split:"),mn=d(),u(Wt.$$.fragment),gn=d(),Ga=l("p"),ld=r("Or select a percentage of the split with:"),_n=d(),u(Gt.$$.fragment),vn=d(),Qa=l("p"),od=r("You can even select a combination of percentages from each split:"),$n=d(),u(Qt.$$.fragment),yn=d(),Xa=l("p"),nd=r("Finally, create cross-validated dataset splits by:"),wn=d(),u(Xt.$$.fragment),jn=d(),cs=l("h3"),Ks=l("a"),_l=l("span"),u(Kt.$$.fragment),rd=d(),vl=l("span"),id=r("Percent slicing and rounding"),bn=d(),Ka=l("p"),pd=r("For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),xn=d(),u(Zt.$$.fragment),kn=d(),Zs=l("p"),dd=r("If you want equal sized splits, use "),$l=l("code"),fd=r("pct1_dropremainder"),cd=r(" rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),En=d(),u(sa.$$.fragment),qn=d(),u(st.$$.fragment),Pn=d(),Za=l("a"),An=d(),hs=l("h2"),tt=l("a"),yl=l("span"),u(ta.$$.fragment),hd=d(),wl=l("span"),ud=r("Troubleshooting"),Sn=d(),se=l("p"),md=r("Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),Tn=d(),us=l("h3"),at=l("a"),jl=l("span"),u(aa.$$.fragment),gd=d(),bl=l("span"),_d=r("Manual download"),Dn=d(),N=l("p"),vd=r("Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),te=l("a"),$d=r("load_dataset()"),yd=r(" to throw an "),xl=l("code"),wd=r("AssertionError"),jd=r(". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),kl=l("code"),bd=r("data_dir"),xd=r(" argument to specify the path to the files you just downloaded."),Cn=d(),et=l("p"),kd=r("For example, if you try to download a configuration from the "),ea=l("a"),Ed=r("MATINF"),qd=r(" dataset:"),In=d(),u(la.$$.fragment),Nn=d(),ms=l("h3"),lt=l("a"),El=l("span"),u(oa.$$.fragment),Pd=d(),ql=l("span"),Ad=r("Specify features"),On=d(),B=l("p"),Sd=r("When you create a dataset from local files, the "),ae=l("a"),Td=r("Features"),Dd=r(" are automatically inferred by "),na=l("a"),Cd=r("Apache Arrow"),Id=r(". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),Hn=d(),W=l("p"),Nd=r("The following example shows how you can add custom labels with "),ee=l("a"),Od=r("ClassLabel"),Hd=r(". First, define your own labels using the "),le=l("a"),Ld=r("Features"),Fd=r(" class:"),Ln=d(),u(ra.$$.fragment),Fn=d(),G=l("p"),Rd=r("Next, specify the "),Pl=l("code"),Md=r("features"),Vd=r(" argument in "),oe=l("a"),zd=r("load_dataset()"),Ud=r(" with the features you just created:"),Rn=d(),u(ia.$$.fragment),Mn=d(),ne=l("p"),Jd=r("Now when you look at your dataset features, you can see it uses the custom labels you defined:"),Vn=d(),u(pa.$$.fragment),zn=d(),gs=l("h2"),ot=l("a"),Al=l("span"),u(da.$$.fragment),Yd=d(),Sl=l("span"),Bd=r("Metrics"),Un=d(),re=l("p"),Wd=r("When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),Jn=d(),u(fa.$$.fragment),Yn=d(),u(nt.$$.fragment),Bn=d(),_s=l("h3"),rt=l("a"),Tl=l("span"),u(ca.$$.fragment),Gd=d(),Dl=l("span"),Qd=r("Load configurations"),Wn=d(),Q=l("p"),Xd=r("It is possible for a metric to have different configurations. The configurations are stored in the "),Cl=l("code"),Kd=r("config_name"),Zd=r(" parameter in "),ie=l("a"),sf=r("MetricInfo"),tf=r(" attribute. When you load a metric, provide the configuration name as shown in the following:"),Gn=d(),u(ha.$$.fragment),Qn=d(),vs=l("h3"),it=l("a"),Il=l("span"),u(ua.$$.fragment),af=d(),Nl=l("span"),ef=r("Distributed setup"),Xn=d(),pe=l("p"),lf=r("When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),Kn=d(),de=l("p"),of=r("For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),Zn=d(),X=l("ol"),Ol=l("li"),ma=l("p"),nf=r("Define the total number of processes with the "),Hl=l("code"),rf=r("num_process"),pf=r(" argument."),df=d(),Ll=l("li"),$s=l("p"),ff=r("Set the process "),Fl=l("code"),cf=r("rank"),hf=r(" as an integer between zero and "),Rl=l("code"),uf=r("num_process - 1"),mf=r("."),gf=d(),Ml=l("li"),ga=l("p"),_f=r("Load your metric with "),fe=l("a"),vf=r("load_metric()"),$f=r(" with these arguments:"),sr=d(),u(_a.$$.fragment),tr=d(),u(pt.$$.fragment),ar=d(),dt=l("p"),yf=r("In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Vl=l("code"),wf=r("experiment_id"),jf=r(" to distinguish the separate evaluations:"),er=d(),u(va.$$.fragment),this.h()},l(s){const e=lu('[data-svelte="svelte-1phssyn"]',document.head);h=o(e,"META",{name:!0,content:!0}),e.forEach(t),k=f(s),y=o(s,"H1",{class:!0});var $a=n(y);b=o($a,"A",{id:!0,class:!0,href:!0});var zl=n(b);x=o(zl,"SPAN",{});var Ul=n(x);m(w.$$.fragment,Ul),Ul.forEach(t),zl.forEach(t),j=f($a),q=o($a,"SPAN",{});var Jl=n(q);K=i(Jl,"Load"),Jl.forEach(t),$a.forEach(t),ys=f(s),F=o(s,"P",{});var Yl=n(F);Z=i(Yl,"You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),Yl.forEach(t),ws=f(s),R=o(s,"P",{});var Bl=n(R);M=i(Bl,"This guide will show you how to load a dataset from:"),Bl.forEach(t),js=f(s),P=o(s,"UL",{});var I=n(P);L=o(I,"LI",{});var bf=n(L);S=i(bf,"The Hub without a dataset loading script"),bf.forEach(t),wa=f(I),bs=o(I,"LI",{});var xf=n(bs);ja=i(xf,"Local loading script"),xf.forEach(t),ba=f(I),xs=o(I,"LI",{});var kf=n(xs);Rr=i(kf,"Local files"),kf.forEach(t),Mr=f(I),be=o(I,"LI",{});var Ef=n(be);Vr=i(Ef,"In-memory data"),Ef.forEach(t),zr=f(I),xe=o(I,"LI",{});var qf=n(xe);Ur=i(qf,"Offline"),qf.forEach(t),Jr=f(I),ke=o(I,"LI",{});var Pf=n(ke);Yr=i(Pf,"A specific slice of a split"),Pf.forEach(t),I.forEach(t),Gl=f(s),xa=o(s,"P",{});var Af=n(xa);Br=i(Af,"You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Af.forEach(t),Ql=f(s),ka=o(s,"A",{id:!0}),n(ka).forEach(t),Xl=f(s),ss=o(s,"H2",{class:!0});var or=n(ss);ks=o(or,"A",{id:!0,class:!0,href:!0});var Sf=n(ks);Ee=o(Sf,"SPAN",{});var Tf=n(Ee);m(ct.$$.fragment,Tf),Tf.forEach(t),Sf.forEach(t),Wr=f(or),qe=o(or,"SPAN",{});var Df=n(qe);Gr=i(Df,"Hugging Face Hub"),Df.forEach(t),or.forEach(t),Kl=f(s),Es=o(s,"P",{});var nr=n(Es);Qr=i(nr,"In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),Pe=o(nr,"STRONG",{});var Cf=n(Pe);Xr=i(Cf,"without"),Cf.forEach(t),Kr=i(nr," a loading script!"),nr.forEach(t),Zl=f(s),V=o(s,"P",{});var ce=n(V);Zr=i(ce,"First, create a dataset repository and upload your data files. Then you can use "),Ea=o(ce,"A",{href:!0});var If=n(Ea);si=i(If,"load_dataset()"),If.forEach(t),ti=i(ce," like you learned in the tutorial. For example, load the files from this "),ht=o(ce,"A",{href:!0,rel:!0});var Nf=n(ht);ai=i(Nf,"demo repository"),Nf.forEach(t),ei=i(ce," by providing the repository namespace and dataset name:"),ce.forEach(t),so=f(s),m(ut.$$.fragment,s),to=f(s),qa=o(s,"P",{});var Of=n(qa);li=i(Of,"This dataset repository contains CSV files, and this code loads all the data from the CSV files."),Of.forEach(t),ao=f(s),qs=o(s,"P",{});var rr=n(qs);oi=i(rr,"Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),Ae=o(rr,"CODE",{});var Hf=n(Ae);ni=i(Hf,"revision"),Hf.forEach(t),ri=i(rr," flag to specify which dataset version you want to load:"),rr.forEach(t),eo=f(s),m(mt.$$.fragment,s),lo=f(s),m(Ps.$$.fragment,s),oo=f(s),D=o(s,"P",{});var O=n(D);ii=i(O,"If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),Se=o(O,"CODE",{});var Lf=n(Se);pi=i(Lf,"train"),Lf.forEach(t),di=i(O," split. Use the "),Te=o(O,"CODE",{});var Ff=n(Te);fi=i(Ff,"data_files"),Ff.forEach(t),ci=i(O," parameter to map data files to splits like "),De=o(O,"CODE",{});var Rf=n(De);hi=i(Rf,"train"),Rf.forEach(t),ui=i(O,", "),Ce=o(O,"CODE",{});var Mf=n(Ce);mi=i(Mf,"validation"),Mf.forEach(t),gi=i(O," and "),Ie=o(O,"CODE",{});var Vf=n(Ie);_i=i(Vf,"test"),Vf.forEach(t),vi=i(O,":"),O.forEach(t),no=f(s),m(gt.$$.fragment,s),ro=f(s),m(As.$$.fragment,s),io=f(s),z=o(s,"P",{});var he=n(z);$i=i(he,"You can also load a specific subset of the files with the "),Ne=o(he,"CODE",{});var zf=n(Ne);yi=i(zf,"data_files"),zf.forEach(t),wi=i(he," parameter. The example below loads files from the "),_t=o(he,"A",{href:!0,rel:!0});var Uf=n(_t);ji=i(Uf,"C4 dataset"),Uf.forEach(t),bi=i(he,":"),he.forEach(t),po=f(s),m(vt.$$.fragment,s),fo=f(s),Ss=o(s,"P",{});var ir=n(Ss);xi=i(ir,"Specify a custom split with the "),Oe=o(ir,"CODE",{});var Jf=n(Oe);ki=i(Jf,"split"),Jf.forEach(t),Ei=i(ir," parameter:"),ir.forEach(t),co=f(s),m($t.$$.fragment,s),ho=f(s),ts=o(s,"H2",{class:!0});var pr=n(ts);Ts=o(pr,"A",{id:!0,class:!0,href:!0});var Yf=n(Ts);He=o(Yf,"SPAN",{});var Bf=n(He);m(yt.$$.fragment,Bf),Bf.forEach(t),Yf.forEach(t),qi=f(pr),Le=o(pr,"SPAN",{});var Wf=n(Le);Pi=i(Wf,"Local loading script"),Wf.forEach(t),pr.forEach(t),uo=f(s),Ds=o(s,"P",{});var dr=n(Ds);Ai=i(dr,"You may have a \u{1F917} Datasets loading script locally on your computer. You can load this dataset by passing to "),Pa=o(dr,"A",{href:!0});var Gf=n(Pa);Si=i(Gf,"load_dataset()"),Gf.forEach(t),Ti=i(dr," one of the following paths:"),dr.forEach(t),mo=f(s),Cs=o(s,"UL",{});var fr=n(Cs);Fe=o(fr,"LI",{});var Qf=n(Fe);Di=i(Qf,"The local path to the loading script file."),Qf.forEach(t),Ci=f(fr),Re=o(fr,"LI",{});var Xf=n(Re);Ii=i(Xf,"The local path to the directory containing the loading script file (only if the script file has the same name as the directory)."),Xf.forEach(t),fr.forEach(t),go=f(s),m(wt.$$.fragment,s),_o=f(s),as=o(s,"H2",{class:!0});var cr=n(as);Is=o(cr,"A",{id:!0,class:!0,href:!0});var Kf=n(Is);Me=o(Kf,"SPAN",{});var Zf=n(Me);m(jt.$$.fragment,Zf),Zf.forEach(t),Kf.forEach(t),Ni=f(cr),Ve=o(cr,"SPAN",{});var sc=n(Ve);Oi=i(sc,"Local and remote files"),sc.forEach(t),cr.forEach(t),vo=f(s),C=o(s,"P",{});var H=n(C);Hi=i(H,"\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),ze=o(H,"CODE",{});var tc=n(ze);Li=i(tc,"csv"),tc.forEach(t),Fi=i(H,", "),Ue=o(H,"CODE",{});var ac=n(Ue);Ri=i(ac,"json"),ac.forEach(t),Mi=i(H,", "),Je=o(H,"CODE",{});var ec=n(Je);Vi=i(ec,"txt"),ec.forEach(t),zi=i(H," or "),Ye=o(H,"CODE",{});var lc=n(Ye);Ui=i(lc,"parquet"),lc.forEach(t),Ji=i(H," file. The "),Aa=o(H,"A",{href:!0});var oc=n(Aa);Yi=i(oc,"load_dataset()"),oc.forEach(t),Bi=i(H," method is able to load each of these file types."),H.forEach(t),$o=f(s),m(Ns.$$.fragment,s),yo=f(s),es=o(s,"H3",{class:!0});var hr=n(es);Os=o(hr,"A",{id:!0,class:!0,href:!0});var nc=n(Os);Be=o(nc,"SPAN",{});var rc=n(Be);m(bt.$$.fragment,rc),rc.forEach(t),nc.forEach(t),Wi=f(hr),We=o(hr,"SPAN",{});var ic=n(We);Gi=i(ic,"CSV"),ic.forEach(t),hr.forEach(t),wo=f(s),Sa=o(s,"P",{});var pc=n(Sa);Qi=i(pc,"\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),pc.forEach(t),jo=f(s),m(xt.$$.fragment,s),bo=f(s),Ta=o(s,"P",{});var dc=n(Ta);Xi=i(dc,"If you have more than one CSV file:"),dc.forEach(t),xo=f(s),m(kt.$$.fragment,s),ko=f(s),Da=o(s,"P",{});var fc=n(Da);Ki=i(fc,"You can also map the training and test splits to specific CSV files:"),fc.forEach(t),Eo=f(s),m(Et.$$.fragment,s),qo=f(s),Ca=o(s,"P",{});var cc=n(Ca);Zi=i(cc,"To load remote CSV files via HTTP, you can pass the URLs:"),cc.forEach(t),Po=f(s),m(qt.$$.fragment,s),Ao=f(s),Ia=o(s,"P",{});var hc=n(Ia);sp=i(hc,"To load zipped CSV files:"),hc.forEach(t),So=f(s),m(Pt.$$.fragment,s),To=f(s),ls=o(s,"H3",{class:!0});var ur=n(ls);Hs=o(ur,"A",{id:!0,class:!0,href:!0});var uc=n(Hs);Ge=o(uc,"SPAN",{});var mc=n(Ge);m(At.$$.fragment,mc),mc.forEach(t),uc.forEach(t),tp=f(ur),Qe=o(ur,"SPAN",{});var gc=n(Qe);ap=i(gc,"JSON"),gc.forEach(t),ur.forEach(t),Do=f(s),Ls=o(s,"P",{});var mr=n(Ls);ep=i(mr,"JSON files are loaded directly with "),Na=o(mr,"A",{href:!0});var _c=n(Na);lp=i(_c,"load_dataset()"),_c.forEach(t),op=i(mr," as shown below:"),mr.forEach(t),Co=f(s),m(St.$$.fragment,s),Io=f(s),Oa=o(s,"P",{});var vc=n(Oa);np=i(vc,"JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),vc.forEach(t),No=f(s),m(Tt.$$.fragment,s),Oo=f(s),Fs=o(s,"P",{});var gr=n(Fs);rp=i(gr,"Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Xe=o(gr,"CODE",{});var $c=n(Xe);ip=i($c,"field"),$c.forEach(t),pp=i(gr," argument as shown in the following:"),gr.forEach(t),Ho=f(s),m(Dt.$$.fragment,s),Lo=f(s),Ha=o(s,"P",{});var yc=n(Ha);dp=i(yc,"To load remote JSON files via HTTP, you can pass the URLs:"),yc.forEach(t),Fo=f(s),m(Ct.$$.fragment,s),Ro=f(s),La=o(s,"P",{});var wc=n(La);fp=i(wc,"While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),wc.forEach(t),Mo=f(s),os=o(s,"H3",{class:!0});var _r=n(os);Rs=o(_r,"A",{id:!0,class:!0,href:!0});var jc=n(Rs);Ke=o(jc,"SPAN",{});var bc=n(Ke);m(It.$$.fragment,bc),bc.forEach(t),jc.forEach(t),cp=f(_r),Ze=o(_r,"SPAN",{});var xc=n(Ze);hp=i(xc,"Text files"),xc.forEach(t),_r.forEach(t),Vo=f(s),Fa=o(s,"P",{});var kc=n(Fa);up=i(kc,"Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),kc.forEach(t),zo=f(s),m(Nt.$$.fragment,s),Uo=f(s),Ra=o(s,"P",{});var Ec=n(Ra);mp=i(Ec,"To load remote TXT files via HTTP, you can pass the URLs:"),Ec.forEach(t),Jo=f(s),m(Ot.$$.fragment,s),Yo=f(s),ns=o(s,"H3",{class:!0});var vr=n(ns);Ms=o(vr,"A",{id:!0,class:!0,href:!0});var qc=n(Ms);sl=o(qc,"SPAN",{});var Pc=n(sl);m(Ht.$$.fragment,Pc),Pc.forEach(t),qc.forEach(t),gp=f(vr),tl=o(vr,"SPAN",{});var Ac=n(tl);_p=i(Ac,"Parquet"),Ac.forEach(t),vr.forEach(t),Bo=f(s),Ma=o(s,"P",{});var Sc=n(Ma);vp=i(Sc,"Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Sc.forEach(t),Wo=f(s),m(Lt.$$.fragment,s),Go=f(s),Va=o(s,"P",{});var Tc=n(Va);$p=i(Tc,"To load remote parquet files via HTTP, you can pass the URLs:"),Tc.forEach(t),Qo=f(s),m(Ft.$$.fragment,s),Xo=f(s),rs=o(s,"H2",{class:!0});var $r=n(rs);Vs=o($r,"A",{id:!0,class:!0,href:!0});var Dc=n(Vs);al=o(Dc,"SPAN",{});var Cc=n(al);m(Rt.$$.fragment,Cc),Cc.forEach(t),Dc.forEach(t),yp=f($r),el=o($r,"SPAN",{});var Ic=n(el);wp=i(Ic,"In-memory data"),Ic.forEach(t),$r.forEach(t),Ko=f(s),zs=o(s,"P",{});var yr=n(zs);jp=i(yr,"\u{1F917} Datasets will also allow you to create a "),za=o(yr,"A",{href:!0});var Nc=n(za);bp=i(Nc,"Dataset"),Nc.forEach(t),xp=i(yr," directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),yr.forEach(t),Zo=f(s),is=o(s,"H3",{class:!0});var wr=n(is);Us=o(wr,"A",{id:!0,class:!0,href:!0});var Oc=n(Us);ll=o(Oc,"SPAN",{});var Hc=n(ll);m(Mt.$$.fragment,Hc),Hc.forEach(t),Oc.forEach(t),kp=f(wr),ol=o(wr,"SPAN",{});var Lc=n(ol);Ep=i(Lc,"Python dictionary"),Lc.forEach(t),wr.forEach(t),sn=f(s),Js=o(s,"P",{});var jr=n(Js);qp=i(jr,"Load Python dictionaries with "),Ua=o(jr,"A",{href:!0});var Fc=n(Ua);Pp=i(Fc,"Dataset.from_dict()"),Fc.forEach(t),Ap=i(jr,":"),jr.forEach(t),tn=f(s),m(Vt.$$.fragment,s),an=f(s),ps=o(s,"H3",{class:!0});var br=n(ps);Ys=o(br,"A",{id:!0,class:!0,href:!0});var Rc=n(Ys);nl=o(Rc,"SPAN",{});var Mc=n(nl);m(zt.$$.fragment,Mc),Mc.forEach(t),Rc.forEach(t),Sp=f(br),rl=o(br,"SPAN",{});var Vc=n(rl);Tp=i(Vc,"Pandas DataFrame"),Vc.forEach(t),br.forEach(t),en=f(s),Bs=o(s,"P",{});var xr=n(Bs);Dp=i(xr,"Load Pandas DataFrames with "),Ja=o(xr,"A",{href:!0});var zc=n(Ja);Cp=i(zc,"Dataset.from_pandas()"),zc.forEach(t),Ip=i(xr,":"),xr.forEach(t),ln=f(s),m(Ut.$$.fragment,s),on=f(s),m(Ws.$$.fragment,s),nn=f(s),ds=o(s,"H2",{class:!0});var kr=n(ds);Gs=o(kr,"A",{id:!0,class:!0,href:!0});var Uc=n(Gs);il=o(Uc,"SPAN",{});var Jc=n(il);m(Jt.$$.fragment,Jc),Jc.forEach(t),Uc.forEach(t),Np=f(kr),pl=o(kr,"SPAN",{});var Yc=n(pl);Op=i(Yc,"Offline"),Yc.forEach(t),kr.forEach(t),rn=f(s),Ya=o(s,"P",{});var Bc=n(Ya);Hp=i(Bc,"Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),Bc.forEach(t),pn=f(s),U=o(s,"P",{});var ue=n(U);Lp=i(ue,"If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),dl=o(ue,"CODE",{});var Wc=n(dl);Fp=i(Wc,"HF_DATASETS_OFFLINE"),Wc.forEach(t),Rp=i(ue," to "),fl=o(ue,"CODE",{});var Gc=n(fl);Mp=i(Gc,"1"),Gc.forEach(t),Vp=i(ue," to enable full offline mode."),ue.forEach(t),dn=f(s),fs=o(s,"H2",{class:!0});var Er=n(fs);Qs=o(Er,"A",{id:!0,class:!0,href:!0});var Qc=n(Qs);cl=o(Qc,"SPAN",{});var Xc=n(cl);m(Yt.$$.fragment,Xc),Xc.forEach(t),Qc.forEach(t),zp=f(Er),hl=o(Er,"SPAN",{});var Kc=n(hl);Up=i(Kc,"Slice splits"),Kc.forEach(t),Er.forEach(t),fn=f(s),J=o(s,"P",{});var me=n(J);Jp=i(me,"For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Ba=o(me,"A",{href:!0});var Zc=n(Ba);Yp=i(Zc,"ReadInstruction"),Zc.forEach(t),Bp=i(me,". Strings are more compact and readable for simple cases, while "),Wa=o(me,"A",{href:!0});var sh=n(Wa);Wp=i(sh,"ReadInstruction"),sh.forEach(t),Gp=i(me," is easier to use with variable slicing parameters."),me.forEach(t),cn=f(s),Y=o(s,"P",{});var ge=n(Y);Qp=i(ge,"Concatenate the "),ul=o(ge,"CODE",{});var th=n(ul);Xp=i(th,"train"),th.forEach(t),Kp=i(ge," and "),ml=o(ge,"CODE",{});var ah=n(ml);Zp=i(ah,"test"),ah.forEach(t),sd=i(ge," split by:"),ge.forEach(t),hn=f(s),m(Bt.$$.fragment,s),un=f(s),Xs=o(s,"P",{});var qr=n(Xs);td=i(qr,"Select specific rows of the "),gl=o(qr,"CODE",{});var eh=n(gl);ad=i(eh,"train"),eh.forEach(t),ed=i(qr," split:"),qr.forEach(t),mn=f(s),m(Wt.$$.fragment,s),gn=f(s),Ga=o(s,"P",{});var lh=n(Ga);ld=i(lh,"Or select a percentage of the split with:"),lh.forEach(t),_n=f(s),m(Gt.$$.fragment,s),vn=f(s),Qa=o(s,"P",{});var oh=n(Qa);od=i(oh,"You can even select a combination of percentages from each split:"),oh.forEach(t),$n=f(s),m(Qt.$$.fragment,s),yn=f(s),Xa=o(s,"P",{});var nh=n(Xa);nd=i(nh,"Finally, create cross-validated dataset splits by:"),nh.forEach(t),wn=f(s),m(Xt.$$.fragment,s),jn=f(s),cs=o(s,"H3",{class:!0});var Pr=n(cs);Ks=o(Pr,"A",{id:!0,class:!0,href:!0});var rh=n(Ks);_l=o(rh,"SPAN",{});var ih=n(_l);m(Kt.$$.fragment,ih),ih.forEach(t),rh.forEach(t),rd=f(Pr),vl=o(Pr,"SPAN",{});var ph=n(vl);id=i(ph,"Percent slicing and rounding"),ph.forEach(t),Pr.forEach(t),bn=f(s),Ka=o(s,"P",{});var dh=n(Ka);pd=i(dh,"For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),dh.forEach(t),xn=f(s),m(Zt.$$.fragment,s),kn=f(s),Zs=o(s,"P",{});var Ar=n(Zs);dd=i(Ar,"If you want equal sized splits, use "),$l=o(Ar,"CODE",{});var fh=n($l);fd=i(fh,"pct1_dropremainder"),fh.forEach(t),cd=i(Ar," rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),Ar.forEach(t),En=f(s),m(sa.$$.fragment,s),qn=f(s),m(st.$$.fragment,s),Pn=f(s),Za=o(s,"A",{id:!0}),n(Za).forEach(t),An=f(s),hs=o(s,"H2",{class:!0});var Sr=n(hs);tt=o(Sr,"A",{id:!0,class:!0,href:!0});var ch=n(tt);yl=o(ch,"SPAN",{});var hh=n(yl);m(ta.$$.fragment,hh),hh.forEach(t),ch.forEach(t),hd=f(Sr),wl=o(Sr,"SPAN",{});var uh=n(wl);ud=i(uh,"Troubleshooting"),uh.forEach(t),Sr.forEach(t),Sn=f(s),se=o(s,"P",{});var mh=n(se);md=i(mh,"Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),mh.forEach(t),Tn=f(s),us=o(s,"H3",{class:!0});var Tr=n(us);at=o(Tr,"A",{id:!0,class:!0,href:!0});var gh=n(at);jl=o(gh,"SPAN",{});var _h=n(jl);m(aa.$$.fragment,_h),_h.forEach(t),gh.forEach(t),gd=f(Tr),bl=o(Tr,"SPAN",{});var vh=n(bl);_d=i(vh,"Manual download"),vh.forEach(t),Tr.forEach(t),Dn=f(s),N=o(s,"P",{});var ft=n(N);vd=i(ft,"Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),te=o(ft,"A",{href:!0});var $h=n(te);$d=i($h,"load_dataset()"),$h.forEach(t),yd=i(ft," to throw an "),xl=o(ft,"CODE",{});var yh=n(xl);wd=i(yh,"AssertionError"),yh.forEach(t),jd=i(ft,". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),kl=o(ft,"CODE",{});var wh=n(kl);bd=i(wh,"data_dir"),wh.forEach(t),xd=i(ft," argument to specify the path to the files you just downloaded."),ft.forEach(t),Cn=f(s),et=o(s,"P",{});var Dr=n(et);kd=i(Dr,"For example, if you try to download a configuration from the "),ea=o(Dr,"A",{href:!0,rel:!0});var jh=n(ea);Ed=i(jh,"MATINF"),jh.forEach(t),qd=i(Dr," dataset:"),Dr.forEach(t),In=f(s),m(la.$$.fragment,s),Nn=f(s),ms=o(s,"H3",{class:!0});var Cr=n(ms);lt=o(Cr,"A",{id:!0,class:!0,href:!0});var bh=n(lt);El=o(bh,"SPAN",{});var xh=n(El);m(oa.$$.fragment,xh),xh.forEach(t),bh.forEach(t),Pd=f(Cr),ql=o(Cr,"SPAN",{});var kh=n(ql);Ad=i(kh,"Specify features"),kh.forEach(t),Cr.forEach(t),On=f(s),B=o(s,"P",{});var _e=n(B);Sd=i(_e,"When you create a dataset from local files, the "),ae=o(_e,"A",{href:!0});var Eh=n(ae);Td=i(Eh,"Features"),Eh.forEach(t),Dd=i(_e," are automatically inferred by "),na=o(_e,"A",{href:!0,rel:!0});var qh=n(na);Cd=i(qh,"Apache Arrow"),qh.forEach(t),Id=i(_e,". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),_e.forEach(t),Hn=f(s),W=o(s,"P",{});var ve=n(W);Nd=i(ve,"The following example shows how you can add custom labels with "),ee=o(ve,"A",{href:!0});var Ph=n(ee);Od=i(Ph,"ClassLabel"),Ph.forEach(t),Hd=i(ve,". First, define your own labels using the "),le=o(ve,"A",{href:!0});var Ah=n(le);Ld=i(Ah,"Features"),Ah.forEach(t),Fd=i(ve," class:"),ve.forEach(t),Ln=f(s),m(ra.$$.fragment,s),Fn=f(s),G=o(s,"P",{});var $e=n(G);Rd=i($e,"Next, specify the "),Pl=o($e,"CODE",{});var Sh=n(Pl);Md=i(Sh,"features"),Sh.forEach(t),Vd=i($e," argument in "),oe=o($e,"A",{href:!0});var Th=n(oe);zd=i(Th,"load_dataset()"),Th.forEach(t),Ud=i($e," with the features you just created:"),$e.forEach(t),Rn=f(s),m(ia.$$.fragment,s),Mn=f(s),ne=o(s,"P",{});var Dh=n(ne);Jd=i(Dh,"Now when you look at your dataset features, you can see it uses the custom labels you defined:"),Dh.forEach(t),Vn=f(s),m(pa.$$.fragment,s),zn=f(s),gs=o(s,"H2",{class:!0});var Ir=n(gs);ot=o(Ir,"A",{id:!0,class:!0,href:!0});var Ch=n(ot);Al=o(Ch,"SPAN",{});var Ih=n(Al);m(da.$$.fragment,Ih),Ih.forEach(t),Ch.forEach(t),Yd=f(Ir),Sl=o(Ir,"SPAN",{});var Nh=n(Sl);Bd=i(Nh,"Metrics"),Nh.forEach(t),Ir.forEach(t),Un=f(s),re=o(s,"P",{});var Oh=n(re);Wd=i(Oh,"When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),Oh.forEach(t),Jn=f(s),m(fa.$$.fragment,s),Yn=f(s),m(nt.$$.fragment,s),Bn=f(s),_s=o(s,"H3",{class:!0});var Nr=n(_s);rt=o(Nr,"A",{id:!0,class:!0,href:!0});var Hh=n(rt);Tl=o(Hh,"SPAN",{});var Lh=n(Tl);m(ca.$$.fragment,Lh),Lh.forEach(t),Hh.forEach(t),Gd=f(Nr),Dl=o(Nr,"SPAN",{});var Fh=n(Dl);Qd=i(Fh,"Load configurations"),Fh.forEach(t),Nr.forEach(t),Wn=f(s),Q=o(s,"P",{});var ye=n(Q);Xd=i(ye,"It is possible for a metric to have different configurations. The configurations are stored in the "),Cl=o(ye,"CODE",{});var Rh=n(Cl);Kd=i(Rh,"config_name"),Rh.forEach(t),Zd=i(ye," parameter in "),ie=o(ye,"A",{href:!0});var Mh=n(ie);sf=i(Mh,"MetricInfo"),Mh.forEach(t),tf=i(ye," attribute. When you load a metric, provide the configuration name as shown in the following:"),ye.forEach(t),Gn=f(s),m(ha.$$.fragment,s),Qn=f(s),vs=o(s,"H3",{class:!0});var Or=n(vs);it=o(Or,"A",{id:!0,class:!0,href:!0});var Vh=n(it);Il=o(Vh,"SPAN",{});var zh=n(Il);m(ua.$$.fragment,zh),zh.forEach(t),Vh.forEach(t),af=f(Or),Nl=o(Or,"SPAN",{});var Uh=n(Nl);ef=i(Uh,"Distributed setup"),Uh.forEach(t),Or.forEach(t),Xn=f(s),pe=o(s,"P",{});var Jh=n(pe);lf=i(Jh,"When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),Jh.forEach(t),Kn=f(s),de=o(s,"P",{});var Yh=n(de);of=i(Yh,"For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),Yh.forEach(t),Zn=f(s),X=o(s,"OL",{});var we=n(X);Ol=o(we,"LI",{});var Bh=n(Ol);ma=o(Bh,"P",{});var Hr=n(ma);nf=i(Hr,"Define the total number of processes with the "),Hl=o(Hr,"CODE",{});var Wh=n(Hl);rf=i(Wh,"num_process"),Wh.forEach(t),pf=i(Hr," argument."),Hr.forEach(t),Bh.forEach(t),df=f(we),Ll=o(we,"LI",{});var Gh=n(Ll);$s=o(Gh,"P",{});var je=n($s);ff=i(je,"Set the process "),Fl=o(je,"CODE",{});var Qh=n(Fl);cf=i(Qh,"rank"),Qh.forEach(t),hf=i(je," as an integer between zero and "),Rl=o(je,"CODE",{});var Xh=n(Rl);uf=i(Xh,"num_process - 1"),Xh.forEach(t),mf=i(je,"."),je.forEach(t),Gh.forEach(t),gf=f(we),Ml=o(we,"LI",{});var Kh=n(Ml);ga=o(Kh,"P",{});var Lr=n(ga);_f=i(Lr,"Load your metric with "),fe=o(Lr,"A",{href:!0});var Zh=n(fe);vf=i(Zh,"load_metric()"),Zh.forEach(t),$f=i(Lr," with these arguments:"),Lr.forEach(t),Kh.forEach(t),we.forEach(t),sr=f(s),m(_a.$$.fragment,s),tr=f(s),m(pt.$$.fragment,s),ar=f(s),dt=o(s,"P",{});var Fr=n(dt);yf=i(Fr,"In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Vl=o(Fr,"CODE",{});var su=n(Vl);wf=i(su,"experiment_id"),su.forEach(t),jf=i(Fr," to distinguish the separate evaluations:"),Fr.forEach(t),er=f(s),m(va.$$.fragment,s),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(uu)),c(b,"id","load"),c(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b,"href","#load"),c(y,"class","relative group"),c(ka,"id","load-from-the-hub"),c(ks,"id","hugging-face-hub"),c(ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ks,"href","#hugging-face-hub"),c(ss,"class","relative group"),c(Ea,"href","/docs/datasets/pr_4598/en/package_reference/loading_methods#datasets.load_dataset"),c(ht,"href","https://huggingface.co/datasets/lhoestq/demo1"),c(ht,"rel","nofollow"),c(_t,"href","https://huggingface.co/datasets/allenai/c4"),c(_t,"rel","nofollow"),c(Ts,"id","local-loading-script"),c(Ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ts,"href","#local-loading-script"),c(ts,"class","relative group"),c(Pa,"href","/docs/datasets/pr_4598/en/package_reference/loading_methods#datasets.load_dataset"),c(Is,"id","local-and-remote-files"),c(Is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Is,"href","#local-and-remote-files"),c(as,"class","relative group"),c(Aa,"href","/docs/datasets/pr_4598/en/package_reference/loading_methods#datasets.load_dataset"),c(Os,"id","csv"),c(Os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Os,"href","#csv"),c(es,"class","relative group"),c(Hs,"id","json"),c(Hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Hs,"href","#json"),c(ls,"class","relative group"),c(Na,"href","/docs/datasets/pr_4598/en/package_reference/loading_methods#datasets.load_dataset"),c(Rs,"id","text-files"),c(Rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rs,"href","#text-files"),c(os,"class","relative group"),c(Ms,"id","parquet"),c(Ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ms,"href","#parquet"),c(ns,"class","relative group"),c(Vs,"id","inmemory-data"),c(Vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vs,"href","#inmemory-data"),c(rs,"class","relative group"),c(za,"href","/docs/datasets/pr_4598/en/package_reference/main_classes#datasets.Dataset"),c(Us,"id","python-dictionary"),c(Us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Us,"href","#python-dictionary"),c(is,"class","relative group"),c(Ua,"href","/docs/datasets/pr_4598/en/package_reference/main_classes#datasets.Dataset.from_dict"),c(Ys,"id","pandas-dataframe"),c(Ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ys,"href","#pandas-dataframe"),c(ps,"class","relative group"),c(Ja,"href","/docs/datasets/pr_4598/en/package_reference/main_classes#datasets.Dataset.from_pandas"),c(Gs,"id","offline"),c(Gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gs,"href","#offline"),c(ds,"class","relative group"),c(Qs,"id","slice-splits"),c(Qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qs,"href","#slice-splits"),c(fs,"class","relative group"),c(Ba,"href","/docs/datasets/pr_4598/en/package_reference/builder_classes#datasets.ReadInstruction"),c(Wa,"href","/docs/datasets/pr_4598/en/package_reference/builder_classes#datasets.ReadInstruction"),c(Ks,"id","percent-slicing-and-rounding"),c(Ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ks,"href","#percent-slicing-and-rounding"),c(cs,"class","relative group"),c(Za,"id","troubleshoot"),c(tt,"id","troubleshooting"),c(tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(tt,"href","#troubleshooting"),c(hs,"class","relative group"),c(at,"id","manual-download"),c(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(at,"href","#manual-download"),c(us,"class","relative group"),c(te,"href","/docs/datasets/pr_4598/en/package_reference/loading_methods#datasets.load_dataset"),c(ea,"href","https://huggingface.co/datasets/matinf"),c(ea,"rel","nofollow"),c(lt,"id","specify-features"),c(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lt,"href","#specify-features"),c(ms,"class","relative group"),c(ae,"href","/docs/datasets/pr_4598/en/package_reference/main_classes#datasets.Features"),c(na,"href","https://arrow.apache.org/docs/"),c(na,"rel","nofollow"),c(ee,"href","/docs/datasets/pr_4598/en/package_reference/main_classes#datasets.ClassLabel"),c(le,"href","/docs/datasets/pr_4598/en/package_reference/main_classes#datasets.Features"),c(oe,"href","/docs/datasets/pr_4598/en/package_reference/loading_methods#datasets.load_dataset"),c(ot,"id","metrics"),c(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ot,"href","#metrics"),c(gs,"class","relative group"),c(rt,"id","load-configurations"),c(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(rt,"href","#load-configurations"),c(_s,"class","relative group"),c(ie,"href","/docs/datasets/pr_4598/en/package_reference/main_classes#datasets.MetricInfo"),c(it,"id","distributed-setup"),c(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(it,"href","#distributed-setup"),c(vs,"class","relative group"),c(fe,"href","/docs/datasets/pr_4598/en/package_reference/loading_methods#datasets.load_metric")},m(s,e){a(document.head,h),p(s,k,e),p(s,y,e),a(y,b),a(b,x),g(w,x,null),a(y,j),a(y,q),a(q,K),p(s,ys,e),p(s,F,e),a(F,Z),p(s,ws,e),p(s,R,e),a(R,M),p(s,js,e),p(s,P,e),a(P,L),a(L,S),a(P,wa),a(P,bs),a(bs,ja),a(P,ba),a(P,xs),a(xs,Rr),a(P,Mr),a(P,be),a(be,Vr),a(P,zr),a(P,xe),a(xe,Ur),a(P,Jr),a(P,ke),a(ke,Yr),p(s,Gl,e),p(s,xa,e),a(xa,Br),p(s,Ql,e),p(s,ka,e),p(s,Xl,e),p(s,ss,e),a(ss,ks),a(ks,Ee),g(ct,Ee,null),a(ss,Wr),a(ss,qe),a(qe,Gr),p(s,Kl,e),p(s,Es,e),a(Es,Qr),a(Es,Pe),a(Pe,Xr),a(Es,Kr),p(s,Zl,e),p(s,V,e),a(V,Zr),a(V,Ea),a(Ea,si),a(V,ti),a(V,ht),a(ht,ai),a(V,ei),p(s,so,e),g(ut,s,e),p(s,to,e),p(s,qa,e),a(qa,li),p(s,ao,e),p(s,qs,e),a(qs,oi),a(qs,Ae),a(Ae,ni),a(qs,ri),p(s,eo,e),g(mt,s,e),p(s,lo,e),g(Ps,s,e),p(s,oo,e),p(s,D,e),a(D,ii),a(D,Se),a(Se,pi),a(D,di),a(D,Te),a(Te,fi),a(D,ci),a(D,De),a(De,hi),a(D,ui),a(D,Ce),a(Ce,mi),a(D,gi),a(D,Ie),a(Ie,_i),a(D,vi),p(s,no,e),g(gt,s,e),p(s,ro,e),g(As,s,e),p(s,io,e),p(s,z,e),a(z,$i),a(z,Ne),a(Ne,yi),a(z,wi),a(z,_t),a(_t,ji),a(z,bi),p(s,po,e),g(vt,s,e),p(s,fo,e),p(s,Ss,e),a(Ss,xi),a(Ss,Oe),a(Oe,ki),a(Ss,Ei),p(s,co,e),g($t,s,e),p(s,ho,e),p(s,ts,e),a(ts,Ts),a(Ts,He),g(yt,He,null),a(ts,qi),a(ts,Le),a(Le,Pi),p(s,uo,e),p(s,Ds,e),a(Ds,Ai),a(Ds,Pa),a(Pa,Si),a(Ds,Ti),p(s,mo,e),p(s,Cs,e),a(Cs,Fe),a(Fe,Di),a(Cs,Ci),a(Cs,Re),a(Re,Ii),p(s,go,e),g(wt,s,e),p(s,_o,e),p(s,as,e),a(as,Is),a(Is,Me),g(jt,Me,null),a(as,Ni),a(as,Ve),a(Ve,Oi),p(s,vo,e),p(s,C,e),a(C,Hi),a(C,ze),a(ze,Li),a(C,Fi),a(C,Ue),a(Ue,Ri),a(C,Mi),a(C,Je),a(Je,Vi),a(C,zi),a(C,Ye),a(Ye,Ui),a(C,Ji),a(C,Aa),a(Aa,Yi),a(C,Bi),p(s,$o,e),g(Ns,s,e),p(s,yo,e),p(s,es,e),a(es,Os),a(Os,Be),g(bt,Be,null),a(es,Wi),a(es,We),a(We,Gi),p(s,wo,e),p(s,Sa,e),a(Sa,Qi),p(s,jo,e),g(xt,s,e),p(s,bo,e),p(s,Ta,e),a(Ta,Xi),p(s,xo,e),g(kt,s,e),p(s,ko,e),p(s,Da,e),a(Da,Ki),p(s,Eo,e),g(Et,s,e),p(s,qo,e),p(s,Ca,e),a(Ca,Zi),p(s,Po,e),g(qt,s,e),p(s,Ao,e),p(s,Ia,e),a(Ia,sp),p(s,So,e),g(Pt,s,e),p(s,To,e),p(s,ls,e),a(ls,Hs),a(Hs,Ge),g(At,Ge,null),a(ls,tp),a(ls,Qe),a(Qe,ap),p(s,Do,e),p(s,Ls,e),a(Ls,ep),a(Ls,Na),a(Na,lp),a(Ls,op),p(s,Co,e),g(St,s,e),p(s,Io,e),p(s,Oa,e),a(Oa,np),p(s,No,e),g(Tt,s,e),p(s,Oo,e),p(s,Fs,e),a(Fs,rp),a(Fs,Xe),a(Xe,ip),a(Fs,pp),p(s,Ho,e),g(Dt,s,e),p(s,Lo,e),p(s,Ha,e),a(Ha,dp),p(s,Fo,e),g(Ct,s,e),p(s,Ro,e),p(s,La,e),a(La,fp),p(s,Mo,e),p(s,os,e),a(os,Rs),a(Rs,Ke),g(It,Ke,null),a(os,cp),a(os,Ze),a(Ze,hp),p(s,Vo,e),p(s,Fa,e),a(Fa,up),p(s,zo,e),g(Nt,s,e),p(s,Uo,e),p(s,Ra,e),a(Ra,mp),p(s,Jo,e),g(Ot,s,e),p(s,Yo,e),p(s,ns,e),a(ns,Ms),a(Ms,sl),g(Ht,sl,null),a(ns,gp),a(ns,tl),a(tl,_p),p(s,Bo,e),p(s,Ma,e),a(Ma,vp),p(s,Wo,e),g(Lt,s,e),p(s,Go,e),p(s,Va,e),a(Va,$p),p(s,Qo,e),g(Ft,s,e),p(s,Xo,e),p(s,rs,e),a(rs,Vs),a(Vs,al),g(Rt,al,null),a(rs,yp),a(rs,el),a(el,wp),p(s,Ko,e),p(s,zs,e),a(zs,jp),a(zs,za),a(za,bp),a(zs,xp),p(s,Zo,e),p(s,is,e),a(is,Us),a(Us,ll),g(Mt,ll,null),a(is,kp),a(is,ol),a(ol,Ep),p(s,sn,e),p(s,Js,e),a(Js,qp),a(Js,Ua),a(Ua,Pp),a(Js,Ap),p(s,tn,e),g(Vt,s,e),p(s,an,e),p(s,ps,e),a(ps,Ys),a(Ys,nl),g(zt,nl,null),a(ps,Sp),a(ps,rl),a(rl,Tp),p(s,en,e),p(s,Bs,e),a(Bs,Dp),a(Bs,Ja),a(Ja,Cp),a(Bs,Ip),p(s,ln,e),g(Ut,s,e),p(s,on,e),g(Ws,s,e),p(s,nn,e),p(s,ds,e),a(ds,Gs),a(Gs,il),g(Jt,il,null),a(ds,Np),a(ds,pl),a(pl,Op),p(s,rn,e),p(s,Ya,e),a(Ya,Hp),p(s,pn,e),p(s,U,e),a(U,Lp),a(U,dl),a(dl,Fp),a(U,Rp),a(U,fl),a(fl,Mp),a(U,Vp),p(s,dn,e),p(s,fs,e),a(fs,Qs),a(Qs,cl),g(Yt,cl,null),a(fs,zp),a(fs,hl),a(hl,Up),p(s,fn,e),p(s,J,e),a(J,Jp),a(J,Ba),a(Ba,Yp),a(J,Bp),a(J,Wa),a(Wa,Wp),a(J,Gp),p(s,cn,e),p(s,Y,e),a(Y,Qp),a(Y,ul),a(ul,Xp),a(Y,Kp),a(Y,ml),a(ml,Zp),a(Y,sd),p(s,hn,e),g(Bt,s,e),p(s,un,e),p(s,Xs,e),a(Xs,td),a(Xs,gl),a(gl,ad),a(Xs,ed),p(s,mn,e),g(Wt,s,e),p(s,gn,e),p(s,Ga,e),a(Ga,ld),p(s,_n,e),g(Gt,s,e),p(s,vn,e),p(s,Qa,e),a(Qa,od),p(s,$n,e),g(Qt,s,e),p(s,yn,e),p(s,Xa,e),a(Xa,nd),p(s,wn,e),g(Xt,s,e),p(s,jn,e),p(s,cs,e),a(cs,Ks),a(Ks,_l),g(Kt,_l,null),a(cs,rd),a(cs,vl),a(vl,id),p(s,bn,e),p(s,Ka,e),a(Ka,pd),p(s,xn,e),g(Zt,s,e),p(s,kn,e),p(s,Zs,e),a(Zs,dd),a(Zs,$l),a($l,fd),a(Zs,cd),p(s,En,e),g(sa,s,e),p(s,qn,e),g(st,s,e),p(s,Pn,e),p(s,Za,e),p(s,An,e),p(s,hs,e),a(hs,tt),a(tt,yl),g(ta,yl,null),a(hs,hd),a(hs,wl),a(wl,ud),p(s,Sn,e),p(s,se,e),a(se,md),p(s,Tn,e),p(s,us,e),a(us,at),a(at,jl),g(aa,jl,null),a(us,gd),a(us,bl),a(bl,_d),p(s,Dn,e),p(s,N,e),a(N,vd),a(N,te),a(te,$d),a(N,yd),a(N,xl),a(xl,wd),a(N,jd),a(N,kl),a(kl,bd),a(N,xd),p(s,Cn,e),p(s,et,e),a(et,kd),a(et,ea),a(ea,Ed),a(et,qd),p(s,In,e),g(la,s,e),p(s,Nn,e),p(s,ms,e),a(ms,lt),a(lt,El),g(oa,El,null),a(ms,Pd),a(ms,ql),a(ql,Ad),p(s,On,e),p(s,B,e),a(B,Sd),a(B,ae),a(ae,Td),a(B,Dd),a(B,na),a(na,Cd),a(B,Id),p(s,Hn,e),p(s,W,e),a(W,Nd),a(W,ee),a(ee,Od),a(W,Hd),a(W,le),a(le,Ld),a(W,Fd),p(s,Ln,e),g(ra,s,e),p(s,Fn,e),p(s,G,e),a(G,Rd),a(G,Pl),a(Pl,Md),a(G,Vd),a(G,oe),a(oe,zd),a(G,Ud),p(s,Rn,e),g(ia,s,e),p(s,Mn,e),p(s,ne,e),a(ne,Jd),p(s,Vn,e),g(pa,s,e),p(s,zn,e),p(s,gs,e),a(gs,ot),a(ot,Al),g(da,Al,null),a(gs,Yd),a(gs,Sl),a(Sl,Bd),p(s,Un,e),p(s,re,e),a(re,Wd),p(s,Jn,e),g(fa,s,e),p(s,Yn,e),g(nt,s,e),p(s,Bn,e),p(s,_s,e),a(_s,rt),a(rt,Tl),g(ca,Tl,null),a(_s,Gd),a(_s,Dl),a(Dl,Qd),p(s,Wn,e),p(s,Q,e),a(Q,Xd),a(Q,Cl),a(Cl,Kd),a(Q,Zd),a(Q,ie),a(ie,sf),a(Q,tf),p(s,Gn,e),g(ha,s,e),p(s,Qn,e),p(s,vs,e),a(vs,it),a(it,Il),g(ua,Il,null),a(vs,af),a(vs,Nl),a(Nl,ef),p(s,Xn,e),p(s,pe,e),a(pe,lf),p(s,Kn,e),p(s,de,e),a(de,of),p(s,Zn,e),p(s,X,e),a(X,Ol),a(Ol,ma),a(ma,nf),a(ma,Hl),a(Hl,rf),a(ma,pf),a(X,df),a(X,Ll),a(Ll,$s),a($s,ff),a($s,Fl),a(Fl,cf),a($s,hf),a($s,Rl),a(Rl,uf),a($s,mf),a(X,gf),a(X,Ml),a(Ml,ga),a(ga,_f),a(ga,fe),a(fe,vf),a(ga,$f),p(s,sr,e),g(_a,s,e),p(s,tr,e),g(pt,s,e),p(s,ar,e),p(s,dt,e),a(dt,yf),a(dt,Vl),a(Vl,wf),a(dt,jf),p(s,er,e),g(va,s,e),lr=!0},p(s,[e]){const $a={};e&2&&($a.$$scope={dirty:e,ctx:s}),Ps.$set($a);const zl={};e&2&&(zl.$$scope={dirty:e,ctx:s}),As.$set(zl);const Ul={};e&2&&(Ul.$$scope={dirty:e,ctx:s}),Ns.$set(Ul);const Jl={};e&2&&(Jl.$$scope={dirty:e,ctx:s}),Ws.$set(Jl);const Yl={};e&2&&(Yl.$$scope={dirty:e,ctx:s}),st.$set(Yl);const Bl={};e&2&&(Bl.$$scope={dirty:e,ctx:s}),nt.$set(Bl);const I={};e&2&&(I.$$scope={dirty:e,ctx:s}),pt.$set(I)},i(s){lr||(_(w.$$.fragment,s),_(ct.$$.fragment,s),_(ut.$$.fragment,s),_(mt.$$.fragment,s),_(Ps.$$.fragment,s),_(gt.$$.fragment,s),_(As.$$.fragment,s),_(vt.$$.fragment,s),_($t.$$.fragment,s),_(yt.$$.fragment,s),_(wt.$$.fragment,s),_(jt.$$.fragment,s),_(Ns.$$.fragment,s),_(bt.$$.fragment,s),_(xt.$$.fragment,s),_(kt.$$.fragment,s),_(Et.$$.fragment,s),_(qt.$$.fragment,s),_(Pt.$$.fragment,s),_(At.$$.fragment,s),_(St.$$.fragment,s),_(Tt.$$.fragment,s),_(Dt.$$.fragment,s),_(Ct.$$.fragment,s),_(It.$$.fragment,s),_(Nt.$$.fragment,s),_(Ot.$$.fragment,s),_(Ht.$$.fragment,s),_(Lt.$$.fragment,s),_(Ft.$$.fragment,s),_(Rt.$$.fragment,s),_(Mt.$$.fragment,s),_(Vt.$$.fragment,s),_(zt.$$.fragment,s),_(Ut.$$.fragment,s),_(Ws.$$.fragment,s),_(Jt.$$.fragment,s),_(Yt.$$.fragment,s),_(Bt.$$.fragment,s),_(Wt.$$.fragment,s),_(Gt.$$.fragment,s),_(Qt.$$.fragment,s),_(Xt.$$.fragment,s),_(Kt.$$.fragment,s),_(Zt.$$.fragment,s),_(sa.$$.fragment,s),_(st.$$.fragment,s),_(ta.$$.fragment,s),_(aa.$$.fragment,s),_(la.$$.fragment,s),_(oa.$$.fragment,s),_(ra.$$.fragment,s),_(ia.$$.fragment,s),_(pa.$$.fragment,s),_(da.$$.fragment,s),_(fa.$$.fragment,s),_(nt.$$.fragment,s),_(ca.$$.fragment,s),_(ha.$$.fragment,s),_(ua.$$.fragment,s),_(_a.$$.fragment,s),_(pt.$$.fragment,s),_(va.$$.fragment,s),lr=!0)},o(s){v(w.$$.fragment,s),v(ct.$$.fragment,s),v(ut.$$.fragment,s),v(mt.$$.fragment,s),v(Ps.$$.fragment,s),v(gt.$$.fragment,s),v(As.$$.fragment,s),v(vt.$$.fragment,s),v($t.$$.fragment,s),v(yt.$$.fragment,s),v(wt.$$.fragment,s),v(jt.$$.fragment,s),v(Ns.$$.fragment,s),v(bt.$$.fragment,s),v(xt.$$.fragment,s),v(kt.$$.fragment,s),v(Et.$$.fragment,s),v(qt.$$.fragment,s),v(Pt.$$.fragment,s),v(At.$$.fragment,s),v(St.$$.fragment,s),v(Tt.$$.fragment,s),v(Dt.$$.fragment,s),v(Ct.$$.fragment,s),v(It.$$.fragment,s),v(Nt.$$.fragment,s),v(Ot.$$.fragment,s),v(Ht.$$.fragment,s),v(Lt.$$.fragment,s),v(Ft.$$.fragment,s),v(Rt.$$.fragment,s),v(Mt.$$.fragment,s),v(Vt.$$.fragment,s),v(zt.$$.fragment,s),v(Ut.$$.fragment,s),v(Ws.$$.fragment,s),v(Jt.$$.fragment,s),v(Yt.$$.fragment,s),v(Bt.$$.fragment,s),v(Wt.$$.fragment,s),v(Gt.$$.fragment,s),v(Qt.$$.fragment,s),v(Xt.$$.fragment,s),v(Kt.$$.fragment,s),v(Zt.$$.fragment,s),v(sa.$$.fragment,s),v(st.$$.fragment,s),v(ta.$$.fragment,s),v(aa.$$.fragment,s),v(la.$$.fragment,s),v(oa.$$.fragment,s),v(ra.$$.fragment,s),v(ia.$$.fragment,s),v(pa.$$.fragment,s),v(da.$$.fragment,s),v(fa.$$.fragment,s),v(nt.$$.fragment,s),v(ca.$$.fragment,s),v(ha.$$.fragment,s),v(ua.$$.fragment,s),v(_a.$$.fragment,s),v(pt.$$.fragment,s),v(va.$$.fragment,s),lr=!1},d(s){t(h),s&&t(k),s&&t(y),$(w),s&&t(ys),s&&t(F),s&&t(ws),s&&t(R),s&&t(js),s&&t(P),s&&t(Gl),s&&t(xa),s&&t(Ql),s&&t(ka),s&&t(Xl),s&&t(ss),$(ct),s&&t(Kl),s&&t(Es),s&&t(Zl),s&&t(V),s&&t(so),$(ut,s),s&&t(to),s&&t(qa),s&&t(ao),s&&t(qs),s&&t(eo),$(mt,s),s&&t(lo),$(Ps,s),s&&t(oo),s&&t(D),s&&t(no),$(gt,s),s&&t(ro),$(As,s),s&&t(io),s&&t(z),s&&t(po),$(vt,s),s&&t(fo),s&&t(Ss),s&&t(co),$($t,s),s&&t(ho),s&&t(ts),$(yt),s&&t(uo),s&&t(Ds),s&&t(mo),s&&t(Cs),s&&t(go),$(wt,s),s&&t(_o),s&&t(as),$(jt),s&&t(vo),s&&t(C),s&&t($o),$(Ns,s),s&&t(yo),s&&t(es),$(bt),s&&t(wo),s&&t(Sa),s&&t(jo),$(xt,s),s&&t(bo),s&&t(Ta),s&&t(xo),$(kt,s),s&&t(ko),s&&t(Da),s&&t(Eo),$(Et,s),s&&t(qo),s&&t(Ca),s&&t(Po),$(qt,s),s&&t(Ao),s&&t(Ia),s&&t(So),$(Pt,s),s&&t(To),s&&t(ls),$(At),s&&t(Do),s&&t(Ls),s&&t(Co),$(St,s),s&&t(Io),s&&t(Oa),s&&t(No),$(Tt,s),s&&t(Oo),s&&t(Fs),s&&t(Ho),$(Dt,s),s&&t(Lo),s&&t(Ha),s&&t(Fo),$(Ct,s),s&&t(Ro),s&&t(La),s&&t(Mo),s&&t(os),$(It),s&&t(Vo),s&&t(Fa),s&&t(zo),$(Nt,s),s&&t(Uo),s&&t(Ra),s&&t(Jo),$(Ot,s),s&&t(Yo),s&&t(ns),$(Ht),s&&t(Bo),s&&t(Ma),s&&t(Wo),$(Lt,s),s&&t(Go),s&&t(Va),s&&t(Qo),$(Ft,s),s&&t(Xo),s&&t(rs),$(Rt),s&&t(Ko),s&&t(zs),s&&t(Zo),s&&t(is),$(Mt),s&&t(sn),s&&t(Js),s&&t(tn),$(Vt,s),s&&t(an),s&&t(ps),$(zt),s&&t(en),s&&t(Bs),s&&t(ln),$(Ut,s),s&&t(on),$(Ws,s),s&&t(nn),s&&t(ds),$(Jt),s&&t(rn),s&&t(Ya),s&&t(pn),s&&t(U),s&&t(dn),s&&t(fs),$(Yt),s&&t(fn),s&&t(J),s&&t(cn),s&&t(Y),s&&t(hn),$(Bt,s),s&&t(un),s&&t(Xs),s&&t(mn),$(Wt,s),s&&t(gn),s&&t(Ga),s&&t(_n),$(Gt,s),s&&t(vn),s&&t(Qa),s&&t($n),$(Qt,s),s&&t(yn),s&&t(Xa),s&&t(wn),$(Xt,s),s&&t(jn),s&&t(cs),$(Kt),s&&t(bn),s&&t(Ka),s&&t(xn),$(Zt,s),s&&t(kn),s&&t(Zs),s&&t(En),$(sa,s),s&&t(qn),$(st,s),s&&t(Pn),s&&t(Za),s&&t(An),s&&t(hs),$(ta),s&&t(Sn),s&&t(se),s&&t(Tn),s&&t(us),$(aa),s&&t(Dn),s&&t(N),s&&t(Cn),s&&t(et),s&&t(In),$(la,s),s&&t(Nn),s&&t(ms),$(oa),s&&t(On),s&&t(B),s&&t(Hn),s&&t(W),s&&t(Ln),$(ra,s),s&&t(Fn),s&&t(G),s&&t(Rn),$(ia,s),s&&t(Mn),s&&t(ne),s&&t(Vn),$(pa,s),s&&t(zn),s&&t(gs),$(da),s&&t(Un),s&&t(re),s&&t(Jn),$(fa,s),s&&t(Yn),$(nt,s),s&&t(Bn),s&&t(_s),$(ca),s&&t(Wn),s&&t(Q),s&&t(Gn),$(ha,s),s&&t(Qn),s&&t(vs),$(ua),s&&t(Xn),s&&t(pe),s&&t(Kn),s&&t(de),s&&t(Zn),s&&t(X),s&&t(sr),$(_a,s),s&&t(tr),$(pt,s),s&&t(ar),s&&t(dt),s&&t(er),$(va,s)}}}const uu={local:"load",sections:[{local:"hugging-face-hub",title:"Hugging Face Hub"},{local:"local-loading-script",title:"Local loading script"},{local:"local-and-remote-files",sections:[{local:"csv",title:"CSV"},{local:"json",title:"JSON"},{local:"text-files",title:"Text files"},{local:"parquet",title:"Parquet"}],title:"Local and remote files"},{local:"inmemory-data",sections:[{local:"python-dictionary",title:"Python dictionary"},{local:"pandas-dataframe",title:"Pandas DataFrame"}],title:"In-memory data"},{local:"offline",title:"Offline"},{local:"slice-splits",sections:[{local:"percent-slicing-and-rounding",title:"Percent slicing and rounding"}],title:"Slice splits"},{local:"troubleshooting",sections:[{local:"manual-download",title:"Manual download"},{local:"specify-features",title:"Specify features"}],title:"Troubleshooting"},{local:"metrics",sections:[{local:"load-configurations",title:"Load configurations"},{local:"distributed-setup",title:"Distributed setup"}],title:"Metrics"}],title:"Load"};function mu(T){return ou(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ju extends tu{constructor(h){super();au(this,h,mu,hu,eu,{})}}export{ju as default,uu as metadata};
