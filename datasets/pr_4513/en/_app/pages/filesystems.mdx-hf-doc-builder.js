import{S as lo,i as oo,s as ro,e as a,k as c,w as f,t as i,M as no,c as l,d as t,m as d,a as o,x as h,h as p,b as m,G as e,g as n,y as g,q as u,o as _,B as y,v as io}from"../chunks/vendor-hf-doc-builder.js";import{T as po}from"../chunks/Tip-hf-doc-builder.js";import{I as Ps}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as b}from"../chunks/CodeBlock-hf-doc-builder.js";function co(Dt){let v,B,$,k,F,S,V,D;return{c(){v=a("p"),B=i("Remember to include your "),$=a("code"),k=i("aws_access_key_id"),F=i(" and "),S=a("code"),V=i("aws_secret_access_key"),D=i(" whenever you are interacting with a private S3 bucket.")},l(L){v=l(L,"P",{});var j=o(v);B=p(j,"Remember to include your "),$=l(j,"CODE",{});var E=o($);k=p(E,"aws_access_key_id"),E.forEach(t),F=p(j," and "),S=l(j,"CODE",{});var Os=o(S);V=p(Os,"aws_secret_access_key"),Os.forEach(t),D=p(j," whenever you are interacting with a private S3 bucket."),j.forEach(t)},m(L,j){n(L,v,j),e(v,B),e(v,$),e($,k),e(v,F),e(v,S),e(S,V),e(v,D)},d(L){L&&t(v)}}}function mo(Dt){let v,B,$,k,F,S,V,D,L,j,E,Os,Bs,Xe,ze,Lt,G,Ms,Z,Ys,Pe,Oe,Js,Be,Ge,w,ss,Us,Ie,Ne,Ks,ts,qe,He,es,Qs,Re,We,Vs,as,Me,Ye,ls,Zs,Je,Ue,st,os,Ke,Qe,rs,tt,Ve,Ze,et,ns,sa,ta,is,at,ea,aa,lt,ps,la,Tt,I,oa,ot,ra,na,Ct,T,N,rt,cs,ia,nt,pa,Xt,Gs,it,ca,zt,ds,Pt,ms,fs,da,pt,ma,fa,Ot,hs,Bt,A,ha,ct,ga,ua,dt,_a,ya,Gt,gs,It,C,q,mt,us,va,ft,$a,Nt,H,Sa,Is,ja,wa,qt,_s,Ht,R,Rt,W,ba,ht,ka,Ea,Wt,ys,Mt,X,M,gt,vs,Aa,ut,xa,Yt,Y,Fa,Ns,Da,La,Jt,$s,Ut,J,Ta,_t,Ca,Xa,Kt,Ss,Qt,z,U,yt,js,za,vt,Pa,Vt,qs,Oa,Zt,P,K,$t,ws,Ba,St,Ga,se,Hs,jt,Ia,te,bs,ee,ks,wt,Na,ae,Es,le,As,bt,qa,oe,xs,re,O,Q,kt,Fs,Ha,Et,Ra,ne,Rs,At,Wa,ie,Ds,pe,Ls,xt,Ma,ce,Ts,de,Cs,Ft,Ya,me,Xs,fe;return S=new Ps({}),cs=new Ps({}),ds=new b({props:{code:"pip install datasets[s3]",highlighted:'&gt;&gt;&gt; pip <span class="hljs-keyword">install </span>datasets[<span class="hljs-built_in">s3</span>]'}}),hs=new b({props:{code:`import datasets
s3 = datasets.filesystems.S3FileSystem(anon=True)  
s3.ls('public-datasets/imdb/train')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = datasets.filesystems.S3FileSystem(anon=<span class="hljs-literal">True</span>)  
<span class="hljs-meta">&gt;&gt;&gt; </span>s3.ls(<span class="hljs-string">&#x27;public-datasets/imdb/train&#x27;</span>)
[<span class="hljs-string">&#x27;dataset_info.json.json&#x27;</span>,<span class="hljs-string">&#x27;dataset.arrow&#x27;</span>,<span class="hljs-string">&#x27;state.json&#x27;</span>]`}}),gs=new b({props:{code:`import datasets
s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)  
s3.ls('my-private-datasets/imdb/train')  `,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)  
<span class="hljs-meta">&gt;&gt;&gt; </span>s3.ls(<span class="hljs-string">&#x27;my-private-datasets/imdb/train&#x27;</span>)  
[<span class="hljs-string">&#x27;dataset_info.json.json&#x27;</span>,<span class="hljs-string">&#x27;dataset.arrow&#x27;</span>,<span class="hljs-string">&#x27;state.json&#x27;</span>]`}}),us=new Ps({}),_s=new b({props:{code:`from datasets.filesystems import S3FileSystem

# create S3FileSystem instance
s3 = S3FileSystem(anon=True)  

# saves encoded_dataset to your s3 bucket
encoded_dataset.save_to_disk('s3://my-private-datasets/imdb/train', fs=s3)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create S3FileSystem instance</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(anon=<span class="hljs-literal">True</span>)  

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saves encoded_dataset to your s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&#x27;s3://my-private-datasets/imdb/train&#x27;</span>, fs=s3)`}}),R=new po({props:{$$slots:{default:[co]},$$scope:{ctx:Dt}}}),ys=new b({props:{code:`import botocore
from datasets.filesystems import S3FileSystem

# creates a botocore session with the provided AWS profile
s3_session = botocore.session.Session(profile='my_profile_name')

# create S3FileSystem instance with s3_session
s3 = S3FileSystem(session=s3_session)  

# saves encoded_dataset to your s3 bucket
encoded_dataset.save_to_disk('s3://my-private-datasets/imdb/train',fs=s3)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> botocore
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># creates a botocore session with the provided AWS profile</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3_session = botocore.session.Session(profile=<span class="hljs-string">&#x27;my_profile_name&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create S3FileSystem instance with s3_session</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(session=s3_session)  

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saves encoded_dataset to your s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&#x27;s3://my-private-datasets/imdb/train&#x27;</span>,fs=s3)`}}),vs=new Ps({}),$s=new b({props:{code:`from datasets import load_from_disk
from datasets.filesystems import S3FileSystem

# create S3FileSystem without credentials
s3 = S3FileSystem(anon=True)  

# load encoded_dataset to from s3 bucket
dataset = load_from_disk('s3://a-public-datasets/imdb/train',fs=s3)  

print(len(dataset))
# 25000`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create S3FileSystem without credentials</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(anon=<span class="hljs-literal">True</span>)  

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load encoded_dataset to from s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;s3://a-public-datasets/imdb/train&#x27;</span>,fs=s3)  

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 25000</span>`}}),Ss=new b({props:{code:`import botocore
from datasets.filesystems import S3FileSystem

# create S3FileSystem instance with aws_access_key_id and aws_secret_access_key
s3_session = botocore.session.Session(profile='my_profile_name')

# create S3FileSystem instance with s3_session
s3 = S3FileSystem(session=s3_session)

# load encoded_dataset to from s3 bucket
dataset = load_from_disk('s3://my-private-datasets/imdb/train',fs=s3)  

print(len(dataset))
# 25000`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> botocore
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create S3FileSystem instance with aws_access_key_id and aws_secret_access_key</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3_session = botocore.session.Session(profile=<span class="hljs-string">&#x27;my_profile_name&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create S3FileSystem instance with s3_session</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(session=s3_session)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load encoded_dataset to from s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;s3://my-private-datasets/imdb/train&#x27;</span>,fs=s3)  

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 25000</span>`}}),js=new Ps({}),ws=new Ps({}),bs=new b({props:{code:`conda install -c conda-forge gcsfs
pip install gcsfs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">conda install -c conda-forge gcsfs</span>
# or install with pip
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pip install gcsfs</span>`}}),Es=new b({props:{code:`import gcsfs

# create GCSFileSystem instance using default gcloud credentials with project
gcs = gcsfs.GCSFileSystem(project='my-google-project')

# saves encoded_dataset to your gcs bucket
encoded_dataset.save_to_disk('gcs://my-private-datasets/imdb/train', fs=gcs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> gcsfs

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create GCSFileSystem instance using default gcloud credentials with project</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>gcs = gcsfs.GCSFileSystem(project=<span class="hljs-string">&#x27;my-google-project&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saves encoded_dataset to your gcs bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&#x27;gcs://my-private-datasets/imdb/train&#x27;</span>, fs=gcs)`}}),xs=new b({props:{code:`import gcsfs
from datasets import load_from_disk

# create GCSFileSystem instance using default gcloud credentials with project
gcs = gcsfs.GCSFileSystem(project='my-google-project')

# loads encoded_dataset from your gcs bucket
dataset = load_from_disk('gcs://my-private-datasets/imdb/train', fs=gcs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> gcsfs
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create GCSFileSystem instance using default gcloud credentials with project</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>gcs = gcsfs.GCSFileSystem(project=<span class="hljs-string">&#x27;my-google-project&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># loads encoded_dataset from your gcs bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;gcs://my-private-datasets/imdb/train&#x27;</span>, fs=gcs)`}}),Fs=new Ps({}),Ds=new b({props:{code:`conda install -c conda-forge adlfs
pip install adlfs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">conda install -c conda-forge adlfs</span>
# or install with pip
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pip install adlfs</span>`}}),Ts=new b({props:{code:`import adlfs

# create AzureBlobFileSystem instance with account_name and account_key
abfs = adlfs.AzureBlobFileSystem(account_name="XXXX", account_key="XXXX")

# saves encoded_dataset to your azure container
encoded_dataset.save_to_disk('abfs://my-private-datasets/imdb/train', fs=abfs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> adlfs

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create AzureBlobFileSystem instance with account_name and account_key</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>abfs = adlfs.AzureBlobFileSystem(account_name=<span class="hljs-string">&quot;XXXX&quot;</span>, account_key=<span class="hljs-string">&quot;XXXX&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saves encoded_dataset to your azure container</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&#x27;abfs://my-private-datasets/imdb/train&#x27;</span>, fs=abfs)`}}),Xs=new b({props:{code:`import adlfs
from datasets import load_from_disk

# create AzureBlobFileSystem instance with account_name and account_key
abfs = adlfs.AzureBlobFileSystem(account_name="XXXX", account_key="XXXX")

# loads encoded_dataset from your azure container
dataset = load_from_disk('abfs://my-private-datasets/imdb/train', fs=abfs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> adlfs
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create AzureBlobFileSystem instance with account_name and account_key</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>abfs = adlfs.AzureBlobFileSystem(account_name=<span class="hljs-string">&quot;XXXX&quot;</span>, account_key=<span class="hljs-string">&quot;XXXX&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># loads encoded_dataset from your azure container</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;abfs://my-private-datasets/imdb/train&#x27;</span>, fs=abfs)`}}),{c(){v=a("meta"),B=c(),$=a("h1"),k=a("a"),F=a("span"),f(S.$$.fragment),V=c(),D=a("span"),L=i("Cloud storage"),j=c(),E=a("p"),Os=i("\u{1F917} Datasets supports access to cloud storage providers through a S3 filesystem implementation: "),Bs=a("a"),Xe=i("filesystems.S3FileSystem"),ze=i(". You can save and load datasets from your Amazon S3 bucket in a Pythonic way. Take a look at the following table for other supported cloud storage providers:"),Lt=c(),G=a("table"),Ms=a("thead"),Z=a("tr"),Ys=a("th"),Pe=i("Storage provider"),Oe=c(),Js=a("th"),Be=i("Filesystem implementation"),Ge=c(),w=a("tbody"),ss=a("tr"),Us=a("td"),Ie=i("Amazon S3"),Ne=c(),Ks=a("td"),ts=a("a"),qe=i("s3fs"),He=c(),es=a("tr"),Qs=a("td"),Re=i("Google Cloud Storage"),We=c(),Vs=a("td"),as=a("a"),Me=i("gcsfs"),Ye=c(),ls=a("tr"),Zs=a("td"),Je=i("Azure Blob/DataLake"),Ue=c(),st=a("td"),os=a("a"),Ke=i("adlfs"),Qe=c(),rs=a("tr"),tt=a("td"),Ve=i("Dropbox"),Ze=c(),et=a("td"),ns=a("a"),sa=i("dropboxdrivefs"),ta=c(),is=a("tr"),at=a("td"),ea=i("Google Drive"),aa=c(),lt=a("td"),ps=a("a"),la=i("gdrivefs"),Tt=c(),I=a("p"),oa=i("This guide will show you how to save and load datasets with "),ot=a("strong"),ra=i("s3fs"),na=i(" to a S3 bucket, but other filesystem implementations can be used similarly."),Ct=c(),T=a("h2"),N=a("a"),rt=a("span"),f(cs.$$.fragment),ia=c(),nt=a("span"),pa=i("Listing datasets"),Xt=c(),Gs=a("ol"),it=a("li"),ca=i("Install the S3 dependency with \u{1F917} Datasets:"),zt=c(),f(ds.$$.fragment),Pt=c(),ms=a("ol"),fs=a("li"),da=i("List files from a public S3 bucket with "),pt=a("code"),ma=i("s3.ls"),fa=i(":"),Ot=c(),f(hs.$$.fragment),Bt=c(),A=a("p"),ha=i("Access a private S3 bucket by entering your "),ct=a("code"),ga=i("aws_access_key_id"),ua=i(" and "),dt=a("code"),_a=i("aws_secret_access_key"),ya=i(":"),Gt=c(),f(gs.$$.fragment),It=c(),C=a("h2"),q=a("a"),mt=a("span"),f(us.$$.fragment),va=c(),ft=a("span"),$a=i("Saving datasets"),Nt=c(),H=a("p"),Sa=i("After you have processed your dataset, you can save it to S3 with "),Is=a("a"),ja=i("Dataset.save_to_disk()"),wa=i(":"),qt=c(),f(_s.$$.fragment),Ht=c(),f(R.$$.fragment),Rt=c(),W=a("p"),ba=i("Save your dataset with "),ht=a("code"),ka=i("botocore.session.Session"),Ea=i(" and a custom AWS profile:"),Wt=c(),f(ys.$$.fragment),Mt=c(),X=a("h2"),M=a("a"),gt=a("span"),f(vs.$$.fragment),Aa=c(),ut=a("span"),xa=i("Loading datasets"),Yt=c(),Y=a("p"),Fa=i("When you are ready to use your dataset again, reload it with "),Ns=a("a"),Da=i("Dataset.load_from_disk()"),La=i(":"),Jt=c(),f($s.$$.fragment),Ut=c(),J=a("p"),Ta=i("Load with "),_t=a("code"),Ca=i("botocore.session.Session"),Xa=i(" and custom AWS profile:"),Kt=c(),f(Ss.$$.fragment),Qt=c(),z=a("h2"),U=a("a"),yt=a("span"),f(js.$$.fragment),za=c(),vt=a("span"),Pa=i("Other filesystems"),Vt=c(),qs=a("p"),Oa=i("Other filesystem implementations, like Google Cloud Storage or Azure Blob Storage, are used similarly:"),Zt=c(),P=a("h3"),K=a("a"),$t=a("span"),f(ws.$$.fragment),Ba=c(),St=a("span"),Ga=i("Google Cloud Storage"),se=c(),Hs=a("ol"),jt=a("li"),Ia=i("Install the Google Cloud Storage implementation:"),te=c(),f(bs.$$.fragment),ee=c(),ks=a("ol"),wt=a("li"),Na=i("Save your dataset:"),ae=c(),f(Es.$$.fragment),le=c(),As=a("ol"),bt=a("li"),qa=i("Load your dataset:"),oe=c(),f(xs.$$.fragment),re=c(),O=a("h3"),Q=a("a"),kt=a("span"),f(Fs.$$.fragment),Ha=c(),Et=a("span"),Ra=i("Azure Blob Storage"),ne=c(),Rs=a("ol"),At=a("li"),Wa=i("Install the Azure Blob Storage implementation:"),ie=c(),f(Ds.$$.fragment),pe=c(),Ls=a("ol"),xt=a("li"),Ma=i("Save your dataset:"),ce=c(),f(Ts.$$.fragment),de=c(),Cs=a("ol"),Ft=a("li"),Ya=i("Load your dataset:"),me=c(),f(Xs.$$.fragment),this.h()},l(s){const r=no('[data-svelte="svelte-1phssyn"]',document.head);v=l(r,"META",{name:!0,content:!0}),r.forEach(t),B=d(s),$=l(s,"H1",{class:!0});var zs=o($);k=l(zs,"A",{id:!0,class:!0,href:!0});var Ja=o(k);F=l(Ja,"SPAN",{});var Ua=o(F);h(S.$$.fragment,Ua),Ua.forEach(t),Ja.forEach(t),V=d(zs),D=l(zs,"SPAN",{});var Ka=o(D);L=p(Ka,"Cloud storage"),Ka.forEach(t),zs.forEach(t),j=d(s),E=l(s,"P",{});var he=o(E);Os=p(he,"\u{1F917} Datasets supports access to cloud storage providers through a S3 filesystem implementation: "),Bs=l(he,"A",{href:!0});var Qa=o(Bs);Xe=p(Qa,"filesystems.S3FileSystem"),Qa.forEach(t),ze=p(he,". You can save and load datasets from your Amazon S3 bucket in a Pythonic way. Take a look at the following table for other supported cloud storage providers:"),he.forEach(t),Lt=d(s),G=l(s,"TABLE",{});var ge=o(G);Ms=l(ge,"THEAD",{});var Va=o(Ms);Z=l(Va,"TR",{});var ue=o(Z);Ys=l(ue,"TH",{});var Za=o(Ys);Pe=p(Za,"Storage provider"),Za.forEach(t),Oe=d(ue),Js=l(ue,"TH",{});var sl=o(Js);Be=p(sl,"Filesystem implementation"),sl.forEach(t),ue.forEach(t),Va.forEach(t),Ge=d(ge),w=l(ge,"TBODY",{});var x=o(w);ss=l(x,"TR",{});var _e=o(ss);Us=l(_e,"TD",{});var tl=o(Us);Ie=p(tl,"Amazon S3"),tl.forEach(t),Ne=d(_e),Ks=l(_e,"TD",{});var el=o(Ks);ts=l(el,"A",{href:!0,rel:!0});var al=o(ts);qe=p(al,"s3fs"),al.forEach(t),el.forEach(t),_e.forEach(t),He=d(x),es=l(x,"TR",{});var ye=o(es);Qs=l(ye,"TD",{});var ll=o(Qs);Re=p(ll,"Google Cloud Storage"),ll.forEach(t),We=d(ye),Vs=l(ye,"TD",{});var ol=o(Vs);as=l(ol,"A",{href:!0,rel:!0});var rl=o(as);Me=p(rl,"gcsfs"),rl.forEach(t),ol.forEach(t),ye.forEach(t),Ye=d(x),ls=l(x,"TR",{});var ve=o(ls);Zs=l(ve,"TD",{});var nl=o(Zs);Je=p(nl,"Azure Blob/DataLake"),nl.forEach(t),Ue=d(ve),st=l(ve,"TD",{});var il=o(st);os=l(il,"A",{href:!0,rel:!0});var pl=o(os);Ke=p(pl,"adlfs"),pl.forEach(t),il.forEach(t),ve.forEach(t),Qe=d(x),rs=l(x,"TR",{});var $e=o(rs);tt=l($e,"TD",{});var cl=o(tt);Ve=p(cl,"Dropbox"),cl.forEach(t),Ze=d($e),et=l($e,"TD",{});var dl=o(et);ns=l(dl,"A",{href:!0,rel:!0});var ml=o(ns);sa=p(ml,"dropboxdrivefs"),ml.forEach(t),dl.forEach(t),$e.forEach(t),ta=d(x),is=l(x,"TR",{});var Se=o(is);at=l(Se,"TD",{});var fl=o(at);ea=p(fl,"Google Drive"),fl.forEach(t),aa=d(Se),lt=l(Se,"TD",{});var hl=o(lt);ps=l(hl,"A",{href:!0,rel:!0});var gl=o(ps);la=p(gl,"gdrivefs"),gl.forEach(t),hl.forEach(t),Se.forEach(t),x.forEach(t),ge.forEach(t),Tt=d(s),I=l(s,"P",{});var je=o(I);oa=p(je,"This guide will show you how to save and load datasets with "),ot=l(je,"STRONG",{});var ul=o(ot);ra=p(ul,"s3fs"),ul.forEach(t),na=p(je," to a S3 bucket, but other filesystem implementations can be used similarly."),je.forEach(t),Ct=d(s),T=l(s,"H2",{class:!0});var we=o(T);N=l(we,"A",{id:!0,class:!0,href:!0});var _l=o(N);rt=l(_l,"SPAN",{});var yl=o(rt);h(cs.$$.fragment,yl),yl.forEach(t),_l.forEach(t),ia=d(we),nt=l(we,"SPAN",{});var vl=o(nt);pa=p(vl,"Listing datasets"),vl.forEach(t),we.forEach(t),Xt=d(s),Gs=l(s,"OL",{});var $l=o(Gs);it=l($l,"LI",{});var Sl=o(it);ca=p(Sl,"Install the S3 dependency with \u{1F917} Datasets:"),Sl.forEach(t),$l.forEach(t),zt=d(s),h(ds.$$.fragment,s),Pt=d(s),ms=l(s,"OL",{start:!0});var jl=o(ms);fs=l(jl,"LI",{});var be=o(fs);da=p(be,"List files from a public S3 bucket with "),pt=l(be,"CODE",{});var wl=o(pt);ma=p(wl,"s3.ls"),wl.forEach(t),fa=p(be,":"),be.forEach(t),jl.forEach(t),Ot=d(s),h(hs.$$.fragment,s),Bt=d(s),A=l(s,"P",{});var Ws=o(A);ha=p(Ws,"Access a private S3 bucket by entering your "),ct=l(Ws,"CODE",{});var bl=o(ct);ga=p(bl,"aws_access_key_id"),bl.forEach(t),ua=p(Ws," and "),dt=l(Ws,"CODE",{});var kl=o(dt);_a=p(kl,"aws_secret_access_key"),kl.forEach(t),ya=p(Ws,":"),Ws.forEach(t),Gt=d(s),h(gs.$$.fragment,s),It=d(s),C=l(s,"H2",{class:!0});var ke=o(C);q=l(ke,"A",{id:!0,class:!0,href:!0});var El=o(q);mt=l(El,"SPAN",{});var Al=o(mt);h(us.$$.fragment,Al),Al.forEach(t),El.forEach(t),va=d(ke),ft=l(ke,"SPAN",{});var xl=o(ft);$a=p(xl,"Saving datasets"),xl.forEach(t),ke.forEach(t),Nt=d(s),H=l(s,"P",{});var Ee=o(H);Sa=p(Ee,"After you have processed your dataset, you can save it to S3 with "),Is=l(Ee,"A",{href:!0});var Fl=o(Is);ja=p(Fl,"Dataset.save_to_disk()"),Fl.forEach(t),wa=p(Ee,":"),Ee.forEach(t),qt=d(s),h(_s.$$.fragment,s),Ht=d(s),h(R.$$.fragment,s),Rt=d(s),W=l(s,"P",{});var Ae=o(W);ba=p(Ae,"Save your dataset with "),ht=l(Ae,"CODE",{});var Dl=o(ht);ka=p(Dl,"botocore.session.Session"),Dl.forEach(t),Ea=p(Ae," and a custom AWS profile:"),Ae.forEach(t),Wt=d(s),h(ys.$$.fragment,s),Mt=d(s),X=l(s,"H2",{class:!0});var xe=o(X);M=l(xe,"A",{id:!0,class:!0,href:!0});var Ll=o(M);gt=l(Ll,"SPAN",{});var Tl=o(gt);h(vs.$$.fragment,Tl),Tl.forEach(t),Ll.forEach(t),Aa=d(xe),ut=l(xe,"SPAN",{});var Cl=o(ut);xa=p(Cl,"Loading datasets"),Cl.forEach(t),xe.forEach(t),Yt=d(s),Y=l(s,"P",{});var Fe=o(Y);Fa=p(Fe,"When you are ready to use your dataset again, reload it with "),Ns=l(Fe,"A",{href:!0});var Xl=o(Ns);Da=p(Xl,"Dataset.load_from_disk()"),Xl.forEach(t),La=p(Fe,":"),Fe.forEach(t),Jt=d(s),h($s.$$.fragment,s),Ut=d(s),J=l(s,"P",{});var De=o(J);Ta=p(De,"Load with "),_t=l(De,"CODE",{});var zl=o(_t);Ca=p(zl,"botocore.session.Session"),zl.forEach(t),Xa=p(De," and custom AWS profile:"),De.forEach(t),Kt=d(s),h(Ss.$$.fragment,s),Qt=d(s),z=l(s,"H2",{class:!0});var Le=o(z);U=l(Le,"A",{id:!0,class:!0,href:!0});var Pl=o(U);yt=l(Pl,"SPAN",{});var Ol=o(yt);h(js.$$.fragment,Ol),Ol.forEach(t),Pl.forEach(t),za=d(Le),vt=l(Le,"SPAN",{});var Bl=o(vt);Pa=p(Bl,"Other filesystems"),Bl.forEach(t),Le.forEach(t),Vt=d(s),qs=l(s,"P",{});var Gl=o(qs);Oa=p(Gl,"Other filesystem implementations, like Google Cloud Storage or Azure Blob Storage, are used similarly:"),Gl.forEach(t),Zt=d(s),P=l(s,"H3",{class:!0});var Te=o(P);K=l(Te,"A",{id:!0,class:!0,href:!0});var Il=o(K);$t=l(Il,"SPAN",{});var Nl=o($t);h(ws.$$.fragment,Nl),Nl.forEach(t),Il.forEach(t),Ba=d(Te),St=l(Te,"SPAN",{});var ql=o(St);Ga=p(ql,"Google Cloud Storage"),ql.forEach(t),Te.forEach(t),se=d(s),Hs=l(s,"OL",{});var Hl=o(Hs);jt=l(Hl,"LI",{});var Rl=o(jt);Ia=p(Rl,"Install the Google Cloud Storage implementation:"),Rl.forEach(t),Hl.forEach(t),te=d(s),h(bs.$$.fragment,s),ee=d(s),ks=l(s,"OL",{start:!0});var Wl=o(ks);wt=l(Wl,"LI",{});var Ml=o(wt);Na=p(Ml,"Save your dataset:"),Ml.forEach(t),Wl.forEach(t),ae=d(s),h(Es.$$.fragment,s),le=d(s),As=l(s,"OL",{start:!0});var Yl=o(As);bt=l(Yl,"LI",{});var Jl=o(bt);qa=p(Jl,"Load your dataset:"),Jl.forEach(t),Yl.forEach(t),oe=d(s),h(xs.$$.fragment,s),re=d(s),O=l(s,"H3",{class:!0});var Ce=o(O);Q=l(Ce,"A",{id:!0,class:!0,href:!0});var Ul=o(Q);kt=l(Ul,"SPAN",{});var Kl=o(kt);h(Fs.$$.fragment,Kl),Kl.forEach(t),Ul.forEach(t),Ha=d(Ce),Et=l(Ce,"SPAN",{});var Ql=o(Et);Ra=p(Ql,"Azure Blob Storage"),Ql.forEach(t),Ce.forEach(t),ne=d(s),Rs=l(s,"OL",{});var Vl=o(Rs);At=l(Vl,"LI",{});var Zl=o(At);Wa=p(Zl,"Install the Azure Blob Storage implementation:"),Zl.forEach(t),Vl.forEach(t),ie=d(s),h(Ds.$$.fragment,s),pe=d(s),Ls=l(s,"OL",{start:!0});var so=o(Ls);xt=l(so,"LI",{});var to=o(xt);Ma=p(to,"Save your dataset:"),to.forEach(t),so.forEach(t),ce=d(s),h(Ts.$$.fragment,s),de=d(s),Cs=l(s,"OL",{start:!0});var eo=o(Cs);Ft=l(eo,"LI",{});var ao=o(Ft);Ya=p(ao,"Load your dataset:"),ao.forEach(t),eo.forEach(t),me=d(s),h(Xs.$$.fragment,s),this.h()},h(){m(v,"name","hf:doc:metadata"),m(v,"content",JSON.stringify(fo)),m(k,"id","cloud-storage"),m(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(k,"href","#cloud-storage"),m($,"class","relative group"),m(Bs,"href","/docs/datasets/pr_4513/en/package_reference/main_classes#datasets.filesystems.S3FileSystem"),m(ts,"href","https://s3fs.readthedocs.io/en/latest/"),m(ts,"rel","nofollow"),m(as,"href","https://gcsfs.readthedocs.io/en/latest/"),m(as,"rel","nofollow"),m(os,"href","https://github.com/fsspec/adlfs"),m(os,"rel","nofollow"),m(ns,"href","https://github.com/MarineChap/dropboxdrivefs"),m(ns,"rel","nofollow"),m(ps,"href","https://github.com/intake/gdrivefs"),m(ps,"rel","nofollow"),m(N,"id","listing-datasets"),m(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(N,"href","#listing-datasets"),m(T,"class","relative group"),m(ms,"start","2"),m(q,"id","saving-datasets"),m(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(q,"href","#saving-datasets"),m(C,"class","relative group"),m(Is,"href","/docs/datasets/pr_4513/en/package_reference/main_classes#datasets.Dataset.save_to_disk"),m(M,"id","loading-datasets"),m(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(M,"href","#loading-datasets"),m(X,"class","relative group"),m(Ns,"href","/docs/datasets/pr_4513/en/package_reference/main_classes#datasets.Dataset.load_from_disk"),m(U,"id","other-filesystems"),m(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(U,"href","#other-filesystems"),m(z,"class","relative group"),m(K,"id","google-cloud-storage"),m(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(K,"href","#google-cloud-storage"),m(P,"class","relative group"),m(ks,"start","2"),m(As,"start","3"),m(Q,"id","azure-blob-storage"),m(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Q,"href","#azure-blob-storage"),m(O,"class","relative group"),m(Ls,"start","2"),m(Cs,"start","3")},m(s,r){e(document.head,v),n(s,B,r),n(s,$,r),e($,k),e(k,F),g(S,F,null),e($,V),e($,D),e(D,L),n(s,j,r),n(s,E,r),e(E,Os),e(E,Bs),e(Bs,Xe),e(E,ze),n(s,Lt,r),n(s,G,r),e(G,Ms),e(Ms,Z),e(Z,Ys),e(Ys,Pe),e(Z,Oe),e(Z,Js),e(Js,Be),e(G,Ge),e(G,w),e(w,ss),e(ss,Us),e(Us,Ie),e(ss,Ne),e(ss,Ks),e(Ks,ts),e(ts,qe),e(w,He),e(w,es),e(es,Qs),e(Qs,Re),e(es,We),e(es,Vs),e(Vs,as),e(as,Me),e(w,Ye),e(w,ls),e(ls,Zs),e(Zs,Je),e(ls,Ue),e(ls,st),e(st,os),e(os,Ke),e(w,Qe),e(w,rs),e(rs,tt),e(tt,Ve),e(rs,Ze),e(rs,et),e(et,ns),e(ns,sa),e(w,ta),e(w,is),e(is,at),e(at,ea),e(is,aa),e(is,lt),e(lt,ps),e(ps,la),n(s,Tt,r),n(s,I,r),e(I,oa),e(I,ot),e(ot,ra),e(I,na),n(s,Ct,r),n(s,T,r),e(T,N),e(N,rt),g(cs,rt,null),e(T,ia),e(T,nt),e(nt,pa),n(s,Xt,r),n(s,Gs,r),e(Gs,it),e(it,ca),n(s,zt,r),g(ds,s,r),n(s,Pt,r),n(s,ms,r),e(ms,fs),e(fs,da),e(fs,pt),e(pt,ma),e(fs,fa),n(s,Ot,r),g(hs,s,r),n(s,Bt,r),n(s,A,r),e(A,ha),e(A,ct),e(ct,ga),e(A,ua),e(A,dt),e(dt,_a),e(A,ya),n(s,Gt,r),g(gs,s,r),n(s,It,r),n(s,C,r),e(C,q),e(q,mt),g(us,mt,null),e(C,va),e(C,ft),e(ft,$a),n(s,Nt,r),n(s,H,r),e(H,Sa),e(H,Is),e(Is,ja),e(H,wa),n(s,qt,r),g(_s,s,r),n(s,Ht,r),g(R,s,r),n(s,Rt,r),n(s,W,r),e(W,ba),e(W,ht),e(ht,ka),e(W,Ea),n(s,Wt,r),g(ys,s,r),n(s,Mt,r),n(s,X,r),e(X,M),e(M,gt),g(vs,gt,null),e(X,Aa),e(X,ut),e(ut,xa),n(s,Yt,r),n(s,Y,r),e(Y,Fa),e(Y,Ns),e(Ns,Da),e(Y,La),n(s,Jt,r),g($s,s,r),n(s,Ut,r),n(s,J,r),e(J,Ta),e(J,_t),e(_t,Ca),e(J,Xa),n(s,Kt,r),g(Ss,s,r),n(s,Qt,r),n(s,z,r),e(z,U),e(U,yt),g(js,yt,null),e(z,za),e(z,vt),e(vt,Pa),n(s,Vt,r),n(s,qs,r),e(qs,Oa),n(s,Zt,r),n(s,P,r),e(P,K),e(K,$t),g(ws,$t,null),e(P,Ba),e(P,St),e(St,Ga),n(s,se,r),n(s,Hs,r),e(Hs,jt),e(jt,Ia),n(s,te,r),g(bs,s,r),n(s,ee,r),n(s,ks,r),e(ks,wt),e(wt,Na),n(s,ae,r),g(Es,s,r),n(s,le,r),n(s,As,r),e(As,bt),e(bt,qa),n(s,oe,r),g(xs,s,r),n(s,re,r),n(s,O,r),e(O,Q),e(Q,kt),g(Fs,kt,null),e(O,Ha),e(O,Et),e(Et,Ra),n(s,ne,r),n(s,Rs,r),e(Rs,At),e(At,Wa),n(s,ie,r),g(Ds,s,r),n(s,pe,r),n(s,Ls,r),e(Ls,xt),e(xt,Ma),n(s,ce,r),g(Ts,s,r),n(s,de,r),n(s,Cs,r),e(Cs,Ft),e(Ft,Ya),n(s,me,r),g(Xs,s,r),fe=!0},p(s,[r]){const zs={};r&2&&(zs.$$scope={dirty:r,ctx:s}),R.$set(zs)},i(s){fe||(u(S.$$.fragment,s),u(cs.$$.fragment,s),u(ds.$$.fragment,s),u(hs.$$.fragment,s),u(gs.$$.fragment,s),u(us.$$.fragment,s),u(_s.$$.fragment,s),u(R.$$.fragment,s),u(ys.$$.fragment,s),u(vs.$$.fragment,s),u($s.$$.fragment,s),u(Ss.$$.fragment,s),u(js.$$.fragment,s),u(ws.$$.fragment,s),u(bs.$$.fragment,s),u(Es.$$.fragment,s),u(xs.$$.fragment,s),u(Fs.$$.fragment,s),u(Ds.$$.fragment,s),u(Ts.$$.fragment,s),u(Xs.$$.fragment,s),fe=!0)},o(s){_(S.$$.fragment,s),_(cs.$$.fragment,s),_(ds.$$.fragment,s),_(hs.$$.fragment,s),_(gs.$$.fragment,s),_(us.$$.fragment,s),_(_s.$$.fragment,s),_(R.$$.fragment,s),_(ys.$$.fragment,s),_(vs.$$.fragment,s),_($s.$$.fragment,s),_(Ss.$$.fragment,s),_(js.$$.fragment,s),_(ws.$$.fragment,s),_(bs.$$.fragment,s),_(Es.$$.fragment,s),_(xs.$$.fragment,s),_(Fs.$$.fragment,s),_(Ds.$$.fragment,s),_(Ts.$$.fragment,s),_(Xs.$$.fragment,s),fe=!1},d(s){t(v),s&&t(B),s&&t($),y(S),s&&t(j),s&&t(E),s&&t(Lt),s&&t(G),s&&t(Tt),s&&t(I),s&&t(Ct),s&&t(T),y(cs),s&&t(Xt),s&&t(Gs),s&&t(zt),y(ds,s),s&&t(Pt),s&&t(ms),s&&t(Ot),y(hs,s),s&&t(Bt),s&&t(A),s&&t(Gt),y(gs,s),s&&t(It),s&&t(C),y(us),s&&t(Nt),s&&t(H),s&&t(qt),y(_s,s),s&&t(Ht),y(R,s),s&&t(Rt),s&&t(W),s&&t(Wt),y(ys,s),s&&t(Mt),s&&t(X),y(vs),s&&t(Yt),s&&t(Y),s&&t(Jt),y($s,s),s&&t(Ut),s&&t(J),s&&t(Kt),y(Ss,s),s&&t(Qt),s&&t(z),y(js),s&&t(Vt),s&&t(qs),s&&t(Zt),s&&t(P),y(ws),s&&t(se),s&&t(Hs),s&&t(te),y(bs,s),s&&t(ee),s&&t(ks),s&&t(ae),y(Es,s),s&&t(le),s&&t(As),s&&t(oe),y(xs,s),s&&t(re),s&&t(O),y(Fs),s&&t(ne),s&&t(Rs),s&&t(ie),y(Ds,s),s&&t(pe),s&&t(Ls),s&&t(ce),y(Ts,s),s&&t(de),s&&t(Cs),s&&t(me),y(Xs,s)}}}const fo={local:"cloud-storage",sections:[{local:"listing-datasets",title:"Listing datasets"},{local:"saving-datasets",title:"Saving datasets"},{local:"loading-datasets",title:"Loading datasets"},{local:"other-filesystems",sections:[{local:"google-cloud-storage",title:"Google Cloud Storage"},{local:"azure-blob-storage",title:"Azure Blob Storage"}],title:"Other filesystems"}],title:"Cloud storage"};function ho(Dt){return io(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class vo extends lo{constructor(v){super();oo(this,v,ho,mo,ro,{})}}export{vo as default,fo as metadata};
