import{S as co,i as mo,s as fo,e as a,k as c,w as f,t as i,M as ho,c as l,d as t,m as d,a as o,x as h,h as p,b as m,G as e,g as n,y as g,q as u,o as _,B as y,v as go}from"../chunks/vendor-hf-doc-builder.js";import{T as uo}from"../chunks/Tip-hf-doc-builder.js";import{I as zs}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as b}from"../chunks/CodeBlock-hf-doc-builder.js";function _o(Dt){let v,B,$,k,F,S,V,L;return{c(){v=a("p"),B=i("Remember to include your "),$=a("code"),k=i("aws_access_key_id"),F=i(" and "),S=a("code"),V=i("aws_secret_access_key"),L=i(" whenever you are interacting with a private S3 bucket.")},l(D){v=l(D,"P",{});var j=o(v);B=p(j,"Remember to include your "),$=l(j,"CODE",{});var E=o($);k=p(E,"aws_access_key_id"),E.forEach(t),F=p(j," and "),S=l(j,"CODE",{});var Os=o(S);V=p(Os,"aws_secret_access_key"),Os.forEach(t),L=p(j," whenever you are interacting with a private S3 bucket."),j.forEach(t)},m(D,j){n(D,v,j),e(v,B),e(v,$),e($,k),e(v,F),e(v,S),e(S,V),e(v,L)},d(D){D&&t(v)}}}function yo(Dt){let v,B,$,k,F,S,V,L,D,j,E,Os,Bs,Be,Ge,Tt,G,Ys,Z,Js,Ie,Ne,Us,qe,He,w,ss,Ks,Re,We,Qs,ts,Me,Ye,es,Vs,Je,Ue,Zs,as,Ke,Qe,ls,st,Ve,Ze,tt,os,sa,ta,rs,et,ea,aa,at,ns,la,oa,is,lt,ra,na,ot,ps,ia,Ct,I,pa,rt,ca,da,Xt,T,N,nt,cs,ma,it,fa,Pt,Gs,pt,ha,zt,ds,Ot,ms,fs,ga,ct,ua,_a,Bt,hs,Gt,A,ya,dt,va,$a,mt,Sa,ja,It,gs,Nt,C,q,ft,us,wa,ht,ba,qt,H,ka,Is,Ea,Aa,Ht,_s,Rt,R,Wt,W,xa,gt,Fa,La,Mt,ys,Yt,X,M,ut,vs,Da,_t,Ta,Jt,Y,Ca,Ns,Xa,Pa,Ut,$s,Kt,J,za,yt,Oa,Ba,Qt,Ss,Vt,Zt,se,P,U,vt,js,Ga,$t,Ia,te,z,K,St,ws,Na,jt,qa,ee,qs,Ha,ae,Hs,wt,Ra,le,bs,oe,ks,bt,Wa,re,Es,ne,As,kt,Ma,ie,xs,pe,O,Q,Et,Fs,Ya,At,Ja,ce,Rs,Ua,de,Ws,xt,Ka,me,Ls,fe,Ds,Ft,Qa,he,Ts,ge,Cs,Lt,Va,ue,Xs,_e;return S=new zs({}),cs=new zs({}),ds=new b({props:{code:"pip install datasets[s3]",highlighted:'&gt;&gt;&gt; pip <span class="hljs-keyword">install </span>datasets[<span class="hljs-built_in">s3</span>]'}}),hs=new b({props:{code:`import datasets
s3 = datasets.filesystems.S3FileSystem(anon=True)  
s3.ls('public-datasets/imdb/train')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = datasets.filesystems.S3FileSystem(anon=<span class="hljs-literal">True</span>)  
<span class="hljs-meta">&gt;&gt;&gt; </span>s3.ls(<span class="hljs-string">&#x27;public-datasets/imdb/train&#x27;</span>)
[<span class="hljs-string">&#x27;dataset_info.json.json&#x27;</span>,<span class="hljs-string">&#x27;dataset.arrow&#x27;</span>,<span class="hljs-string">&#x27;state.json&#x27;</span>]`}}),gs=new b({props:{code:`import datasets
s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)  
s3.ls('my-private-datasets/imdb/train')  `,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)  
<span class="hljs-meta">&gt;&gt;&gt; </span>s3.ls(<span class="hljs-string">&#x27;my-private-datasets/imdb/train&#x27;</span>)  
[<span class="hljs-string">&#x27;dataset_info.json.json&#x27;</span>,<span class="hljs-string">&#x27;dataset.arrow&#x27;</span>,<span class="hljs-string">&#x27;state.json&#x27;</span>]`}}),us=new zs({}),_s=new b({props:{code:`from datasets.filesystems import S3FileSystem

# create S3FileSystem instance
s3 = S3FileSystem(anon=True)  

# saves encoded_dataset to your s3 bucket
encoded_dataset.save_to_disk('s3://my-private-datasets/imdb/train', fs=s3)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create S3FileSystem instance</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(anon=<span class="hljs-literal">True</span>)  

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saves encoded_dataset to your s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&#x27;s3://my-private-datasets/imdb/train&#x27;</span>, fs=s3)`}}),R=new uo({props:{$$slots:{default:[_o]},$$scope:{ctx:Dt}}}),ys=new b({props:{code:`import botocore
from datasets.filesystems import S3FileSystem

# creates a botocore session with the provided AWS profile
s3_session = botocore.session.Session(profile='my_profile_name')

# create S3FileSystem instance with s3_session
s3 = S3FileSystem(session=s3_session)  

# saves encoded_dataset to your s3 bucket
encoded_dataset.save_to_disk('s3://my-private-datasets/imdb/train',fs=s3)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> botocore
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># creates a botocore session with the provided AWS profile</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3_session = botocore.session.Session(profile=<span class="hljs-string">&#x27;my_profile_name&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create S3FileSystem instance with s3_session</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(session=s3_session)  

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saves encoded_dataset to your s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&#x27;s3://my-private-datasets/imdb/train&#x27;</span>,fs=s3)`}}),vs=new zs({}),$s=new b({props:{code:`from datasets import load_from_disk
from datasets.filesystems import S3FileSystem

# create S3FileSystem without credentials
s3 = S3FileSystem(anon=True)  

# load encoded_dataset to from s3 bucket
dataset = load_from_disk('s3://a-public-datasets/imdb/train',fs=s3)  

print(len(dataset))
# 25000`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create S3FileSystem without credentials</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(anon=<span class="hljs-literal">True</span>)  

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load encoded_dataset to from s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;s3://a-public-datasets/imdb/train&#x27;</span>,fs=s3)  

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 25000</span>`}}),Ss=new b({props:{code:`import botocore
from datasets.filesystems import S3FileSystem

# create S3FileSystem instance with aws_access_key_id and aws_secret_access_key
s3_session = botocore.session.Session(profile='my_profile_name')

# create S3FileSystem instance with s3_session
s3 = S3FileSystem(session=s3_session)

# load encoded_dataset to from s3 bucket
dataset = load_from_disk('s3://my-private-datasets/imdb/train',fs=s3)  

print(len(dataset))
# 25000`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> botocore
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create S3FileSystem instance with aws_access_key_id and aws_secret_access_key</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3_session = botocore.session.Session(profile=<span class="hljs-string">&#x27;my_profile_name&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create S3FileSystem instance with s3_session</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(session=s3_session)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load encoded_dataset to from s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;s3://my-private-datasets/imdb/train&#x27;</span>,fs=s3)  

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 25000</span>`}}),js=new zs({}),ws=new zs({}),bs=new b({props:{code:`conda install -c conda-forge gcsfs
pip install gcsfs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">conda install -c conda-forge gcsfs</span>
# or install with pip
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pip install gcsfs</span>`}}),Es=new b({props:{code:`import gcsfs

# create GCSFileSystem instance using default gcloud credentials with project
gcs = gcsfs.GCSFileSystem(project='my-google-project')

# saves dataset to your gcs bucket
dataset.save_to_disk('gcs://my-private-datasets/imdb/train', fs=gcs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> gcsfs

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create GCSFileSystem instance using default gcloud credentials with project</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>gcs = gcsfs.GCSFileSystem(project=<span class="hljs-string">&#x27;my-google-project&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saves dataset to your gcs bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.save_to_disk(<span class="hljs-string">&#x27;gcs://my-private-datasets/imdb/train&#x27;</span>, fs=gcs)`}}),xs=new b({props:{code:`import gcsfs
from datasets import load_from_disk

# create GCSFileSystem instance using default gcloud credentials with project
gcs = gcsfs.GCSFileSystem(project='my-google-project')

# loads dataset from your gcs bucket
dataset = load_from_disk('gcs://my-private-datasets/imdb/train', fs=gcs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> gcsfs
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create GCSFileSystem instance using default gcloud credentials with project</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>gcs = gcsfs.GCSFileSystem(project=<span class="hljs-string">&#x27;my-google-project&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># loads dataset from your gcs bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;gcs://my-private-datasets/imdb/train&#x27;</span>, fs=gcs)`}}),Fs=new zs({}),Ls=new b({props:{code:`conda install -c conda-forge adlfs
pip install adlfs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">conda install -c conda-forge adlfs</span>
# or install with pip
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pip install adlfs</span>`}}),Ts=new b({props:{code:`import adlfs

# create AzureBlobFileSystem instance with account_name and account_key
abfs = adlfs.AzureBlobFileSystem(account_name="XXXX", account_key="XXXX")

# saves dataset to your azure container
dataset.save_to_disk('abfs://my-private-datasets/imdb/train', fs=abfs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> adlfs

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create AzureBlobFileSystem instance with account_name and account_key</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>abfs = adlfs.AzureBlobFileSystem(account_name=<span class="hljs-string">&quot;XXXX&quot;</span>, account_key=<span class="hljs-string">&quot;XXXX&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saves dataset to your azure container</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.save_to_disk(<span class="hljs-string">&#x27;abfs://my-private-datasets/imdb/train&#x27;</span>, fs=abfs)`}}),Xs=new b({props:{code:`import adlfs
from datasets import load_from_disk

# create AzureBlobFileSystem instance with account_name and account_key
abfs = adlfs.AzureBlobFileSystem(account_name="XXXX", account_key="XXXX")

# loads dataset from your azure container
dataset = load_from_disk('abfs://my-private-datasets/imdb/train', fs=abfs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> adlfs
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create AzureBlobFileSystem instance with account_name and account_key</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>abfs = adlfs.AzureBlobFileSystem(account_name=<span class="hljs-string">&quot;XXXX&quot;</span>, account_key=<span class="hljs-string">&quot;XXXX&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># loads dataset from your azure container</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;abfs://my-private-datasets/imdb/train&#x27;</span>, fs=abfs)`}}),{c(){v=a("meta"),B=c(),$=a("h1"),k=a("a"),F=a("span"),f(S.$$.fragment),V=c(),L=a("span"),D=i("Cloud storage"),j=c(),E=a("p"),Os=i("\u{1F917} Datasets supports access to cloud storage providers through a S3 filesystem implementation: "),Bs=a("a"),Be=i("filesystems.S3FileSystem"),Ge=i(". You can save and load datasets from your Amazon S3 bucket in a Pythonic way. Take a look at the following table for other supported cloud storage providers:"),Tt=c(),G=a("table"),Ys=a("thead"),Z=a("tr"),Js=a("th"),Ie=i("Storage provider"),Ne=c(),Us=a("th"),qe=i("Filesystem implementation"),He=c(),w=a("tbody"),ss=a("tr"),Ks=a("td"),Re=i("Amazon S3"),We=c(),Qs=a("td"),ts=a("a"),Me=i("s3fs"),Ye=c(),es=a("tr"),Vs=a("td"),Je=i("Google Cloud Storage"),Ue=c(),Zs=a("td"),as=a("a"),Ke=i("gcsfs"),Qe=c(),ls=a("tr"),st=a("td"),Ve=i("Azure Blob/DataLake"),Ze=c(),tt=a("td"),os=a("a"),sa=i("adlfs"),ta=c(),rs=a("tr"),et=a("td"),ea=i("Dropbox"),aa=c(),at=a("td"),ns=a("a"),la=i("dropboxdrivefs"),oa=c(),is=a("tr"),lt=a("td"),ra=i("Google Drive"),na=c(),ot=a("td"),ps=a("a"),ia=i("gdrivefs"),Ct=c(),I=a("p"),pa=i("This guide will show you how to save and load datasets with "),rt=a("strong"),ca=i("s3fs"),da=i(" to a S3 bucket, but other filesystem implementations can be used similarly."),Xt=c(),T=a("h2"),N=a("a"),nt=a("span"),f(cs.$$.fragment),ma=c(),it=a("span"),fa=i("Listing datasets"),Pt=c(),Gs=a("ol"),pt=a("li"),ha=i("Install the S3 dependency with \u{1F917} Datasets:"),zt=c(),f(ds.$$.fragment),Ot=c(),ms=a("ol"),fs=a("li"),ga=i("List files from a public S3 bucket with "),ct=a("code"),ua=i("s3.ls"),_a=i(":"),Bt=c(),f(hs.$$.fragment),Gt=c(),A=a("p"),ya=i("Access a private S3 bucket by entering your "),dt=a("code"),va=i("aws_access_key_id"),$a=i(" and "),mt=a("code"),Sa=i("aws_secret_access_key"),ja=i(":"),It=c(),f(gs.$$.fragment),Nt=c(),C=a("h2"),q=a("a"),ft=a("span"),f(us.$$.fragment),wa=c(),ht=a("span"),ba=i("Saving datasets"),qt=c(),H=a("p"),ka=i("After you have processed your dataset, you can save it to S3 with "),Is=a("a"),Ea=i("Dataset.save_to_disk()"),Aa=i(":"),Ht=c(),f(_s.$$.fragment),Rt=c(),f(R.$$.fragment),Wt=c(),W=a("p"),xa=i("Save your dataset with "),gt=a("code"),Fa=i("botocore.session.Session"),La=i(" and a custom AWS profile:"),Mt=c(),f(ys.$$.fragment),Yt=c(),X=a("h2"),M=a("a"),ut=a("span"),f(vs.$$.fragment),Da=c(),_t=a("span"),Ta=i("Loading datasets"),Jt=c(),Y=a("p"),Ca=i("When you are ready to use your dataset again, reload it with "),Ns=a("a"),Xa=i("Dataset.load_from_disk()"),Pa=i(":"),Ut=c(),f($s.$$.fragment),Kt=c(),J=a("p"),za=i("Load with "),yt=a("code"),Oa=i("botocore.session.Session"),Ba=i(" and custom AWS profile:"),Qt=c(),f(Ss.$$.fragment),Vt=c(),Zt=a("hr"),se=c(),P=a("h2"),U=a("a"),vt=a("span"),f(js.$$.fragment),Ga=c(),$t=a("span"),Ia=i("Other filesystems"),te=c(),z=a("h3"),K=a("a"),St=a("span"),f(ws.$$.fragment),Na=c(),jt=a("span"),qa=i("Google Cloud Storage"),ee=c(),qs=a("p"),Ha=i("Other filesystem implementations, like Google Cloud Storage, are used similarly:"),ae=c(),Hs=a("ol"),wt=a("li"),Ra=i("Install the Google Cloud Storage implementation:"),le=c(),f(bs.$$.fragment),oe=c(),ks=a("ol"),bt=a("li"),Wa=i("Save your dataset:"),re=c(),f(Es.$$.fragment),ne=c(),As=a("ol"),kt=a("li"),Ma=i("Load your dataset:"),ie=c(),f(xs.$$.fragment),pe=c(),O=a("h3"),Q=a("a"),Et=a("span"),f(Fs.$$.fragment),Ya=c(),At=a("span"),Ja=i("Azure Blob Storage"),ce=c(),Rs=a("p"),Ua=i("Other filesystem implementations, like Azure Blob Storage, are used similarly:"),de=c(),Ws=a("ol"),xt=a("li"),Ka=i("Install the Azure Blob Storage implementation:"),me=c(),f(Ls.$$.fragment),fe=c(),Ds=a("ol"),Ft=a("li"),Qa=i("Load your dataset:"),he=c(),f(Ts.$$.fragment),ge=c(),Cs=a("ol"),Lt=a("li"),Va=i("Load your dataset:"),ue=c(),f(Xs.$$.fragment),this.h()},l(s){const r=ho('[data-svelte="svelte-1phssyn"]',document.head);v=l(r,"META",{name:!0,content:!0}),r.forEach(t),B=d(s),$=l(s,"H1",{class:!0});var Ps=o($);k=l(Ps,"A",{id:!0,class:!0,href:!0});var Za=o(k);F=l(Za,"SPAN",{});var sl=o(F);h(S.$$.fragment,sl),sl.forEach(t),Za.forEach(t),V=d(Ps),L=l(Ps,"SPAN",{});var tl=o(L);D=p(tl,"Cloud storage"),tl.forEach(t),Ps.forEach(t),j=d(s),E=l(s,"P",{});var ye=o(E);Os=p(ye,"\u{1F917} Datasets supports access to cloud storage providers through a S3 filesystem implementation: "),Bs=l(ye,"A",{href:!0});var el=o(Bs);Be=p(el,"filesystems.S3FileSystem"),el.forEach(t),Ge=p(ye,". You can save and load datasets from your Amazon S3 bucket in a Pythonic way. Take a look at the following table for other supported cloud storage providers:"),ye.forEach(t),Tt=d(s),G=l(s,"TABLE",{});var ve=o(G);Ys=l(ve,"THEAD",{});var al=o(Ys);Z=l(al,"TR",{});var $e=o(Z);Js=l($e,"TH",{});var ll=o(Js);Ie=p(ll,"Storage provider"),ll.forEach(t),Ne=d($e),Us=l($e,"TH",{});var ol=o(Us);qe=p(ol,"Filesystem implementation"),ol.forEach(t),$e.forEach(t),al.forEach(t),He=d(ve),w=l(ve,"TBODY",{});var x=o(w);ss=l(x,"TR",{});var Se=o(ss);Ks=l(Se,"TD",{});var rl=o(Ks);Re=p(rl,"Amazon S3"),rl.forEach(t),We=d(Se),Qs=l(Se,"TD",{});var nl=o(Qs);ts=l(nl,"A",{href:!0,rel:!0});var il=o(ts);Me=p(il,"s3fs"),il.forEach(t),nl.forEach(t),Se.forEach(t),Ye=d(x),es=l(x,"TR",{});var je=o(es);Vs=l(je,"TD",{});var pl=o(Vs);Je=p(pl,"Google Cloud Storage"),pl.forEach(t),Ue=d(je),Zs=l(je,"TD",{});var cl=o(Zs);as=l(cl,"A",{href:!0,rel:!0});var dl=o(as);Ke=p(dl,"gcsfs"),dl.forEach(t),cl.forEach(t),je.forEach(t),Qe=d(x),ls=l(x,"TR",{});var we=o(ls);st=l(we,"TD",{});var ml=o(st);Ve=p(ml,"Azure Blob/DataLake"),ml.forEach(t),Ze=d(we),tt=l(we,"TD",{});var fl=o(tt);os=l(fl,"A",{href:!0,rel:!0});var hl=o(os);sa=p(hl,"adlfs"),hl.forEach(t),fl.forEach(t),we.forEach(t),ta=d(x),rs=l(x,"TR",{});var be=o(rs);et=l(be,"TD",{});var gl=o(et);ea=p(gl,"Dropbox"),gl.forEach(t),aa=d(be),at=l(be,"TD",{});var ul=o(at);ns=l(ul,"A",{href:!0,rel:!0});var _l=o(ns);la=p(_l,"dropboxdrivefs"),_l.forEach(t),ul.forEach(t),be.forEach(t),oa=d(x),is=l(x,"TR",{});var ke=o(is);lt=l(ke,"TD",{});var yl=o(lt);ra=p(yl,"Google Drive"),yl.forEach(t),na=d(ke),ot=l(ke,"TD",{});var vl=o(ot);ps=l(vl,"A",{href:!0,rel:!0});var $l=o(ps);ia=p($l,"gdrivefs"),$l.forEach(t),vl.forEach(t),ke.forEach(t),x.forEach(t),ve.forEach(t),Ct=d(s),I=l(s,"P",{});var Ee=o(I);pa=p(Ee,"This guide will show you how to save and load datasets with "),rt=l(Ee,"STRONG",{});var Sl=o(rt);ca=p(Sl,"s3fs"),Sl.forEach(t),da=p(Ee," to a S3 bucket, but other filesystem implementations can be used similarly."),Ee.forEach(t),Xt=d(s),T=l(s,"H2",{class:!0});var Ae=o(T);N=l(Ae,"A",{id:!0,class:!0,href:!0});var jl=o(N);nt=l(jl,"SPAN",{});var wl=o(nt);h(cs.$$.fragment,wl),wl.forEach(t),jl.forEach(t),ma=d(Ae),it=l(Ae,"SPAN",{});var bl=o(it);fa=p(bl,"Listing datasets"),bl.forEach(t),Ae.forEach(t),Pt=d(s),Gs=l(s,"OL",{});var kl=o(Gs);pt=l(kl,"LI",{});var El=o(pt);ha=p(El,"Install the S3 dependency with \u{1F917} Datasets:"),El.forEach(t),kl.forEach(t),zt=d(s),h(ds.$$.fragment,s),Ot=d(s),ms=l(s,"OL",{start:!0});var Al=o(ms);fs=l(Al,"LI",{});var xe=o(fs);ga=p(xe,"List files from a public S3 bucket with "),ct=l(xe,"CODE",{});var xl=o(ct);ua=p(xl,"s3.ls"),xl.forEach(t),_a=p(xe,":"),xe.forEach(t),Al.forEach(t),Bt=d(s),h(hs.$$.fragment,s),Gt=d(s),A=l(s,"P",{});var Ms=o(A);ya=p(Ms,"Access a private S3 bucket by entering your "),dt=l(Ms,"CODE",{});var Fl=o(dt);va=p(Fl,"aws_access_key_id"),Fl.forEach(t),$a=p(Ms," and "),mt=l(Ms,"CODE",{});var Ll=o(mt);Sa=p(Ll,"aws_secret_access_key"),Ll.forEach(t),ja=p(Ms,":"),Ms.forEach(t),It=d(s),h(gs.$$.fragment,s),Nt=d(s),C=l(s,"H2",{class:!0});var Fe=o(C);q=l(Fe,"A",{id:!0,class:!0,href:!0});var Dl=o(q);ft=l(Dl,"SPAN",{});var Tl=o(ft);h(us.$$.fragment,Tl),Tl.forEach(t),Dl.forEach(t),wa=d(Fe),ht=l(Fe,"SPAN",{});var Cl=o(ht);ba=p(Cl,"Saving datasets"),Cl.forEach(t),Fe.forEach(t),qt=d(s),H=l(s,"P",{});var Le=o(H);ka=p(Le,"After you have processed your dataset, you can save it to S3 with "),Is=l(Le,"A",{href:!0});var Xl=o(Is);Ea=p(Xl,"Dataset.save_to_disk()"),Xl.forEach(t),Aa=p(Le,":"),Le.forEach(t),Ht=d(s),h(_s.$$.fragment,s),Rt=d(s),h(R.$$.fragment,s),Wt=d(s),W=l(s,"P",{});var De=o(W);xa=p(De,"Save your dataset with "),gt=l(De,"CODE",{});var Pl=o(gt);Fa=p(Pl,"botocore.session.Session"),Pl.forEach(t),La=p(De," and a custom AWS profile:"),De.forEach(t),Mt=d(s),h(ys.$$.fragment,s),Yt=d(s),X=l(s,"H2",{class:!0});var Te=o(X);M=l(Te,"A",{id:!0,class:!0,href:!0});var zl=o(M);ut=l(zl,"SPAN",{});var Ol=o(ut);h(vs.$$.fragment,Ol),Ol.forEach(t),zl.forEach(t),Da=d(Te),_t=l(Te,"SPAN",{});var Bl=o(_t);Ta=p(Bl,"Loading datasets"),Bl.forEach(t),Te.forEach(t),Jt=d(s),Y=l(s,"P",{});var Ce=o(Y);Ca=p(Ce,"When you are ready to use your dataset again, reload it with "),Ns=l(Ce,"A",{href:!0});var Gl=o(Ns);Xa=p(Gl,"Dataset.load_from_disk()"),Gl.forEach(t),Pa=p(Ce,":"),Ce.forEach(t),Ut=d(s),h($s.$$.fragment,s),Kt=d(s),J=l(s,"P",{});var Xe=o(J);za=p(Xe,"Load with "),yt=l(Xe,"CODE",{});var Il=o(yt);Oa=p(Il,"botocore.session.Session"),Il.forEach(t),Ba=p(Xe," and custom AWS profile:"),Xe.forEach(t),Qt=d(s),h(Ss.$$.fragment,s),Vt=d(s),Zt=l(s,"HR",{}),se=d(s),P=l(s,"H2",{class:!0});var Pe=o(P);U=l(Pe,"A",{id:!0,class:!0,href:!0});var Nl=o(U);vt=l(Nl,"SPAN",{});var ql=o(vt);h(js.$$.fragment,ql),ql.forEach(t),Nl.forEach(t),Ga=d(Pe),$t=l(Pe,"SPAN",{});var Hl=o($t);Ia=p(Hl,"Other filesystems"),Hl.forEach(t),Pe.forEach(t),te=d(s),z=l(s,"H3",{class:!0});var ze=o(z);K=l(ze,"A",{id:!0,class:!0,href:!0});var Rl=o(K);St=l(Rl,"SPAN",{});var Wl=o(St);h(ws.$$.fragment,Wl),Wl.forEach(t),Rl.forEach(t),Na=d(ze),jt=l(ze,"SPAN",{});var Ml=o(jt);qa=p(Ml,"Google Cloud Storage"),Ml.forEach(t),ze.forEach(t),ee=d(s),qs=l(s,"P",{});var Yl=o(qs);Ha=p(Yl,"Other filesystem implementations, like Google Cloud Storage, are used similarly:"),Yl.forEach(t),ae=d(s),Hs=l(s,"OL",{});var Jl=o(Hs);wt=l(Jl,"LI",{});var Ul=o(wt);Ra=p(Ul,"Install the Google Cloud Storage implementation:"),Ul.forEach(t),Jl.forEach(t),le=d(s),h(bs.$$.fragment,s),oe=d(s),ks=l(s,"OL",{start:!0});var Kl=o(ks);bt=l(Kl,"LI",{});var Ql=o(bt);Wa=p(Ql,"Save your dataset:"),Ql.forEach(t),Kl.forEach(t),re=d(s),h(Es.$$.fragment,s),ne=d(s),As=l(s,"OL",{start:!0});var Vl=o(As);kt=l(Vl,"LI",{});var Zl=o(kt);Ma=p(Zl,"Load your dataset:"),Zl.forEach(t),Vl.forEach(t),ie=d(s),h(xs.$$.fragment,s),pe=d(s),O=l(s,"H3",{class:!0});var Oe=o(O);Q=l(Oe,"A",{id:!0,class:!0,href:!0});var so=o(Q);Et=l(so,"SPAN",{});var to=o(Et);h(Fs.$$.fragment,to),to.forEach(t),so.forEach(t),Ya=d(Oe),At=l(Oe,"SPAN",{});var eo=o(At);Ja=p(eo,"Azure Blob Storage"),eo.forEach(t),Oe.forEach(t),ce=d(s),Rs=l(s,"P",{});var ao=o(Rs);Ua=p(ao,"Other filesystem implementations, like Azure Blob Storage, are used similarly:"),ao.forEach(t),de=d(s),Ws=l(s,"OL",{});var lo=o(Ws);xt=l(lo,"LI",{});var oo=o(xt);Ka=p(oo,"Install the Azure Blob Storage implementation:"),oo.forEach(t),lo.forEach(t),me=d(s),h(Ls.$$.fragment,s),fe=d(s),Ds=l(s,"OL",{start:!0});var ro=o(Ds);Ft=l(ro,"LI",{});var no=o(Ft);Qa=p(no,"Load your dataset:"),no.forEach(t),ro.forEach(t),he=d(s),h(Ts.$$.fragment,s),ge=d(s),Cs=l(s,"OL",{start:!0});var io=o(Cs);Lt=l(io,"LI",{});var po=o(Lt);Va=p(po,"Load your dataset:"),po.forEach(t),io.forEach(t),ue=d(s),h(Xs.$$.fragment,s),this.h()},h(){m(v,"name","hf:doc:metadata"),m(v,"content",JSON.stringify(vo)),m(k,"id","cloud-storage"),m(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(k,"href","#cloud-storage"),m($,"class","relative group"),m(Bs,"href","/docs/datasets/pr_4513/en/package_reference/main_classes#datasets.filesystems.S3FileSystem"),m(ts,"href","https://s3fs.readthedocs.io/en/latest/"),m(ts,"rel","nofollow"),m(as,"href","https://gcsfs.readthedocs.io/en/latest/"),m(as,"rel","nofollow"),m(os,"href","https://github.com/fsspec/adlfs"),m(os,"rel","nofollow"),m(ns,"href","https://github.com/MarineChap/dropboxdrivefs"),m(ns,"rel","nofollow"),m(ps,"href","https://github.com/intake/gdrivefs"),m(ps,"rel","nofollow"),m(N,"id","listing-datasets"),m(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(N,"href","#listing-datasets"),m(T,"class","relative group"),m(ms,"start","2"),m(q,"id","saving-datasets"),m(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(q,"href","#saving-datasets"),m(C,"class","relative group"),m(Is,"href","/docs/datasets/pr_4513/en/package_reference/main_classes#datasets.Dataset.save_to_disk"),m(M,"id","loading-datasets"),m(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(M,"href","#loading-datasets"),m(X,"class","relative group"),m(Ns,"href","/docs/datasets/pr_4513/en/package_reference/main_classes#datasets.Dataset.load_from_disk"),m(U,"id","other-filesystems"),m(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(U,"href","#other-filesystems"),m(P,"class","relative group"),m(K,"id","google-cloud-storage"),m(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(K,"href","#google-cloud-storage"),m(z,"class","relative group"),m(ks,"start","2"),m(As,"start","3"),m(Q,"id","azure-blob-storage"),m(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Q,"href","#azure-blob-storage"),m(O,"class","relative group"),m(Ds,"start","2"),m(Cs,"start","3")},m(s,r){e(document.head,v),n(s,B,r),n(s,$,r),e($,k),e(k,F),g(S,F,null),e($,V),e($,L),e(L,D),n(s,j,r),n(s,E,r),e(E,Os),e(E,Bs),e(Bs,Be),e(E,Ge),n(s,Tt,r),n(s,G,r),e(G,Ys),e(Ys,Z),e(Z,Js),e(Js,Ie),e(Z,Ne),e(Z,Us),e(Us,qe),e(G,He),e(G,w),e(w,ss),e(ss,Ks),e(Ks,Re),e(ss,We),e(ss,Qs),e(Qs,ts),e(ts,Me),e(w,Ye),e(w,es),e(es,Vs),e(Vs,Je),e(es,Ue),e(es,Zs),e(Zs,as),e(as,Ke),e(w,Qe),e(w,ls),e(ls,st),e(st,Ve),e(ls,Ze),e(ls,tt),e(tt,os),e(os,sa),e(w,ta),e(w,rs),e(rs,et),e(et,ea),e(rs,aa),e(rs,at),e(at,ns),e(ns,la),e(w,oa),e(w,is),e(is,lt),e(lt,ra),e(is,na),e(is,ot),e(ot,ps),e(ps,ia),n(s,Ct,r),n(s,I,r),e(I,pa),e(I,rt),e(rt,ca),e(I,da),n(s,Xt,r),n(s,T,r),e(T,N),e(N,nt),g(cs,nt,null),e(T,ma),e(T,it),e(it,fa),n(s,Pt,r),n(s,Gs,r),e(Gs,pt),e(pt,ha),n(s,zt,r),g(ds,s,r),n(s,Ot,r),n(s,ms,r),e(ms,fs),e(fs,ga),e(fs,ct),e(ct,ua),e(fs,_a),n(s,Bt,r),g(hs,s,r),n(s,Gt,r),n(s,A,r),e(A,ya),e(A,dt),e(dt,va),e(A,$a),e(A,mt),e(mt,Sa),e(A,ja),n(s,It,r),g(gs,s,r),n(s,Nt,r),n(s,C,r),e(C,q),e(q,ft),g(us,ft,null),e(C,wa),e(C,ht),e(ht,ba),n(s,qt,r),n(s,H,r),e(H,ka),e(H,Is),e(Is,Ea),e(H,Aa),n(s,Ht,r),g(_s,s,r),n(s,Rt,r),g(R,s,r),n(s,Wt,r),n(s,W,r),e(W,xa),e(W,gt),e(gt,Fa),e(W,La),n(s,Mt,r),g(ys,s,r),n(s,Yt,r),n(s,X,r),e(X,M),e(M,ut),g(vs,ut,null),e(X,Da),e(X,_t),e(_t,Ta),n(s,Jt,r),n(s,Y,r),e(Y,Ca),e(Y,Ns),e(Ns,Xa),e(Y,Pa),n(s,Ut,r),g($s,s,r),n(s,Kt,r),n(s,J,r),e(J,za),e(J,yt),e(yt,Oa),e(J,Ba),n(s,Qt,r),g(Ss,s,r),n(s,Vt,r),n(s,Zt,r),n(s,se,r),n(s,P,r),e(P,U),e(U,vt),g(js,vt,null),e(P,Ga),e(P,$t),e($t,Ia),n(s,te,r),n(s,z,r),e(z,K),e(K,St),g(ws,St,null),e(z,Na),e(z,jt),e(jt,qa),n(s,ee,r),n(s,qs,r),e(qs,Ha),n(s,ae,r),n(s,Hs,r),e(Hs,wt),e(wt,Ra),n(s,le,r),g(bs,s,r),n(s,oe,r),n(s,ks,r),e(ks,bt),e(bt,Wa),n(s,re,r),g(Es,s,r),n(s,ne,r),n(s,As,r),e(As,kt),e(kt,Ma),n(s,ie,r),g(xs,s,r),n(s,pe,r),n(s,O,r),e(O,Q),e(Q,Et),g(Fs,Et,null),e(O,Ya),e(O,At),e(At,Ja),n(s,ce,r),n(s,Rs,r),e(Rs,Ua),n(s,de,r),n(s,Ws,r),e(Ws,xt),e(xt,Ka),n(s,me,r),g(Ls,s,r),n(s,fe,r),n(s,Ds,r),e(Ds,Ft),e(Ft,Qa),n(s,he,r),g(Ts,s,r),n(s,ge,r),n(s,Cs,r),e(Cs,Lt),e(Lt,Va),n(s,ue,r),g(Xs,s,r),_e=!0},p(s,[r]){const Ps={};r&2&&(Ps.$$scope={dirty:r,ctx:s}),R.$set(Ps)},i(s){_e||(u(S.$$.fragment,s),u(cs.$$.fragment,s),u(ds.$$.fragment,s),u(hs.$$.fragment,s),u(gs.$$.fragment,s),u(us.$$.fragment,s),u(_s.$$.fragment,s),u(R.$$.fragment,s),u(ys.$$.fragment,s),u(vs.$$.fragment,s),u($s.$$.fragment,s),u(Ss.$$.fragment,s),u(js.$$.fragment,s),u(ws.$$.fragment,s),u(bs.$$.fragment,s),u(Es.$$.fragment,s),u(xs.$$.fragment,s),u(Fs.$$.fragment,s),u(Ls.$$.fragment,s),u(Ts.$$.fragment,s),u(Xs.$$.fragment,s),_e=!0)},o(s){_(S.$$.fragment,s),_(cs.$$.fragment,s),_(ds.$$.fragment,s),_(hs.$$.fragment,s),_(gs.$$.fragment,s),_(us.$$.fragment,s),_(_s.$$.fragment,s),_(R.$$.fragment,s),_(ys.$$.fragment,s),_(vs.$$.fragment,s),_($s.$$.fragment,s),_(Ss.$$.fragment,s),_(js.$$.fragment,s),_(ws.$$.fragment,s),_(bs.$$.fragment,s),_(Es.$$.fragment,s),_(xs.$$.fragment,s),_(Fs.$$.fragment,s),_(Ls.$$.fragment,s),_(Ts.$$.fragment,s),_(Xs.$$.fragment,s),_e=!1},d(s){t(v),s&&t(B),s&&t($),y(S),s&&t(j),s&&t(E),s&&t(Tt),s&&t(G),s&&t(Ct),s&&t(I),s&&t(Xt),s&&t(T),y(cs),s&&t(Pt),s&&t(Gs),s&&t(zt),y(ds,s),s&&t(Ot),s&&t(ms),s&&t(Bt),y(hs,s),s&&t(Gt),s&&t(A),s&&t(It),y(gs,s),s&&t(Nt),s&&t(C),y(us),s&&t(qt),s&&t(H),s&&t(Ht),y(_s,s),s&&t(Rt),y(R,s),s&&t(Wt),s&&t(W),s&&t(Mt),y(ys,s),s&&t(Yt),s&&t(X),y(vs),s&&t(Jt),s&&t(Y),s&&t(Ut),y($s,s),s&&t(Kt),s&&t(J),s&&t(Qt),y(Ss,s),s&&t(Vt),s&&t(Zt),s&&t(se),s&&t(P),y(js),s&&t(te),s&&t(z),y(ws),s&&t(ee),s&&t(qs),s&&t(ae),s&&t(Hs),s&&t(le),y(bs,s),s&&t(oe),s&&t(ks),s&&t(re),y(Es,s),s&&t(ne),s&&t(As),s&&t(ie),y(xs,s),s&&t(pe),s&&t(O),y(Fs),s&&t(ce),s&&t(Rs),s&&t(de),s&&t(Ws),s&&t(me),y(Ls,s),s&&t(fe),s&&t(Ds),s&&t(he),y(Ts,s),s&&t(ge),s&&t(Cs),s&&t(ue),y(Xs,s)}}}const vo={local:"cloud-storage",sections:[{local:"listing-datasets",title:"Listing datasets"},{local:"saving-datasets",title:"Saving datasets"},{local:"loading-datasets",title:"Loading datasets"},{local:"other-filesystems",sections:[{local:"google-cloud-storage",title:"Google Cloud Storage"},{local:"azure-blob-storage",title:"Azure Blob Storage"}],title:"Other filesystems"}],title:"Cloud storage"};function $o(Dt){return go(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ko extends co{constructor(v){super();mo(this,v,$o,yo,fo,{})}}export{ko as default,vo as metadata};
