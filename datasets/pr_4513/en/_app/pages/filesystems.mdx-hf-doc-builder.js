import{S as lo,i as oo,s as ro,e as a,k as d,w as m,t as i,M as no,c as l,d as t,m as f,a as o,x as h,h as p,b as c,G as e,g as n,y as g,q as u,o as _,B as y,v as io}from"../chunks/vendor-hf-doc-builder.js";import{T as po}from"../chunks/Tip-hf-doc-builder.js";import{I as zs}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as k}from"../chunks/CodeBlock-hf-doc-builder.js";function fo(Dt){let v,B,$,j,F,S,V,D;return{c(){v=a("p"),B=i("Remember to include your "),$=a("code"),j=i("aws_access_key_id"),F=i(" and "),S=a("code"),V=i("aws_secret_access_key"),D=i(" whenever you are interacting with a private S3 bucket.")},l(L){v=l(L,"P",{});var b=o(v);B=p(b,"Remember to include your "),$=l(b,"CODE",{});var E=o($);j=p(E,"aws_access_key_id"),E.forEach(t),F=p(b," and "),S=l(b,"CODE",{});var Os=o(S);V=p(Os,"aws_secret_access_key"),Os.forEach(t),D=p(b," whenever you are interacting with a private S3 bucket."),b.forEach(t)},m(L,b){n(L,v,b),e(v,B),e(v,$),e($,j),e(v,F),e(v,S),e(S,V),e(v,D)},d(L){L&&t(v)}}}function co(Dt){let v,B,$,j,F,S,V,D,L,b,E,Os,Bs,Ce,Pe,Lt,G,Ms,Z,Ys,ze,Oe,Js,Be,Ge,w,ss,Us,Ie,Ne,Ks,ts,qe,He,es,Qs,Re,We,Vs,as,Me,Ye,ls,Zs,Je,Ue,st,os,Ke,Qe,rs,tt,Ve,Ze,et,ns,sa,ta,is,at,ea,aa,lt,ps,la,Tt,I,oa,ot,ra,na,Xt,T,N,rt,ds,ia,nt,pa,Ct,Gs,it,da,Pt,fs,zt,cs,ms,fa,pt,ca,ma,Ot,hs,Bt,A,ha,dt,ga,ua,ft,_a,ya,Gt,gs,It,X,q,ct,us,va,mt,$a,Nt,H,Sa,Is,ba,wa,qt,_s,Ht,R,Rt,W,ka,ht,ja,Ea,Wt,ys,Mt,C,M,gt,vs,Aa,ut,xa,Yt,Y,Fa,Ns,Da,La,Jt,$s,Ut,J,Ta,_t,Xa,Ca,Kt,Ss,Qt,P,U,yt,bs,Pa,vt,za,Vt,qs,Oa,Zt,z,K,$t,ws,Ba,St,Ga,se,Hs,bt,Ia,te,ks,ee,js,wt,Na,ae,Es,le,As,kt,qa,oe,xs,re,O,Q,jt,Fs,Ha,Et,Ra,ne,Rs,At,Wa,ie,Ds,pe,Ls,xt,Ma,de,Ts,fe,Xs,Ft,Ya,ce,Cs,me;return S=new zs({}),ds=new zs({}),fs=new k({props:{code:"pip install datasets[s3]",highlighted:'&gt;&gt;&gt; pip <span class="hljs-keyword">install </span>datasets[<span class="hljs-built_in">s3</span>]'}}),hs=new k({props:{code:`import datasets
s3 = datasets.filesystems.S3FileSystem(anon=True)  
s3.ls('public-datasets/imdb/train')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = datasets.filesystems.S3FileSystem(anon=<span class="hljs-literal">True</span>)  
<span class="hljs-meta">&gt;&gt;&gt; </span>s3.ls(<span class="hljs-string">&#x27;public-datasets/imdb/train&#x27;</span>)
[<span class="hljs-string">&#x27;dataset_info.json.json&#x27;</span>,<span class="hljs-string">&#x27;dataset.arrow&#x27;</span>,<span class="hljs-string">&#x27;state.json&#x27;</span>]`}}),gs=new k({props:{code:`import datasets
s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)  
s3.ls('my-private-datasets/imdb/train')  `,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)  
<span class="hljs-meta">&gt;&gt;&gt; </span>s3.ls(<span class="hljs-string">&#x27;my-private-datasets/imdb/train&#x27;</span>)  
[<span class="hljs-string">&#x27;dataset_info.json.json&#x27;</span>,<span class="hljs-string">&#x27;dataset.arrow&#x27;</span>,<span class="hljs-string">&#x27;state.json&#x27;</span>]`}}),us=new zs({}),_s=new k({props:{code:`from datasets.filesystems import S3FileSystem

s3 = S3FileSystem(anon=True)  

encoded_dataset.save_to_disk('s3://my-private-datasets/imdb/train', fs=s3)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-comment"># create S3FileSystem instance</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(anon=<span class="hljs-literal">True</span>)  

<span class="hljs-comment"># saves encoded_dataset to your s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&#x27;s3://my-private-datasets/imdb/train&#x27;</span>, fs=s3)`}}),R=new po({props:{$$slots:{default:[fo]},$$scope:{ctx:Dt}}}),ys=new k({props:{code:`import botocore
from datasets.filesystems import S3FileSystem

s3_session = botocore.session.Session(profile='my_profile_name')

s3 = S3FileSystem(session=s3_session)  

encoded_dataset.save_to_disk('s3://my-private-datasets/imdb/train',fs=s3)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> botocore
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-comment"># creates a botocore session with the provided AWS profile</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3_session = botocore.session.Session(profile=<span class="hljs-string">&#x27;my_profile_name&#x27;</span>)

<span class="hljs-comment"># create S3FileSystem instance with s3_session</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(session=s3_session)  

<span class="hljs-comment"># saves encoded_dataset to your s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&#x27;s3://my-private-datasets/imdb/train&#x27;</span>,fs=s3)`}}),vs=new zs({}),$s=new k({props:{code:`from datasets import load_from_disk
from datasets.filesystems import S3FileSystem

s3 = S3FileSystem(anon=True)  

dataset = load_from_disk('s3://a-public-datasets/imdb/train',fs=s3)  

print(len(dataset))
# 25000`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-comment"># create S3FileSystem without credentials</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(anon=<span class="hljs-literal">True</span>)  

<span class="hljs-comment"># load encoded_dataset to from s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;s3://a-public-datasets/imdb/train&#x27;</span>,fs=s3)  

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 25000</span>`}}),Ss=new k({props:{code:`import botocore
from datasets.filesystems import S3FileSystem

s3_session = botocore.session.Session(profile='my_profile_name')

s3 = S3FileSystem(session=s3_session)

dataset = load_from_disk('s3://my-private-datasets/imdb/train',fs=s3)  

print(len(dataset))
# 25000`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> botocore
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-comment"># create S3FileSystem instance with aws_access_key_id and aws_secret_access_key</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3_session = botocore.session.Session(profile=<span class="hljs-string">&#x27;my_profile_name&#x27;</span>)

<span class="hljs-comment"># create S3FileSystem instance with s3_session</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(session=s3_session)

<span class="hljs-comment"># load encoded_dataset to from s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;s3://my-private-datasets/imdb/train&#x27;</span>,fs=s3)  

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 25000</span>`}}),bs=new zs({}),ws=new zs({}),ks=new k({props:{code:`conda install -c conda-forge gcsfs
pip install gcsfs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">conda install -c conda-forge gcsfs</span>
# or install with pip
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pip install gcsfs</span>`}}),Es=new k({props:{code:`import gcsfs

gcs = gcsfs.GCSFileSystem(project='my-google-project')

encoded_dataset.save_to_disk('gcs://my-private-datasets/imdb/train', fs=gcs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> gcsfs

<span class="hljs-comment"># create GCSFileSystem instance using default gcloud credentials with project</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>gcs = gcsfs.GCSFileSystem(project=<span class="hljs-string">&#x27;my-google-project&#x27;</span>)

<span class="hljs-comment"># saves encoded_dataset to your gcs bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&#x27;gcs://my-private-datasets/imdb/train&#x27;</span>, fs=gcs)`}}),xs=new k({props:{code:`import gcsfs
from datasets import load_from_disk

gcs = gcsfs.GCSFileSystem(project='my-google-project')

dataset = load_from_disk('gcs://my-private-datasets/imdb/train', fs=gcs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> gcsfs
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk

<span class="hljs-comment"># create GCSFileSystem instance using default gcloud credentials with project</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>gcs = gcsfs.GCSFileSystem(project=<span class="hljs-string">&#x27;my-google-project&#x27;</span>)

<span class="hljs-comment"># loads encoded_dataset from your gcs bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;gcs://my-private-datasets/imdb/train&#x27;</span>, fs=gcs)`}}),Fs=new zs({}),Ds=new k({props:{code:`conda install -c conda-forge adlfs
pip install adlfs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">conda install -c conda-forge adlfs</span>
# or install with pip
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pip install adlfs</span>`}}),Ts=new k({props:{code:`import adlfs

abfs = adlfs.AzureBlobFileSystem(account_name="XXXX", account_key="XXXX")

encoded_dataset.save_to_disk('abfs://my-private-datasets/imdb/train', fs=abfs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> adlfs

<span class="hljs-comment"># create AzureBlobFileSystem instance with account_name and account_key</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>abfs = adlfs.AzureBlobFileSystem(account_name=<span class="hljs-string">&quot;XXXX&quot;</span>, account_key=<span class="hljs-string">&quot;XXXX&quot;</span>)

<span class="hljs-comment"># saves encoded_dataset to your azure container</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&#x27;abfs://my-private-datasets/imdb/train&#x27;</span>, fs=abfs)`}}),Cs=new k({props:{code:`import adlfs
from datasets import load_from_disk

abfs = adlfs.AzureBlobFileSystem(account_name="XXXX", account_key="XXXX")

dataset = load_from_disk('abfs://my-private-datasets/imdb/train', fs=abfs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> adlfs
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk

<span class="hljs-comment"># create AzureBlobFileSystem instance with account_name and account_key</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>abfs = adlfs.AzureBlobFileSystem(account_name=<span class="hljs-string">&quot;XXXX&quot;</span>, account_key=<span class="hljs-string">&quot;XXXX&quot;</span>)

<span class="hljs-comment"># loads encoded_dataset from your azure container</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;abfs://my-private-datasets/imdb/train&#x27;</span>, fs=abfs)`}}),{c(){v=a("meta"),B=d(),$=a("h1"),j=a("a"),F=a("span"),m(S.$$.fragment),V=d(),D=a("span"),L=i("Cloud storage"),b=d(),E=a("p"),Os=i("\u{1F917} Datasets supports access to cloud storage providers through a S3 filesystem implementation: "),Bs=a("a"),Ce=i("filesystems.S3FileSystem"),Pe=i(". You can save and load datasets from your Amazon S3 bucket in a Pythonic way. Take a look at the following table for other supported cloud storage providers:"),Lt=d(),G=a("table"),Ms=a("thead"),Z=a("tr"),Ys=a("th"),ze=i("Storage provider"),Oe=d(),Js=a("th"),Be=i("Filesystem implementation"),Ge=d(),w=a("tbody"),ss=a("tr"),Us=a("td"),Ie=i("Amazon S3"),Ne=d(),Ks=a("td"),ts=a("a"),qe=i("s3fs"),He=d(),es=a("tr"),Qs=a("td"),Re=i("Google Cloud Storage"),We=d(),Vs=a("td"),as=a("a"),Me=i("gcsfs"),Ye=d(),ls=a("tr"),Zs=a("td"),Je=i("Azure Blob/DataLake"),Ue=d(),st=a("td"),os=a("a"),Ke=i("adlfs"),Qe=d(),rs=a("tr"),tt=a("td"),Ve=i("Dropbox"),Ze=d(),et=a("td"),ns=a("a"),sa=i("dropboxdrivefs"),ta=d(),is=a("tr"),at=a("td"),ea=i("Google Drive"),aa=d(),lt=a("td"),ps=a("a"),la=i("gdrivefs"),Tt=d(),I=a("p"),oa=i("This guide will show you how to save and load datasets with "),ot=a("strong"),ra=i("s3fs"),na=i(" to a S3 bucket, but other filesystem implementations can be used similarly."),Xt=d(),T=a("h2"),N=a("a"),rt=a("span"),m(ds.$$.fragment),ia=d(),nt=a("span"),pa=i("Listing datasets"),Ct=d(),Gs=a("ol"),it=a("li"),da=i("Install the S3 dependency with \u{1F917} Datasets:"),Pt=d(),m(fs.$$.fragment),zt=d(),cs=a("ol"),ms=a("li"),fa=i("List files from a public S3 bucket with "),pt=a("code"),ca=i("s3.ls"),ma=i(":"),Ot=d(),m(hs.$$.fragment),Bt=d(),A=a("p"),ha=i("Access a private S3 bucket by entering your "),dt=a("code"),ga=i("aws_access_key_id"),ua=i(" and "),ft=a("code"),_a=i("aws_secret_access_key"),ya=i(":"),Gt=d(),m(gs.$$.fragment),It=d(),X=a("h2"),q=a("a"),ct=a("span"),m(us.$$.fragment),va=d(),mt=a("span"),$a=i("Saving datasets"),Nt=d(),H=a("p"),Sa=i("After you have processed your dataset, you can save it to S3 with "),Is=a("a"),ba=i("Dataset.save_to_disk()"),wa=i(":"),qt=d(),m(_s.$$.fragment),Ht=d(),m(R.$$.fragment),Rt=d(),W=a("p"),ka=i("Save your dataset with "),ht=a("code"),ja=i("botocore.session.Session"),Ea=i(" and a custom AWS profile:"),Wt=d(),m(ys.$$.fragment),Mt=d(),C=a("h2"),M=a("a"),gt=a("span"),m(vs.$$.fragment),Aa=d(),ut=a("span"),xa=i("Loading datasets"),Yt=d(),Y=a("p"),Fa=i("When you are ready to use your dataset again, reload it with "),Ns=a("a"),Da=i("Dataset.load_from_disk()"),La=i(":"),Jt=d(),m($s.$$.fragment),Ut=d(),J=a("p"),Ta=i("Load with "),_t=a("code"),Xa=i("botocore.session.Session"),Ca=i(" and custom AWS profile:"),Kt=d(),m(Ss.$$.fragment),Qt=d(),P=a("h2"),U=a("a"),yt=a("span"),m(bs.$$.fragment),Pa=d(),vt=a("span"),za=i("Other filesystems"),Vt=d(),qs=a("p"),Oa=i("Other filesystem implementations, like Google Cloud Storage or Azure Blob Storage, are used similarly:"),Zt=d(),z=a("h3"),K=a("a"),$t=a("span"),m(ws.$$.fragment),Ba=d(),St=a("span"),Ga=i("Google Cloud Storage"),se=d(),Hs=a("ol"),bt=a("li"),Ia=i("Install the Google Cloud Storage implementation:"),te=d(),m(ks.$$.fragment),ee=d(),js=a("ol"),wt=a("li"),Na=i("Save your dataset:"),ae=d(),m(Es.$$.fragment),le=d(),As=a("ol"),kt=a("li"),qa=i("Load your dataset:"),oe=d(),m(xs.$$.fragment),re=d(),O=a("h3"),Q=a("a"),jt=a("span"),m(Fs.$$.fragment),Ha=d(),Et=a("span"),Ra=i("Azure Blob Storage"),ne=d(),Rs=a("ol"),At=a("li"),Wa=i("Install the Azure Blob Storage implementation:"),ie=d(),m(Ds.$$.fragment),pe=d(),Ls=a("ol"),xt=a("li"),Ma=i("Save your dataset:"),de=d(),m(Ts.$$.fragment),fe=d(),Xs=a("ol"),Ft=a("li"),Ya=i("Load your dataset:"),ce=d(),m(Cs.$$.fragment),this.h()},l(s){const r=no('[data-svelte="svelte-1phssyn"]',document.head);v=l(r,"META",{name:!0,content:!0}),r.forEach(t),B=f(s),$=l(s,"H1",{class:!0});var Ps=o($);j=l(Ps,"A",{id:!0,class:!0,href:!0});var Ja=o(j);F=l(Ja,"SPAN",{});var Ua=o(F);h(S.$$.fragment,Ua),Ua.forEach(t),Ja.forEach(t),V=f(Ps),D=l(Ps,"SPAN",{});var Ka=o(D);L=p(Ka,"Cloud storage"),Ka.forEach(t),Ps.forEach(t),b=f(s),E=l(s,"P",{});var he=o(E);Os=p(he,"\u{1F917} Datasets supports access to cloud storage providers through a S3 filesystem implementation: "),Bs=l(he,"A",{href:!0});var Qa=o(Bs);Ce=p(Qa,"filesystems.S3FileSystem"),Qa.forEach(t),Pe=p(he,". You can save and load datasets from your Amazon S3 bucket in a Pythonic way. Take a look at the following table for other supported cloud storage providers:"),he.forEach(t),Lt=f(s),G=l(s,"TABLE",{});var ge=o(G);Ms=l(ge,"THEAD",{});var Va=o(Ms);Z=l(Va,"TR",{});var ue=o(Z);Ys=l(ue,"TH",{});var Za=o(Ys);ze=p(Za,"Storage provider"),Za.forEach(t),Oe=f(ue),Js=l(ue,"TH",{});var sl=o(Js);Be=p(sl,"Filesystem implementation"),sl.forEach(t),ue.forEach(t),Va.forEach(t),Ge=f(ge),w=l(ge,"TBODY",{});var x=o(w);ss=l(x,"TR",{});var _e=o(ss);Us=l(_e,"TD",{});var tl=o(Us);Ie=p(tl,"Amazon S3"),tl.forEach(t),Ne=f(_e),Ks=l(_e,"TD",{});var el=o(Ks);ts=l(el,"A",{href:!0,rel:!0});var al=o(ts);qe=p(al,"s3fs"),al.forEach(t),el.forEach(t),_e.forEach(t),He=f(x),es=l(x,"TR",{});var ye=o(es);Qs=l(ye,"TD",{});var ll=o(Qs);Re=p(ll,"Google Cloud Storage"),ll.forEach(t),We=f(ye),Vs=l(ye,"TD",{});var ol=o(Vs);as=l(ol,"A",{href:!0,rel:!0});var rl=o(as);Me=p(rl,"gcsfs"),rl.forEach(t),ol.forEach(t),ye.forEach(t),Ye=f(x),ls=l(x,"TR",{});var ve=o(ls);Zs=l(ve,"TD",{});var nl=o(Zs);Je=p(nl,"Azure Blob/DataLake"),nl.forEach(t),Ue=f(ve),st=l(ve,"TD",{});var il=o(st);os=l(il,"A",{href:!0,rel:!0});var pl=o(os);Ke=p(pl,"adlfs"),pl.forEach(t),il.forEach(t),ve.forEach(t),Qe=f(x),rs=l(x,"TR",{});var $e=o(rs);tt=l($e,"TD",{});var dl=o(tt);Ve=p(dl,"Dropbox"),dl.forEach(t),Ze=f($e),et=l($e,"TD",{});var fl=o(et);ns=l(fl,"A",{href:!0,rel:!0});var cl=o(ns);sa=p(cl,"dropboxdrivefs"),cl.forEach(t),fl.forEach(t),$e.forEach(t),ta=f(x),is=l(x,"TR",{});var Se=o(is);at=l(Se,"TD",{});var ml=o(at);ea=p(ml,"Google Drive"),ml.forEach(t),aa=f(Se),lt=l(Se,"TD",{});var hl=o(lt);ps=l(hl,"A",{href:!0,rel:!0});var gl=o(ps);la=p(gl,"gdrivefs"),gl.forEach(t),hl.forEach(t),Se.forEach(t),x.forEach(t),ge.forEach(t),Tt=f(s),I=l(s,"P",{});var be=o(I);oa=p(be,"This guide will show you how to save and load datasets with "),ot=l(be,"STRONG",{});var ul=o(ot);ra=p(ul,"s3fs"),ul.forEach(t),na=p(be," to a S3 bucket, but other filesystem implementations can be used similarly."),be.forEach(t),Xt=f(s),T=l(s,"H2",{class:!0});var we=o(T);N=l(we,"A",{id:!0,class:!0,href:!0});var _l=o(N);rt=l(_l,"SPAN",{});var yl=o(rt);h(ds.$$.fragment,yl),yl.forEach(t),_l.forEach(t),ia=f(we),nt=l(we,"SPAN",{});var vl=o(nt);pa=p(vl,"Listing datasets"),vl.forEach(t),we.forEach(t),Ct=f(s),Gs=l(s,"OL",{});var $l=o(Gs);it=l($l,"LI",{});var Sl=o(it);da=p(Sl,"Install the S3 dependency with \u{1F917} Datasets:"),Sl.forEach(t),$l.forEach(t),Pt=f(s),h(fs.$$.fragment,s),zt=f(s),cs=l(s,"OL",{start:!0});var bl=o(cs);ms=l(bl,"LI",{});var ke=o(ms);fa=p(ke,"List files from a public S3 bucket with "),pt=l(ke,"CODE",{});var wl=o(pt);ca=p(wl,"s3.ls"),wl.forEach(t),ma=p(ke,":"),ke.forEach(t),bl.forEach(t),Ot=f(s),h(hs.$$.fragment,s),Bt=f(s),A=l(s,"P",{});var Ws=o(A);ha=p(Ws,"Access a private S3 bucket by entering your "),dt=l(Ws,"CODE",{});var kl=o(dt);ga=p(kl,"aws_access_key_id"),kl.forEach(t),ua=p(Ws," and "),ft=l(Ws,"CODE",{});var jl=o(ft);_a=p(jl,"aws_secret_access_key"),jl.forEach(t),ya=p(Ws,":"),Ws.forEach(t),Gt=f(s),h(gs.$$.fragment,s),It=f(s),X=l(s,"H2",{class:!0});var je=o(X);q=l(je,"A",{id:!0,class:!0,href:!0});var El=o(q);ct=l(El,"SPAN",{});var Al=o(ct);h(us.$$.fragment,Al),Al.forEach(t),El.forEach(t),va=f(je),mt=l(je,"SPAN",{});var xl=o(mt);$a=p(xl,"Saving datasets"),xl.forEach(t),je.forEach(t),Nt=f(s),H=l(s,"P",{});var Ee=o(H);Sa=p(Ee,"After you have processed your dataset, you can save it to S3 with "),Is=l(Ee,"A",{href:!0});var Fl=o(Is);ba=p(Fl,"Dataset.save_to_disk()"),Fl.forEach(t),wa=p(Ee,":"),Ee.forEach(t),qt=f(s),h(_s.$$.fragment,s),Ht=f(s),h(R.$$.fragment,s),Rt=f(s),W=l(s,"P",{});var Ae=o(W);ka=p(Ae,"Save your dataset with "),ht=l(Ae,"CODE",{});var Dl=o(ht);ja=p(Dl,"botocore.session.Session"),Dl.forEach(t),Ea=p(Ae," and a custom AWS profile:"),Ae.forEach(t),Wt=f(s),h(ys.$$.fragment,s),Mt=f(s),C=l(s,"H2",{class:!0});var xe=o(C);M=l(xe,"A",{id:!0,class:!0,href:!0});var Ll=o(M);gt=l(Ll,"SPAN",{});var Tl=o(gt);h(vs.$$.fragment,Tl),Tl.forEach(t),Ll.forEach(t),Aa=f(xe),ut=l(xe,"SPAN",{});var Xl=o(ut);xa=p(Xl,"Loading datasets"),Xl.forEach(t),xe.forEach(t),Yt=f(s),Y=l(s,"P",{});var Fe=o(Y);Fa=p(Fe,"When you are ready to use your dataset again, reload it with "),Ns=l(Fe,"A",{href:!0});var Cl=o(Ns);Da=p(Cl,"Dataset.load_from_disk()"),Cl.forEach(t),La=p(Fe,":"),Fe.forEach(t),Jt=f(s),h($s.$$.fragment,s),Ut=f(s),J=l(s,"P",{});var De=o(J);Ta=p(De,"Load with "),_t=l(De,"CODE",{});var Pl=o(_t);Xa=p(Pl,"botocore.session.Session"),Pl.forEach(t),Ca=p(De," and custom AWS profile:"),De.forEach(t),Kt=f(s),h(Ss.$$.fragment,s),Qt=f(s),P=l(s,"H2",{class:!0});var Le=o(P);U=l(Le,"A",{id:!0,class:!0,href:!0});var zl=o(U);yt=l(zl,"SPAN",{});var Ol=o(yt);h(bs.$$.fragment,Ol),Ol.forEach(t),zl.forEach(t),Pa=f(Le),vt=l(Le,"SPAN",{});var Bl=o(vt);za=p(Bl,"Other filesystems"),Bl.forEach(t),Le.forEach(t),Vt=f(s),qs=l(s,"P",{});var Gl=o(qs);Oa=p(Gl,"Other filesystem implementations, like Google Cloud Storage or Azure Blob Storage, are used similarly:"),Gl.forEach(t),Zt=f(s),z=l(s,"H3",{class:!0});var Te=o(z);K=l(Te,"A",{id:!0,class:!0,href:!0});var Il=o(K);$t=l(Il,"SPAN",{});var Nl=o($t);h(ws.$$.fragment,Nl),Nl.forEach(t),Il.forEach(t),Ba=f(Te),St=l(Te,"SPAN",{});var ql=o(St);Ga=p(ql,"Google Cloud Storage"),ql.forEach(t),Te.forEach(t),se=f(s),Hs=l(s,"OL",{});var Hl=o(Hs);bt=l(Hl,"LI",{});var Rl=o(bt);Ia=p(Rl,"Install the Google Cloud Storage implementation:"),Rl.forEach(t),Hl.forEach(t),te=f(s),h(ks.$$.fragment,s),ee=f(s),js=l(s,"OL",{start:!0});var Wl=o(js);wt=l(Wl,"LI",{});var Ml=o(wt);Na=p(Ml,"Save your dataset:"),Ml.forEach(t),Wl.forEach(t),ae=f(s),h(Es.$$.fragment,s),le=f(s),As=l(s,"OL",{start:!0});var Yl=o(As);kt=l(Yl,"LI",{});var Jl=o(kt);qa=p(Jl,"Load your dataset:"),Jl.forEach(t),Yl.forEach(t),oe=f(s),h(xs.$$.fragment,s),re=f(s),O=l(s,"H3",{class:!0});var Xe=o(O);Q=l(Xe,"A",{id:!0,class:!0,href:!0});var Ul=o(Q);jt=l(Ul,"SPAN",{});var Kl=o(jt);h(Fs.$$.fragment,Kl),Kl.forEach(t),Ul.forEach(t),Ha=f(Xe),Et=l(Xe,"SPAN",{});var Ql=o(Et);Ra=p(Ql,"Azure Blob Storage"),Ql.forEach(t),Xe.forEach(t),ne=f(s),Rs=l(s,"OL",{});var Vl=o(Rs);At=l(Vl,"LI",{});var Zl=o(At);Wa=p(Zl,"Install the Azure Blob Storage implementation:"),Zl.forEach(t),Vl.forEach(t),ie=f(s),h(Ds.$$.fragment,s),pe=f(s),Ls=l(s,"OL",{start:!0});var so=o(Ls);xt=l(so,"LI",{});var to=o(xt);Ma=p(to,"Save your dataset:"),to.forEach(t),so.forEach(t),de=f(s),h(Ts.$$.fragment,s),fe=f(s),Xs=l(s,"OL",{start:!0});var eo=o(Xs);Ft=l(eo,"LI",{});var ao=o(Ft);Ya=p(ao,"Load your dataset:"),ao.forEach(t),eo.forEach(t),ce=f(s),h(Cs.$$.fragment,s),this.h()},h(){c(v,"name","hf:doc:metadata"),c(v,"content",JSON.stringify(mo)),c(j,"id","cloud-storage"),c(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(j,"href","#cloud-storage"),c($,"class","relative group"),c(Bs,"href","/docs/datasets/pr_4513/en/package_reference/main_classes#datasets.filesystems.S3FileSystem"),c(ts,"href","https://s3fs.readthedocs.io/en/latest/"),c(ts,"rel","nofollow"),c(as,"href","https://gcsfs.readthedocs.io/en/latest/"),c(as,"rel","nofollow"),c(os,"href","https://github.com/fsspec/adlfs"),c(os,"rel","nofollow"),c(ns,"href","https://github.com/MarineChap/dropboxdrivefs"),c(ns,"rel","nofollow"),c(ps,"href","https://github.com/intake/gdrivefs"),c(ps,"rel","nofollow"),c(N,"id","listing-datasets"),c(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(N,"href","#listing-datasets"),c(T,"class","relative group"),c(cs,"start","2"),c(q,"id","saving-datasets"),c(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(q,"href","#saving-datasets"),c(X,"class","relative group"),c(Is,"href","/docs/datasets/pr_4513/en/package_reference/main_classes#datasets.Dataset.save_to_disk"),c(M,"id","loading-datasets"),c(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(M,"href","#loading-datasets"),c(C,"class","relative group"),c(Ns,"href","/docs/datasets/pr_4513/en/package_reference/main_classes#datasets.Dataset.load_from_disk"),c(U,"id","other-filesystems"),c(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(U,"href","#other-filesystems"),c(P,"class","relative group"),c(K,"id","google-cloud-storage"),c(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K,"href","#google-cloud-storage"),c(z,"class","relative group"),c(js,"start","2"),c(As,"start","3"),c(Q,"id","azure-blob-storage"),c(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Q,"href","#azure-blob-storage"),c(O,"class","relative group"),c(Ls,"start","2"),c(Xs,"start","3")},m(s,r){e(document.head,v),n(s,B,r),n(s,$,r),e($,j),e(j,F),g(S,F,null),e($,V),e($,D),e(D,L),n(s,b,r),n(s,E,r),e(E,Os),e(E,Bs),e(Bs,Ce),e(E,Pe),n(s,Lt,r),n(s,G,r),e(G,Ms),e(Ms,Z),e(Z,Ys),e(Ys,ze),e(Z,Oe),e(Z,Js),e(Js,Be),e(G,Ge),e(G,w),e(w,ss),e(ss,Us),e(Us,Ie),e(ss,Ne),e(ss,Ks),e(Ks,ts),e(ts,qe),e(w,He),e(w,es),e(es,Qs),e(Qs,Re),e(es,We),e(es,Vs),e(Vs,as),e(as,Me),e(w,Ye),e(w,ls),e(ls,Zs),e(Zs,Je),e(ls,Ue),e(ls,st),e(st,os),e(os,Ke),e(w,Qe),e(w,rs),e(rs,tt),e(tt,Ve),e(rs,Ze),e(rs,et),e(et,ns),e(ns,sa),e(w,ta),e(w,is),e(is,at),e(at,ea),e(is,aa),e(is,lt),e(lt,ps),e(ps,la),n(s,Tt,r),n(s,I,r),e(I,oa),e(I,ot),e(ot,ra),e(I,na),n(s,Xt,r),n(s,T,r),e(T,N),e(N,rt),g(ds,rt,null),e(T,ia),e(T,nt),e(nt,pa),n(s,Ct,r),n(s,Gs,r),e(Gs,it),e(it,da),n(s,Pt,r),g(fs,s,r),n(s,zt,r),n(s,cs,r),e(cs,ms),e(ms,fa),e(ms,pt),e(pt,ca),e(ms,ma),n(s,Ot,r),g(hs,s,r),n(s,Bt,r),n(s,A,r),e(A,ha),e(A,dt),e(dt,ga),e(A,ua),e(A,ft),e(ft,_a),e(A,ya),n(s,Gt,r),g(gs,s,r),n(s,It,r),n(s,X,r),e(X,q),e(q,ct),g(us,ct,null),e(X,va),e(X,mt),e(mt,$a),n(s,Nt,r),n(s,H,r),e(H,Sa),e(H,Is),e(Is,ba),e(H,wa),n(s,qt,r),g(_s,s,r),n(s,Ht,r),g(R,s,r),n(s,Rt,r),n(s,W,r),e(W,ka),e(W,ht),e(ht,ja),e(W,Ea),n(s,Wt,r),g(ys,s,r),n(s,Mt,r),n(s,C,r),e(C,M),e(M,gt),g(vs,gt,null),e(C,Aa),e(C,ut),e(ut,xa),n(s,Yt,r),n(s,Y,r),e(Y,Fa),e(Y,Ns),e(Ns,Da),e(Y,La),n(s,Jt,r),g($s,s,r),n(s,Ut,r),n(s,J,r),e(J,Ta),e(J,_t),e(_t,Xa),e(J,Ca),n(s,Kt,r),g(Ss,s,r),n(s,Qt,r),n(s,P,r),e(P,U),e(U,yt),g(bs,yt,null),e(P,Pa),e(P,vt),e(vt,za),n(s,Vt,r),n(s,qs,r),e(qs,Oa),n(s,Zt,r),n(s,z,r),e(z,K),e(K,$t),g(ws,$t,null),e(z,Ba),e(z,St),e(St,Ga),n(s,se,r),n(s,Hs,r),e(Hs,bt),e(bt,Ia),n(s,te,r),g(ks,s,r),n(s,ee,r),n(s,js,r),e(js,wt),e(wt,Na),n(s,ae,r),g(Es,s,r),n(s,le,r),n(s,As,r),e(As,kt),e(kt,qa),n(s,oe,r),g(xs,s,r),n(s,re,r),n(s,O,r),e(O,Q),e(Q,jt),g(Fs,jt,null),e(O,Ha),e(O,Et),e(Et,Ra),n(s,ne,r),n(s,Rs,r),e(Rs,At),e(At,Wa),n(s,ie,r),g(Ds,s,r),n(s,pe,r),n(s,Ls,r),e(Ls,xt),e(xt,Ma),n(s,de,r),g(Ts,s,r),n(s,fe,r),n(s,Xs,r),e(Xs,Ft),e(Ft,Ya),n(s,ce,r),g(Cs,s,r),me=!0},p(s,[r]){const Ps={};r&2&&(Ps.$$scope={dirty:r,ctx:s}),R.$set(Ps)},i(s){me||(u(S.$$.fragment,s),u(ds.$$.fragment,s),u(fs.$$.fragment,s),u(hs.$$.fragment,s),u(gs.$$.fragment,s),u(us.$$.fragment,s),u(_s.$$.fragment,s),u(R.$$.fragment,s),u(ys.$$.fragment,s),u(vs.$$.fragment,s),u($s.$$.fragment,s),u(Ss.$$.fragment,s),u(bs.$$.fragment,s),u(ws.$$.fragment,s),u(ks.$$.fragment,s),u(Es.$$.fragment,s),u(xs.$$.fragment,s),u(Fs.$$.fragment,s),u(Ds.$$.fragment,s),u(Ts.$$.fragment,s),u(Cs.$$.fragment,s),me=!0)},o(s){_(S.$$.fragment,s),_(ds.$$.fragment,s),_(fs.$$.fragment,s),_(hs.$$.fragment,s),_(gs.$$.fragment,s),_(us.$$.fragment,s),_(_s.$$.fragment,s),_(R.$$.fragment,s),_(ys.$$.fragment,s),_(vs.$$.fragment,s),_($s.$$.fragment,s),_(Ss.$$.fragment,s),_(bs.$$.fragment,s),_(ws.$$.fragment,s),_(ks.$$.fragment,s),_(Es.$$.fragment,s),_(xs.$$.fragment,s),_(Fs.$$.fragment,s),_(Ds.$$.fragment,s),_(Ts.$$.fragment,s),_(Cs.$$.fragment,s),me=!1},d(s){t(v),s&&t(B),s&&t($),y(S),s&&t(b),s&&t(E),s&&t(Lt),s&&t(G),s&&t(Tt),s&&t(I),s&&t(Xt),s&&t(T),y(ds),s&&t(Ct),s&&t(Gs),s&&t(Pt),y(fs,s),s&&t(zt),s&&t(cs),s&&t(Ot),y(hs,s),s&&t(Bt),s&&t(A),s&&t(Gt),y(gs,s),s&&t(It),s&&t(X),y(us),s&&t(Nt),s&&t(H),s&&t(qt),y(_s,s),s&&t(Ht),y(R,s),s&&t(Rt),s&&t(W),s&&t(Wt),y(ys,s),s&&t(Mt),s&&t(C),y(vs),s&&t(Yt),s&&t(Y),s&&t(Jt),y($s,s),s&&t(Ut),s&&t(J),s&&t(Kt),y(Ss,s),s&&t(Qt),s&&t(P),y(bs),s&&t(Vt),s&&t(qs),s&&t(Zt),s&&t(z),y(ws),s&&t(se),s&&t(Hs),s&&t(te),y(ks,s),s&&t(ee),s&&t(js),s&&t(ae),y(Es,s),s&&t(le),s&&t(As),s&&t(oe),y(xs,s),s&&t(re),s&&t(O),y(Fs),s&&t(ne),s&&t(Rs),s&&t(ie),y(Ds,s),s&&t(pe),s&&t(Ls),s&&t(de),y(Ts,s),s&&t(fe),s&&t(Xs),s&&t(ce),y(Cs,s)}}}const mo={local:"cloud-storage",sections:[{local:"listing-datasets",title:"Listing datasets"},{local:"saving-datasets",title:"Saving datasets"},{local:"loading-datasets",title:"Loading datasets"},{local:"other-filesystems",sections:[{local:"google-cloud-storage",title:"Google Cloud Storage"},{local:"azure-blob-storage",title:"Azure Blob Storage"}],title:"Other filesystems"}],title:"Cloud storage"};function ho(Dt){return io(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class vo extends lo{constructor(v){super();oo(this,v,ho,co,ro,{})}}export{vo as default,mo as metadata};
