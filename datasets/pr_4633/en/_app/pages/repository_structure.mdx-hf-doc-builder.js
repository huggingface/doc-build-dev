import{S as Ml,i as Cl,s as Ol,e as i,k as f,w as _,t as a,M as gl,c as r,d as s,m as d,a as o,x as y,h as l,b as c,G as e,g as p,y as E,q as $,o as w,B as b,v as Nl}from"../chunks/vendor-hf-doc-builder.js";import{T as Tl}from"../chunks/Tip-hf-doc-builder.js";import{I as Pt}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as St}from"../chunks/CodeBlock-hf-doc-builder.js";function Hl(ce){let h,F,v,x,N;return{c(){h=i("p"),F=a("Eventually, you\u2019ll also be able to structure your repository to specify different dataset configurations. Stay tuned on this "),v=i("a"),x=a("issue"),N=a(" for the latest updates!"),this.h()},l(m){h=r(m,"P",{});var D=o(h);F=l(D,"Eventually, you\u2019ll also be able to structure your repository to specify different dataset configurations. Stay tuned on this "),v=r(D,"A",{href:!0,rel:!0});var q=o(v);x=l(q,"issue"),q.forEach(s),N=l(D," for the latest updates!"),D.forEach(s),this.h()},h(){c(v,"href","https://github.com/huggingface/datasets/issues/4578"),c(v,"rel","nofollow")},m(m,D){p(m,h,D),e(h,F),e(h,v),e(v,x),e(h,N)},d(m){m&&s(h)}}}function Rl(ce){let h,F,v,x,N,m,D,q,ts,he,ut,es,ve,J,ss,mt,as,ls,ue,U,is,_t,rs,os,me,T,B,Mt,et,ns,Ct,ps,_e,j,fs,Ot,ds,cs,gt,hs,vs,ye,P,us,Nt,ms,_s,yt,ys,Es,Ee,st,$e,H,V,Tt,at,$s,Ht,ws,we,Et,bs,be,A,xs,Rt,As,ks,It,Ds,js,Lt,Ps,Ss,xe,Y,S,Ms,Ft,Cs,Os,qt,gs,Ns,Jt,Ts,Hs,M,Rs,Ut,Is,Ls,Bt,Fs,qs,Vt,Js,Ae,G,Us,Yt,Bs,Vs,ke,lt,De,C,Ys,Gt,Gs,zs,zt,Ks,Qs,je,R,z,Kt,it,Ws,Qt,Xs,Pe,$t,Zs,Se,rt,Me,u,ta,Wt,ea,sa,Xt,aa,la,Zt,ia,ra,te,oa,na,Ce,wt,pa,Oe,ot,ge,K,Ne,I,Q,ee,nt,fa,se,da,Te,bt,ca,He,O,ae,ha,va,le,ua,ma,ie,_a,Re,xt,ya,Ie,pt,Le,L,W,re,ft,Ea,oe,$a,Fe,X,wa,ne,ba,xa,qe,k,Aa,pe,ka,Da,fe,ja,Pa,de,Sa,Ma,Je,dt,Ue;return m=new Pt({}),et=new Pt({}),st=new St({props:{code:`my_dataset_repository/
\u251C\u2500\u2500 README.md
\u251C\u2500\u2500 train.csv
\u2514\u2500\u2500 test.csv`,highlighted:`my_dataset_repository/
\u251C\u2500\u2500 README.md
\u251C\u2500\u2500 train.<span class="hljs-built_in">csv</span>
\u2514\u2500\u2500 test.<span class="hljs-built_in">csv</span>`}}),at=new Pt({}),lt=new St({props:{code:`my_dataset_repository/
\u251C\u2500\u2500 README.md
\u2514\u2500\u2500 data/
    \u251C\u2500\u2500 train.csv
    \u251C\u2500\u2500 test.csv
    \u2514\u2500\u2500 validation.csv`,highlighted:`my_dataset_repository/
\u251C\u2500\u2500 README.md
\u2514\u2500\u2500 data/
    \u251C\u2500\u2500 train.<span class="hljs-built_in">csv</span>
    \u251C\u2500\u2500 test.<span class="hljs-built_in">csv</span>
    \u2514\u2500\u2500 validation.<span class="hljs-built_in">csv</span>`}}),it=new Pt({}),rt=new St({props:{code:`my_dataset_repository/
\u251C\u2500\u2500 README.md
\u251C\u2500\u2500 train_0.csv
\u251C\u2500\u2500 train_1.csv
\u251C\u2500\u2500 train_2.csv
\u251C\u2500\u2500 train_3.csv
\u251C\u2500\u2500 test_0.csv
\u2514\u2500\u2500 test_1.csv`,highlighted:`my_dataset_repository/
\u251C\u2500\u2500 README.md
\u251C\u2500\u2500 train_0.<span class="hljs-built_in">csv</span>
\u251C\u2500\u2500 train_1.<span class="hljs-built_in">csv</span>
\u251C\u2500\u2500 train_2.<span class="hljs-built_in">csv</span>
\u251C\u2500\u2500 train_3.<span class="hljs-built_in">csv</span>
\u251C\u2500\u2500 test_0.<span class="hljs-built_in">csv</span>
\u2514\u2500\u2500 test_1.<span class="hljs-built_in">csv</span>`}}),ot=new St({props:{code:`my_dataset_repository/
\u251C\u2500\u2500 README.md
\u2514\u2500\u2500 data/
    \u251C\u2500\u2500 train/
    \u2502   \u251C\u2500\u2500 shard_0.csv
    \u2502   \u251C\u2500\u2500 shard_1.csv
    \u2502   \u251C\u2500\u2500 shard_2.csv
    \u2502   \u2514\u2500\u2500 shard_3.csv
    \u2514\u2500\u2500 test/
        \u251C\u2500\u2500 shard_0.csv
        \u2514\u2500\u2500 shard_1.csv`,highlighted:`my_dataset_repository/
\u251C\u2500\u2500 README.md
\u2514\u2500\u2500 data/
    \u251C\u2500\u2500 train/
    \u2502   \u251C\u2500\u2500 shard_0.<span class="hljs-built_in">csv</span>
    \u2502   \u251C\u2500\u2500 shard_1.<span class="hljs-built_in">csv</span>
    \u2502   \u251C\u2500\u2500 shard_2.<span class="hljs-built_in">csv</span>
    \u2502   \u2514\u2500\u2500 shard_3.<span class="hljs-built_in">csv</span>
    \u2514\u2500\u2500 test/
        \u251C\u2500\u2500 shard_0.<span class="hljs-built_in">csv</span>
        \u2514\u2500\u2500 shard_1.<span class="hljs-built_in">csv</span>`}}),K=new Tl({props:{$$slots:{default:[Hl]},$$scope:{ctx:ce}}}),nt=new Pt({}),pt=new St({props:{code:`my_dataset_repository/
\u251C\u2500\u2500 README.md
\u2514\u2500\u2500 data/
    \u251C\u2500\u2500 training.csv
    \u251C\u2500\u2500 eval.csv
    \u2514\u2500\u2500 valid.csv`,highlighted:`my_dataset_repository/
\u251C\u2500\u2500 README.md
\u2514\u2500\u2500 data/
    \u251C\u2500\u2500 training.<span class="hljs-built_in">csv</span>
    \u251C\u2500\u2500 <span class="hljs-built_in">eval</span>.<span class="hljs-built_in">csv</span>
    \u2514\u2500\u2500 valid.<span class="hljs-built_in">csv</span>`}}),ft=new Pt({}),dt=new St({props:{code:`my_dataset_repository/
\u251C\u2500\u2500 README.md
\u2514\u2500\u2500 data/
    \u251C\u2500\u2500 train-00000-of-00003.csv
    \u251C\u2500\u2500 train-00001-of-00003.csv
    \u251C\u2500\u2500 train-00002-of-00003.csv
    \u251C\u2500\u2500 test-00000-of-00001.csv
    \u251C\u2500\u2500 random-00000-of-00003.csv
    \u251C\u2500\u2500 random-00001-of-00003.csv
    \u2514\u2500\u2500 random-00002-of-00003.csv`,highlighted:`my_dataset_repository/
\u251C\u2500\u2500 README.md
\u2514\u2500\u2500 data/
    \u251C\u2500\u2500 train<span class="hljs-string">-00000</span>-of<span class="hljs-string">-00003</span>.csv
    \u251C\u2500\u2500 train<span class="hljs-string">-00001</span>-of<span class="hljs-string">-00003</span>.csv
    \u251C\u2500\u2500 train<span class="hljs-string">-00002</span>-of<span class="hljs-string">-00003</span>.csv
    \u251C\u2500\u2500 test<span class="hljs-string">-00000</span>-of<span class="hljs-string">-00001</span>.csv
    \u251C\u2500\u2500 random<span class="hljs-string">-00000</span>-of<span class="hljs-string">-00003</span>.csv
    \u251C\u2500\u2500 random<span class="hljs-string">-00001</span>-of<span class="hljs-string">-00003</span>.csv
    \u2514\u2500\u2500 random<span class="hljs-string">-00002</span>-of<span class="hljs-string">-00003</span>.csv`}}),{c(){h=i("meta"),F=f(),v=i("h1"),x=i("a"),N=i("span"),_(m.$$.fragment),D=f(),q=i("span"),ts=a("Structure your repository"),he=f(),ut=i("p"),es=a("To host and share your dataset, you can create a dataset repository on the Hugging Face Dataset Hub and upload your data files."),ve=f(),J=i("p"),ss=a(`This guide will show you how to structure your dataset repository when you upload it.
A dataset with a supported structure and file format (text, JSON, JSON Lines, CSV, Parquet) can be loaded automatically with `),mt=i("a"),as=a("load_dataset()"),ls=a(", and it\u2019ll have a preview on its dataset page on the Hub."),ue=f(),U=i("p"),is=a("For more flexibility over how to load and generate a dataset, you can also write a "),_t=i("a"),rs=a("dataset loading script"),os=a("."),me=f(),T=i("h2"),B=i("a"),Mt=i("span"),_(et.$$.fragment),ns=f(),Ct=i("span"),ps=a("Main use-case"),_e=f(),j=i("p"),fs=a("The simplest dataset structure has two files: "),Ot=i("code"),ds=a("train.csv"),cs=a(" and "),gt=i("code"),hs=a("test.csv"),vs=a("."),ye=f(),P=i("p"),us=a("Your repository will also contain a "),Nt=i("code"),ms=a("README.md"),_s=a(" file, the "),yt=i("a"),ys=a("dataset card"),Es=a(" displayed on your dataset page."),Ee=f(),_(st.$$.fragment),$e=f(),H=i("h2"),V=i("a"),Tt=i("span"),_(at.$$.fragment),$s=f(),Ht=i("span"),ws=a("Splits and file names"),we=f(),Et=i("p"),bs=a("\u{1F917} Datasets automatically infer a dataset\u2019s train, validation, and test splits from the file names."),be=f(),A=i("p"),xs=a("Files that contain "),Rt=i("em"),As=a("train"),ks=a(" in their names are considered part of the train split, e.g. "),It=i("code"),Ds=a("train.csv"),js=a(", "),Lt=i("code"),Ps=a("my_train_file.csv"),Ss=a(`, etc.
The same idea applies to the test and validation split:`),xe=f(),Y=i("ul"),S=i("li"),Ms=a("All the files that contain "),Ft=i("em"),Cs=a("test"),Os=a(" in their names are considered part of the test split, e.g. "),qt=i("code"),gs=a("test.csv"),Ns=a(", "),Jt=i("code"),Ts=a("my_test_file.csv"),Hs=f(),M=i("li"),Rs=a("All the files that contain "),Ut=i("em"),Is=a("validation"),Ls=a(" in their names are considered part of the validation split, e.g. "),Bt=i("code"),Fs=a("validation.csv"),qs=a(", "),Vt=i("code"),Js=a("my_validation_file.csv"),Ae=f(),G=i("p"),Us=a("Here is an example where all the files are placed into a directory named "),Yt=i("code"),Bs=a("data"),Vs=a(":"),ke=f(),_(lt.$$.fragment),De=f(),C=i("p"),Ys=a("Note that if a file contains "),Gt=i("em"),Gs=a("test"),zs=a(" but is embedded in another word (e.g. "),zt=i("code"),Ks=a("contest.csv"),Qs=a("), it\u2019s not counted as a test file."),je=f(),R=i("h2"),z=i("a"),Kt=i("span"),_(it.$$.fragment),Ws=f(),Qt=i("span"),Xs=a("Multiple files per split"),Pe=f(),$t=i("p"),Zs=a(`If one of your splits comprises several files, \u{1F917} Datasets can still infer whether it is the train, validation, and test split from the file name.
For example, if your train and test splits span several files:`),Se=f(),_(rt.$$.fragment),Me=f(),u=i("p"),ta=a("Make sure all the files of your "),Wt=i("code"),ea=a("train"),sa=a(" set have "),Xt=i("em"),aa=a("train"),la=a(` in their names (same for test and validation).
Even if you add a prefix or suffix to `),Zt=i("code"),ia=a("train"),ra=a(" in the file name (like "),te=i("code"),oa=a("my_train_file_00001.csv"),na=a(` for example),
\u{1F917} Datasets can still infer the appropriate split.`),Ce=f(),wt=i("p"),pa=a(`For convenience, you can also place your data files into different directories.
In this case, the split name is inferred from the directory name.`),Oe=f(),_(ot.$$.fragment),ge=f(),_(K.$$.fragment),Ne=f(),I=i("h2"),Q=i("a"),ee=i("span"),_(nt.$$.fragment),fa=f(),se=i("span"),da=a("Split names keywords"),Te=f(),bt=i("p"),ca=a(`Train/validation/test splits are sometimes called train/dev/test, or sometimes train & eval sets.
These other names are also supported.
In particular, these keywords are equivalent:`),He=f(),O=i("ul"),ae=i("li"),ha=a("train, training"),va=f(),le=i("li"),ua=a("validation, valid, dev"),ma=f(),ie=i("li"),_a=a("test, eval"),Re=f(),xt=i("p"),ya=a("Therefore this is also a valid repository:"),Ie=f(),_(pt.$$.fragment),Le=f(),L=i("h2"),W=i("a"),re=i("span"),_(ft.$$.fragment),Ea=f(),oe=i("span"),$a=a("Custom split names"),Fe=f(),X=i("p"),wa=a(`If you have other data files in addition to the traditional train, validation, and test sets, you must use a different structure.
Use this exact file name format for this structure type: `),ne=i("code"),ba=a("data/<split_name>-xxxxx-of-xxxxx.csv"),xa=a("."),qe=f(),k=i("p"),Aa=a("Here is an example with three splits: "),pe=i("code"),ka=a("train"),Da=a(", "),fe=i("code"),ja=a("test"),Pa=a(", and "),de=i("code"),Sa=a("random"),Ma=a(":"),Je=f(),_(dt.$$.fragment),this.h()},l(t){const n=gl('[data-svelte="svelte-1phssyn"]',document.head);h=r(n,"META",{name:!0,content:!0}),n.forEach(s),F=d(t),v=r(t,"H1",{class:!0});var ct=o(v);x=r(ct,"A",{id:!0,class:!0,href:!0});var Ca=o(x);N=r(Ca,"SPAN",{});var Oa=o(N);y(m.$$.fragment,Oa),Oa.forEach(s),Ca.forEach(s),D=d(ct),q=r(ct,"SPAN",{});var ga=o(q);ts=l(ga,"Structure your repository"),ga.forEach(s),ct.forEach(s),he=d(t),ut=r(t,"P",{});var Na=o(ut);es=l(Na,"To host and share your dataset, you can create a dataset repository on the Hugging Face Dataset Hub and upload your data files."),Na.forEach(s),ve=d(t),J=r(t,"P",{});var Be=o(J);ss=l(Be,`This guide will show you how to structure your dataset repository when you upload it.
A dataset with a supported structure and file format (text, JSON, JSON Lines, CSV, Parquet) can be loaded automatically with `),mt=r(Be,"A",{href:!0});var Ta=o(mt);as=l(Ta,"load_dataset()"),Ta.forEach(s),ls=l(Be,", and it\u2019ll have a preview on its dataset page on the Hub."),Be.forEach(s),ue=d(t),U=r(t,"P",{});var Ve=o(U);is=l(Ve,"For more flexibility over how to load and generate a dataset, you can also write a "),_t=r(Ve,"A",{href:!0});var Ha=o(_t);rs=l(Ha,"dataset loading script"),Ha.forEach(s),os=l(Ve,"."),Ve.forEach(s),me=d(t),T=r(t,"H2",{class:!0});var Ye=o(T);B=r(Ye,"A",{id:!0,class:!0,href:!0});var Ra=o(B);Mt=r(Ra,"SPAN",{});var Ia=o(Mt);y(et.$$.fragment,Ia),Ia.forEach(s),Ra.forEach(s),ns=d(Ye),Ct=r(Ye,"SPAN",{});var La=o(Ct);ps=l(La,"Main use-case"),La.forEach(s),Ye.forEach(s),_e=d(t),j=r(t,"P",{});var At=o(j);fs=l(At,"The simplest dataset structure has two files: "),Ot=r(At,"CODE",{});var Fa=o(Ot);ds=l(Fa,"train.csv"),Fa.forEach(s),cs=l(At," and "),gt=r(At,"CODE",{});var qa=o(gt);hs=l(qa,"test.csv"),qa.forEach(s),vs=l(At,"."),At.forEach(s),ye=d(t),P=r(t,"P",{});var kt=o(P);us=l(kt,"Your repository will also contain a "),Nt=r(kt,"CODE",{});var Ja=o(Nt);ms=l(Ja,"README.md"),Ja.forEach(s),_s=l(kt," file, the "),yt=r(kt,"A",{href:!0});var Ua=o(yt);ys=l(Ua,"dataset card"),Ua.forEach(s),Es=l(kt," displayed on your dataset page."),kt.forEach(s),Ee=d(t),y(st.$$.fragment,t),$e=d(t),H=r(t,"H2",{class:!0});var Ge=o(H);V=r(Ge,"A",{id:!0,class:!0,href:!0});var Ba=o(V);Tt=r(Ba,"SPAN",{});var Va=o(Tt);y(at.$$.fragment,Va),Va.forEach(s),Ba.forEach(s),$s=d(Ge),Ht=r(Ge,"SPAN",{});var Ya=o(Ht);ws=l(Ya,"Splits and file names"),Ya.forEach(s),Ge.forEach(s),we=d(t),Et=r(t,"P",{});var Ga=o(Et);bs=l(Ga,"\u{1F917} Datasets automatically infer a dataset\u2019s train, validation, and test splits from the file names."),Ga.forEach(s),be=d(t),A=r(t,"P",{});var Z=o(A);xs=l(Z,"Files that contain "),Rt=r(Z,"EM",{});var za=o(Rt);As=l(za,"train"),za.forEach(s),ks=l(Z," in their names are considered part of the train split, e.g. "),It=r(Z,"CODE",{});var Ka=o(It);Ds=l(Ka,"train.csv"),Ka.forEach(s),js=l(Z,", "),Lt=r(Z,"CODE",{});var Qa=o(Lt);Ps=l(Qa,"my_train_file.csv"),Qa.forEach(s),Ss=l(Z,`, etc.
The same idea applies to the test and validation split:`),Z.forEach(s),xe=d(t),Y=r(t,"UL",{});var ze=o(Y);S=r(ze,"LI",{});var ht=o(S);Ms=l(ht,"All the files that contain "),Ft=r(ht,"EM",{});var Wa=o(Ft);Cs=l(Wa,"test"),Wa.forEach(s),Os=l(ht," in their names are considered part of the test split, e.g. "),qt=r(ht,"CODE",{});var Xa=o(qt);gs=l(Xa,"test.csv"),Xa.forEach(s),Ns=l(ht,", "),Jt=r(ht,"CODE",{});var Za=o(Jt);Ts=l(Za,"my_test_file.csv"),Za.forEach(s),ht.forEach(s),Hs=d(ze),M=r(ze,"LI",{});var vt=o(M);Rs=l(vt,"All the files that contain "),Ut=r(vt,"EM",{});var tl=o(Ut);Is=l(tl,"validation"),tl.forEach(s),Ls=l(vt," in their names are considered part of the validation split, e.g. "),Bt=r(vt,"CODE",{});var el=o(Bt);Fs=l(el,"validation.csv"),el.forEach(s),qs=l(vt,", "),Vt=r(vt,"CODE",{});var sl=o(Vt);Js=l(sl,"my_validation_file.csv"),sl.forEach(s),vt.forEach(s),ze.forEach(s),Ae=d(t),G=r(t,"P",{});var Ke=o(G);Us=l(Ke,"Here is an example where all the files are placed into a directory named "),Yt=r(Ke,"CODE",{});var al=o(Yt);Bs=l(al,"data"),al.forEach(s),Vs=l(Ke,":"),Ke.forEach(s),ke=d(t),y(lt.$$.fragment,t),De=d(t),C=r(t,"P",{});var Dt=o(C);Ys=l(Dt,"Note that if a file contains "),Gt=r(Dt,"EM",{});var ll=o(Gt);Gs=l(ll,"test"),ll.forEach(s),zs=l(Dt," but is embedded in another word (e.g. "),zt=r(Dt,"CODE",{});var il=o(zt);Ks=l(il,"contest.csv"),il.forEach(s),Qs=l(Dt,"), it\u2019s not counted as a test file."),Dt.forEach(s),je=d(t),R=r(t,"H2",{class:!0});var Qe=o(R);z=r(Qe,"A",{id:!0,class:!0,href:!0});var rl=o(z);Kt=r(rl,"SPAN",{});var ol=o(Kt);y(it.$$.fragment,ol),ol.forEach(s),rl.forEach(s),Ws=d(Qe),Qt=r(Qe,"SPAN",{});var nl=o(Qt);Xs=l(nl,"Multiple files per split"),nl.forEach(s),Qe.forEach(s),Pe=d(t),$t=r(t,"P",{});var pl=o($t);Zs=l(pl,`If one of your splits comprises several files, \u{1F917} Datasets can still infer whether it is the train, validation, and test split from the file name.
For example, if your train and test splits span several files:`),pl.forEach(s),Se=d(t),y(rt.$$.fragment,t),Me=d(t),u=r(t,"P",{});var g=o(u);ta=l(g,"Make sure all the files of your "),Wt=r(g,"CODE",{});var fl=o(Wt);ea=l(fl,"train"),fl.forEach(s),sa=l(g," set have "),Xt=r(g,"EM",{});var dl=o(Xt);aa=l(dl,"train"),dl.forEach(s),la=l(g,` in their names (same for test and validation).
Even if you add a prefix or suffix to `),Zt=r(g,"CODE",{});var cl=o(Zt);ia=l(cl,"train"),cl.forEach(s),ra=l(g," in the file name (like "),te=r(g,"CODE",{});var hl=o(te);oa=l(hl,"my_train_file_00001.csv"),hl.forEach(s),na=l(g,` for example),
\u{1F917} Datasets can still infer the appropriate split.`),g.forEach(s),Ce=d(t),wt=r(t,"P",{});var vl=o(wt);pa=l(vl,`For convenience, you can also place your data files into different directories.
In this case, the split name is inferred from the directory name.`),vl.forEach(s),Oe=d(t),y(ot.$$.fragment,t),ge=d(t),y(K.$$.fragment,t),Ne=d(t),I=r(t,"H2",{class:!0});var We=o(I);Q=r(We,"A",{id:!0,class:!0,href:!0});var ul=o(Q);ee=r(ul,"SPAN",{});var ml=o(ee);y(nt.$$.fragment,ml),ml.forEach(s),ul.forEach(s),fa=d(We),se=r(We,"SPAN",{});var _l=o(se);da=l(_l,"Split names keywords"),_l.forEach(s),We.forEach(s),Te=d(t),bt=r(t,"P",{});var yl=o(bt);ca=l(yl,`Train/validation/test splits are sometimes called train/dev/test, or sometimes train & eval sets.
These other names are also supported.
In particular, these keywords are equivalent:`),yl.forEach(s),He=d(t),O=r(t,"UL",{});var jt=o(O);ae=r(jt,"LI",{});var El=o(ae);ha=l(El,"train, training"),El.forEach(s),va=d(jt),le=r(jt,"LI",{});var $l=o(le);ua=l($l,"validation, valid, dev"),$l.forEach(s),ma=d(jt),ie=r(jt,"LI",{});var wl=o(ie);_a=l(wl,"test, eval"),wl.forEach(s),jt.forEach(s),Re=d(t),xt=r(t,"P",{});var bl=o(xt);ya=l(bl,"Therefore this is also a valid repository:"),bl.forEach(s),Ie=d(t),y(pt.$$.fragment,t),Le=d(t),L=r(t,"H2",{class:!0});var Xe=o(L);W=r(Xe,"A",{id:!0,class:!0,href:!0});var xl=o(W);re=r(xl,"SPAN",{});var Al=o(re);y(ft.$$.fragment,Al),Al.forEach(s),xl.forEach(s),Ea=d(Xe),oe=r(Xe,"SPAN",{});var kl=o(oe);$a=l(kl,"Custom split names"),kl.forEach(s),Xe.forEach(s),Fe=d(t),X=r(t,"P",{});var Ze=o(X);wa=l(Ze,`If you have other data files in addition to the traditional train, validation, and test sets, you must use a different structure.
Use this exact file name format for this structure type: `),ne=r(Ze,"CODE",{});var Dl=o(ne);ba=l(Dl,"data/<split_name>-xxxxx-of-xxxxx.csv"),Dl.forEach(s),xa=l(Ze,"."),Ze.forEach(s),qe=d(t),k=r(t,"P",{});var tt=o(k);Aa=l(tt,"Here is an example with three splits: "),pe=r(tt,"CODE",{});var jl=o(pe);ka=l(jl,"train"),jl.forEach(s),Da=l(tt,", "),fe=r(tt,"CODE",{});var Pl=o(fe);ja=l(Pl,"test"),Pl.forEach(s),Pa=l(tt,", and "),de=r(tt,"CODE",{});var Sl=o(de);Sa=l(Sl,"random"),Sl.forEach(s),Ma=l(tt,":"),tt.forEach(s),Je=d(t),y(dt.$$.fragment,t),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(Il)),c(x,"id","structure-your-repository"),c(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(x,"href","#structure-your-repository"),c(v,"class","relative group"),c(mt,"href","/docs/datasets/pr_4633/en/package_reference/loading_methods#datasets.load_dataset"),c(_t,"href","./dataset_script"),c(B,"id","main-usecase"),c(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(B,"href","#main-usecase"),c(T,"class","relative group"),c(yt,"href","dataset_card"),c(V,"id","splits-and-file-names"),c(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(V,"href","#splits-and-file-names"),c(H,"class","relative group"),c(z,"id","multiple-files-per-split"),c(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(z,"href","#multiple-files-per-split"),c(R,"class","relative group"),c(Q,"id","split-names-keywords"),c(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Q,"href","#split-names-keywords"),c(I,"class","relative group"),c(W,"id","custom-split-names"),c(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(W,"href","#custom-split-names"),c(L,"class","relative group")},m(t,n){e(document.head,h),p(t,F,n),p(t,v,n),e(v,x),e(x,N),E(m,N,null),e(v,D),e(v,q),e(q,ts),p(t,he,n),p(t,ut,n),e(ut,es),p(t,ve,n),p(t,J,n),e(J,ss),e(J,mt),e(mt,as),e(J,ls),p(t,ue,n),p(t,U,n),e(U,is),e(U,_t),e(_t,rs),e(U,os),p(t,me,n),p(t,T,n),e(T,B),e(B,Mt),E(et,Mt,null),e(T,ns),e(T,Ct),e(Ct,ps),p(t,_e,n),p(t,j,n),e(j,fs),e(j,Ot),e(Ot,ds),e(j,cs),e(j,gt),e(gt,hs),e(j,vs),p(t,ye,n),p(t,P,n),e(P,us),e(P,Nt),e(Nt,ms),e(P,_s),e(P,yt),e(yt,ys),e(P,Es),p(t,Ee,n),E(st,t,n),p(t,$e,n),p(t,H,n),e(H,V),e(V,Tt),E(at,Tt,null),e(H,$s),e(H,Ht),e(Ht,ws),p(t,we,n),p(t,Et,n),e(Et,bs),p(t,be,n),p(t,A,n),e(A,xs),e(A,Rt),e(Rt,As),e(A,ks),e(A,It),e(It,Ds),e(A,js),e(A,Lt),e(Lt,Ps),e(A,Ss),p(t,xe,n),p(t,Y,n),e(Y,S),e(S,Ms),e(S,Ft),e(Ft,Cs),e(S,Os),e(S,qt),e(qt,gs),e(S,Ns),e(S,Jt),e(Jt,Ts),e(Y,Hs),e(Y,M),e(M,Rs),e(M,Ut),e(Ut,Is),e(M,Ls),e(M,Bt),e(Bt,Fs),e(M,qs),e(M,Vt),e(Vt,Js),p(t,Ae,n),p(t,G,n),e(G,Us),e(G,Yt),e(Yt,Bs),e(G,Vs),p(t,ke,n),E(lt,t,n),p(t,De,n),p(t,C,n),e(C,Ys),e(C,Gt),e(Gt,Gs),e(C,zs),e(C,zt),e(zt,Ks),e(C,Qs),p(t,je,n),p(t,R,n),e(R,z),e(z,Kt),E(it,Kt,null),e(R,Ws),e(R,Qt),e(Qt,Xs),p(t,Pe,n),p(t,$t,n),e($t,Zs),p(t,Se,n),E(rt,t,n),p(t,Me,n),p(t,u,n),e(u,ta),e(u,Wt),e(Wt,ea),e(u,sa),e(u,Xt),e(Xt,aa),e(u,la),e(u,Zt),e(Zt,ia),e(u,ra),e(u,te),e(te,oa),e(u,na),p(t,Ce,n),p(t,wt,n),e(wt,pa),p(t,Oe,n),E(ot,t,n),p(t,ge,n),E(K,t,n),p(t,Ne,n),p(t,I,n),e(I,Q),e(Q,ee),E(nt,ee,null),e(I,fa),e(I,se),e(se,da),p(t,Te,n),p(t,bt,n),e(bt,ca),p(t,He,n),p(t,O,n),e(O,ae),e(ae,ha),e(O,va),e(O,le),e(le,ua),e(O,ma),e(O,ie),e(ie,_a),p(t,Re,n),p(t,xt,n),e(xt,ya),p(t,Ie,n),E(pt,t,n),p(t,Le,n),p(t,L,n),e(L,W),e(W,re),E(ft,re,null),e(L,Ea),e(L,oe),e(oe,$a),p(t,Fe,n),p(t,X,n),e(X,wa),e(X,ne),e(ne,ba),e(X,xa),p(t,qe,n),p(t,k,n),e(k,Aa),e(k,pe),e(pe,ka),e(k,Da),e(k,fe),e(fe,ja),e(k,Pa),e(k,de),e(de,Sa),e(k,Ma),p(t,Je,n),E(dt,t,n),Ue=!0},p(t,[n]){const ct={};n&2&&(ct.$$scope={dirty:n,ctx:t}),K.$set(ct)},i(t){Ue||($(m.$$.fragment,t),$(et.$$.fragment,t),$(st.$$.fragment,t),$(at.$$.fragment,t),$(lt.$$.fragment,t),$(it.$$.fragment,t),$(rt.$$.fragment,t),$(ot.$$.fragment,t),$(K.$$.fragment,t),$(nt.$$.fragment,t),$(pt.$$.fragment,t),$(ft.$$.fragment,t),$(dt.$$.fragment,t),Ue=!0)},o(t){w(m.$$.fragment,t),w(et.$$.fragment,t),w(st.$$.fragment,t),w(at.$$.fragment,t),w(lt.$$.fragment,t),w(it.$$.fragment,t),w(rt.$$.fragment,t),w(ot.$$.fragment,t),w(K.$$.fragment,t),w(nt.$$.fragment,t),w(pt.$$.fragment,t),w(ft.$$.fragment,t),w(dt.$$.fragment,t),Ue=!1},d(t){s(h),t&&s(F),t&&s(v),b(m),t&&s(he),t&&s(ut),t&&s(ve),t&&s(J),t&&s(ue),t&&s(U),t&&s(me),t&&s(T),b(et),t&&s(_e),t&&s(j),t&&s(ye),t&&s(P),t&&s(Ee),b(st,t),t&&s($e),t&&s(H),b(at),t&&s(we),t&&s(Et),t&&s(be),t&&s(A),t&&s(xe),t&&s(Y),t&&s(Ae),t&&s(G),t&&s(ke),b(lt,t),t&&s(De),t&&s(C),t&&s(je),t&&s(R),b(it),t&&s(Pe),t&&s($t),t&&s(Se),b(rt,t),t&&s(Me),t&&s(u),t&&s(Ce),t&&s(wt),t&&s(Oe),b(ot,t),t&&s(ge),b(K,t),t&&s(Ne),t&&s(I),b(nt),t&&s(Te),t&&s(bt),t&&s(He),t&&s(O),t&&s(Re),t&&s(xt),t&&s(Ie),b(pt,t),t&&s(Le),t&&s(L),b(ft),t&&s(Fe),t&&s(X),t&&s(qe),t&&s(k),t&&s(Je),b(dt,t)}}}const Il={local:"structure-your-repository",sections:[{local:"main-usecase",title:"Main use-case"},{local:"splits-and-file-names",title:"Splits and file names"},{local:"multiple-files-per-split",title:"Multiple files per split"},{local:"split-names-keywords",title:"Split names keywords"},{local:"custom-split-names",title:"Custom split names"}],title:"Structure your repository"};function Ll(ce){return Nl(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Bl extends Ml{constructor(h){super();Cl(this,h,Ll,Rl,Ol,{})}}export{Bl as default,Il as metadata};
