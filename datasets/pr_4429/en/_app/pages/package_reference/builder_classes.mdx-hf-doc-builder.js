import{S as ac,i as sc,s as rc,e as n,k as p,w as v,t as m,M as nc,c as o,d as s,m as c,a as l,x as b,h as g,b as D,G as e,g as $,y as x,q as w,o as E,B as y,v as oc,L as S}from"../../chunks/vendor-hf-doc-builder.js";import{D as j}from"../../chunks/Docstring-hf-doc-builder.js";import{C as I}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Pr}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as T}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function lc(k){let d,_,f,r,u;return r=new I({props:{code:`from datasets import load_dataset_builder
builder = load_dataset_builder('rotten_tomatoes')
ds = builder.download_and_prepare()
ds = builder.as_dataset(split='train')
ds`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.download_and_prepare()
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.as_dataset(split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds
Dataset({
    features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
    num_rows: <span class="hljs-number">8530</span>
})`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function dc(k){let d,_,f,r,u;return r=new I({props:{code:`from datasets import load_dataset_builder
builder = load_dataset_builder('rotten_tomatoes')
ds = builder.download_and_prepare()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.download_and_prepare()`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function ic(k){let d,_,f,r,u;return r=new I({props:{code:`from datasets import load_dataset_builder
ds_builder = load_dataset_builder('rotten_tomatoes')
ds_builder.get_all_exported_dataset_infos()
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder.get_all_exported_dataset_infos()
{<span class="hljs-string">&#x27;default&#x27;</span>: DatasetInfo(description=<span class="hljs-string">&quot;Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, \`\`Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.&#x27;&#x27;, Proceedings of the
5.
ion=&#x27;@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age=&#x27;http://www.cs.cornell.edu/people/pabo/movie-review-data/&#x27;, license=&#x27;&#x27;, features={&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=&#x27;&#x27;, output=&#x27;&#x27;), task_templates=[TextClassification(task=&#x27;text-classification&#x27;, text_column=&#x27;text&#x27;, label_column=&#x27;label&#x27;)], builder_name=&#x27;rotten_tomatoes_movie_review&#x27;, config_name=&#x27;default&#x27;, version=1.0.0, splits={&#x27;train&#x27;: SplitInfo(name=&#x27;train&#x27;, num_bytes=1074810, num_examples=8530, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;validation&#x27;: SplitInfo(name=&#x27;validation&#x27;, num_bytes=134679, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;test&#x27;: SplitInfo(name=&#x27;test&#x27;, num_bytes=135972, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;)}, download_checksums={&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;: {&#x27;num_bytes&#x27;: 487770, &#x27;checksum&#x27;: &#x27;a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9&#x27;}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)}</span>`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function pc(k){let d,_,f,r,u;return r=new I({props:{code:`from datasets import load_dataset_builder
ds_builder = load_dataset_builder('rotten_tomatoes')
ds_builder.get_exported_dataset_info()
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder.get_exported_dataset_info()
DatasetInfo(description=<span class="hljs-string">&quot;Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, \`\`Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.&#x27;&#x27;, Proceedings of the
5.
ion=&#x27;@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age=&#x27;http://www.cs.cornell.edu/people/pabo/movie-review-data/&#x27;, license=&#x27;&#x27;, features={&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=&#x27;&#x27;, output=&#x27;&#x27;), task_templates=[TextClassification(task=&#x27;text-classification&#x27;, text_column=&#x27;text&#x27;, label_column=&#x27;label&#x27;)], builder_name=&#x27;rotten_tomatoes_movie_review&#x27;, config_name=&#x27;default&#x27;, version=1.0.0, splits={&#x27;train&#x27;: SplitInfo(name=&#x27;train&#x27;, num_bytes=1074810, num_examples=8530, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;validation&#x27;: SplitInfo(name=&#x27;validation&#x27;, num_bytes=134679, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;test&#x27;: SplitInfo(name=&#x27;test&#x27;, num_bytes=135972, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;)}, download_checksums={&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;: {&#x27;num_bytes&#x27;: 487770, &#x27;checksum&#x27;: &#x27;a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9&#x27;}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)</span>`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function cc(k){let d,_,f,r,u;return r=new I({props:{code:"downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)'}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function mc(k){let d,_,f,r,u;return r=new I({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),{c(){d=n("p"),_=m("Is roughly equivalent to:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Is roughly equivalent to:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function gc(k){let d,_,f,r,u;return r=new I({props:{code:"downloaded_files = dl_manager.download_custom('s3://my-bucket/data.zip', custom_download_for_my_private_bucket)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download_custom(<span class="hljs-string">&#x27;s3://my-bucket/data.zip&#x27;</span>, custom_download_for_my_private_bucket)'}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function fc(k){let d,_,f,r,u;return r=new I({props:{code:`downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
extracted_files = dl_manager.extract(downloaded_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>extracted_files = dl_manager.extract(downloaded_files)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function uc(k){let d,_,f,r,u;return r=new I({props:{code:`archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
files = dl_manager.iter_archive(archive)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>archive = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_archive(archive)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function _c(k){let d,_,f,r,u;return r=new I({props:{code:`files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
files = dl_manager.iter_files(files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.download_and_extract(<span class="hljs-string">&#x27;https://huggingface.co/datasets/beans/resolve/main/data/train.zip&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_files(files)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function hc(k){let d,_,f,r,u;return r=new I({props:{code:"downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)'}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function $c(k){let d,_,f,r,u;return r=new I({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),{c(){d=n("p"),_=m("Is roughly equivalent to:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Is roughly equivalent to:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function vc(k){let d,_,f,r,u;return r=new I({props:{code:`downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
extracted_files = dl_manager.extract(downloaded_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>extracted_files = dl_manager.extract(downloaded_files)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function bc(k){let d,_,f,r,u;return r=new I({props:{code:`archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
files = dl_manager.iter_archive(archive)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>archive = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_archive(archive)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function xc(k){let d,_,f,r,u;return r=new I({props:{code:`files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
files = dl_manager.iter_files(files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.download_and_extract(<span class="hljs-string">&#x27;https://huggingface.co/datasets/beans/resolve/main/data/train.zip&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_files(files)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function wc(k){let d,_,f,r,u;return r=new I({props:{code:`datasets.SplitGenerator(
    name=datasets.Split.TRAIN,
    gen_kwargs={"split_key": "train", "files": dl_manager.download_and_extract(url)},
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TRAIN,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and_extract(url)},
<span class="hljs-meta">... </span>)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function Ec(k){let d,_,f,r,u;return r=new I({props:{code:`datasets.SplitGenerator(
    name=datasets.Split.TRAIN,
    gen_kwargs={"split_key": "train", "files": dl_manager.download_and extract(url)},
),
datasets.SplitGenerator(
    name=datasets.Split.VALIDATION,
    gen_kwargs={"split_key": "validation", "files": dl_manager.download_and extract(url)},
),
datasets.SplitGenerator(
    name=datasets.Split.TEST,
    gen_kwargs={"split_key": "test", "files": dl_manager.download_and extract(url)},
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TRAIN,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>),
<span class="hljs-meta">... </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.VALIDATION,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;validation&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>),
<span class="hljs-meta">... </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TEST,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;test&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function yc(k){let d,_,f,r,u;return r=new I({props:{code:`Each descriptor can be composed with other using addition or slice. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST

The resulting split will correspond to 25% of the train split merged with
100% of the test split.`,highlighted:`Each descriptor can be composed <span class="hljs-keyword">with</span> other using addition <span class="hljs-keyword">or</span> <span class="hljs-built_in">slice</span>. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">0</span>:<span class="hljs-number">25</span>]) + datasets.Split.TEST

The resulting split will correspond to <span class="hljs-number">25</span>% of the train split merged <span class="hljs-keyword">with</span>
<span class="hljs-number">100</span>% of the test split.`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function Dc(k){let d,_,f,r,u;return r=new I({props:{code:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error`,highlighted:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">75</span>:])
)  <span class="hljs-comment"># Error</span>
split = datasets.Split.TEST + datasets.Split.ALL  <span class="hljs-comment"># Error</span>`}}),{c(){d=n("p"),_=m("A split cannot be added twice, so the following will fail:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"A split cannot be added twice, so the following will fail:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function kc(k){let d,_,f,r,u;return r=new I({props:{code:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])`,highlighted:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
datasets.Split.TEST.subsplit(datasets.percent[:<span class="hljs-number">50</span>])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){d=n("p"),_=m("The slices can be applied only one time. So the following are valid:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"The slices can be applied only one time. So the following are valid:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function jc(k){let d,_,f,r,u;return r=new I({props:{code:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])`,highlighted:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]).subsplit(datasets.percent[:<span class="hljs-number">25</span>])
split = (train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) + test).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){d=n("p"),_=m("But not:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"But not:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function Tc(k){let d,_,f,r,u;return r=new I({props:{code:`# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(<span class="hljs-string">&#x27;test[:33%]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=(
datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) +
datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">1</span>, to=-<span class="hljs-number">1</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>)))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))

<span class="hljs-comment"># 10-fold validation:</span>
tests = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
trains = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),{c(){d=n("p"),_=m("Examples:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Examples:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function Sc(k){let d,_,f,r,u;return r=new I({props:{code:'VERSION = datasets.Version("1.0.0")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>VERSION = datasets.Version(<span class="hljs-string">&quot;1.0.0&quot;</span>)'}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function Ic(k){let d,_,f,r,u,t,i,Wa,vn,Rr,he,Oe,Xa,$t,bn,Ja,xn,Ar,J,wn,fa,En,yn,ua,Dn,kn,Cr,P,vt,jn,Ka,Tn,Sn,K,bt,In,Ya,Nn,Bn,Le,Pn,Y,xt,Rn,Qa,An,Cn,Ve,On,Q,wt,Ln,Za,Vn,Mn,Me,qn,Z,Et,Fn,es,zn,Un,qe,Gn,Fe,yt,Hn,ts,Wn,Or,W,Dt,Xn,as,Jn,Kn,ee,ss,Yn,Qn,rs,Zn,eo,ns,to,ao,Lr,$e,kt,so,os,ro,Vr,ve,jt,no,ls,oo,Mr,z,Tt,lo,St,io,_a,po,co,mo,It,go,ha,fo,uo,_o,te,Nt,ho,ds,$o,vo,be,is,bo,xo,ps,wo,Eo,cs,yo,qr,xe,ze,ms,Bt,Do,gs,ko,Fr,N,Pt,jo,ae,Rt,To,fs,So,Io,Ue,No,se,At,Bo,us,Po,Ro,Ge,Ao,re,Ct,Co,Ot,Oo,_s,Lo,Vo,Mo,He,qo,ne,Lt,Fo,hs,zo,Uo,We,Go,oe,Vt,Ho,$s,Wo,Xo,Xe,Jo,le,Mt,Ko,vs,Yo,Qo,Je,Zo,Ke,qt,el,bs,tl,zr,R,Ft,al,U,sl,xs,rl,nl,ws,ol,ll,Es,dl,il,ys,pl,cl,ml,de,zt,gl,Ds,fl,ul,Ye,_l,ie,Ut,hl,ks,$l,vl,Qe,bl,pe,Gt,xl,js,wl,El,Ze,yl,ce,Ht,Dl,Ts,kl,jl,et,Tl,me,Wt,Sl,Ss,Il,Nl,tt,Ur,we,Xt,Bl,Is,Pl,Gr,V,Jt,Rl,$a,Ns,Al,Cl,Ol,Kt,Ll,Bs,Vl,Ml,ql,Ps,Fl,zl,Yt,Rs,Ee,Hr,Ul,As,Gl,Hl,Cs,Wl,Xl,ye,De,va,Os,Jl,Kl,Yl,Ls,Ql,Zl,Vs,ed,td,ke,Ms,qs,ad,sd,Fs,rd,nd,zs,od,ld,je,Us,Gs,dd,id,Hs,pd,cd,Ws,md,Wr,Te,at,Xs,Qt,gd,Js,fd,Xr,G,Zt,ud,Ks,_d,hd,Se,$d,Ys,vd,bd,Qs,xd,wd,Ed,st,Jr,A,ea,yd,ba,Zs,Dd,kd,jd,er,Td,Sd,X,xa,tr,Id,Nd,Bd,wa,ar,Pd,Rd,Ad,Ea,sr,Cd,Od,Ld,ya,rr,Vd,Md,qd,Da,Fd,nr,zd,Ud,ta,Gd,or,Hd,Wd,Xd,rt,Kr,B,aa,Jd,lr,Kd,Yd,nt,Qd,dr,Zd,ei,ot,ti,ir,ai,si,lt,ri,dt,Yr,Ie,sa,ni,pr,oi,Qr,M,ra,li,cr,di,ii,it,pi,pt,na,ci,mr,mi,gi,ge,oa,fi,gr,ui,_i,fr,hi,Zr,Ne,ct,ur,la,$i,_r,vi,en,H,da,bi,hr,xi,wi,mt,Ei,gt,ia,yi,$r,Di,tn;return t=new Pr({}),$t=new Pr({}),vt=new j({props:{name:"class datasets.DatasetBuilder",anchor:"datasets.DatasetBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"config_name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"name",val:" = 'deprecated'"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L180"}}),bt=new j({props:{name:"as_dataset",anchor:"datasets.DatasetBuilder.as_dataset",parameters:[{name:"split",val:": typing.Optional[datasets.splits.Split] = None"},{name:"run_post_process",val:" = True"},{name:"ignore_verifications",val:" = False"},{name:"in_memory",val:" = False"}],parametersDescription:[{anchor:"datasets.DatasetBuilder.as_dataset.split",description:"<strong>split</strong> (<code>datasets.Split</code>) &#x2014; Which subset of the data to return.",name:"split"},{anchor:"datasets.DatasetBuilder.as_dataset.run_post_process",description:`<strong>run_post_process</strong> (bool, default=True) &#x2014; Whether to run post-processing dataset transforms and/or add
indexes.`,name:"run_post_process"},{anchor:"datasets.DatasetBuilder.as_dataset.ignore_verifications",description:`<strong>ignore_verifications</strong> (bool, default=False) &#x2014; Whether to ignore the verifications of the
downloaded/processed dataset information (checksums/size/splits/&#x2026;).`,name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.as_dataset.in_memory",description:"<strong>in_memory</strong> (bool, default=False) &#x2014; Whether to copy the data in-memory.",name:"in_memory"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L802",returnDescription:`
<p>datasets.Dataset</p>
`}}),Le=new T({props:{anchor:"datasets.DatasetBuilder.as_dataset.example",$$slots:{default:[lc]},$$scope:{ctx:k}}}),xt=new j({props:{name:"download_and_prepare",anchor:"datasets.DatasetBuilder.download_and_prepare",parameters:[{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.download.download_manager.DownloadMode] = None"},{name:"ignore_verifications",val:": bool = False"},{name:"try_from_hf_gcs",val:": bool = True"},{name:"dl_manager",val:": typing.Optional[datasets.download.download_manager.DownloadManager] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"**download_and_prepare_kwargs",val:""}],parametersDescription:[{anchor:"datasets.DatasetBuilder.download_and_prepare.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; specific download configuration parameters.',name:"download_config"},{anchor:"datasets.DatasetBuilder.download_and_prepare.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, optional) &#x2014; select the download/generate mode - Default to <code>REUSE_DATASET_IF_EXISTS</code>',name:"download_mode"},{anchor:"datasets.DatasetBuilder.download_and_prepare.ignore_verifications",description:"<strong>ignore_verifications</strong> (<code>bool</code>) &#x2014; Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/&#x2026;)",name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.download_and_prepare.try_from_hf_gcs",description:"<strong>try_from_hf_gcs</strong> (<code>bool</code>) &#x2014; If True, it will try to download the already prepared dataset from the Hf google cloud storage",name:"try_from_hf_gcs"},{anchor:"datasets.DatasetBuilder.download_and_prepare.dl_manager",description:'<strong>dl_manager</strong> (<a href="/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a>, optional) &#x2014; specific Download Manger to use',name:"dl_manager"},{anchor:"datasets.DatasetBuilder.download_and_prepare.base_path",description:`<strong>base_path</strong> (<code>str</code>, optional) &#x2014; base path for relative paths that are used to download files. This can be a remote url.
If not specified, the value of the <em>base_path</em> attribute (<em>self.base_path</em>) will be used instead.`,name:"base_path"},{anchor:"datasets.DatasetBuilder.download_and_prepare.use_auth_token",description:`<strong>use_auth_token</strong> (<code>Union[str, bool]</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from ~/.huggingface.`,name:"use_auth_token"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L531"}}),Ve=new T({props:{anchor:"datasets.DatasetBuilder.download_and_prepare.example",$$slots:{default:[dc]},$$scope:{ctx:k}}}),wt=new j({props:{name:"get_all_exported_dataset_infos",anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos",parameters:[],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L335"}}),Me=new T({props:{anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos.example",$$slots:{default:[ic]},$$scope:{ctx:k}}}),Et=new j({props:{name:"get_exported_dataset_info",anchor:"datasets.DatasetBuilder.get_exported_dataset_info",parameters:[],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L353"}}),qe=new T({props:{anchor:"datasets.DatasetBuilder.get_exported_dataset_info.example",$$slots:{default:[pc]},$$scope:{ctx:k}}}),yt=new j({props:{name:"get_imported_module_dir",anchor:"datasets.DatasetBuilder.get_imported_module_dir",parameters:[],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L526"}}),Dt=new j({props:{name:"class datasets.GeneratorBasedBuilder",anchor:"datasets.GeneratorBasedBuilder",parameters:[{name:"*args",val:""},{name:"writer_batch_size",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L1089"}}),kt=new j({props:{name:"class datasets.BeamBasedBuilder",anchor:"datasets.BeamBasedBuilder",parameters:[{name:"*args",val:""},{name:"beam_runner",val:" = None"},{name:"beam_options",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L1249"}}),jt=new j({props:{name:"class datasets.ArrowBasedBuilder",anchor:"datasets.ArrowBasedBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"config_name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"name",val:" = 'deprecated'"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L1187"}}),Tt=new j({props:{name:"class datasets.BuilderConfig",anchor:"datasets.BuilderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = 0.0.0"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.BuilderConfig.name",description:"<strong>name</strong> (<code>str</code>, default <code>&quot;default&quot;</code>) &#x2014;",name:"name"},{anchor:"datasets.BuilderConfig.version",description:'<strong>version</strong> (<a href="/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014;',name:"version"},{anchor:"datasets.BuilderConfig.data_dir",description:"<strong>data_dir</strong> (<code>str</code>, optional) &#x2014;",name:"data_dir"},{anchor:"datasets.BuilderConfig.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.BuilderConfig.description",description:"<strong>description</strong> (<code>str</code>, optional) &#x2014;",name:"description"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L84"}}),Nt=new j({props:{name:"create_config_id",anchor:"datasets.BuilderConfig.create_config_id",parameters:[{name:"config_kwargs",val:": dict"},{name:"custom_features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L123"}}),Bt=new Pr({}),Pt=new j({props:{name:"class datasets.DownloadManager",anchor:"datasets.DownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"record_checksums",val:" = True"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L136"}}),Rt=new j({props:{name:"download",anchor:"datasets.DownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L268",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Ue=new T({props:{anchor:"datasets.DownloadManager.download.example",$$slots:{default:[cc]},$$scope:{ctx:k}}}),At=new j({props:{name:"download_and_extract",anchor:"datasets.DownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L403",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Ge=new T({props:{anchor:"datasets.DownloadManager.download_and_extract.example",$$slots:{default:[mc]},$$scope:{ctx:k}}}),Ct=new j({props:{name:"download_custom",anchor:"datasets.DownloadManager.download_custom",parameters:[{name:"url_or_urls",val:""},{name:"custom_download",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L221",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),He=new T({props:{anchor:"datasets.DownloadManager.download_custom.example",$$slots:{default:[gc]},$$scope:{ctx:k}}}),Lt=new j({props:{name:"extract",anchor:"datasets.DownloadManager.extract",parameters:[{name:"path_or_paths",val:""},{name:"num_proc",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L366",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),We=new T({props:{anchor:"datasets.DownloadManager.extract.example",$$slots:{default:[fc]},$$scope:{ctx:k}}}),Vt=new j({props:{name:"iter_archive",anchor:"datasets.DownloadManager.iter_archive",parameters:[{name:"path_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_archive.path_or_buf",description:"<strong>path_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"path_or_buf"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L325"}}),Xe=new T({props:{anchor:"datasets.DownloadManager.iter_archive.example",$$slots:{default:[uc]},$$scope:{ctx:k}}}),Mt=new j({props:{name:"iter_files",anchor:"datasets.DownloadManager.iter_files",parameters:[{name:"paths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_files.paths",description:"<strong>paths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"paths"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L348"}}),Je=new T({props:{anchor:"datasets.DownloadManager.iter_files.example",$$slots:{default:[_c]},$$scope:{ctx:k}}}),qt=new j({props:{name:"ship_files_with_pipeline",anchor:"datasets.DownloadManager.ship_files_with_pipeline",parameters:[{name:"downloaded_path_or_paths",val:""},{name:"pipeline",val:""}],parametersDescription:[{anchor:"datasets.DownloadManager.ship_files_with_pipeline.downloaded_path_or_paths",description:`<strong>downloaded_path_or_paths</strong> (<code>str</code> or <code>list[str]</code> or <code>dict[str, str]</code>) &#x2014; Nested structure containing the
downloaded path(s).`,name:"downloaded_path_or_paths"},{anchor:"datasets.DownloadManager.ship_files_with_pipeline.pipeline",description:"<strong>pipeline</strong> (<code>utils.beam_utils.BeamPipeline</code>) &#x2014; Apache Beam Pipeline.",name:"pipeline"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L178",returnDescription:`
<p><code>str</code> or <code>list[str]</code> or <code>dict[str, str]</code></p>
`}}),Ft=new j({props:{name:"class datasets.StreamingDownloadManager",anchor:"datasets.StreamingDownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/streaming_download_manager.py#L767"}}),zt=new j({props:{name:"download",anchor:"datasets.StreamingDownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/streaming_download_manager.py#L793",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Ye=new T({props:{anchor:"datasets.StreamingDownloadManager.download.example",$$slots:{default:[hc]},$$scope:{ctx:k}}}),Ut=new j({props:{name:"download_and_extract",anchor:"datasets.StreamingDownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/streaming_download_manager.py#L861",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Qe=new T({props:{anchor:"datasets.StreamingDownloadManager.download_and_extract.example",$$slots:{default:[$c]},$$scope:{ctx:k}}}),Gt=new j({props:{name:"extract",anchor:"datasets.StreamingDownloadManager.extract",parameters:[{name:"path_or_paths",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/streaming_download_manager.py#L820",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Ze=new T({props:{anchor:"datasets.StreamingDownloadManager.extract.example",$$slots:{default:[vc]},$$scope:{ctx:k}}}),Ht=new j({props:{name:"iter_archive",anchor:"datasets.StreamingDownloadManager.iter_archive",parameters:[{name:"urlpath_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.StreamingDownloadManager.iter_archive.urlpath_or_buf",description:"<strong>urlpath_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"urlpath_or_buf"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/streaming_download_manager.py#L879"}}),et=new T({props:{anchor:"datasets.StreamingDownloadManager.iter_archive.example",$$slots:{default:[bc]},$$scope:{ctx:k}}}),Wt=new j({props:{name:"iter_files",anchor:"datasets.StreamingDownloadManager.iter_files",parameters:[{name:"urlpaths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.StreamingDownloadManager.iter_files.urlpaths",description:"<strong>urlpaths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"urlpaths"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/streaming_download_manager.py#L902"}}),tt=new T({props:{anchor:"datasets.StreamingDownloadManager.iter_files.example",$$slots:{default:[xc]},$$scope:{ctx:k}}}),Xt=new j({props:{name:"class datasets.DownloadConfig",anchor:"datasets.DownloadConfig",parameters:[{name:"cache_dir",val:": typing.Union[str, pathlib.Path, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"resume_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"proxies",val:": typing.Optional[typing.Dict] = None"},{name:"user_agent",val:": typing.Optional[str] = None"},{name:"extract_compressed_file",val:": bool = False"},{name:"force_extract",val:": bool = False"},{name:"delete_extracted",val:": bool = False"},{name:"use_etag",val:": bool = True"},{name:"num_proc",val:": typing.Optional[int] = None"},{name:"max_retries",val:": int = 1"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"ignore_url_params",val:": bool = False"},{name:"download_desc",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.DownloadConfig.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>Path</code>, optional) &#x2014; Specify a cache directory to save the file to (overwrite the
default cache dir).`,name:"cache_dir"},{anchor:"datasets.DownloadConfig.force_download",description:`<strong>force_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, re-dowload the file even if it&#x2019;s already cached in
the cache dir.`,name:"force_download"},{anchor:"datasets.DownloadConfig.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, resume the download if incompletly recieved file is
found.`,name:"resume_download"},{anchor:"datasets.DownloadConfig.proxies",description:"<strong>proxies</strong> (<code>dict</code>, optional) &#x2014;",name:"proxies"},{anchor:"datasets.DownloadConfig.user_agent",description:`<strong>user_agent</strong> (<code>str</code>, optional) &#x2014; Optional string or dict that will be appended to the user-agent on remote
requests.`,name:"user_agent"},{anchor:"datasets.DownloadConfig.extract_compressed_file",description:`<strong>extract_compressed_file</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True and the path point to a zip or tar file,
extract the compressed file in a folder along the archive.`,name:"extract_compressed_file"},{anchor:"datasets.DownloadConfig.force_extract",description:`<strong>force_extract</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True when extract_compressed_file is True and the archive
was already extracted, re-extract the archive and override the folder where it was extracted.`,name:"force_extract"},{anchor:"datasets.DownloadConfig.delete_extracted",description:"<strong>delete_extracted</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to delete (or keep) the extracted files.",name:"delete_extracted"},{anchor:"datasets.DownloadConfig.use_etag",description:"<strong>use_etag</strong> (<code>bool</code>, default <code>True</code>) &#x2014; Whether to use the ETag HTTP response header to validate the cached files.",name:"use_etag"},{anchor:"datasets.DownloadConfig.num_proc",description:"<strong>num_proc</strong> (<code>int</code>, optional) &#x2014; The number of processes to launch to download the files in parallel.",name:"num_proc"},{anchor:"datasets.DownloadConfig.max_retries",description:"<strong>max_retries</strong> (<code>int</code>, default <code>1</code>) &#x2014; The number of times to retry an HTTP request if it fails.",name:"max_retries"},{anchor:"datasets.DownloadConfig.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token
for remote files on the Datasets Hub. If True, will get token from ~/.huggingface.`,name:"use_auth_token"},{anchor:"datasets.DownloadConfig.ignore_url_params",description:`<strong>ignore_url_params</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to strip all query parameters and #fragments from
the download URL before using it for caching the file.`,name:"ignore_url_params"},{anchor:"datasets.DownloadConfig.download_desc",description:"<strong>download_desc</strong> (<code>str</code>, optional) &#x2014; A description to be displayed alongside with the progress bar while downloading the files.",name:"download_desc"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_config.py#L8"}}),Jt=new j({props:{name:"class datasets.DownloadMode",anchor:"datasets.DownloadMode",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L39"}}),Qt=new Pr({}),Zt=new j({props:{name:"class datasets.SplitGenerator",anchor:"datasets.SplitGenerator",parameters:[{name:"name",val:": str"},{name:"gen_kwargs",val:": typing.Dict = <factory>"}],parametersDescription:[{anchor:"datasets.SplitGenerator.name",description:`<strong>name</strong> (str) &#x2014; Name of the Split for which the generator will
create the examples.
**gen_kwargs &#x2014; Keyword arguments to forward to the <code>DatasetBuilder._generate_examples</code> method
of the builder.`,name:"name"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/splits.py#L566"}}),st=new T({props:{anchor:"datasets.SplitGenerator.example",$$slots:{default:[wc]},$$scope:{ctx:k}}}),ea=new j({props:{name:"class datasets.Split",anchor:"datasets.Split",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/splits.py#L387"}}),rt=new T({props:{anchor:"datasets.Split.example",$$slots:{default:[Ec]},$$scope:{ctx:k}}}),aa=new j({props:{name:"class datasets.NamedSplit",anchor:"datasets.NamedSplit",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/splits.py#L303"}}),nt=new T({props:{anchor:"datasets.NamedSplit.example",$$slots:{default:[yc]},$$scope:{ctx:k}}}),ot=new T({props:{anchor:"datasets.NamedSplit.example-2",$$slots:{default:[Dc]},$$scope:{ctx:k}}}),lt=new T({props:{anchor:"datasets.NamedSplit.example-3",$$slots:{default:[kc]},$$scope:{ctx:k}}}),dt=new T({props:{anchor:"datasets.NamedSplit.example-4",$$slots:{default:[jc]},$$scope:{ctx:k}}}),sa=new j({props:{name:"class datasets.NamedSplitAll",anchor:"datasets.NamedSplitAll",parameters:[],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/splits.py#L372"}}),ra=new j({props:{name:"class datasets.ReadInstruction",anchor:"datasets.ReadInstruction",parameters:[{name:"split_name",val:""},{name:"rounding",val:" = None"},{name:"from_",val:" = None"},{name:"to",val:" = None"},{name:"unit",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/arrow_reader.py#L457"}}),it=new T({props:{anchor:"datasets.ReadInstruction.example",$$slots:{default:[Tc]},$$scope:{ctx:k}}}),na=new j({props:{name:"from_spec",anchor:"datasets.ReadInstruction.from_spec",parameters:[{name:"spec",val:""}],parametersDescription:[{anchor:"datasets.ReadInstruction.from_spec.spec",description:`<strong>spec</strong> (str) &#x2014; split(s) + optional slice(s) to read + optional rounding
if percents are used as the slicing unit. A slice can be specified,
using absolute numbers (int) or percentages (int). E.g.
<code>test</code>: test split.
<code>test + validation</code>: test split + validation split.
<code>test[10:]</code>: test split, minus its first 10 records.
<code>test[:10%]</code>: first 10% records of test split.
<code>test[:20%](pct1_dropremainder)</code>: first 10% records, rounded with
the <code>pct1_dropremainder</code> rounding.
<code>test[:-5%]+train[40%:60%]</code>: first 95% of test + middle 20% of
train.`,name:"spec"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/arrow_reader.py#L537",returnDescription:`
<p>ReadInstruction instance.</p>
`}}),oa=new j({props:{name:"to_absolute",anchor:"datasets.ReadInstruction.to_absolute",parameters:[{name:"name2len",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/arrow_reader.py#L605",returnDescription:`
<p>list of _AbsoluteInstruction instances (corresponds to the + in spec).</p>
`}}),la=new Pr({}),da=new j({props:{name:"class datasets.Version",anchor:"datasets.Version",parameters:[{name:"version_str",val:": str"},{name:"description",val:": typing.Optional[str] = None"},{name:"major",val:": typing.Union[str, int, NoneType] = None"},{name:"minor",val:": typing.Union[str, int, NoneType] = None"},{name:"patch",val:": typing.Union[str, int, NoneType] = None"}],parametersDescription:[{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.major",description:"<strong>major</strong> (<code>str</code>) &#x2014;",name:"major"},{anchor:"datasets.Version.minor",description:"<strong>minor</strong> (<code>str</code>) &#x2014;",name:"minor"},{anchor:"datasets.Version.patch",description:"<strong>patch</strong> (<code>str</code>) &#x2014;",name:"patch"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/utils/version.py#L30"}}),mt=new T({props:{anchor:"datasets.Version.example",$$slots:{default:[Sc]},$$scope:{ctx:k}}}),ia=new j({props:{name:"match",anchor:"datasets.Version.match",parameters:[{name:"other_version",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/utils/version.py#L101"}}),{c(){d=n("meta"),_=p(),f=n("h1"),r=n("a"),u=n("span"),v(t.$$.fragment),i=p(),Wa=n("span"),vn=m("Builder classes"),Rr=p(),he=n("h2"),Oe=n("a"),Xa=n("span"),v($t.$$.fragment),bn=p(),Ja=n("span"),xn=m("Builders"),Ar=p(),J=n("p"),wn=m("\u{1F917} Datasets relies on two main classes during the dataset building process: "),fa=n("a"),En=m("DatasetBuilder"),yn=m(" and "),ua=n("a"),Dn=m("BuilderConfig"),kn=m("."),Cr=p(),P=n("div"),v(vt.$$.fragment),jn=p(),Ka=n("p"),Tn=m("Abstract base class for all datasets."),Sn=p(),K=n("div"),v(bt.$$.fragment),In=p(),Ya=n("p"),Nn=m("Return a Dataset for the specified split."),Bn=p(),v(Le.$$.fragment),Pn=p(),Y=n("div"),v(xt.$$.fragment),Rn=p(),Qa=n("p"),An=m("Downloads and prepares dataset for reading."),Cn=p(),v(Ve.$$.fragment),On=p(),Q=n("div"),v(wt.$$.fragment),Ln=p(),Za=n("p"),Vn=m("Empty dict if doesn\u2019t exist"),Mn=p(),v(Me.$$.fragment),qn=p(),Z=n("div"),v(Et.$$.fragment),Fn=p(),es=n("p"),zn=m("Empty DatasetInfo if doesn\u2019t exist"),Un=p(),v(qe.$$.fragment),Gn=p(),Fe=n("div"),v(yt.$$.fragment),Hn=p(),ts=n("p"),Wn=m("Return the path of the module of this class or subclass."),Or=p(),W=n("div"),v(Dt.$$.fragment),Xn=p(),as=n("p"),Jn=m("Base class for datasets with data generation based on dict generators."),Kn=p(),ee=n("p"),ss=n("code"),Yn=m("GeneratorBasedBuilder"),Qn=m(` is a convenience class that abstracts away much
of the data writing and reading of `),rs=n("code"),Zn=m("DatasetBuilder"),eo=m(`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),ns=n("code"),to=m("_split_generators"),ao=m("). See the method docstrings for details."),Lr=p(),$e=n("div"),v(kt.$$.fragment),so=p(),os=n("p"),ro=m("Beam based Builder."),Vr=p(),ve=n("div"),v(jt.$$.fragment),no=p(),ls=n("p"),oo=m("Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Mr=p(),z=n("div"),v(Tt.$$.fragment),lo=p(),St=n("p"),io=m("Base class for "),_a=n("a"),po=m("DatasetBuilder"),co=m(" data configuration."),mo=p(),It=n("p"),go=m(`DatasetBuilder subclasses with data configuration options should subclass
`),ha=n("a"),fo=m("BuilderConfig"),uo=m(" and add their own properties."),_o=p(),te=n("div"),v(Nt.$$.fragment),ho=p(),ds=n("p"),$o=m(`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),vo=p(),be=n("ul"),is=n("li"),bo=m("the config kwargs that can be used to overwrite attributes"),xo=p(),ps=n("li"),wo=m("the custom features used to write the dataset"),Eo=p(),cs=n("li"),yo=m(`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),qr=p(),xe=n("h2"),ze=n("a"),ms=n("span"),v(Bt.$$.fragment),Do=p(),gs=n("span"),ko=m("Download"),Fr=p(),N=n("div"),v(Pt.$$.fragment),jo=p(),ae=n("div"),v(Rt.$$.fragment),To=p(),fs=n("p"),So=m("Download given url(s)."),Io=p(),v(Ue.$$.fragment),No=p(),se=n("div"),v(At.$$.fragment),Bo=p(),us=n("p"),Po=m("Download and extract given url_or_urls."),Ro=p(),v(Ge.$$.fragment),Ao=p(),re=n("div"),v(Ct.$$.fragment),Co=p(),Ot=n("p"),Oo=m("Download given urls(s) by calling "),_s=n("code"),Lo=m("custom_download"),Vo=m("."),Mo=p(),v(He.$$.fragment),qo=p(),ne=n("div"),v(Lt.$$.fragment),Fo=p(),hs=n("p"),zo=m("Extract given path(s)."),Uo=p(),v(We.$$.fragment),Go=p(),oe=n("div"),v(Vt.$$.fragment),Ho=p(),$s=n("p"),Wo=m("Iterate over files within an archive."),Xo=p(),v(Xe.$$.fragment),Jo=p(),le=n("div"),v(Mt.$$.fragment),Ko=p(),vs=n("p"),Yo=m("Iterate over file paths."),Qo=p(),v(Je.$$.fragment),Zo=p(),Ke=n("div"),v(qt.$$.fragment),el=p(),bs=n("p"),tl=m("Ship the files using Beam FileSystems to the pipeline temp dir."),zr=p(),R=n("div"),v(Ft.$$.fragment),al=p(),U=n("p"),sl=m(`Download manager that uses the \u201D::\u201D separator to navigate through (possibly remote) compressed archives.
Contrary to the regular DownloadManager, the `),xs=n("code"),rl=m("download"),nl=m(" and "),ws=n("code"),ol=m("extract"),ll=m(` methods don\u2019t actually download nor extract
data, but they rather return the path or url that could be opened using the `),Es=n("code"),dl=m("xopen"),il=m(` function which extends the
builtin `),ys=n("code"),pl=m("open"),cl=m(" function to stream data from remote files."),ml=p(),de=n("div"),v(zt.$$.fragment),gl=p(),Ds=n("p"),fl=m("Download given url(s)."),ul=p(),v(Ye.$$.fragment),_l=p(),ie=n("div"),v(Ut.$$.fragment),hl=p(),ks=n("p"),$l=m("Download and extract given url_or_urls."),vl=p(),v(Qe.$$.fragment),bl=p(),pe=n("div"),v(Gt.$$.fragment),xl=p(),js=n("p"),wl=m("Extract given path(s)."),El=p(),v(Ze.$$.fragment),yl=p(),ce=n("div"),v(Ht.$$.fragment),Dl=p(),Ts=n("p"),kl=m("Iterate over files within an archive."),jl=p(),v(et.$$.fragment),Tl=p(),me=n("div"),v(Wt.$$.fragment),Sl=p(),Ss=n("p"),Il=m("Iterate over files."),Nl=p(),v(tt.$$.fragment),Ur=p(),we=n("div"),v(Xt.$$.fragment),Bl=p(),Is=n("p"),Pl=m("Configuration for our cached path manager."),Gr=p(),V=n("div"),v(Jt.$$.fragment),Rl=p(),$a=n("p"),Ns=n("code"),Al=m("Enum"),Cl=m(" for how to treat pre-existing downloads and data."),Ol=p(),Kt=n("p"),Ll=m("The default mode is "),Bs=n("code"),Vl=m("REUSE_DATASET_IF_EXISTS"),Ml=m(`, which will reuse both
raw downloads and the prepared dataset if they exist.`),ql=p(),Ps=n("p"),Fl=m("The generations modes:"),zl=p(),Yt=n("table"),Rs=n("thead"),Ee=n("tr"),Hr=n("th"),Ul=p(),As=n("th"),Gl=m("Downloads"),Hl=p(),Cs=n("th"),Wl=m("Dataset"),Xl=p(),ye=n("tbody"),De=n("tr"),va=n("td"),Os=n("code"),Jl=m("REUSE_DATASET_IF_EXISTS"),Kl=m(" (default)"),Yl=p(),Ls=n("td"),Ql=m("Reuse"),Zl=p(),Vs=n("td"),ed=m("Reuse"),td=p(),ke=n("tr"),Ms=n("td"),qs=n("code"),ad=m("REUSE_CACHE_IF_EXISTS"),sd=p(),Fs=n("td"),rd=m("Reuse"),nd=p(),zs=n("td"),od=m("Fresh"),ld=p(),je=n("tr"),Us=n("td"),Gs=n("code"),dd=m("FORCE_REDOWNLOAD"),id=p(),Hs=n("td"),pd=m("Fresh"),cd=p(),Ws=n("td"),md=m("Fresh"),Wr=p(),Te=n("h2"),at=n("a"),Xs=n("span"),v(Qt.$$.fragment),gd=p(),Js=n("span"),fd=m("Splits"),Xr=p(),G=n("div"),v(Zt.$$.fragment),ud=p(),Ks=n("p"),_d=m("Defines the split information for the generator."),hd=p(),Se=n("p"),$d=m(`This should be used as returned value of
`),Ys=n("code"),vd=m("GeneratorBasedBuilder._split_generators()"),bd=m(`.
See `),Qs=n("code"),xd=m("GeneratorBasedBuilder._split_generators()"),wd=m(` for more info and example
of usage.`),Ed=p(),v(st.$$.fragment),Jr=p(),A=n("div"),v(ea.$$.fragment),yd=p(),ba=n("p"),Zs=n("code"),Dd=m("Enum"),kd=m(" for dataset splits."),jd=p(),er=n("p"),Td=m(`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),Sd=p(),X=n("ul"),xa=n("li"),tr=n("code"),Id=m("TRAIN"),Nd=m(": the training data."),Bd=p(),wa=n("li"),ar=n("code"),Pd=m("VALIDATION"),Rd=m(`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Ad=p(),Ea=n("li"),sr=n("code"),Cd=m("TEST"),Od=m(`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Ld=p(),ya=n("li"),rr=n("code"),Vd=m("ALL"),Md=m(": the union of all defined dataset splits."),qd=p(),Da=n("p"),Fd=m("Note: All splits, including compositions inherit from "),nr=n("code"),zd=m("datasets.SplitBase"),Ud=p(),ta=n("p"),Gd=m("See the :doc:"),or=n("code"),Hd=m("guide on splits </loading>"),Wd=m(" for more information."),Xd=p(),v(rt.$$.fragment),Kr=p(),B=n("div"),v(aa.$$.fragment),Jd=p(),lr=n("p"),Kd=m("Descriptor corresponding to a named split (train, test, \u2026)."),Yd=p(),v(nt.$$.fragment),Qd=p(),dr=n("p"),Zd=m("Warning:"),ei=p(),v(ot.$$.fragment),ti=p(),ir=n("p"),ai=m("Warning:"),si=p(),v(lt.$$.fragment),ri=p(),v(dt.$$.fragment),Yr=p(),Ie=n("div"),v(sa.$$.fragment),ni=p(),pr=n("p"),oi=m("Split corresponding to the union of all defined dataset splits."),Qr=p(),M=n("div"),v(ra.$$.fragment),li=p(),cr=n("p"),di=m("Reading instruction for a dataset."),ii=p(),v(it.$$.fragment),pi=p(),pt=n("div"),v(na.$$.fragment),ci=p(),mr=n("p"),mi=m("Creates a ReadInstruction instance out of a string spec."),gi=p(),ge=n("div"),v(oa.$$.fragment),fi=p(),gr=n("p"),ui=m("Translate instruction into a list of absolute instructions."),_i=p(),fr=n("p"),hi=m("Those absolute instructions are then to be added together."),Zr=p(),Ne=n("h2"),ct=n("a"),ur=n("span"),v(la.$$.fragment),$i=p(),_r=n("span"),vi=m("Version"),en=p(),H=n("div"),v(da.$$.fragment),bi=p(),hr=n("p"),xi=m("Dataset version MAJOR.MINOR.PATCH."),wi=p(),v(mt.$$.fragment),Ei=p(),gt=n("div"),v(ia.$$.fragment),yi=p(),$r=n("p"),Di=m("Returns True if other_version matches."),this.h()},l(a){const h=nc('[data-svelte="svelte-1phssyn"]',document.head);d=o(h,"META",{name:!0,content:!0}),h.forEach(s),_=c(a),f=o(a,"H1",{class:!0});var pa=l(f);r=o(pa,"A",{id:!0,class:!0,href:!0});var vr=l(r);u=o(vr,"SPAN",{});var br=l(u);b(t.$$.fragment,br),br.forEach(s),vr.forEach(s),i=c(pa),Wa=o(pa,"SPAN",{});var xr=l(Wa);vn=g(xr,"Builder classes"),xr.forEach(s),pa.forEach(s),Rr=c(a),he=o(a,"H2",{class:!0});var ca=l(he);Oe=o(ca,"A",{id:!0,class:!0,href:!0});var wr=l(Oe);Xa=o(wr,"SPAN",{});var Er=l(Xa);b($t.$$.fragment,Er),Er.forEach(s),wr.forEach(s),bn=c(ca),Ja=o(ca,"SPAN",{});var yr=l(Ja);xn=g(yr,"Builders"),yr.forEach(s),ca.forEach(s),Ar=c(a),J=o(a,"P",{});var Be=l(J);wn=g(Be,"\u{1F917} Datasets relies on two main classes during the dataset building process: "),fa=o(Be,"A",{href:!0});var Dr=l(fa);En=g(Dr,"DatasetBuilder"),Dr.forEach(s),yn=g(Be," and "),ua=o(Be,"A",{href:!0});var kr=l(ua);Dn=g(kr,"BuilderConfig"),kr.forEach(s),kn=g(Be,"."),Be.forEach(s),Cr=c(a),P=o(a,"DIV",{class:!0});var C=l(P);b(vt.$$.fragment,C),jn=c(C),Ka=o(C,"P",{});var jr=l(Ka);Tn=g(jr,"Abstract base class for all datasets."),jr.forEach(s),Sn=c(C),K=o(C,"DIV",{class:!0});var Pe=l(K);b(bt.$$.fragment,Pe),In=c(Pe),Ya=o(Pe,"P",{});var Tr=l(Ya);Nn=g(Tr,"Return a Dataset for the specified split."),Tr.forEach(s),Bn=c(Pe),b(Le.$$.fragment,Pe),Pe.forEach(s),Pn=c(C),Y=o(C,"DIV",{class:!0});var Re=l(Y);b(xt.$$.fragment,Re),Rn=c(Re),Qa=o(Re,"P",{});var Sr=l(Qa);An=g(Sr,"Downloads and prepares dataset for reading."),Sr.forEach(s),Cn=c(Re),b(Ve.$$.fragment,Re),Re.forEach(s),On=c(C),Q=o(C,"DIV",{class:!0});var Ae=l(Q);b(wt.$$.fragment,Ae),Ln=c(Ae),Za=o(Ae,"P",{});var Ir=l(Za);Vn=g(Ir,"Empty dict if doesn\u2019t exist"),Ir.forEach(s),Mn=c(Ae),b(Me.$$.fragment,Ae),Ae.forEach(s),qn=c(C),Z=o(C,"DIV",{class:!0});var Ce=l(Z);b(Et.$$.fragment,Ce),Fn=c(Ce),es=o(Ce,"P",{});var Nr=l(es);zn=g(Nr,"Empty DatasetInfo if doesn\u2019t exist"),Nr.forEach(s),Un=c(Ce),b(qe.$$.fragment,Ce),Ce.forEach(s),Gn=c(C),Fe=o(C,"DIV",{class:!0});var ma=l(Fe);b(yt.$$.fragment,ma),Hn=c(ma),ts=o(ma,"P",{});var Br=l(ts);Wn=g(Br,"Return the path of the module of this class or subclass."),Br.forEach(s),ma.forEach(s),C.forEach(s),Or=c(a),W=o(a,"DIV",{class:!0});var ka=l(W);b(Dt.$$.fragment,ka),Xn=c(ka),as=o(ka,"P",{});var Ri=l(as);Jn=g(Ri,"Base class for datasets with data generation based on dict generators."),Ri.forEach(s),Kn=c(ka),ee=o(ka,"P",{});var ga=l(ee);ss=o(ga,"CODE",{});var Ai=l(ss);Yn=g(Ai,"GeneratorBasedBuilder"),Ai.forEach(s),Qn=g(ga,` is a convenience class that abstracts away much
of the data writing and reading of `),rs=o(ga,"CODE",{});var Ci=l(rs);Zn=g(Ci,"DatasetBuilder"),Ci.forEach(s),eo=g(ga,`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),ns=o(ga,"CODE",{});var Oi=l(ns);to=g(Oi,"_split_generators"),Oi.forEach(s),ao=g(ga,"). See the method docstrings for details."),ga.forEach(s),ka.forEach(s),Lr=c(a),$e=o(a,"DIV",{class:!0});var an=l($e);b(kt.$$.fragment,an),so=c(an),os=o(an,"P",{});var Li=l(os);ro=g(Li,"Beam based Builder."),Li.forEach(s),an.forEach(s),Vr=c(a),ve=o(a,"DIV",{class:!0});var sn=l(ve);b(jt.$$.fragment,sn),no=c(sn),ls=o(sn,"P",{});var Vi=l(ls);oo=g(Vi,"Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Vi.forEach(s),sn.forEach(s),Mr=c(a),z=o(a,"DIV",{class:!0});var ft=l(z);b(Tt.$$.fragment,ft),lo=c(ft),St=o(ft,"P",{});var rn=l(St);io=g(rn,"Base class for "),_a=o(rn,"A",{href:!0});var Mi=l(_a);po=g(Mi,"DatasetBuilder"),Mi.forEach(s),co=g(rn," data configuration."),rn.forEach(s),mo=c(ft),It=o(ft,"P",{});var nn=l(It);go=g(nn,`DatasetBuilder subclasses with data configuration options should subclass
`),ha=o(nn,"A",{href:!0});var qi=l(ha);fo=g(qi,"BuilderConfig"),qi.forEach(s),uo=g(nn," and add their own properties."),nn.forEach(s),_o=c(ft),te=o(ft,"DIV",{class:!0});var ja=l(te);b(Nt.$$.fragment,ja),ho=c(ja),ds=o(ja,"P",{});var Fi=l(ds);$o=g(Fi,`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),Fi.forEach(s),vo=c(ja),be=o(ja,"UL",{});var Ta=l(be);is=o(Ta,"LI",{});var zi=l(is);bo=g(zi,"the config kwargs that can be used to overwrite attributes"),zi.forEach(s),xo=c(Ta),ps=o(Ta,"LI",{});var Ui=l(ps);wo=g(Ui,"the custom features used to write the dataset"),Ui.forEach(s),Eo=c(Ta),cs=o(Ta,"LI",{});var Gi=l(cs);yo=g(Gi,`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),Gi.forEach(s),Ta.forEach(s),ja.forEach(s),ft.forEach(s),qr=c(a),xe=o(a,"H2",{class:!0});var on=l(xe);ze=o(on,"A",{id:!0,class:!0,href:!0});var Hi=l(ze);ms=o(Hi,"SPAN",{});var Wi=l(ms);b(Bt.$$.fragment,Wi),Wi.forEach(s),Hi.forEach(s),Do=c(on),gs=o(on,"SPAN",{});var Xi=l(gs);ko=g(Xi,"Download"),Xi.forEach(s),on.forEach(s),Fr=c(a),N=o(a,"DIV",{class:!0});var O=l(N);b(Pt.$$.fragment,O),jo=c(O),ae=o(O,"DIV",{class:!0});var Sa=l(ae);b(Rt.$$.fragment,Sa),To=c(Sa),fs=o(Sa,"P",{});var Ji=l(fs);So=g(Ji,"Download given url(s)."),Ji.forEach(s),Io=c(Sa),b(Ue.$$.fragment,Sa),Sa.forEach(s),No=c(O),se=o(O,"DIV",{class:!0});var Ia=l(se);b(At.$$.fragment,Ia),Bo=c(Ia),us=o(Ia,"P",{});var Ki=l(us);Po=g(Ki,"Download and extract given url_or_urls."),Ki.forEach(s),Ro=c(Ia),b(Ge.$$.fragment,Ia),Ia.forEach(s),Ao=c(O),re=o(O,"DIV",{class:!0});var Na=l(re);b(Ct.$$.fragment,Na),Co=c(Na),Ot=o(Na,"P",{});var ln=l(Ot);Oo=g(ln,"Download given urls(s) by calling "),_s=o(ln,"CODE",{});var Yi=l(_s);Lo=g(Yi,"custom_download"),Yi.forEach(s),Vo=g(ln,"."),ln.forEach(s),Mo=c(Na),b(He.$$.fragment,Na),Na.forEach(s),qo=c(O),ne=o(O,"DIV",{class:!0});var Ba=l(ne);b(Lt.$$.fragment,Ba),Fo=c(Ba),hs=o(Ba,"P",{});var Qi=l(hs);zo=g(Qi,"Extract given path(s)."),Qi.forEach(s),Uo=c(Ba),b(We.$$.fragment,Ba),Ba.forEach(s),Go=c(O),oe=o(O,"DIV",{class:!0});var Pa=l(oe);b(Vt.$$.fragment,Pa),Ho=c(Pa),$s=o(Pa,"P",{});var Zi=l($s);Wo=g(Zi,"Iterate over files within an archive."),Zi.forEach(s),Xo=c(Pa),b(Xe.$$.fragment,Pa),Pa.forEach(s),Jo=c(O),le=o(O,"DIV",{class:!0});var Ra=l(le);b(Mt.$$.fragment,Ra),Ko=c(Ra),vs=o(Ra,"P",{});var ep=l(vs);Yo=g(ep,"Iterate over file paths."),ep.forEach(s),Qo=c(Ra),b(Je.$$.fragment,Ra),Ra.forEach(s),Zo=c(O),Ke=o(O,"DIV",{class:!0});var dn=l(Ke);b(qt.$$.fragment,dn),el=c(dn),bs=o(dn,"P",{});var tp=l(bs);tl=g(tp,"Ship the files using Beam FileSystems to the pipeline temp dir."),tp.forEach(s),dn.forEach(s),O.forEach(s),zr=c(a),R=o(a,"DIV",{class:!0});var q=l(R);b(Ft.$$.fragment,q),al=c(q),U=o(q,"P",{});var fe=l(U);sl=g(fe,`Download manager that uses the \u201D::\u201D separator to navigate through (possibly remote) compressed archives.
Contrary to the regular DownloadManager, the `),xs=o(fe,"CODE",{});var ap=l(xs);rl=g(ap,"download"),ap.forEach(s),nl=g(fe," and "),ws=o(fe,"CODE",{});var sp=l(ws);ol=g(sp,"extract"),sp.forEach(s),ll=g(fe,` methods don\u2019t actually download nor extract
data, but they rather return the path or url that could be opened using the `),Es=o(fe,"CODE",{});var rp=l(Es);dl=g(rp,"xopen"),rp.forEach(s),il=g(fe,` function which extends the
builtin `),ys=o(fe,"CODE",{});var np=l(ys);pl=g(np,"open"),np.forEach(s),cl=g(fe," function to stream data from remote files."),fe.forEach(s),ml=c(q),de=o(q,"DIV",{class:!0});var Aa=l(de);b(zt.$$.fragment,Aa),gl=c(Aa),Ds=o(Aa,"P",{});var op=l(Ds);fl=g(op,"Download given url(s)."),op.forEach(s),ul=c(Aa),b(Ye.$$.fragment,Aa),Aa.forEach(s),_l=c(q),ie=o(q,"DIV",{class:!0});var Ca=l(ie);b(Ut.$$.fragment,Ca),hl=c(Ca),ks=o(Ca,"P",{});var lp=l(ks);$l=g(lp,"Download and extract given url_or_urls."),lp.forEach(s),vl=c(Ca),b(Qe.$$.fragment,Ca),Ca.forEach(s),bl=c(q),pe=o(q,"DIV",{class:!0});var Oa=l(pe);b(Gt.$$.fragment,Oa),xl=c(Oa),js=o(Oa,"P",{});var dp=l(js);wl=g(dp,"Extract given path(s)."),dp.forEach(s),El=c(Oa),b(Ze.$$.fragment,Oa),Oa.forEach(s),yl=c(q),ce=o(q,"DIV",{class:!0});var La=l(ce);b(Ht.$$.fragment,La),Dl=c(La),Ts=o(La,"P",{});var ip=l(Ts);kl=g(ip,"Iterate over files within an archive."),ip.forEach(s),jl=c(La),b(et.$$.fragment,La),La.forEach(s),Tl=c(q),me=o(q,"DIV",{class:!0});var Va=l(me);b(Wt.$$.fragment,Va),Sl=c(Va),Ss=o(Va,"P",{});var pp=l(Ss);Il=g(pp,"Iterate over files."),pp.forEach(s),Nl=c(Va),b(tt.$$.fragment,Va),Va.forEach(s),q.forEach(s),Ur=c(a),we=o(a,"DIV",{class:!0});var pn=l(we);b(Xt.$$.fragment,pn),Bl=c(pn),Is=o(pn,"P",{});var cp=l(Is);Pl=g(cp,"Configuration for our cached path manager."),cp.forEach(s),pn.forEach(s),Gr=c(a),V=o(a,"DIV",{class:!0});var ue=l(V);b(Jt.$$.fragment,ue),Rl=c(ue),$a=o(ue,"P",{});var ki=l($a);Ns=o(ki,"CODE",{});var mp=l(Ns);Al=g(mp,"Enum"),mp.forEach(s),Cl=g(ki," for how to treat pre-existing downloads and data."),ki.forEach(s),Ol=c(ue),Kt=o(ue,"P",{});var cn=l(Kt);Ll=g(cn,"The default mode is "),Bs=o(cn,"CODE",{});var gp=l(Bs);Vl=g(gp,"REUSE_DATASET_IF_EXISTS"),gp.forEach(s),Ml=g(cn,`, which will reuse both
raw downloads and the prepared dataset if they exist.`),cn.forEach(s),ql=c(ue),Ps=o(ue,"P",{});var fp=l(Ps);Fl=g(fp,"The generations modes:"),fp.forEach(s),zl=c(ue),Yt=o(ue,"TABLE",{});var mn=l(Yt);Rs=o(mn,"THEAD",{});var up=l(Rs);Ee=o(up,"TR",{});var Ma=l(Ee);Hr=o(Ma,"TH",{}),l(Hr).forEach(s),Ul=c(Ma),As=o(Ma,"TH",{});var _p=l(As);Gl=g(_p,"Downloads"),_p.forEach(s),Hl=c(Ma),Cs=o(Ma,"TH",{});var hp=l(Cs);Wl=g(hp,"Dataset"),hp.forEach(s),Ma.forEach(s),up.forEach(s),Xl=c(mn),ye=o(mn,"TBODY",{});var qa=l(ye);De=o(qa,"TR",{});var Fa=l(De);va=o(Fa,"TD",{});var ji=l(va);Os=o(ji,"CODE",{});var $p=l(Os);Jl=g($p,"REUSE_DATASET_IF_EXISTS"),$p.forEach(s),Kl=g(ji," (default)"),ji.forEach(s),Yl=c(Fa),Ls=o(Fa,"TD",{});var vp=l(Ls);Ql=g(vp,"Reuse"),vp.forEach(s),Zl=c(Fa),Vs=o(Fa,"TD",{});var bp=l(Vs);ed=g(bp,"Reuse"),bp.forEach(s),Fa.forEach(s),td=c(qa),ke=o(qa,"TR",{});var za=l(ke);Ms=o(za,"TD",{});var xp=l(Ms);qs=o(xp,"CODE",{});var wp=l(qs);ad=g(wp,"REUSE_CACHE_IF_EXISTS"),wp.forEach(s),xp.forEach(s),sd=c(za),Fs=o(za,"TD",{});var Ep=l(Fs);rd=g(Ep,"Reuse"),Ep.forEach(s),nd=c(za),zs=o(za,"TD",{});var yp=l(zs);od=g(yp,"Fresh"),yp.forEach(s),za.forEach(s),ld=c(qa),je=o(qa,"TR",{});var Ua=l(je);Us=o(Ua,"TD",{});var Dp=l(Us);Gs=o(Dp,"CODE",{});var kp=l(Gs);dd=g(kp,"FORCE_REDOWNLOAD"),kp.forEach(s),Dp.forEach(s),id=c(Ua),Hs=o(Ua,"TD",{});var jp=l(Hs);pd=g(jp,"Fresh"),jp.forEach(s),cd=c(Ua),Ws=o(Ua,"TD",{});var Tp=l(Ws);md=g(Tp,"Fresh"),Tp.forEach(s),Ua.forEach(s),qa.forEach(s),mn.forEach(s),ue.forEach(s),Wr=c(a),Te=o(a,"H2",{class:!0});var gn=l(Te);at=o(gn,"A",{id:!0,class:!0,href:!0});var Sp=l(at);Xs=o(Sp,"SPAN",{});var Ip=l(Xs);b(Qt.$$.fragment,Ip),Ip.forEach(s),Sp.forEach(s),gd=c(gn),Js=o(gn,"SPAN",{});var Np=l(Js);fd=g(Np,"Splits"),Np.forEach(s),gn.forEach(s),Xr=c(a),G=o(a,"DIV",{class:!0});var ut=l(G);b(Zt.$$.fragment,ut),ud=c(ut),Ks=o(ut,"P",{});var Bp=l(Ks);_d=g(Bp,"Defines the split information for the generator."),Bp.forEach(s),hd=c(ut),Se=o(ut,"P",{});var Ga=l(Se);$d=g(Ga,`This should be used as returned value of
`),Ys=o(Ga,"CODE",{});var Pp=l(Ys);vd=g(Pp,"GeneratorBasedBuilder._split_generators()"),Pp.forEach(s),bd=g(Ga,`.
See `),Qs=o(Ga,"CODE",{});var Rp=l(Qs);xd=g(Rp,"GeneratorBasedBuilder._split_generators()"),Rp.forEach(s),wd=g(Ga,` for more info and example
of usage.`),Ga.forEach(s),Ed=c(ut),b(st.$$.fragment,ut),ut.forEach(s),Jr=c(a),A=o(a,"DIV",{class:!0});var F=l(A);b(ea.$$.fragment,F),yd=c(F),ba=o(F,"P",{});var Ti=l(ba);Zs=o(Ti,"CODE",{});var Ap=l(Zs);Dd=g(Ap,"Enum"),Ap.forEach(s),kd=g(Ti," for dataset splits."),Ti.forEach(s),jd=c(F),er=o(F,"P",{});var Cp=l(er);Td=g(Cp,`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),Cp.forEach(s),Sd=c(F),X=o(F,"UL",{});var _t=l(X);xa=o(_t,"LI",{});var Si=l(xa);tr=o(Si,"CODE",{});var Op=l(tr);Id=g(Op,"TRAIN"),Op.forEach(s),Nd=g(Si,": the training data."),Si.forEach(s),Bd=c(_t),wa=o(_t,"LI",{});var Ii=l(wa);ar=o(Ii,"CODE",{});var Lp=l(ar);Pd=g(Lp,"VALIDATION"),Lp.forEach(s),Rd=g(Ii,`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Ii.forEach(s),Ad=c(_t),Ea=o(_t,"LI",{});var Ni=l(Ea);sr=o(Ni,"CODE",{});var Vp=l(sr);Cd=g(Vp,"TEST"),Vp.forEach(s),Od=g(Ni,`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Ni.forEach(s),Ld=c(_t),ya=o(_t,"LI",{});var Bi=l(ya);rr=o(Bi,"CODE",{});var Mp=l(rr);Vd=g(Mp,"ALL"),Mp.forEach(s),Md=g(Bi,": the union of all defined dataset splits."),Bi.forEach(s),_t.forEach(s),qd=c(F),Da=o(F,"P",{});var Pi=l(Da);Fd=g(Pi,"Note: All splits, including compositions inherit from "),nr=o(Pi,"CODE",{});var qp=l(nr);zd=g(qp,"datasets.SplitBase"),qp.forEach(s),Pi.forEach(s),Ud=c(F),ta=o(F,"P",{});var fn=l(ta);Gd=g(fn,"See the :doc:"),or=o(fn,"CODE",{});var Fp=l(or);Hd=g(Fp,"guide on splits </loading>"),Fp.forEach(s),Wd=g(fn," for more information."),fn.forEach(s),Xd=c(F),b(rt.$$.fragment,F),F.forEach(s),Kr=c(a),B=o(a,"DIV",{class:!0});var L=l(B);b(aa.$$.fragment,L),Jd=c(L),lr=o(L,"P",{});var zp=l(lr);Kd=g(zp,"Descriptor corresponding to a named split (train, test, \u2026)."),zp.forEach(s),Yd=c(L),b(nt.$$.fragment,L),Qd=c(L),dr=o(L,"P",{});var Up=l(dr);Zd=g(Up,"Warning:"),Up.forEach(s),ei=c(L),b(ot.$$.fragment,L),ti=c(L),ir=o(L,"P",{});var Gp=l(ir);ai=g(Gp,"Warning:"),Gp.forEach(s),si=c(L),b(lt.$$.fragment,L),ri=c(L),b(dt.$$.fragment,L),L.forEach(s),Yr=c(a),Ie=o(a,"DIV",{class:!0});var un=l(Ie);b(sa.$$.fragment,un),ni=c(un),pr=o(un,"P",{});var Hp=l(pr);oi=g(Hp,"Split corresponding to the union of all defined dataset splits."),Hp.forEach(s),un.forEach(s),Qr=c(a),M=o(a,"DIV",{class:!0});var _e=l(M);b(ra.$$.fragment,_e),li=c(_e),cr=o(_e,"P",{});var Wp=l(cr);di=g(Wp,"Reading instruction for a dataset."),Wp.forEach(s),ii=c(_e),b(it.$$.fragment,_e),pi=c(_e),pt=o(_e,"DIV",{class:!0});var _n=l(pt);b(na.$$.fragment,_n),ci=c(_n),mr=o(_n,"P",{});var Xp=l(mr);mi=g(Xp,"Creates a ReadInstruction instance out of a string spec."),Xp.forEach(s),_n.forEach(s),gi=c(_e),ge=o(_e,"DIV",{class:!0});var Ha=l(ge);b(oa.$$.fragment,Ha),fi=c(Ha),gr=o(Ha,"P",{});var Jp=l(gr);ui=g(Jp,"Translate instruction into a list of absolute instructions."),Jp.forEach(s),_i=c(Ha),fr=o(Ha,"P",{});var Kp=l(fr);hi=g(Kp,"Those absolute instructions are then to be added together."),Kp.forEach(s),Ha.forEach(s),_e.forEach(s),Zr=c(a),Ne=o(a,"H2",{class:!0});var hn=l(Ne);ct=o(hn,"A",{id:!0,class:!0,href:!0});var Yp=l(ct);ur=o(Yp,"SPAN",{});var Qp=l(ur);b(la.$$.fragment,Qp),Qp.forEach(s),Yp.forEach(s),$i=c(hn),_r=o(hn,"SPAN",{});var Zp=l(_r);vi=g(Zp,"Version"),Zp.forEach(s),hn.forEach(s),en=c(a),H=o(a,"DIV",{class:!0});var ht=l(H);b(da.$$.fragment,ht),bi=c(ht),hr=o(ht,"P",{});var ec=l(hr);xi=g(ec,"Dataset version MAJOR.MINOR.PATCH."),ec.forEach(s),wi=c(ht),b(mt.$$.fragment,ht),Ei=c(ht),gt=o(ht,"DIV",{class:!0});var $n=l(gt);b(ia.$$.fragment,$n),yi=c($n),$r=o($n,"P",{});var tc=l($r);Di=g(tc,"Returns True if other_version matches."),tc.forEach(s),$n.forEach(s),ht.forEach(s),this.h()},h(){D(d,"name","hf:doc:metadata"),D(d,"content",JSON.stringify(Nc)),D(r,"id","builder-classes"),D(r,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(r,"href","#builder-classes"),D(f,"class","relative group"),D(Oe,"id","datasets.DatasetBuilder"),D(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(Oe,"href","#datasets.DatasetBuilder"),D(he,"class","relative group"),D(fa,"href","/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.DatasetBuilder"),D(ua,"href","/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.BuilderConfig"),D(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(_a,"href","/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.DatasetBuilder"),D(ha,"href","/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.BuilderConfig"),D(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ze,"id","datasets.DownloadManager"),D(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(ze,"href","#datasets.DownloadManager"),D(xe,"class","relative group"),D(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(at,"id","datasets.SplitGenerator"),D(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(at,"href","#datasets.SplitGenerator"),D(Te,"class","relative group"),D(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ct,"id","datasets.Version"),D(ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(ct,"href","#datasets.Version"),D(Ne,"class","relative group"),D(gt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(a,h){e(document.head,d),$(a,_,h),$(a,f,h),e(f,r),e(r,u),x(t,u,null),e(f,i),e(f,Wa),e(Wa,vn),$(a,Rr,h),$(a,he,h),e(he,Oe),e(Oe,Xa),x($t,Xa,null),e(he,bn),e(he,Ja),e(Ja,xn),$(a,Ar,h),$(a,J,h),e(J,wn),e(J,fa),e(fa,En),e(J,yn),e(J,ua),e(ua,Dn),e(J,kn),$(a,Cr,h),$(a,P,h),x(vt,P,null),e(P,jn),e(P,Ka),e(Ka,Tn),e(P,Sn),e(P,K),x(bt,K,null),e(K,In),e(K,Ya),e(Ya,Nn),e(K,Bn),x(Le,K,null),e(P,Pn),e(P,Y),x(xt,Y,null),e(Y,Rn),e(Y,Qa),e(Qa,An),e(Y,Cn),x(Ve,Y,null),e(P,On),e(P,Q),x(wt,Q,null),e(Q,Ln),e(Q,Za),e(Za,Vn),e(Q,Mn),x(Me,Q,null),e(P,qn),e(P,Z),x(Et,Z,null),e(Z,Fn),e(Z,es),e(es,zn),e(Z,Un),x(qe,Z,null),e(P,Gn),e(P,Fe),x(yt,Fe,null),e(Fe,Hn),e(Fe,ts),e(ts,Wn),$(a,Or,h),$(a,W,h),x(Dt,W,null),e(W,Xn),e(W,as),e(as,Jn),e(W,Kn),e(W,ee),e(ee,ss),e(ss,Yn),e(ee,Qn),e(ee,rs),e(rs,Zn),e(ee,eo),e(ee,ns),e(ns,to),e(ee,ao),$(a,Lr,h),$(a,$e,h),x(kt,$e,null),e($e,so),e($e,os),e(os,ro),$(a,Vr,h),$(a,ve,h),x(jt,ve,null),e(ve,no),e(ve,ls),e(ls,oo),$(a,Mr,h),$(a,z,h),x(Tt,z,null),e(z,lo),e(z,St),e(St,io),e(St,_a),e(_a,po),e(St,co),e(z,mo),e(z,It),e(It,go),e(It,ha),e(ha,fo),e(It,uo),e(z,_o),e(z,te),x(Nt,te,null),e(te,ho),e(te,ds),e(ds,$o),e(te,vo),e(te,be),e(be,is),e(is,bo),e(be,xo),e(be,ps),e(ps,wo),e(be,Eo),e(be,cs),e(cs,yo),$(a,qr,h),$(a,xe,h),e(xe,ze),e(ze,ms),x(Bt,ms,null),e(xe,Do),e(xe,gs),e(gs,ko),$(a,Fr,h),$(a,N,h),x(Pt,N,null),e(N,jo),e(N,ae),x(Rt,ae,null),e(ae,To),e(ae,fs),e(fs,So),e(ae,Io),x(Ue,ae,null),e(N,No),e(N,se),x(At,se,null),e(se,Bo),e(se,us),e(us,Po),e(se,Ro),x(Ge,se,null),e(N,Ao),e(N,re),x(Ct,re,null),e(re,Co),e(re,Ot),e(Ot,Oo),e(Ot,_s),e(_s,Lo),e(Ot,Vo),e(re,Mo),x(He,re,null),e(N,qo),e(N,ne),x(Lt,ne,null),e(ne,Fo),e(ne,hs),e(hs,zo),e(ne,Uo),x(We,ne,null),e(N,Go),e(N,oe),x(Vt,oe,null),e(oe,Ho),e(oe,$s),e($s,Wo),e(oe,Xo),x(Xe,oe,null),e(N,Jo),e(N,le),x(Mt,le,null),e(le,Ko),e(le,vs),e(vs,Yo),e(le,Qo),x(Je,le,null),e(N,Zo),e(N,Ke),x(qt,Ke,null),e(Ke,el),e(Ke,bs),e(bs,tl),$(a,zr,h),$(a,R,h),x(Ft,R,null),e(R,al),e(R,U),e(U,sl),e(U,xs),e(xs,rl),e(U,nl),e(U,ws),e(ws,ol),e(U,ll),e(U,Es),e(Es,dl),e(U,il),e(U,ys),e(ys,pl),e(U,cl),e(R,ml),e(R,de),x(zt,de,null),e(de,gl),e(de,Ds),e(Ds,fl),e(de,ul),x(Ye,de,null),e(R,_l),e(R,ie),x(Ut,ie,null),e(ie,hl),e(ie,ks),e(ks,$l),e(ie,vl),x(Qe,ie,null),e(R,bl),e(R,pe),x(Gt,pe,null),e(pe,xl),e(pe,js),e(js,wl),e(pe,El),x(Ze,pe,null),e(R,yl),e(R,ce),x(Ht,ce,null),e(ce,Dl),e(ce,Ts),e(Ts,kl),e(ce,jl),x(et,ce,null),e(R,Tl),e(R,me),x(Wt,me,null),e(me,Sl),e(me,Ss),e(Ss,Il),e(me,Nl),x(tt,me,null),$(a,Ur,h),$(a,we,h),x(Xt,we,null),e(we,Bl),e(we,Is),e(Is,Pl),$(a,Gr,h),$(a,V,h),x(Jt,V,null),e(V,Rl),e(V,$a),e($a,Ns),e(Ns,Al),e($a,Cl),e(V,Ol),e(V,Kt),e(Kt,Ll),e(Kt,Bs),e(Bs,Vl),e(Kt,Ml),e(V,ql),e(V,Ps),e(Ps,Fl),e(V,zl),e(V,Yt),e(Yt,Rs),e(Rs,Ee),e(Ee,Hr),e(Ee,Ul),e(Ee,As),e(As,Gl),e(Ee,Hl),e(Ee,Cs),e(Cs,Wl),e(Yt,Xl),e(Yt,ye),e(ye,De),e(De,va),e(va,Os),e(Os,Jl),e(va,Kl),e(De,Yl),e(De,Ls),e(Ls,Ql),e(De,Zl),e(De,Vs),e(Vs,ed),e(ye,td),e(ye,ke),e(ke,Ms),e(Ms,qs),e(qs,ad),e(ke,sd),e(ke,Fs),e(Fs,rd),e(ke,nd),e(ke,zs),e(zs,od),e(ye,ld),e(ye,je),e(je,Us),e(Us,Gs),e(Gs,dd),e(je,id),e(je,Hs),e(Hs,pd),e(je,cd),e(je,Ws),e(Ws,md),$(a,Wr,h),$(a,Te,h),e(Te,at),e(at,Xs),x(Qt,Xs,null),e(Te,gd),e(Te,Js),e(Js,fd),$(a,Xr,h),$(a,G,h),x(Zt,G,null),e(G,ud),e(G,Ks),e(Ks,_d),e(G,hd),e(G,Se),e(Se,$d),e(Se,Ys),e(Ys,vd),e(Se,bd),e(Se,Qs),e(Qs,xd),e(Se,wd),e(G,Ed),x(st,G,null),$(a,Jr,h),$(a,A,h),x(ea,A,null),e(A,yd),e(A,ba),e(ba,Zs),e(Zs,Dd),e(ba,kd),e(A,jd),e(A,er),e(er,Td),e(A,Sd),e(A,X),e(X,xa),e(xa,tr),e(tr,Id),e(xa,Nd),e(X,Bd),e(X,wa),e(wa,ar),e(ar,Pd),e(wa,Rd),e(X,Ad),e(X,Ea),e(Ea,sr),e(sr,Cd),e(Ea,Od),e(X,Ld),e(X,ya),e(ya,rr),e(rr,Vd),e(ya,Md),e(A,qd),e(A,Da),e(Da,Fd),e(Da,nr),e(nr,zd),e(A,Ud),e(A,ta),e(ta,Gd),e(ta,or),e(or,Hd),e(ta,Wd),e(A,Xd),x(rt,A,null),$(a,Kr,h),$(a,B,h),x(aa,B,null),e(B,Jd),e(B,lr),e(lr,Kd),e(B,Yd),x(nt,B,null),e(B,Qd),e(B,dr),e(dr,Zd),e(B,ei),x(ot,B,null),e(B,ti),e(B,ir),e(ir,ai),e(B,si),x(lt,B,null),e(B,ri),x(dt,B,null),$(a,Yr,h),$(a,Ie,h),x(sa,Ie,null),e(Ie,ni),e(Ie,pr),e(pr,oi),$(a,Qr,h),$(a,M,h),x(ra,M,null),e(M,li),e(M,cr),e(cr,di),e(M,ii),x(it,M,null),e(M,pi),e(M,pt),x(na,pt,null),e(pt,ci),e(pt,mr),e(mr,mi),e(M,gi),e(M,ge),x(oa,ge,null),e(ge,fi),e(ge,gr),e(gr,ui),e(ge,_i),e(ge,fr),e(fr,hi),$(a,Zr,h),$(a,Ne,h),e(Ne,ct),e(ct,ur),x(la,ur,null),e(Ne,$i),e(Ne,_r),e(_r,vi),$(a,en,h),$(a,H,h),x(da,H,null),e(H,bi),e(H,hr),e(hr,xi),e(H,wi),x(mt,H,null),e(H,Ei),e(H,gt),x(ia,gt,null),e(gt,yi),e(gt,$r),e($r,Di),tn=!0},p(a,[h]){const pa={};h&2&&(pa.$$scope={dirty:h,ctx:a}),Le.$set(pa);const vr={};h&2&&(vr.$$scope={dirty:h,ctx:a}),Ve.$set(vr);const br={};h&2&&(br.$$scope={dirty:h,ctx:a}),Me.$set(br);const xr={};h&2&&(xr.$$scope={dirty:h,ctx:a}),qe.$set(xr);const ca={};h&2&&(ca.$$scope={dirty:h,ctx:a}),Ue.$set(ca);const wr={};h&2&&(wr.$$scope={dirty:h,ctx:a}),Ge.$set(wr);const Er={};h&2&&(Er.$$scope={dirty:h,ctx:a}),He.$set(Er);const yr={};h&2&&(yr.$$scope={dirty:h,ctx:a}),We.$set(yr);const Be={};h&2&&(Be.$$scope={dirty:h,ctx:a}),Xe.$set(Be);const Dr={};h&2&&(Dr.$$scope={dirty:h,ctx:a}),Je.$set(Dr);const kr={};h&2&&(kr.$$scope={dirty:h,ctx:a}),Ye.$set(kr);const C={};h&2&&(C.$$scope={dirty:h,ctx:a}),Qe.$set(C);const jr={};h&2&&(jr.$$scope={dirty:h,ctx:a}),Ze.$set(jr);const Pe={};h&2&&(Pe.$$scope={dirty:h,ctx:a}),et.$set(Pe);const Tr={};h&2&&(Tr.$$scope={dirty:h,ctx:a}),tt.$set(Tr);const Re={};h&2&&(Re.$$scope={dirty:h,ctx:a}),st.$set(Re);const Sr={};h&2&&(Sr.$$scope={dirty:h,ctx:a}),rt.$set(Sr);const Ae={};h&2&&(Ae.$$scope={dirty:h,ctx:a}),nt.$set(Ae);const Ir={};h&2&&(Ir.$$scope={dirty:h,ctx:a}),ot.$set(Ir);const Ce={};h&2&&(Ce.$$scope={dirty:h,ctx:a}),lt.$set(Ce);const Nr={};h&2&&(Nr.$$scope={dirty:h,ctx:a}),dt.$set(Nr);const ma={};h&2&&(ma.$$scope={dirty:h,ctx:a}),it.$set(ma);const Br={};h&2&&(Br.$$scope={dirty:h,ctx:a}),mt.$set(Br)},i(a){tn||(w(t.$$.fragment,a),w($t.$$.fragment,a),w(vt.$$.fragment,a),w(bt.$$.fragment,a),w(Le.$$.fragment,a),w(xt.$$.fragment,a),w(Ve.$$.fragment,a),w(wt.$$.fragment,a),w(Me.$$.fragment,a),w(Et.$$.fragment,a),w(qe.$$.fragment,a),w(yt.$$.fragment,a),w(Dt.$$.fragment,a),w(kt.$$.fragment,a),w(jt.$$.fragment,a),w(Tt.$$.fragment,a),w(Nt.$$.fragment,a),w(Bt.$$.fragment,a),w(Pt.$$.fragment,a),w(Rt.$$.fragment,a),w(Ue.$$.fragment,a),w(At.$$.fragment,a),w(Ge.$$.fragment,a),w(Ct.$$.fragment,a),w(He.$$.fragment,a),w(Lt.$$.fragment,a),w(We.$$.fragment,a),w(Vt.$$.fragment,a),w(Xe.$$.fragment,a),w(Mt.$$.fragment,a),w(Je.$$.fragment,a),w(qt.$$.fragment,a),w(Ft.$$.fragment,a),w(zt.$$.fragment,a),w(Ye.$$.fragment,a),w(Ut.$$.fragment,a),w(Qe.$$.fragment,a),w(Gt.$$.fragment,a),w(Ze.$$.fragment,a),w(Ht.$$.fragment,a),w(et.$$.fragment,a),w(Wt.$$.fragment,a),w(tt.$$.fragment,a),w(Xt.$$.fragment,a),w(Jt.$$.fragment,a),w(Qt.$$.fragment,a),w(Zt.$$.fragment,a),w(st.$$.fragment,a),w(ea.$$.fragment,a),w(rt.$$.fragment,a),w(aa.$$.fragment,a),w(nt.$$.fragment,a),w(ot.$$.fragment,a),w(lt.$$.fragment,a),w(dt.$$.fragment,a),w(sa.$$.fragment,a),w(ra.$$.fragment,a),w(it.$$.fragment,a),w(na.$$.fragment,a),w(oa.$$.fragment,a),w(la.$$.fragment,a),w(da.$$.fragment,a),w(mt.$$.fragment,a),w(ia.$$.fragment,a),tn=!0)},o(a){E(t.$$.fragment,a),E($t.$$.fragment,a),E(vt.$$.fragment,a),E(bt.$$.fragment,a),E(Le.$$.fragment,a),E(xt.$$.fragment,a),E(Ve.$$.fragment,a),E(wt.$$.fragment,a),E(Me.$$.fragment,a),E(Et.$$.fragment,a),E(qe.$$.fragment,a),E(yt.$$.fragment,a),E(Dt.$$.fragment,a),E(kt.$$.fragment,a),E(jt.$$.fragment,a),E(Tt.$$.fragment,a),E(Nt.$$.fragment,a),E(Bt.$$.fragment,a),E(Pt.$$.fragment,a),E(Rt.$$.fragment,a),E(Ue.$$.fragment,a),E(At.$$.fragment,a),E(Ge.$$.fragment,a),E(Ct.$$.fragment,a),E(He.$$.fragment,a),E(Lt.$$.fragment,a),E(We.$$.fragment,a),E(Vt.$$.fragment,a),E(Xe.$$.fragment,a),E(Mt.$$.fragment,a),E(Je.$$.fragment,a),E(qt.$$.fragment,a),E(Ft.$$.fragment,a),E(zt.$$.fragment,a),E(Ye.$$.fragment,a),E(Ut.$$.fragment,a),E(Qe.$$.fragment,a),E(Gt.$$.fragment,a),E(Ze.$$.fragment,a),E(Ht.$$.fragment,a),E(et.$$.fragment,a),E(Wt.$$.fragment,a),E(tt.$$.fragment,a),E(Xt.$$.fragment,a),E(Jt.$$.fragment,a),E(Qt.$$.fragment,a),E(Zt.$$.fragment,a),E(st.$$.fragment,a),E(ea.$$.fragment,a),E(rt.$$.fragment,a),E(aa.$$.fragment,a),E(nt.$$.fragment,a),E(ot.$$.fragment,a),E(lt.$$.fragment,a),E(dt.$$.fragment,a),E(sa.$$.fragment,a),E(ra.$$.fragment,a),E(it.$$.fragment,a),E(na.$$.fragment,a),E(oa.$$.fragment,a),E(la.$$.fragment,a),E(da.$$.fragment,a),E(mt.$$.fragment,a),E(ia.$$.fragment,a),tn=!1},d(a){s(d),a&&s(_),a&&s(f),y(t),a&&s(Rr),a&&s(he),y($t),a&&s(Ar),a&&s(J),a&&s(Cr),a&&s(P),y(vt),y(bt),y(Le),y(xt),y(Ve),y(wt),y(Me),y(Et),y(qe),y(yt),a&&s(Or),a&&s(W),y(Dt),a&&s(Lr),a&&s($e),y(kt),a&&s(Vr),a&&s(ve),y(jt),a&&s(Mr),a&&s(z),y(Tt),y(Nt),a&&s(qr),a&&s(xe),y(Bt),a&&s(Fr),a&&s(N),y(Pt),y(Rt),y(Ue),y(At),y(Ge),y(Ct),y(He),y(Lt),y(We),y(Vt),y(Xe),y(Mt),y(Je),y(qt),a&&s(zr),a&&s(R),y(Ft),y(zt),y(Ye),y(Ut),y(Qe),y(Gt),y(Ze),y(Ht),y(et),y(Wt),y(tt),a&&s(Ur),a&&s(we),y(Xt),a&&s(Gr),a&&s(V),y(Jt),a&&s(Wr),a&&s(Te),y(Qt),a&&s(Xr),a&&s(G),y(Zt),y(st),a&&s(Jr),a&&s(A),y(ea),y(rt),a&&s(Kr),a&&s(B),y(aa),y(nt),y(ot),y(lt),y(dt),a&&s(Yr),a&&s(Ie),y(sa),a&&s(Qr),a&&s(M),y(ra),y(it),y(na),y(oa),a&&s(Zr),a&&s(Ne),y(la),a&&s(en),a&&s(H),y(da),y(mt),y(ia)}}}const Nc={local:"builder-classes",sections:[{local:"datasets.DatasetBuilder",title:"Builders"},{local:"datasets.DownloadManager",title:"Download"},{local:"datasets.SplitGenerator",title:"Splits"},{local:"datasets.Version",title:"Version"}],title:"Builder classes"};function Bc(k){return oc(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Lc extends ac{constructor(d){super();sc(this,d,Bc,Ic,rc,{})}}export{Lc as default,Nc as metadata};
