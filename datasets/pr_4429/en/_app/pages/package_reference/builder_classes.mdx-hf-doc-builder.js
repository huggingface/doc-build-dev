import{S as Qp,i as Zp,s as ec,e as n,k as p,w as v,t as m,M as tc,c as o,d as s,m as c,a as l,x as b,h as g,b as D,G as e,g as $,y as x,q as w,o as E,B as y,v as ac,L as S}from"../../chunks/vendor-hf-doc-builder.js";import{D as j}from"../../chunks/Docstring-hf-doc-builder.js";import{C as I}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Nr}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as T}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function sc(k){let d,_,f,r,u;return r=new I({props:{code:`from datasets import load_dataset_builder
builder = load_dataset_builder('rotten_tomatoes')
ds = builder.download_and_prepare()
ds = builder.as_dataset(split='train')
ds`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.download_and_prepare()
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.as_dataset(split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds
Dataset({
    features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
    num_rows: <span class="hljs-number">8530</span>
})`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function rc(k){let d,_,f,r,u;return r=new I({props:{code:`from datasets import load_dataset_builder
builder = load_dataset_builder('rotten_tomatoes')
ds = builder.download_and_prepare()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.download_and_prepare()`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function nc(k){let d,_,f,r,u;return r=new I({props:{code:`from datasets import load_dataset_builder
ds_builder = load_dataset_builder('rotten_tomatoes')
ds_builder.get_all_exported_dataset_infos()
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder.get_all_exported_dataset_infos()
{<span class="hljs-string">&#x27;default&#x27;</span>: DatasetInfo(description=<span class="hljs-string">&quot;Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, \`\`Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.&#x27;&#x27;, Proceedings of the
5.
ion=&#x27;@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age=&#x27;http://www.cs.cornell.edu/people/pabo/movie-review-data/&#x27;, license=&#x27;&#x27;, features={&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=&#x27;&#x27;, output=&#x27;&#x27;), task_templates=[TextClassification(task=&#x27;text-classification&#x27;, text_column=&#x27;text&#x27;, label_column=&#x27;label&#x27;)], builder_name=&#x27;rotten_tomatoes_movie_review&#x27;, config_name=&#x27;default&#x27;, version=1.0.0, splits={&#x27;train&#x27;: SplitInfo(name=&#x27;train&#x27;, num_bytes=1074810, num_examples=8530, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;validation&#x27;: SplitInfo(name=&#x27;validation&#x27;, num_bytes=134679, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;test&#x27;: SplitInfo(name=&#x27;test&#x27;, num_bytes=135972, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;)}, download_checksums={&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;: {&#x27;num_bytes&#x27;: 487770, &#x27;checksum&#x27;: &#x27;a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9&#x27;}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)}</span>`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function oc(k){let d,_,f,r,u;return r=new I({props:{code:`from datasets import load_dataset_builder
ds_builder = load_dataset_builder('rotten_tomatoes')
ds_builder.get_exported_dataset_info()
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder.get_exported_dataset_info()
DatasetInfo(description=<span class="hljs-string">&quot;Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, \`\`Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.&#x27;&#x27;, Proceedings of the
5.
ion=&#x27;@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age=&#x27;http://www.cs.cornell.edu/people/pabo/movie-review-data/&#x27;, license=&#x27;&#x27;, features={&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=&#x27;&#x27;, output=&#x27;&#x27;), task_templates=[TextClassification(task=&#x27;text-classification&#x27;, text_column=&#x27;text&#x27;, label_column=&#x27;label&#x27;)], builder_name=&#x27;rotten_tomatoes_movie_review&#x27;, config_name=&#x27;default&#x27;, version=1.0.0, splits={&#x27;train&#x27;: SplitInfo(name=&#x27;train&#x27;, num_bytes=1074810, num_examples=8530, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;validation&#x27;: SplitInfo(name=&#x27;validation&#x27;, num_bytes=134679, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;test&#x27;: SplitInfo(name=&#x27;test&#x27;, num_bytes=135972, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;)}, download_checksums={&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;: {&#x27;num_bytes&#x27;: 487770, &#x27;checksum&#x27;: &#x27;a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9&#x27;}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)</span>`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function lc(k){let d,_,f,r,u;return r=new I({props:{code:"downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)'}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function dc(k){let d,_,f,r,u;return r=new I({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),{c(){d=n("p"),_=m("Is roughly equivalent to:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Is roughly equivalent to:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function ic(k){let d,_,f,r,u;return r=new I({props:{code:"downloaded_files = dl_manager.download_custom('s3://my-bucket/data.zip', custom_download_for_my_private_bucket)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download_custom(<span class="hljs-string">&#x27;s3://my-bucket/data.zip&#x27;</span>, custom_download_for_my_private_bucket)'}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function pc(k){let d,_,f,r,u;return r=new I({props:{code:`downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
extracted_files = dl_manager.extract(downloaded_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>extracted_files = dl_manager.extract(downloaded_files)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function cc(k){let d,_,f,r,u;return r=new I({props:{code:`archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
files = dl_manager.iter_archive(archive)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>archive = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_archive(archive)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function mc(k){let d,_,f,r,u;return r=new I({props:{code:`files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
files = dl_manager.iter_files(files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.download_and_extract(<span class="hljs-string">&#x27;https://huggingface.co/datasets/beans/resolve/main/data/train.zip&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_files(files)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function gc(k){let d,_,f,r,u;return r=new I({props:{code:"downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)'}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function fc(k){let d,_,f,r,u;return r=new I({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),{c(){d=n("p"),_=m("Is roughly equivalent to:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Is roughly equivalent to:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function uc(k){let d,_,f,r,u;return r=new I({props:{code:`downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
extracted_files = dl_manager.extract(downloaded_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>extracted_files = dl_manager.extract(downloaded_files)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function _c(k){let d,_,f,r,u;return r=new I({props:{code:`archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
files = dl_manager.iter_archive(archive)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>archive = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_archive(archive)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function hc(k){let d,_,f,r,u;return r=new I({props:{code:`files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
files = dl_manager.iter_files(files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.download_and_extract(<span class="hljs-string">&#x27;https://huggingface.co/datasets/beans/resolve/main/data/train.zip&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_files(files)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function $c(k){let d,_,f,r,u;return r=new I({props:{code:`datasets.SplitGenerator(
    name=datasets.Split.TRAIN,
    gen_kwargs={"split_key": "train", "files": dl_manager.download_and_extract(url)},
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TRAIN,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and_extract(url)},
<span class="hljs-meta">... </span>)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function vc(k){let d,_,f,r,u;return r=new I({props:{code:`datasets.SplitGenerator(
    name=datasets.Split.TRAIN,
    gen_kwargs={"split_key": "train", "files": dl_manager.download_and extract(url)},
),
datasets.SplitGenerator(
    name=datasets.Split.VALIDATION,
    gen_kwargs={"split_key": "validation", "files": dl_manager.download_and extract(url)},
),
datasets.SplitGenerator(
    name=datasets.Split.TEST,
    gen_kwargs={"split_key": "test", "files": dl_manager.download_and extract(url)},
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TRAIN,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>),
<span class="hljs-meta">... </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.VALIDATION,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;validation&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>),
<span class="hljs-meta">... </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TEST,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;test&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function bc(k){let d,_,f,r,u;return r=new I({props:{code:`Each descriptor can be composed with other using addition or slice. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST

The resulting split will correspond to 25% of the train split merged with
100% of the test split.`,highlighted:`Each descriptor can be composed <span class="hljs-keyword">with</span> other using addition <span class="hljs-keyword">or</span> <span class="hljs-built_in">slice</span>. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">0</span>:<span class="hljs-number">25</span>]) + datasets.Split.TEST

The resulting split will correspond to <span class="hljs-number">25</span>% of the train split merged <span class="hljs-keyword">with</span>
<span class="hljs-number">100</span>% of the test split.`}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function xc(k){let d,_,f,r,u;return r=new I({props:{code:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error`,highlighted:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">75</span>:])
)  <span class="hljs-comment"># Error</span>
split = datasets.Split.TEST + datasets.Split.ALL  <span class="hljs-comment"># Error</span>`}}),{c(){d=n("p"),_=m("A split cannot be added twice, so the following will fail:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"A split cannot be added twice, so the following will fail:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function wc(k){let d,_,f,r,u;return r=new I({props:{code:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])`,highlighted:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
datasets.Split.TEST.subsplit(datasets.percent[:<span class="hljs-number">50</span>])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){d=n("p"),_=m("The slices can be applied only one time. So the following are valid:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"The slices can be applied only one time. So the following are valid:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function Ec(k){let d,_,f,r,u;return r=new I({props:{code:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])`,highlighted:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]).subsplit(datasets.percent[:<span class="hljs-number">25</span>])
split = (train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) + test).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){d=n("p"),_=m("But not:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"But not:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function yc(k){let d,_,f,r,u;return r=new I({props:{code:`# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(<span class="hljs-string">&#x27;test[:33%]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=(
datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) +
datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">1</span>, to=-<span class="hljs-number">1</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>)))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))

<span class="hljs-comment"># 10-fold validation:</span>
tests = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
trains = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),{c(){d=n("p"),_=m("Examples:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Examples:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function Dc(k){let d,_,f,r,u;return r=new I({props:{code:'VERSION = datasets.Version("1.0.0")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>VERSION = datasets.Version(<span class="hljs-string">&quot;1.0.0&quot;</span>)'}}),{c(){d=n("p"),_=m("Example:"),f=p(),v(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(s),f=c(t),b(r.$$.fragment,t)},m(t,i){$(t,d,i),e(d,_),$(t,f,i),x(r,t,i),u=!0},p:S,i(t){u||(w(r.$$.fragment,t),u=!0)},o(t){E(r.$$.fragment,t),u=!1},d(t){t&&s(d),t&&s(f),y(r,t)}}}function kc(k){let d,_,f,r,u,t,i,Wa,hn,Br,he,Le,Xa,vt,$n,Ja,vn,Pr,J,bn,ua,xn,wn,_a,En,yn,Rr,A,bt,Dn,K,xt,kn,Ka,jn,Tn,Ve,Sn,Y,wt,In,Ya,Nn,Bn,Me,Pn,Q,Et,Rn,Qa,An,Cn,qe,On,Z,yt,Ln,Za,Vn,Mn,Fe,qn,ze,Dt,Fn,es,zn,Ar,W,kt,Un,ts,Gn,Hn,ee,as,Wn,Xn,ss,Jn,Kn,rs,Yn,Qn,Cr,$e,jt,Zn,ns,eo,Or,ve,Tt,to,os,ao,Lr,z,St,so,It,ro,ha,no,oo,lo,Nt,io,$a,po,co,mo,te,Bt,go,ls,fo,uo,be,ds,_o,ho,is,$o,vo,ps,bo,Vr,xe,Ue,cs,Pt,xo,ms,wo,Mr,N,Rt,Eo,ae,At,yo,gs,Do,ko,Ge,jo,se,Ct,To,fs,So,Io,He,No,re,Ot,Bo,Lt,Po,us,Ro,Ao,Co,We,Oo,ne,Vt,Lo,_s,Vo,Mo,Xe,qo,oe,Mt,Fo,hs,zo,Uo,Je,Go,le,qt,Ho,$s,Wo,Xo,Ke,Jo,Ye,Ft,Ko,vs,Yo,qr,P,zt,Qo,U,Zo,bs,el,tl,xs,al,sl,ws,rl,nl,Es,ol,ll,dl,de,Ut,il,ys,pl,cl,Qe,ml,ie,Gt,gl,Ds,fl,ul,Ze,_l,pe,Ht,hl,ks,$l,vl,et,bl,ce,Wt,xl,js,wl,El,tt,yl,me,Xt,Dl,Ts,kl,jl,at,Fr,we,Jt,Tl,Ss,Sl,zr,L,Kt,Il,va,Is,Nl,Bl,Pl,Yt,Rl,Ns,Al,Cl,Ol,Bs,Ll,Vl,Qt,Ps,Ee,Ur,Ml,Rs,ql,Fl,As,zl,Ul,ye,De,ba,Cs,Gl,Hl,Wl,Os,Xl,Jl,Ls,Kl,Yl,ke,Vs,Ms,Ql,Zl,qs,ed,td,Fs,ad,sd,je,zs,Us,rd,nd,Gs,od,ld,Hs,dd,Gr,Te,st,Ws,Zt,id,Xs,pd,Hr,G,ea,cd,Js,md,gd,Se,fd,Ks,ud,_d,Ys,hd,$d,vd,rt,Wr,R,ta,bd,xa,Qs,xd,wd,Ed,Zs,yd,Dd,X,wa,er,kd,jd,Td,Ea,tr,Sd,Id,Nd,ya,ar,Bd,Pd,Rd,Da,sr,Ad,Cd,Od,ka,Ld,rr,Vd,Md,aa,qd,nr,Fd,zd,Ud,nt,Xr,B,sa,Gd,or,Hd,Wd,ot,Xd,lr,Jd,Kd,lt,Yd,dr,Qd,Zd,dt,ei,it,Jr,Ie,ra,ti,ir,ai,Kr,V,na,si,pr,ri,ni,pt,oi,ct,oa,li,cr,di,ii,ge,la,pi,mr,ci,mi,gr,gi,Yr,Ne,mt,fr,da,fi,ur,ui,Qr,H,ia,_i,_r,hi,$i,gt,vi,ft,pa,bi,hr,xi,Zr;return t=new Nr({}),vt=new Nr({}),bt=new j({props:{name:"class datasets.DatasetBuilder",anchor:"datasets.DatasetBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"config_name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"name",val:" = 'deprecated'"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L180"}}),xt=new j({props:{name:"as_dataset",anchor:"datasets.DatasetBuilder.as_dataset",parameters:[{name:"split",val:": typing.Optional[datasets.splits.Split] = None"},{name:"run_post_process",val:" = True"},{name:"ignore_verifications",val:" = False"},{name:"in_memory",val:" = False"}],parametersDescription:[{anchor:"datasets.DatasetBuilder.as_dataset.split",description:"<strong>split</strong> (<code>datasets.Split</code>) &#x2014; Which subset of the data to return.",name:"split"},{anchor:"datasets.DatasetBuilder.as_dataset.run_post_process",description:`<strong>run_post_process</strong> (bool, default=True) &#x2014; Whether to run post-processing dataset transforms and/or add
indexes.`,name:"run_post_process"},{anchor:"datasets.DatasetBuilder.as_dataset.ignore_verifications",description:`<strong>ignore_verifications</strong> (bool, default=False) &#x2014; Whether to ignore the verifications of the
downloaded/processed dataset information (checksums/size/splits/&#x2026;).`,name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.as_dataset.in_memory",description:"<strong>in_memory</strong> (bool, default=False) &#x2014; Whether to copy the data in-memory.",name:"in_memory"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L815",returnDescription:`
<p>datasets.Dataset</p>
`}}),Ve=new T({props:{anchor:"datasets.DatasetBuilder.as_dataset.example",$$slots:{default:[sc]},$$scope:{ctx:k}}}),wt=new j({props:{name:"download_and_prepare",anchor:"datasets.DatasetBuilder.download_and_prepare",parameters:[{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.download.download_manager.DownloadMode] = None"},{name:"ignore_verifications",val:": bool = False"},{name:"try_from_hf_gcs",val:": bool = True"},{name:"dl_manager",val:": typing.Optional[datasets.download.download_manager.DownloadManager] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"**download_and_prepare_kwargs",val:""}],parametersDescription:[{anchor:"datasets.DatasetBuilder.download_and_prepare.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; specific download configuration parameters.',name:"download_config"},{anchor:"datasets.DatasetBuilder.download_and_prepare.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, optional) &#x2014; select the download/generate mode - Default to <code>REUSE_DATASET_IF_EXISTS</code>',name:"download_mode"},{anchor:"datasets.DatasetBuilder.download_and_prepare.ignore_verifications",description:"<strong>ignore_verifications</strong> (<code>bool</code>) &#x2014; Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/&#x2026;)",name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.download_and_prepare.try_from_hf_gcs",description:"<strong>try_from_hf_gcs</strong> (<code>bool</code>) &#x2014; If True, it will try to download the already prepared dataset from the Hf google cloud storage",name:"try_from_hf_gcs"},{anchor:"datasets.DatasetBuilder.download_and_prepare.dl_manager",description:'<strong>dl_manager</strong> (<a href="/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a>, optional) &#x2014; specific Download Manger to use',name:"dl_manager"},{anchor:"datasets.DatasetBuilder.download_and_prepare.base_path",description:`<strong>base_path</strong> (<code>str</code>, optional) &#x2014; base path for relative paths that are used to download files. This can be a remote url.
If not specified, the value of the <em>base_path</em> attribute (<em>self.base_path</em>) will be used instead.`,name:"base_path"},{anchor:"datasets.DatasetBuilder.download_and_prepare.use_auth_token",description:`<strong>use_auth_token</strong> (<code>Union[str, bool]</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from ~/.huggingface.`,name:"use_auth_token"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L544"}}),Me=new T({props:{anchor:"datasets.DatasetBuilder.download_and_prepare.example",$$slots:{default:[rc]},$$scope:{ctx:k}}}),Et=new j({props:{name:"get_all_exported_dataset_infos",anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos",parameters:[],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L348"}}),qe=new T({props:{anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos.example",$$slots:{default:[nc]},$$scope:{ctx:k}}}),yt=new j({props:{name:"get_exported_dataset_info",anchor:"datasets.DatasetBuilder.get_exported_dataset_info",parameters:[],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L366"}}),Fe=new T({props:{anchor:"datasets.DatasetBuilder.get_exported_dataset_info.example",$$slots:{default:[oc]},$$scope:{ctx:k}}}),Dt=new j({props:{name:"get_imported_module_dir",anchor:"datasets.DatasetBuilder.get_imported_module_dir",parameters:[],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L539"}}),kt=new j({props:{name:"class datasets.GeneratorBasedBuilder",anchor:"datasets.GeneratorBasedBuilder",parameters:[{name:"*args",val:""},{name:"writer_batch_size",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L1102"}}),jt=new j({props:{name:"class datasets.BeamBasedBuilder",anchor:"datasets.BeamBasedBuilder",parameters:[{name:"*args",val:""},{name:"beam_runner",val:" = None"},{name:"beam_options",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L1262"}}),Tt=new j({props:{name:"class datasets.ArrowBasedBuilder",anchor:"datasets.ArrowBasedBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"config_name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"name",val:" = 'deprecated'"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L1200"}}),St=new j({props:{name:"class datasets.BuilderConfig",anchor:"datasets.BuilderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = 0.0.0"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.BuilderConfig.name",description:"<strong>name</strong> (<code>str</code>, default <code>&quot;default&quot;</code>) &#x2014;",name:"name"},{anchor:"datasets.BuilderConfig.version",description:'<strong>version</strong> (<a href="/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014;',name:"version"},{anchor:"datasets.BuilderConfig.data_dir",description:"<strong>data_dir</strong> (<code>str</code>, optional) &#x2014;",name:"data_dir"},{anchor:"datasets.BuilderConfig.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.BuilderConfig.description",description:"<strong>description</strong> (<code>str</code>, optional) &#x2014;",name:"description"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L84"}}),Bt=new j({props:{name:"create_config_id",anchor:"datasets.BuilderConfig.create_config_id",parameters:[{name:"config_kwargs",val:": dict"},{name:"custom_features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/builder.py#L123"}}),Pt=new Nr({}),Rt=new j({props:{name:"class datasets.DownloadManager",anchor:"datasets.DownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"record_checksums",val:" = True"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L136"}}),At=new j({props:{name:"download",anchor:"datasets.DownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L268",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Ge=new T({props:{anchor:"datasets.DownloadManager.download.example",$$slots:{default:[lc]},$$scope:{ctx:k}}}),Ct=new j({props:{name:"download_and_extract",anchor:"datasets.DownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L403",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),He=new T({props:{anchor:"datasets.DownloadManager.download_and_extract.example",$$slots:{default:[dc]},$$scope:{ctx:k}}}),Ot=new j({props:{name:"download_custom",anchor:"datasets.DownloadManager.download_custom",parameters:[{name:"url_or_urls",val:""},{name:"custom_download",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L221",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),We=new T({props:{anchor:"datasets.DownloadManager.download_custom.example",$$slots:{default:[ic]},$$scope:{ctx:k}}}),Vt=new j({props:{name:"extract",anchor:"datasets.DownloadManager.extract",parameters:[{name:"path_or_paths",val:""},{name:"num_proc",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L366",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Xe=new T({props:{anchor:"datasets.DownloadManager.extract.example",$$slots:{default:[pc]},$$scope:{ctx:k}}}),Mt=new j({props:{name:"iter_archive",anchor:"datasets.DownloadManager.iter_archive",parameters:[{name:"path_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_archive.path_or_buf",description:"<strong>path_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"path_or_buf"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L325"}}),Je=new T({props:{anchor:"datasets.DownloadManager.iter_archive.example",$$slots:{default:[cc]},$$scope:{ctx:k}}}),qt=new j({props:{name:"iter_files",anchor:"datasets.DownloadManager.iter_files",parameters:[{name:"paths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_files.paths",description:"<strong>paths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"paths"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L348"}}),Ke=new T({props:{anchor:"datasets.DownloadManager.iter_files.example",$$slots:{default:[mc]},$$scope:{ctx:k}}}),Ft=new j({props:{name:"ship_files_with_pipeline",anchor:"datasets.DownloadManager.ship_files_with_pipeline",parameters:[{name:"downloaded_path_or_paths",val:""},{name:"pipeline",val:""}],parametersDescription:[{anchor:"datasets.DownloadManager.ship_files_with_pipeline.downloaded_path_or_paths",description:`<strong>downloaded_path_or_paths</strong> (<code>str</code> or <code>list[str]</code> or <code>dict[str, str]</code>) &#x2014; Nested structure containing the
downloaded path(s).`,name:"downloaded_path_or_paths"},{anchor:"datasets.DownloadManager.ship_files_with_pipeline.pipeline",description:"<strong>pipeline</strong> (<code>utils.beam_utils.BeamPipeline</code>) &#x2014; Apache Beam Pipeline.",name:"pipeline"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L178",returnDescription:`
<p><code>str</code> or <code>list[str]</code> or <code>dict[str, str]</code></p>
`}}),zt=new j({props:{name:"class datasets.StreamingDownloadManager",anchor:"datasets.StreamingDownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/streaming_download_manager.py#L767"}}),Ut=new j({props:{name:"download",anchor:"datasets.StreamingDownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/streaming_download_manager.py#L793",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Qe=new T({props:{anchor:"datasets.StreamingDownloadManager.download.example",$$slots:{default:[gc]},$$scope:{ctx:k}}}),Gt=new j({props:{name:"download_and_extract",anchor:"datasets.StreamingDownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/streaming_download_manager.py#L861",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Ze=new T({props:{anchor:"datasets.StreamingDownloadManager.download_and_extract.example",$$slots:{default:[fc]},$$scope:{ctx:k}}}),Ht=new j({props:{name:"extract",anchor:"datasets.StreamingDownloadManager.extract",parameters:[{name:"path_or_paths",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/streaming_download_manager.py#L820",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),et=new T({props:{anchor:"datasets.StreamingDownloadManager.extract.example",$$slots:{default:[uc]},$$scope:{ctx:k}}}),Wt=new j({props:{name:"iter_archive",anchor:"datasets.StreamingDownloadManager.iter_archive",parameters:[{name:"urlpath_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.StreamingDownloadManager.iter_archive.urlpath_or_buf",description:"<strong>urlpath_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"urlpath_or_buf"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/streaming_download_manager.py#L879"}}),tt=new T({props:{anchor:"datasets.StreamingDownloadManager.iter_archive.example",$$slots:{default:[_c]},$$scope:{ctx:k}}}),Xt=new j({props:{name:"iter_files",anchor:"datasets.StreamingDownloadManager.iter_files",parameters:[{name:"urlpaths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.StreamingDownloadManager.iter_files.urlpaths",description:"<strong>urlpaths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"urlpaths"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/streaming_download_manager.py#L902"}}),at=new T({props:{anchor:"datasets.StreamingDownloadManager.iter_files.example",$$slots:{default:[hc]},$$scope:{ctx:k}}}),Jt=new j({props:{name:"class datasets.DownloadConfig",anchor:"datasets.DownloadConfig",parameters:[{name:"cache_dir",val:": typing.Union[str, pathlib.Path, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"resume_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"proxies",val:": typing.Optional[typing.Dict] = None"},{name:"user_agent",val:": typing.Optional[str] = None"},{name:"extract_compressed_file",val:": bool = False"},{name:"force_extract",val:": bool = False"},{name:"delete_extracted",val:": bool = False"},{name:"use_etag",val:": bool = True"},{name:"num_proc",val:": typing.Optional[int] = None"},{name:"max_retries",val:": int = 1"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"ignore_url_params",val:": bool = False"},{name:"download_desc",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.DownloadConfig.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>Path</code>, optional) &#x2014; Specify a cache directory to save the file to (overwrite the
default cache dir).`,name:"cache_dir"},{anchor:"datasets.DownloadConfig.force_download",description:`<strong>force_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, re-dowload the file even if it&#x2019;s already cached in
the cache dir.`,name:"force_download"},{anchor:"datasets.DownloadConfig.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, resume the download if incompletly recieved file is
found.`,name:"resume_download"},{anchor:"datasets.DownloadConfig.proxies",description:"<strong>proxies</strong> (<code>dict</code>, optional) &#x2014;",name:"proxies"},{anchor:"datasets.DownloadConfig.user_agent",description:`<strong>user_agent</strong> (<code>str</code>, optional) &#x2014; Optional string or dict that will be appended to the user-agent on remote
requests.`,name:"user_agent"},{anchor:"datasets.DownloadConfig.extract_compressed_file",description:`<strong>extract_compressed_file</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True and the path point to a zip or tar file,
extract the compressed file in a folder along the archive.`,name:"extract_compressed_file"},{anchor:"datasets.DownloadConfig.force_extract",description:`<strong>force_extract</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True when extract_compressed_file is True and the archive
was already extracted, re-extract the archive and override the folder where it was extracted.`,name:"force_extract"},{anchor:"datasets.DownloadConfig.delete_extracted",description:"<strong>delete_extracted</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to delete (or keep) the extracted files.",name:"delete_extracted"},{anchor:"datasets.DownloadConfig.use_etag",description:"<strong>use_etag</strong> (<code>bool</code>, default <code>True</code>) &#x2014; Whether to use the ETag HTTP response header to validate the cached files.",name:"use_etag"},{anchor:"datasets.DownloadConfig.num_proc",description:"<strong>num_proc</strong> (<code>int</code>, optional) &#x2014; The number of processes to launch to download the files in parallel.",name:"num_proc"},{anchor:"datasets.DownloadConfig.max_retries",description:"<strong>max_retries</strong> (<code>int</code>, default <code>1</code>) &#x2014; The number of times to retry an HTTP request if it fails.",name:"max_retries"},{anchor:"datasets.DownloadConfig.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token
for remote files on the Datasets Hub. If True, will get token from ~/.huggingface.`,name:"use_auth_token"},{anchor:"datasets.DownloadConfig.ignore_url_params",description:`<strong>ignore_url_params</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to strip all query parameters and #fragments from
the download URL before using it for caching the file.`,name:"ignore_url_params"},{anchor:"datasets.DownloadConfig.download_desc",description:"<strong>download_desc</strong> (<code>str</code>, optional) &#x2014; A description to be displayed alongside with the progress bar while downloading the files.",name:"download_desc"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_config.py#L8"}}),Kt=new j({props:{name:"class datasets.DownloadMode",anchor:"datasets.DownloadMode",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/download/download_manager.py#L39"}}),Zt=new Nr({}),ea=new j({props:{name:"class datasets.SplitGenerator",anchor:"datasets.SplitGenerator",parameters:[{name:"name",val:": str"},{name:"gen_kwargs",val:": typing.Dict = <factory>"}],parametersDescription:[{anchor:"datasets.SplitGenerator.name",description:`<strong>name</strong> (str) &#x2014; Name of the Split for which the generator will
create the examples.
**gen_kwargs &#x2014; Keyword arguments to forward to the <code>DatasetBuilder._generate_examples</code> method
of the builder.`,name:"name"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/splits.py#L566"}}),rt=new T({props:{anchor:"datasets.SplitGenerator.example",$$slots:{default:[$c]},$$scope:{ctx:k}}}),ta=new j({props:{name:"class datasets.Split",anchor:"datasets.Split",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/splits.py#L387"}}),nt=new T({props:{anchor:"datasets.Split.example",$$slots:{default:[vc]},$$scope:{ctx:k}}}),sa=new j({props:{name:"class datasets.NamedSplit",anchor:"datasets.NamedSplit",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/splits.py#L303"}}),ot=new T({props:{anchor:"datasets.NamedSplit.example",$$slots:{default:[bc]},$$scope:{ctx:k}}}),lt=new T({props:{anchor:"datasets.NamedSplit.example-2",$$slots:{default:[xc]},$$scope:{ctx:k}}}),dt=new T({props:{anchor:"datasets.NamedSplit.example-3",$$slots:{default:[wc]},$$scope:{ctx:k}}}),it=new T({props:{anchor:"datasets.NamedSplit.example-4",$$slots:{default:[Ec]},$$scope:{ctx:k}}}),ra=new j({props:{name:"class datasets.NamedSplitAll",anchor:"datasets.NamedSplitAll",parameters:[],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/splits.py#L372"}}),na=new j({props:{name:"class datasets.ReadInstruction",anchor:"datasets.ReadInstruction",parameters:[{name:"split_name",val:""},{name:"rounding",val:" = None"},{name:"from_",val:" = None"},{name:"to",val:" = None"},{name:"unit",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/arrow_reader.py#L457"}}),pt=new T({props:{anchor:"datasets.ReadInstruction.example",$$slots:{default:[yc]},$$scope:{ctx:k}}}),oa=new j({props:{name:"from_spec",anchor:"datasets.ReadInstruction.from_spec",parameters:[{name:"spec",val:""}],parametersDescription:[{anchor:"datasets.ReadInstruction.from_spec.spec",description:`<strong>spec</strong> (str) &#x2014; split(s) + optional slice(s) to read + optional rounding
if percents are used as the slicing unit. A slice can be specified,
using absolute numbers (int) or percentages (int). E.g.
<code>test</code>: test split.
<code>test + validation</code>: test split + validation split.
<code>test[10:]</code>: test split, minus its first 10 records.
<code>test[:10%]</code>: first 10% records of test split.
<code>test[:20%](pct1_dropremainder)</code>: first 10% records, rounded with
the <code>pct1_dropremainder</code> rounding.
<code>test[:-5%]+train[40%:60%]</code>: first 95% of test + middle 20% of
train.`,name:"spec"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/arrow_reader.py#L537",returnDescription:`
<p>ReadInstruction instance.</p>
`}}),la=new j({props:{name:"to_absolute",anchor:"datasets.ReadInstruction.to_absolute",parameters:[{name:"name2len",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/arrow_reader.py#L605",returnDescription:`
<p>list of _AbsoluteInstruction instances (corresponds to the + in spec).</p>
`}}),da=new Nr({}),ia=new j({props:{name:"class datasets.Version",anchor:"datasets.Version",parameters:[{name:"version_str",val:": str"},{name:"description",val:": typing.Optional[str] = None"},{name:"major",val:": typing.Union[str, int, NoneType] = None"},{name:"minor",val:": typing.Union[str, int, NoneType] = None"},{name:"patch",val:": typing.Union[str, int, NoneType] = None"}],parametersDescription:[{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.major",description:"<strong>major</strong> (<code>str</code>) &#x2014;",name:"major"},{anchor:"datasets.Version.minor",description:"<strong>minor</strong> (<code>str</code>) &#x2014;",name:"minor"},{anchor:"datasets.Version.patch",description:"<strong>patch</strong> (<code>str</code>) &#x2014;",name:"patch"}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/utils/version.py#L30"}}),gt=new T({props:{anchor:"datasets.Version.example",$$slots:{default:[Dc]},$$scope:{ctx:k}}}),pa=new j({props:{name:"match",anchor:"datasets.Version.match",parameters:[{name:"other_version",val:""}],source:"https://github.com/huggingface/datasets/blob/r_4429/src/datasets/utils/version.py#L101"}}),{c(){d=n("meta"),_=p(),f=n("h1"),r=n("a"),u=n("span"),v(t.$$.fragment),i=p(),Wa=n("span"),hn=m("Builder classes"),Br=p(),he=n("h2"),Le=n("a"),Xa=n("span"),v(vt.$$.fragment),$n=p(),Ja=n("span"),vn=m("Builders"),Pr=p(),J=n("p"),bn=m("\u{1F917} Datasets relies on two main classes during the dataset building process: "),ua=n("a"),xn=m("DatasetBuilder"),wn=m(" and "),_a=n("a"),En=m("BuilderConfig"),yn=m("."),Rr=p(),A=n("div"),v(bt.$$.fragment),Dn=p(),K=n("div"),v(xt.$$.fragment),kn=p(),Ka=n("p"),jn=m("Return a Dataset for the specified split."),Tn=p(),v(Ve.$$.fragment),Sn=p(),Y=n("div"),v(wt.$$.fragment),In=p(),Ya=n("p"),Nn=m("Downloads and prepares dataset for reading."),Bn=p(),v(Me.$$.fragment),Pn=p(),Q=n("div"),v(Et.$$.fragment),Rn=p(),Qa=n("p"),An=m("Empty dict if doesn\u2019t exist"),Cn=p(),v(qe.$$.fragment),On=p(),Z=n("div"),v(yt.$$.fragment),Ln=p(),Za=n("p"),Vn=m("Empty DatasetInfo if doesn\u2019t exist"),Mn=p(),v(Fe.$$.fragment),qn=p(),ze=n("div"),v(Dt.$$.fragment),Fn=p(),es=n("p"),zn=m("Return the path of the module of this class or subclass."),Ar=p(),W=n("div"),v(kt.$$.fragment),Un=p(),ts=n("p"),Gn=m("Base class for datasets with data generation based on dict generators."),Hn=p(),ee=n("p"),as=n("code"),Wn=m("GeneratorBasedBuilder"),Xn=m(` is a convenience class that abstracts away much
of the data writing and reading of `),ss=n("code"),Jn=m("DatasetBuilder"),Kn=m(`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),rs=n("code"),Yn=m("_split_generators"),Qn=m("). See the method docstrings for details."),Cr=p(),$e=n("div"),v(jt.$$.fragment),Zn=p(),ns=n("p"),eo=m("Beam based Builder."),Or=p(),ve=n("div"),v(Tt.$$.fragment),to=p(),os=n("p"),ao=m("Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Lr=p(),z=n("div"),v(St.$$.fragment),so=p(),It=n("p"),ro=m("Base class for "),ha=n("a"),no=m("DatasetBuilder"),oo=m(" data configuration."),lo=p(),Nt=n("p"),io=m(`DatasetBuilder subclasses with data configuration options should subclass
`),$a=n("a"),po=m("BuilderConfig"),co=m(" and add their own properties."),mo=p(),te=n("div"),v(Bt.$$.fragment),go=p(),ls=n("p"),fo=m(`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),uo=p(),be=n("ul"),ds=n("li"),_o=m("the config kwargs that can be used to overwrite attributes"),ho=p(),is=n("li"),$o=m("the custom features used to write the dataset"),vo=p(),ps=n("li"),bo=m(`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),Vr=p(),xe=n("h2"),Ue=n("a"),cs=n("span"),v(Pt.$$.fragment),xo=p(),ms=n("span"),wo=m("Download"),Mr=p(),N=n("div"),v(Rt.$$.fragment),Eo=p(),ae=n("div"),v(At.$$.fragment),yo=p(),gs=n("p"),Do=m("Download given url(s)."),ko=p(),v(Ge.$$.fragment),jo=p(),se=n("div"),v(Ct.$$.fragment),To=p(),fs=n("p"),So=m("Download and extract given url_or_urls."),Io=p(),v(He.$$.fragment),No=p(),re=n("div"),v(Ot.$$.fragment),Bo=p(),Lt=n("p"),Po=m("Download given urls(s) by calling "),us=n("code"),Ro=m("custom_download"),Ao=m("."),Co=p(),v(We.$$.fragment),Oo=p(),ne=n("div"),v(Vt.$$.fragment),Lo=p(),_s=n("p"),Vo=m("Extract given path(s)."),Mo=p(),v(Xe.$$.fragment),qo=p(),oe=n("div"),v(Mt.$$.fragment),Fo=p(),hs=n("p"),zo=m("Iterate over files within an archive."),Uo=p(),v(Je.$$.fragment),Go=p(),le=n("div"),v(qt.$$.fragment),Ho=p(),$s=n("p"),Wo=m("Iterate over file paths."),Xo=p(),v(Ke.$$.fragment),Jo=p(),Ye=n("div"),v(Ft.$$.fragment),Ko=p(),vs=n("p"),Yo=m("Ship the files using Beam FileSystems to the pipeline temp dir."),qr=p(),P=n("div"),v(zt.$$.fragment),Qo=p(),U=n("p"),Zo=m(`Download manager that uses the \u201D::\u201D separator to navigate through (possibly remote) compressed archives.
Contrary to the regular DownloadManager, the `),bs=n("code"),el=m("download"),tl=m(" and "),xs=n("code"),al=m("extract"),sl=m(` methods don\u2019t actually download nor extract
data, but they rather return the path or url that could be opened using the `),ws=n("code"),rl=m("xopen"),nl=m(` function which extends the
builtin `),Es=n("code"),ol=m("open"),ll=m(" function to stream data from remote files."),dl=p(),de=n("div"),v(Ut.$$.fragment),il=p(),ys=n("p"),pl=m("Download given url(s)."),cl=p(),v(Qe.$$.fragment),ml=p(),ie=n("div"),v(Gt.$$.fragment),gl=p(),Ds=n("p"),fl=m("Download and extract given url_or_urls."),ul=p(),v(Ze.$$.fragment),_l=p(),pe=n("div"),v(Ht.$$.fragment),hl=p(),ks=n("p"),$l=m("Extract given path(s)."),vl=p(),v(et.$$.fragment),bl=p(),ce=n("div"),v(Wt.$$.fragment),xl=p(),js=n("p"),wl=m("Iterate over files within an archive."),El=p(),v(tt.$$.fragment),yl=p(),me=n("div"),v(Xt.$$.fragment),Dl=p(),Ts=n("p"),kl=m("Iterate over files."),jl=p(),v(at.$$.fragment),Fr=p(),we=n("div"),v(Jt.$$.fragment),Tl=p(),Ss=n("p"),Sl=m("Configuration for our cached path manager."),zr=p(),L=n("div"),v(Kt.$$.fragment),Il=p(),va=n("p"),Is=n("code"),Nl=m("Enum"),Bl=m(" for how to treat pre-existing downloads and data."),Pl=p(),Yt=n("p"),Rl=m("The default mode is "),Ns=n("code"),Al=m("REUSE_DATASET_IF_EXISTS"),Cl=m(`, which will reuse both
raw downloads and the prepared dataset if they exist.`),Ol=p(),Bs=n("p"),Ll=m("The generations modes:"),Vl=p(),Qt=n("table"),Ps=n("thead"),Ee=n("tr"),Ur=n("th"),Ml=p(),Rs=n("th"),ql=m("Downloads"),Fl=p(),As=n("th"),zl=m("Dataset"),Ul=p(),ye=n("tbody"),De=n("tr"),ba=n("td"),Cs=n("code"),Gl=m("REUSE_DATASET_IF_EXISTS"),Hl=m(" (default)"),Wl=p(),Os=n("td"),Xl=m("Reuse"),Jl=p(),Ls=n("td"),Kl=m("Reuse"),Yl=p(),ke=n("tr"),Vs=n("td"),Ms=n("code"),Ql=m("REUSE_CACHE_IF_EXISTS"),Zl=p(),qs=n("td"),ed=m("Reuse"),td=p(),Fs=n("td"),ad=m("Fresh"),sd=p(),je=n("tr"),zs=n("td"),Us=n("code"),rd=m("FORCE_REDOWNLOAD"),nd=p(),Gs=n("td"),od=m("Fresh"),ld=p(),Hs=n("td"),dd=m("Fresh"),Gr=p(),Te=n("h2"),st=n("a"),Ws=n("span"),v(Zt.$$.fragment),id=p(),Xs=n("span"),pd=m("Splits"),Hr=p(),G=n("div"),v(ea.$$.fragment),cd=p(),Js=n("p"),md=m("Defines the split information for the generator."),gd=p(),Se=n("p"),fd=m(`This should be used as returned value of
`),Ks=n("code"),ud=m("GeneratorBasedBuilder._split_generators()"),_d=m(`.
See `),Ys=n("code"),hd=m("GeneratorBasedBuilder._split_generators()"),$d=m(` for more info and example
of usage.`),vd=p(),v(rt.$$.fragment),Wr=p(),R=n("div"),v(ta.$$.fragment),bd=p(),xa=n("p"),Qs=n("code"),xd=m("Enum"),wd=m(" for dataset splits."),Ed=p(),Zs=n("p"),yd=m(`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),Dd=p(),X=n("ul"),wa=n("li"),er=n("code"),kd=m("TRAIN"),jd=m(": the training data."),Td=p(),Ea=n("li"),tr=n("code"),Sd=m("VALIDATION"),Id=m(`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Nd=p(),ya=n("li"),ar=n("code"),Bd=m("TEST"),Pd=m(`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Rd=p(),Da=n("li"),sr=n("code"),Ad=m("ALL"),Cd=m(": the union of all defined dataset splits."),Od=p(),ka=n("p"),Ld=m("Note: All splits, including compositions inherit from "),rr=n("code"),Vd=m("datasets.SplitBase"),Md=p(),aa=n("p"),qd=m("See the :doc:"),nr=n("code"),Fd=m("guide on splits </loading>"),zd=m(" for more information."),Ud=p(),v(nt.$$.fragment),Xr=p(),B=n("div"),v(sa.$$.fragment),Gd=p(),or=n("p"),Hd=m("Descriptor corresponding to a named split (train, test, \u2026)."),Wd=p(),v(ot.$$.fragment),Xd=p(),lr=n("p"),Jd=m("Warning:"),Kd=p(),v(lt.$$.fragment),Yd=p(),dr=n("p"),Qd=m("Warning:"),Zd=p(),v(dt.$$.fragment),ei=p(),v(it.$$.fragment),Jr=p(),Ie=n("div"),v(ra.$$.fragment),ti=p(),ir=n("p"),ai=m("Split corresponding to the union of all defined dataset splits."),Kr=p(),V=n("div"),v(na.$$.fragment),si=p(),pr=n("p"),ri=m("Reading instruction for a dataset."),ni=p(),v(pt.$$.fragment),oi=p(),ct=n("div"),v(oa.$$.fragment),li=p(),cr=n("p"),di=m("Creates a ReadInstruction instance out of a string spec."),ii=p(),ge=n("div"),v(la.$$.fragment),pi=p(),mr=n("p"),ci=m("Translate instruction into a list of absolute instructions."),mi=p(),gr=n("p"),gi=m("Those absolute instructions are then to be added together."),Yr=p(),Ne=n("h2"),mt=n("a"),fr=n("span"),v(da.$$.fragment),fi=p(),ur=n("span"),ui=m("Version"),Qr=p(),H=n("div"),v(ia.$$.fragment),_i=p(),_r=n("p"),hi=m("Dataset version MAJOR.MINOR.PATCH."),$i=p(),v(gt.$$.fragment),vi=p(),ft=n("div"),v(pa.$$.fragment),bi=p(),hr=n("p"),xi=m("Returns True if other_version matches."),this.h()},l(a){const h=tc('[data-svelte="svelte-1phssyn"]',document.head);d=o(h,"META",{name:!0,content:!0}),h.forEach(s),_=c(a),f=o(a,"H1",{class:!0});var ca=l(f);r=o(ca,"A",{id:!0,class:!0,href:!0});var $r=l(r);u=o($r,"SPAN",{});var vr=l(u);b(t.$$.fragment,vr),vr.forEach(s),$r.forEach(s),i=c(ca),Wa=o(ca,"SPAN",{});var br=l(Wa);hn=g(br,"Builder classes"),br.forEach(s),ca.forEach(s),Br=c(a),he=o(a,"H2",{class:!0});var ma=l(he);Le=o(ma,"A",{id:!0,class:!0,href:!0});var xr=l(Le);Xa=o(xr,"SPAN",{});var wr=l(Xa);b(vt.$$.fragment,wr),wr.forEach(s),xr.forEach(s),$n=c(ma),Ja=o(ma,"SPAN",{});var Er=l(Ja);vn=g(Er,"Builders"),Er.forEach(s),ma.forEach(s),Pr=c(a),J=o(a,"P",{});var Be=l(J);bn=g(Be,"\u{1F917} Datasets relies on two main classes during the dataset building process: "),ua=o(Be,"A",{href:!0});var yr=l(ua);xn=g(yr,"DatasetBuilder"),yr.forEach(s),wn=g(Be," and "),_a=o(Be,"A",{href:!0});var Dr=l(_a);En=g(Dr,"BuilderConfig"),Dr.forEach(s),yn=g(Be,"."),Be.forEach(s),Rr=c(a),A=o(a,"DIV",{class:!0});var M=l(A);b(bt.$$.fragment,M),Dn=c(M),K=o(M,"DIV",{class:!0});var Pe=l(K);b(xt.$$.fragment,Pe),kn=c(Pe),Ka=o(Pe,"P",{});var kr=l(Ka);jn=g(kr,"Return a Dataset for the specified split."),kr.forEach(s),Tn=c(Pe),b(Ve.$$.fragment,Pe),Pe.forEach(s),Sn=c(M),Y=o(M,"DIV",{class:!0});var Re=l(Y);b(wt.$$.fragment,Re),In=c(Re),Ya=o(Re,"P",{});var jr=l(Ya);Nn=g(jr,"Downloads and prepares dataset for reading."),jr.forEach(s),Bn=c(Re),b(Me.$$.fragment,Re),Re.forEach(s),Pn=c(M),Q=o(M,"DIV",{class:!0});var Ae=l(Q);b(Et.$$.fragment,Ae),Rn=c(Ae),Qa=o(Ae,"P",{});var Tr=l(Qa);An=g(Tr,"Empty dict if doesn\u2019t exist"),Tr.forEach(s),Cn=c(Ae),b(qe.$$.fragment,Ae),Ae.forEach(s),On=c(M),Z=o(M,"DIV",{class:!0});var Ce=l(Z);b(yt.$$.fragment,Ce),Ln=c(Ce),Za=o(Ce,"P",{});var Sr=l(Za);Vn=g(Sr,"Empty DatasetInfo if doesn\u2019t exist"),Sr.forEach(s),Mn=c(Ce),b(Fe.$$.fragment,Ce),Ce.forEach(s),qn=c(M),ze=o(M,"DIV",{class:!0});var ga=l(ze);b(Dt.$$.fragment,ga),Fn=c(ga),es=o(ga,"P",{});var Ir=l(es);zn=g(Ir,"Return the path of the module of this class or subclass."),Ir.forEach(s),ga.forEach(s),M.forEach(s),Ar=c(a),W=o(a,"DIV",{class:!0});var Oe=l(W);b(kt.$$.fragment,Oe),Un=c(Oe),ts=o(Oe,"P",{});var Ii=l(ts);Gn=g(Ii,"Base class for datasets with data generation based on dict generators."),Ii.forEach(s),Hn=c(Oe),ee=o(Oe,"P",{});var fa=l(ee);as=o(fa,"CODE",{});var Ni=l(as);Wn=g(Ni,"GeneratorBasedBuilder"),Ni.forEach(s),Xn=g(fa,` is a convenience class that abstracts away much
of the data writing and reading of `),ss=o(fa,"CODE",{});var Bi=l(ss);Jn=g(Bi,"DatasetBuilder"),Bi.forEach(s),Kn=g(fa,`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),rs=o(fa,"CODE",{});var Pi=l(rs);Yn=g(Pi,"_split_generators"),Pi.forEach(s),Qn=g(fa,"). See the method docstrings for details."),fa.forEach(s),Oe.forEach(s),Cr=c(a),$e=o(a,"DIV",{class:!0});var en=l($e);b(jt.$$.fragment,en),Zn=c(en),ns=o(en,"P",{});var Ri=l(ns);eo=g(Ri,"Beam based Builder."),Ri.forEach(s),en.forEach(s),Or=c(a),ve=o(a,"DIV",{class:!0});var tn=l(ve);b(Tt.$$.fragment,tn),to=c(tn),os=o(tn,"P",{});var Ai=l(os);ao=g(Ai,"Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Ai.forEach(s),tn.forEach(s),Lr=c(a),z=o(a,"DIV",{class:!0});var ut=l(z);b(St.$$.fragment,ut),so=c(ut),It=o(ut,"P",{});var an=l(It);ro=g(an,"Base class for "),ha=o(an,"A",{href:!0});var Ci=l(ha);no=g(Ci,"DatasetBuilder"),Ci.forEach(s),oo=g(an," data configuration."),an.forEach(s),lo=c(ut),Nt=o(ut,"P",{});var sn=l(Nt);io=g(sn,`DatasetBuilder subclasses with data configuration options should subclass
`),$a=o(sn,"A",{href:!0});var Oi=l($a);po=g(Oi,"BuilderConfig"),Oi.forEach(s),co=g(sn," and add their own properties."),sn.forEach(s),mo=c(ut),te=o(ut,"DIV",{class:!0});var ja=l(te);b(Bt.$$.fragment,ja),go=c(ja),ls=o(ja,"P",{});var Li=l(ls);fo=g(Li,`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),Li.forEach(s),uo=c(ja),be=o(ja,"UL",{});var Ta=l(be);ds=o(Ta,"LI",{});var Vi=l(ds);_o=g(Vi,"the config kwargs that can be used to overwrite attributes"),Vi.forEach(s),ho=c(Ta),is=o(Ta,"LI",{});var Mi=l(is);$o=g(Mi,"the custom features used to write the dataset"),Mi.forEach(s),vo=c(Ta),ps=o(Ta,"LI",{});var qi=l(ps);bo=g(qi,`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),qi.forEach(s),Ta.forEach(s),ja.forEach(s),ut.forEach(s),Vr=c(a),xe=o(a,"H2",{class:!0});var rn=l(xe);Ue=o(rn,"A",{id:!0,class:!0,href:!0});var Fi=l(Ue);cs=o(Fi,"SPAN",{});var zi=l(cs);b(Pt.$$.fragment,zi),zi.forEach(s),Fi.forEach(s),xo=c(rn),ms=o(rn,"SPAN",{});var Ui=l(ms);wo=g(Ui,"Download"),Ui.forEach(s),rn.forEach(s),Mr=c(a),N=o(a,"DIV",{class:!0});var C=l(N);b(Rt.$$.fragment,C),Eo=c(C),ae=o(C,"DIV",{class:!0});var Sa=l(ae);b(At.$$.fragment,Sa),yo=c(Sa),gs=o(Sa,"P",{});var Gi=l(gs);Do=g(Gi,"Download given url(s)."),Gi.forEach(s),ko=c(Sa),b(Ge.$$.fragment,Sa),Sa.forEach(s),jo=c(C),se=o(C,"DIV",{class:!0});var Ia=l(se);b(Ct.$$.fragment,Ia),To=c(Ia),fs=o(Ia,"P",{});var Hi=l(fs);So=g(Hi,"Download and extract given url_or_urls."),Hi.forEach(s),Io=c(Ia),b(He.$$.fragment,Ia),Ia.forEach(s),No=c(C),re=o(C,"DIV",{class:!0});var Na=l(re);b(Ot.$$.fragment,Na),Bo=c(Na),Lt=o(Na,"P",{});var nn=l(Lt);Po=g(nn,"Download given urls(s) by calling "),us=o(nn,"CODE",{});var Wi=l(us);Ro=g(Wi,"custom_download"),Wi.forEach(s),Ao=g(nn,"."),nn.forEach(s),Co=c(Na),b(We.$$.fragment,Na),Na.forEach(s),Oo=c(C),ne=o(C,"DIV",{class:!0});var Ba=l(ne);b(Vt.$$.fragment,Ba),Lo=c(Ba),_s=o(Ba,"P",{});var Xi=l(_s);Vo=g(Xi,"Extract given path(s)."),Xi.forEach(s),Mo=c(Ba),b(Xe.$$.fragment,Ba),Ba.forEach(s),qo=c(C),oe=o(C,"DIV",{class:!0});var Pa=l(oe);b(Mt.$$.fragment,Pa),Fo=c(Pa),hs=o(Pa,"P",{});var Ji=l(hs);zo=g(Ji,"Iterate over files within an archive."),Ji.forEach(s),Uo=c(Pa),b(Je.$$.fragment,Pa),Pa.forEach(s),Go=c(C),le=o(C,"DIV",{class:!0});var Ra=l(le);b(qt.$$.fragment,Ra),Ho=c(Ra),$s=o(Ra,"P",{});var Ki=l($s);Wo=g(Ki,"Iterate over file paths."),Ki.forEach(s),Xo=c(Ra),b(Ke.$$.fragment,Ra),Ra.forEach(s),Jo=c(C),Ye=o(C,"DIV",{class:!0});var on=l(Ye);b(Ft.$$.fragment,on),Ko=c(on),vs=o(on,"P",{});var Yi=l(vs);Yo=g(Yi,"Ship the files using Beam FileSystems to the pipeline temp dir."),Yi.forEach(s),on.forEach(s),C.forEach(s),qr=c(a),P=o(a,"DIV",{class:!0});var q=l(P);b(zt.$$.fragment,q),Qo=c(q),U=o(q,"P",{});var fe=l(U);Zo=g(fe,`Download manager that uses the \u201D::\u201D separator to navigate through (possibly remote) compressed archives.
Contrary to the regular DownloadManager, the `),bs=o(fe,"CODE",{});var Qi=l(bs);el=g(Qi,"download"),Qi.forEach(s),tl=g(fe," and "),xs=o(fe,"CODE",{});var Zi=l(xs);al=g(Zi,"extract"),Zi.forEach(s),sl=g(fe,` methods don\u2019t actually download nor extract
data, but they rather return the path or url that could be opened using the `),ws=o(fe,"CODE",{});var ep=l(ws);rl=g(ep,"xopen"),ep.forEach(s),nl=g(fe,` function which extends the
builtin `),Es=o(fe,"CODE",{});var tp=l(Es);ol=g(tp,"open"),tp.forEach(s),ll=g(fe," function to stream data from remote files."),fe.forEach(s),dl=c(q),de=o(q,"DIV",{class:!0});var Aa=l(de);b(Ut.$$.fragment,Aa),il=c(Aa),ys=o(Aa,"P",{});var ap=l(ys);pl=g(ap,"Download given url(s)."),ap.forEach(s),cl=c(Aa),b(Qe.$$.fragment,Aa),Aa.forEach(s),ml=c(q),ie=o(q,"DIV",{class:!0});var Ca=l(ie);b(Gt.$$.fragment,Ca),gl=c(Ca),Ds=o(Ca,"P",{});var sp=l(Ds);fl=g(sp,"Download and extract given url_or_urls."),sp.forEach(s),ul=c(Ca),b(Ze.$$.fragment,Ca),Ca.forEach(s),_l=c(q),pe=o(q,"DIV",{class:!0});var Oa=l(pe);b(Ht.$$.fragment,Oa),hl=c(Oa),ks=o(Oa,"P",{});var rp=l(ks);$l=g(rp,"Extract given path(s)."),rp.forEach(s),vl=c(Oa),b(et.$$.fragment,Oa),Oa.forEach(s),bl=c(q),ce=o(q,"DIV",{class:!0});var La=l(ce);b(Wt.$$.fragment,La),xl=c(La),js=o(La,"P",{});var np=l(js);wl=g(np,"Iterate over files within an archive."),np.forEach(s),El=c(La),b(tt.$$.fragment,La),La.forEach(s),yl=c(q),me=o(q,"DIV",{class:!0});var Va=l(me);b(Xt.$$.fragment,Va),Dl=c(Va),Ts=o(Va,"P",{});var op=l(Ts);kl=g(op,"Iterate over files."),op.forEach(s),jl=c(Va),b(at.$$.fragment,Va),Va.forEach(s),q.forEach(s),Fr=c(a),we=o(a,"DIV",{class:!0});var ln=l(we);b(Jt.$$.fragment,ln),Tl=c(ln),Ss=o(ln,"P",{});var lp=l(Ss);Sl=g(lp,"Configuration for our cached path manager."),lp.forEach(s),ln.forEach(s),zr=c(a),L=o(a,"DIV",{class:!0});var ue=l(L);b(Kt.$$.fragment,ue),Il=c(ue),va=o(ue,"P",{});var wi=l(va);Is=o(wi,"CODE",{});var dp=l(Is);Nl=g(dp,"Enum"),dp.forEach(s),Bl=g(wi," for how to treat pre-existing downloads and data."),wi.forEach(s),Pl=c(ue),Yt=o(ue,"P",{});var dn=l(Yt);Rl=g(dn,"The default mode is "),Ns=o(dn,"CODE",{});var ip=l(Ns);Al=g(ip,"REUSE_DATASET_IF_EXISTS"),ip.forEach(s),Cl=g(dn,`, which will reuse both
raw downloads and the prepared dataset if they exist.`),dn.forEach(s),Ol=c(ue),Bs=o(ue,"P",{});var pp=l(Bs);Ll=g(pp,"The generations modes:"),pp.forEach(s),Vl=c(ue),Qt=o(ue,"TABLE",{});var pn=l(Qt);Ps=o(pn,"THEAD",{});var cp=l(Ps);Ee=o(cp,"TR",{});var Ma=l(Ee);Ur=o(Ma,"TH",{}),l(Ur).forEach(s),Ml=c(Ma),Rs=o(Ma,"TH",{});var mp=l(Rs);ql=g(mp,"Downloads"),mp.forEach(s),Fl=c(Ma),As=o(Ma,"TH",{});var gp=l(As);zl=g(gp,"Dataset"),gp.forEach(s),Ma.forEach(s),cp.forEach(s),Ul=c(pn),ye=o(pn,"TBODY",{});var qa=l(ye);De=o(qa,"TR",{});var Fa=l(De);ba=o(Fa,"TD",{});var Ei=l(ba);Cs=o(Ei,"CODE",{});var fp=l(Cs);Gl=g(fp,"REUSE_DATASET_IF_EXISTS"),fp.forEach(s),Hl=g(Ei," (default)"),Ei.forEach(s),Wl=c(Fa),Os=o(Fa,"TD",{});var up=l(Os);Xl=g(up,"Reuse"),up.forEach(s),Jl=c(Fa),Ls=o(Fa,"TD",{});var _p=l(Ls);Kl=g(_p,"Reuse"),_p.forEach(s),Fa.forEach(s),Yl=c(qa),ke=o(qa,"TR",{});var za=l(ke);Vs=o(za,"TD",{});var hp=l(Vs);Ms=o(hp,"CODE",{});var $p=l(Ms);Ql=g($p,"REUSE_CACHE_IF_EXISTS"),$p.forEach(s),hp.forEach(s),Zl=c(za),qs=o(za,"TD",{});var vp=l(qs);ed=g(vp,"Reuse"),vp.forEach(s),td=c(za),Fs=o(za,"TD",{});var bp=l(Fs);ad=g(bp,"Fresh"),bp.forEach(s),za.forEach(s),sd=c(qa),je=o(qa,"TR",{});var Ua=l(je);zs=o(Ua,"TD",{});var xp=l(zs);Us=o(xp,"CODE",{});var wp=l(Us);rd=g(wp,"FORCE_REDOWNLOAD"),wp.forEach(s),xp.forEach(s),nd=c(Ua),Gs=o(Ua,"TD",{});var Ep=l(Gs);od=g(Ep,"Fresh"),Ep.forEach(s),ld=c(Ua),Hs=o(Ua,"TD",{});var yp=l(Hs);dd=g(yp,"Fresh"),yp.forEach(s),Ua.forEach(s),qa.forEach(s),pn.forEach(s),ue.forEach(s),Gr=c(a),Te=o(a,"H2",{class:!0});var cn=l(Te);st=o(cn,"A",{id:!0,class:!0,href:!0});var Dp=l(st);Ws=o(Dp,"SPAN",{});var kp=l(Ws);b(Zt.$$.fragment,kp),kp.forEach(s),Dp.forEach(s),id=c(cn),Xs=o(cn,"SPAN",{});var jp=l(Xs);pd=g(jp,"Splits"),jp.forEach(s),cn.forEach(s),Hr=c(a),G=o(a,"DIV",{class:!0});var _t=l(G);b(ea.$$.fragment,_t),cd=c(_t),Js=o(_t,"P",{});var Tp=l(Js);md=g(Tp,"Defines the split information for the generator."),Tp.forEach(s),gd=c(_t),Se=o(_t,"P",{});var Ga=l(Se);fd=g(Ga,`This should be used as returned value of
`),Ks=o(Ga,"CODE",{});var Sp=l(Ks);ud=g(Sp,"GeneratorBasedBuilder._split_generators()"),Sp.forEach(s),_d=g(Ga,`.
See `),Ys=o(Ga,"CODE",{});var Ip=l(Ys);hd=g(Ip,"GeneratorBasedBuilder._split_generators()"),Ip.forEach(s),$d=g(Ga,` for more info and example
of usage.`),Ga.forEach(s),vd=c(_t),b(rt.$$.fragment,_t),_t.forEach(s),Wr=c(a),R=o(a,"DIV",{class:!0});var F=l(R);b(ta.$$.fragment,F),bd=c(F),xa=o(F,"P",{});var yi=l(xa);Qs=o(yi,"CODE",{});var Np=l(Qs);xd=g(Np,"Enum"),Np.forEach(s),wd=g(yi," for dataset splits."),yi.forEach(s),Ed=c(F),Zs=o(F,"P",{});var Bp=l(Zs);yd=g(Bp,`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),Bp.forEach(s),Dd=c(F),X=o(F,"UL",{});var ht=l(X);wa=o(ht,"LI",{});var Di=l(wa);er=o(Di,"CODE",{});var Pp=l(er);kd=g(Pp,"TRAIN"),Pp.forEach(s),jd=g(Di,": the training data."),Di.forEach(s),Td=c(ht),Ea=o(ht,"LI",{});var ki=l(Ea);tr=o(ki,"CODE",{});var Rp=l(tr);Sd=g(Rp,"VALIDATION"),Rp.forEach(s),Id=g(ki,`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),ki.forEach(s),Nd=c(ht),ya=o(ht,"LI",{});var ji=l(ya);ar=o(ji,"CODE",{});var Ap=l(ar);Bd=g(Ap,"TEST"),Ap.forEach(s),Pd=g(ji,`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),ji.forEach(s),Rd=c(ht),Da=o(ht,"LI",{});var Ti=l(Da);sr=o(Ti,"CODE",{});var Cp=l(sr);Ad=g(Cp,"ALL"),Cp.forEach(s),Cd=g(Ti,": the union of all defined dataset splits."),Ti.forEach(s),ht.forEach(s),Od=c(F),ka=o(F,"P",{});var Si=l(ka);Ld=g(Si,"Note: All splits, including compositions inherit from "),rr=o(Si,"CODE",{});var Op=l(rr);Vd=g(Op,"datasets.SplitBase"),Op.forEach(s),Si.forEach(s),Md=c(F),aa=o(F,"P",{});var mn=l(aa);qd=g(mn,"See the :doc:"),nr=o(mn,"CODE",{});var Lp=l(nr);Fd=g(Lp,"guide on splits </loading>"),Lp.forEach(s),zd=g(mn," for more information."),mn.forEach(s),Ud=c(F),b(nt.$$.fragment,F),F.forEach(s),Xr=c(a),B=o(a,"DIV",{class:!0});var O=l(B);b(sa.$$.fragment,O),Gd=c(O),or=o(O,"P",{});var Vp=l(or);Hd=g(Vp,"Descriptor corresponding to a named split (train, test, \u2026)."),Vp.forEach(s),Wd=c(O),b(ot.$$.fragment,O),Xd=c(O),lr=o(O,"P",{});var Mp=l(lr);Jd=g(Mp,"Warning:"),Mp.forEach(s),Kd=c(O),b(lt.$$.fragment,O),Yd=c(O),dr=o(O,"P",{});var qp=l(dr);Qd=g(qp,"Warning:"),qp.forEach(s),Zd=c(O),b(dt.$$.fragment,O),ei=c(O),b(it.$$.fragment,O),O.forEach(s),Jr=c(a),Ie=o(a,"DIV",{class:!0});var gn=l(Ie);b(ra.$$.fragment,gn),ti=c(gn),ir=o(gn,"P",{});var Fp=l(ir);ai=g(Fp,"Split corresponding to the union of all defined dataset splits."),Fp.forEach(s),gn.forEach(s),Kr=c(a),V=o(a,"DIV",{class:!0});var _e=l(V);b(na.$$.fragment,_e),si=c(_e),pr=o(_e,"P",{});var zp=l(pr);ri=g(zp,"Reading instruction for a dataset."),zp.forEach(s),ni=c(_e),b(pt.$$.fragment,_e),oi=c(_e),ct=o(_e,"DIV",{class:!0});var fn=l(ct);b(oa.$$.fragment,fn),li=c(fn),cr=o(fn,"P",{});var Up=l(cr);di=g(Up,"Creates a ReadInstruction instance out of a string spec."),Up.forEach(s),fn.forEach(s),ii=c(_e),ge=o(_e,"DIV",{class:!0});var Ha=l(ge);b(la.$$.fragment,Ha),pi=c(Ha),mr=o(Ha,"P",{});var Gp=l(mr);ci=g(Gp,"Translate instruction into a list of absolute instructions."),Gp.forEach(s),mi=c(Ha),gr=o(Ha,"P",{});var Hp=l(gr);gi=g(Hp,"Those absolute instructions are then to be added together."),Hp.forEach(s),Ha.forEach(s),_e.forEach(s),Yr=c(a),Ne=o(a,"H2",{class:!0});var un=l(Ne);mt=o(un,"A",{id:!0,class:!0,href:!0});var Wp=l(mt);fr=o(Wp,"SPAN",{});var Xp=l(fr);b(da.$$.fragment,Xp),Xp.forEach(s),Wp.forEach(s),fi=c(un),ur=o(un,"SPAN",{});var Jp=l(ur);ui=g(Jp,"Version"),Jp.forEach(s),un.forEach(s),Qr=c(a),H=o(a,"DIV",{class:!0});var $t=l(H);b(ia.$$.fragment,$t),_i=c($t),_r=o($t,"P",{});var Kp=l(_r);hi=g(Kp,"Dataset version MAJOR.MINOR.PATCH."),Kp.forEach(s),$i=c($t),b(gt.$$.fragment,$t),vi=c($t),ft=o($t,"DIV",{class:!0});var _n=l(ft);b(pa.$$.fragment,_n),bi=c(_n),hr=o(_n,"P",{});var Yp=l(hr);xi=g(Yp,"Returns True if other_version matches."),Yp.forEach(s),_n.forEach(s),$t.forEach(s),this.h()},h(){D(d,"name","hf:doc:metadata"),D(d,"content",JSON.stringify(jc)),D(r,"id","builder-classes"),D(r,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(r,"href","#builder-classes"),D(f,"class","relative group"),D(Le,"id","datasets.DatasetBuilder"),D(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(Le,"href","#datasets.DatasetBuilder"),D(he,"class","relative group"),D(ua,"href","/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.DatasetBuilder"),D(_a,"href","/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.BuilderConfig"),D(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ha,"href","/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.DatasetBuilder"),D($a,"href","/docs/datasets/pr_4429/en/package_reference/builder_classes#datasets.BuilderConfig"),D(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Ue,"id","datasets.DownloadManager"),D(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(Ue,"href","#datasets.DownloadManager"),D(xe,"class","relative group"),D(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(st,"id","datasets.SplitGenerator"),D(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(st,"href","#datasets.SplitGenerator"),D(Te,"class","relative group"),D(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(mt,"id","datasets.Version"),D(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(mt,"href","#datasets.Version"),D(Ne,"class","relative group"),D(ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(a,h){e(document.head,d),$(a,_,h),$(a,f,h),e(f,r),e(r,u),x(t,u,null),e(f,i),e(f,Wa),e(Wa,hn),$(a,Br,h),$(a,he,h),e(he,Le),e(Le,Xa),x(vt,Xa,null),e(he,$n),e(he,Ja),e(Ja,vn),$(a,Pr,h),$(a,J,h),e(J,bn),e(J,ua),e(ua,xn),e(J,wn),e(J,_a),e(_a,En),e(J,yn),$(a,Rr,h),$(a,A,h),x(bt,A,null),e(A,Dn),e(A,K),x(xt,K,null),e(K,kn),e(K,Ka),e(Ka,jn),e(K,Tn),x(Ve,K,null),e(A,Sn),e(A,Y),x(wt,Y,null),e(Y,In),e(Y,Ya),e(Ya,Nn),e(Y,Bn),x(Me,Y,null),e(A,Pn),e(A,Q),x(Et,Q,null),e(Q,Rn),e(Q,Qa),e(Qa,An),e(Q,Cn),x(qe,Q,null),e(A,On),e(A,Z),x(yt,Z,null),e(Z,Ln),e(Z,Za),e(Za,Vn),e(Z,Mn),x(Fe,Z,null),e(A,qn),e(A,ze),x(Dt,ze,null),e(ze,Fn),e(ze,es),e(es,zn),$(a,Ar,h),$(a,W,h),x(kt,W,null),e(W,Un),e(W,ts),e(ts,Gn),e(W,Hn),e(W,ee),e(ee,as),e(as,Wn),e(ee,Xn),e(ee,ss),e(ss,Jn),e(ee,Kn),e(ee,rs),e(rs,Yn),e(ee,Qn),$(a,Cr,h),$(a,$e,h),x(jt,$e,null),e($e,Zn),e($e,ns),e(ns,eo),$(a,Or,h),$(a,ve,h),x(Tt,ve,null),e(ve,to),e(ve,os),e(os,ao),$(a,Lr,h),$(a,z,h),x(St,z,null),e(z,so),e(z,It),e(It,ro),e(It,ha),e(ha,no),e(It,oo),e(z,lo),e(z,Nt),e(Nt,io),e(Nt,$a),e($a,po),e(Nt,co),e(z,mo),e(z,te),x(Bt,te,null),e(te,go),e(te,ls),e(ls,fo),e(te,uo),e(te,be),e(be,ds),e(ds,_o),e(be,ho),e(be,is),e(is,$o),e(be,vo),e(be,ps),e(ps,bo),$(a,Vr,h),$(a,xe,h),e(xe,Ue),e(Ue,cs),x(Pt,cs,null),e(xe,xo),e(xe,ms),e(ms,wo),$(a,Mr,h),$(a,N,h),x(Rt,N,null),e(N,Eo),e(N,ae),x(At,ae,null),e(ae,yo),e(ae,gs),e(gs,Do),e(ae,ko),x(Ge,ae,null),e(N,jo),e(N,se),x(Ct,se,null),e(se,To),e(se,fs),e(fs,So),e(se,Io),x(He,se,null),e(N,No),e(N,re),x(Ot,re,null),e(re,Bo),e(re,Lt),e(Lt,Po),e(Lt,us),e(us,Ro),e(Lt,Ao),e(re,Co),x(We,re,null),e(N,Oo),e(N,ne),x(Vt,ne,null),e(ne,Lo),e(ne,_s),e(_s,Vo),e(ne,Mo),x(Xe,ne,null),e(N,qo),e(N,oe),x(Mt,oe,null),e(oe,Fo),e(oe,hs),e(hs,zo),e(oe,Uo),x(Je,oe,null),e(N,Go),e(N,le),x(qt,le,null),e(le,Ho),e(le,$s),e($s,Wo),e(le,Xo),x(Ke,le,null),e(N,Jo),e(N,Ye),x(Ft,Ye,null),e(Ye,Ko),e(Ye,vs),e(vs,Yo),$(a,qr,h),$(a,P,h),x(zt,P,null),e(P,Qo),e(P,U),e(U,Zo),e(U,bs),e(bs,el),e(U,tl),e(U,xs),e(xs,al),e(U,sl),e(U,ws),e(ws,rl),e(U,nl),e(U,Es),e(Es,ol),e(U,ll),e(P,dl),e(P,de),x(Ut,de,null),e(de,il),e(de,ys),e(ys,pl),e(de,cl),x(Qe,de,null),e(P,ml),e(P,ie),x(Gt,ie,null),e(ie,gl),e(ie,Ds),e(Ds,fl),e(ie,ul),x(Ze,ie,null),e(P,_l),e(P,pe),x(Ht,pe,null),e(pe,hl),e(pe,ks),e(ks,$l),e(pe,vl),x(et,pe,null),e(P,bl),e(P,ce),x(Wt,ce,null),e(ce,xl),e(ce,js),e(js,wl),e(ce,El),x(tt,ce,null),e(P,yl),e(P,me),x(Xt,me,null),e(me,Dl),e(me,Ts),e(Ts,kl),e(me,jl),x(at,me,null),$(a,Fr,h),$(a,we,h),x(Jt,we,null),e(we,Tl),e(we,Ss),e(Ss,Sl),$(a,zr,h),$(a,L,h),x(Kt,L,null),e(L,Il),e(L,va),e(va,Is),e(Is,Nl),e(va,Bl),e(L,Pl),e(L,Yt),e(Yt,Rl),e(Yt,Ns),e(Ns,Al),e(Yt,Cl),e(L,Ol),e(L,Bs),e(Bs,Ll),e(L,Vl),e(L,Qt),e(Qt,Ps),e(Ps,Ee),e(Ee,Ur),e(Ee,Ml),e(Ee,Rs),e(Rs,ql),e(Ee,Fl),e(Ee,As),e(As,zl),e(Qt,Ul),e(Qt,ye),e(ye,De),e(De,ba),e(ba,Cs),e(Cs,Gl),e(ba,Hl),e(De,Wl),e(De,Os),e(Os,Xl),e(De,Jl),e(De,Ls),e(Ls,Kl),e(ye,Yl),e(ye,ke),e(ke,Vs),e(Vs,Ms),e(Ms,Ql),e(ke,Zl),e(ke,qs),e(qs,ed),e(ke,td),e(ke,Fs),e(Fs,ad),e(ye,sd),e(ye,je),e(je,zs),e(zs,Us),e(Us,rd),e(je,nd),e(je,Gs),e(Gs,od),e(je,ld),e(je,Hs),e(Hs,dd),$(a,Gr,h),$(a,Te,h),e(Te,st),e(st,Ws),x(Zt,Ws,null),e(Te,id),e(Te,Xs),e(Xs,pd),$(a,Hr,h),$(a,G,h),x(ea,G,null),e(G,cd),e(G,Js),e(Js,md),e(G,gd),e(G,Se),e(Se,fd),e(Se,Ks),e(Ks,ud),e(Se,_d),e(Se,Ys),e(Ys,hd),e(Se,$d),e(G,vd),x(rt,G,null),$(a,Wr,h),$(a,R,h),x(ta,R,null),e(R,bd),e(R,xa),e(xa,Qs),e(Qs,xd),e(xa,wd),e(R,Ed),e(R,Zs),e(Zs,yd),e(R,Dd),e(R,X),e(X,wa),e(wa,er),e(er,kd),e(wa,jd),e(X,Td),e(X,Ea),e(Ea,tr),e(tr,Sd),e(Ea,Id),e(X,Nd),e(X,ya),e(ya,ar),e(ar,Bd),e(ya,Pd),e(X,Rd),e(X,Da),e(Da,sr),e(sr,Ad),e(Da,Cd),e(R,Od),e(R,ka),e(ka,Ld),e(ka,rr),e(rr,Vd),e(R,Md),e(R,aa),e(aa,qd),e(aa,nr),e(nr,Fd),e(aa,zd),e(R,Ud),x(nt,R,null),$(a,Xr,h),$(a,B,h),x(sa,B,null),e(B,Gd),e(B,or),e(or,Hd),e(B,Wd),x(ot,B,null),e(B,Xd),e(B,lr),e(lr,Jd),e(B,Kd),x(lt,B,null),e(B,Yd),e(B,dr),e(dr,Qd),e(B,Zd),x(dt,B,null),e(B,ei),x(it,B,null),$(a,Jr,h),$(a,Ie,h),x(ra,Ie,null),e(Ie,ti),e(Ie,ir),e(ir,ai),$(a,Kr,h),$(a,V,h),x(na,V,null),e(V,si),e(V,pr),e(pr,ri),e(V,ni),x(pt,V,null),e(V,oi),e(V,ct),x(oa,ct,null),e(ct,li),e(ct,cr),e(cr,di),e(V,ii),e(V,ge),x(la,ge,null),e(ge,pi),e(ge,mr),e(mr,ci),e(ge,mi),e(ge,gr),e(gr,gi),$(a,Yr,h),$(a,Ne,h),e(Ne,mt),e(mt,fr),x(da,fr,null),e(Ne,fi),e(Ne,ur),e(ur,ui),$(a,Qr,h),$(a,H,h),x(ia,H,null),e(H,_i),e(H,_r),e(_r,hi),e(H,$i),x(gt,H,null),e(H,vi),e(H,ft),x(pa,ft,null),e(ft,bi),e(ft,hr),e(hr,xi),Zr=!0},p(a,[h]){const ca={};h&2&&(ca.$$scope={dirty:h,ctx:a}),Ve.$set(ca);const $r={};h&2&&($r.$$scope={dirty:h,ctx:a}),Me.$set($r);const vr={};h&2&&(vr.$$scope={dirty:h,ctx:a}),qe.$set(vr);const br={};h&2&&(br.$$scope={dirty:h,ctx:a}),Fe.$set(br);const ma={};h&2&&(ma.$$scope={dirty:h,ctx:a}),Ge.$set(ma);const xr={};h&2&&(xr.$$scope={dirty:h,ctx:a}),He.$set(xr);const wr={};h&2&&(wr.$$scope={dirty:h,ctx:a}),We.$set(wr);const Er={};h&2&&(Er.$$scope={dirty:h,ctx:a}),Xe.$set(Er);const Be={};h&2&&(Be.$$scope={dirty:h,ctx:a}),Je.$set(Be);const yr={};h&2&&(yr.$$scope={dirty:h,ctx:a}),Ke.$set(yr);const Dr={};h&2&&(Dr.$$scope={dirty:h,ctx:a}),Qe.$set(Dr);const M={};h&2&&(M.$$scope={dirty:h,ctx:a}),Ze.$set(M);const Pe={};h&2&&(Pe.$$scope={dirty:h,ctx:a}),et.$set(Pe);const kr={};h&2&&(kr.$$scope={dirty:h,ctx:a}),tt.$set(kr);const Re={};h&2&&(Re.$$scope={dirty:h,ctx:a}),at.$set(Re);const jr={};h&2&&(jr.$$scope={dirty:h,ctx:a}),rt.$set(jr);const Ae={};h&2&&(Ae.$$scope={dirty:h,ctx:a}),nt.$set(Ae);const Tr={};h&2&&(Tr.$$scope={dirty:h,ctx:a}),ot.$set(Tr);const Ce={};h&2&&(Ce.$$scope={dirty:h,ctx:a}),lt.$set(Ce);const Sr={};h&2&&(Sr.$$scope={dirty:h,ctx:a}),dt.$set(Sr);const ga={};h&2&&(ga.$$scope={dirty:h,ctx:a}),it.$set(ga);const Ir={};h&2&&(Ir.$$scope={dirty:h,ctx:a}),pt.$set(Ir);const Oe={};h&2&&(Oe.$$scope={dirty:h,ctx:a}),gt.$set(Oe)},i(a){Zr||(w(t.$$.fragment,a),w(vt.$$.fragment,a),w(bt.$$.fragment,a),w(xt.$$.fragment,a),w(Ve.$$.fragment,a),w(wt.$$.fragment,a),w(Me.$$.fragment,a),w(Et.$$.fragment,a),w(qe.$$.fragment,a),w(yt.$$.fragment,a),w(Fe.$$.fragment,a),w(Dt.$$.fragment,a),w(kt.$$.fragment,a),w(jt.$$.fragment,a),w(Tt.$$.fragment,a),w(St.$$.fragment,a),w(Bt.$$.fragment,a),w(Pt.$$.fragment,a),w(Rt.$$.fragment,a),w(At.$$.fragment,a),w(Ge.$$.fragment,a),w(Ct.$$.fragment,a),w(He.$$.fragment,a),w(Ot.$$.fragment,a),w(We.$$.fragment,a),w(Vt.$$.fragment,a),w(Xe.$$.fragment,a),w(Mt.$$.fragment,a),w(Je.$$.fragment,a),w(qt.$$.fragment,a),w(Ke.$$.fragment,a),w(Ft.$$.fragment,a),w(zt.$$.fragment,a),w(Ut.$$.fragment,a),w(Qe.$$.fragment,a),w(Gt.$$.fragment,a),w(Ze.$$.fragment,a),w(Ht.$$.fragment,a),w(et.$$.fragment,a),w(Wt.$$.fragment,a),w(tt.$$.fragment,a),w(Xt.$$.fragment,a),w(at.$$.fragment,a),w(Jt.$$.fragment,a),w(Kt.$$.fragment,a),w(Zt.$$.fragment,a),w(ea.$$.fragment,a),w(rt.$$.fragment,a),w(ta.$$.fragment,a),w(nt.$$.fragment,a),w(sa.$$.fragment,a),w(ot.$$.fragment,a),w(lt.$$.fragment,a),w(dt.$$.fragment,a),w(it.$$.fragment,a),w(ra.$$.fragment,a),w(na.$$.fragment,a),w(pt.$$.fragment,a),w(oa.$$.fragment,a),w(la.$$.fragment,a),w(da.$$.fragment,a),w(ia.$$.fragment,a),w(gt.$$.fragment,a),w(pa.$$.fragment,a),Zr=!0)},o(a){E(t.$$.fragment,a),E(vt.$$.fragment,a),E(bt.$$.fragment,a),E(xt.$$.fragment,a),E(Ve.$$.fragment,a),E(wt.$$.fragment,a),E(Me.$$.fragment,a),E(Et.$$.fragment,a),E(qe.$$.fragment,a),E(yt.$$.fragment,a),E(Fe.$$.fragment,a),E(Dt.$$.fragment,a),E(kt.$$.fragment,a),E(jt.$$.fragment,a),E(Tt.$$.fragment,a),E(St.$$.fragment,a),E(Bt.$$.fragment,a),E(Pt.$$.fragment,a),E(Rt.$$.fragment,a),E(At.$$.fragment,a),E(Ge.$$.fragment,a),E(Ct.$$.fragment,a),E(He.$$.fragment,a),E(Ot.$$.fragment,a),E(We.$$.fragment,a),E(Vt.$$.fragment,a),E(Xe.$$.fragment,a),E(Mt.$$.fragment,a),E(Je.$$.fragment,a),E(qt.$$.fragment,a),E(Ke.$$.fragment,a),E(Ft.$$.fragment,a),E(zt.$$.fragment,a),E(Ut.$$.fragment,a),E(Qe.$$.fragment,a),E(Gt.$$.fragment,a),E(Ze.$$.fragment,a),E(Ht.$$.fragment,a),E(et.$$.fragment,a),E(Wt.$$.fragment,a),E(tt.$$.fragment,a),E(Xt.$$.fragment,a),E(at.$$.fragment,a),E(Jt.$$.fragment,a),E(Kt.$$.fragment,a),E(Zt.$$.fragment,a),E(ea.$$.fragment,a),E(rt.$$.fragment,a),E(ta.$$.fragment,a),E(nt.$$.fragment,a),E(sa.$$.fragment,a),E(ot.$$.fragment,a),E(lt.$$.fragment,a),E(dt.$$.fragment,a),E(it.$$.fragment,a),E(ra.$$.fragment,a),E(na.$$.fragment,a),E(pt.$$.fragment,a),E(oa.$$.fragment,a),E(la.$$.fragment,a),E(da.$$.fragment,a),E(ia.$$.fragment,a),E(gt.$$.fragment,a),E(pa.$$.fragment,a),Zr=!1},d(a){s(d),a&&s(_),a&&s(f),y(t),a&&s(Br),a&&s(he),y(vt),a&&s(Pr),a&&s(J),a&&s(Rr),a&&s(A),y(bt),y(xt),y(Ve),y(wt),y(Me),y(Et),y(qe),y(yt),y(Fe),y(Dt),a&&s(Ar),a&&s(W),y(kt),a&&s(Cr),a&&s($e),y(jt),a&&s(Or),a&&s(ve),y(Tt),a&&s(Lr),a&&s(z),y(St),y(Bt),a&&s(Vr),a&&s(xe),y(Pt),a&&s(Mr),a&&s(N),y(Rt),y(At),y(Ge),y(Ct),y(He),y(Ot),y(We),y(Vt),y(Xe),y(Mt),y(Je),y(qt),y(Ke),y(Ft),a&&s(qr),a&&s(P),y(zt),y(Ut),y(Qe),y(Gt),y(Ze),y(Ht),y(et),y(Wt),y(tt),y(Xt),y(at),a&&s(Fr),a&&s(we),y(Jt),a&&s(zr),a&&s(L),y(Kt),a&&s(Gr),a&&s(Te),y(Zt),a&&s(Hr),a&&s(G),y(ea),y(rt),a&&s(Wr),a&&s(R),y(ta),y(nt),a&&s(Xr),a&&s(B),y(sa),y(ot),y(lt),y(dt),y(it),a&&s(Jr),a&&s(Ie),y(ra),a&&s(Kr),a&&s(V),y(na),y(pt),y(oa),y(la),a&&s(Yr),a&&s(Ne),y(da),a&&s(Qr),a&&s(H),y(ia),y(gt),y(pa)}}}const jc={local:"builder-classes",sections:[{local:"datasets.DatasetBuilder",title:"Builders"},{local:"datasets.DownloadManager",title:"Download"},{local:"datasets.SplitGenerator",title:"Splits"},{local:"datasets.Version",title:"Version"}],title:"Builder classes"};function Tc(k){return ac(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Rc extends Qp{constructor(d){super();Zp(this,d,Tc,kc,ec,{})}}export{Rc as default,jc as metadata};
