import{S as Rr,i as Fr,s as Sr,e as t,k as c,w as u,t as n,M as Ur,c as l,d as a,m,a as r,x as g,h as o,b as h,N as ua,G as e,g as i,y as d,q as b,o as x,B as _,v as Mr}from"../chunks/vendor-hf-doc-builder.js";import{T as Jr}from"../chunks/Tip-hf-doc-builder.js";import{I as ga}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as k}from"../chunks/CodeBlock-hf-doc-builder.js";function Vr(da){let f,V,j,C,G,v,bs,N,D,H,T;return{c(){f=t("p"),V=n("Feel free to use other data augmentation libraries like "),j=t("a"),C=n("Albumentations"),G=n(", "),v=t("a"),bs=n("Kornia"),N=n(", and "),D=t("a"),H=n("imgaug"),T=n("."),this.h()},l(R){f=l(R,"P",{});var y=r(f);V=o(y,"Feel free to use other data augmentation libraries like "),j=l(y,"A",{href:!0,rel:!0});var P=r(j);C=o(P,"Albumentations"),P.forEach(a),G=o(y,", "),v=l(y,"A",{href:!0,rel:!0});var z=r(v);bs=o(z,"Kornia"),z.forEach(a),N=o(y,", and "),D=l(y,"A",{href:!0,rel:!0});var Ws=r(D);H=o(Ws,"imgaug"),Ws.forEach(a),T=o(y,"."),y.forEach(a),this.h()},h(){h(j,"href","https://albumentations.ai/docs/"),h(j,"rel","nofollow"),h(v,"href","https://kornia.readthedocs.io/en/latest/"),h(v,"rel","nofollow"),h(D,"href","https://imgaug.readthedocs.io/en/latest/"),h(D,"rel","nofollow")},m(R,y){i(R,f,y),e(f,V),e(f,j),e(j,C),e(f,G),e(f,v),e(v,bs),e(f,N),e(f,D),e(D,H),e(f,T)},d(R){R&&a(f)}}}function Hr(da){let f,V,j,C,G,v,bs,N,D,H,T,R,y,P,z,Ws,Xs,Nt,Rt,Ft,xs,St,Zs,Ut,Mt,ba,Y,Jt,_s,Vt,Ht,xa,F,K,Ce,js,Yt,Ie,Kt,_a,Q,Qt,se,Wt,Xt,ja,W,Zt,vs,De,sl,el,va,ys,ya,$,al,ee,tl,ll,Pe,nl,ol,Ae,rl,pl,Te,il,cl,wa,ws,$a,X,ml,ae,hl,fl,Ea,$s,te,ul,gl,ka,Z,le,ne,ze,dl,bl,xl,oe,re,Oe,_l,jl,Ca,ss,vl,pe,yl,wl,Ia,S,es,Le,Es,$l,qe,El,Da,ie,kl,Pa,U,as,Be,ks,Cl,Ge,Il,Aa,ts,Dl,Cs,Pl,Al,Ta,ls,za,ns,Tl,Is,Ne,zl,Ol,Oa,Ds,La,os,Ll,Re,ql,Bl,qa,Ps,Ba,rs,Gl,ce,Nl,Rl,Ga,As,Na,ps,Fl,Fe,Sl,Ul,Ra,Ts,Fa,M,me,go,Ml,he,bo,Sa,J,is,Se,zs,Jl,Ue,Vl,Ua,O,Hl,Os,Yl,Kl,Ls,Ql,Wl,Ma,L,Xl,Me,Zl,sn,Je,en,an,Ja,qs,Va,cs,tn,Bs,Ve,ln,nn,Ha,fe,on,Ya,Gs,Ka,ue,rn,Qa,E,ge,He,pn,cn,mn,de,Ye,hn,fn,un,be,Ke,gn,dn,bn,xe,Qe,xn,_n,jn,Ns,We,vn,yn,A,_e,Xe,wn,$n,En,je,Ze,kn,Cn,In,ms,sa,Dn,Pn,Rs,An,Tn,zn,w,ea,On,Ln,aa,qn,Bn,ta,Gn,Nn,la,Rn,Fn,na,Sn,Un,oa,Mn,Wa,hs,Jn,ra,Vn,Hn,Xa,Fs,Za,Ss,pa,xo,st,fs,Yn,ia,Kn,Qn,et,us,Wn,ca,Xn,Zn,at,Us,tt,ve,so,lt,Ms,nt,Js,ma,_o,ot,ye,eo,rt,Vs,pt,gs,ao,we,to,lo,it,Hs,ct,$e,no,mt,Ys,ht,Ks,ha,jo,ft;return v=new ga({}),js=new ga({}),ys=new k({props:{code:`def transforms(examples):
    examples["pixel_values"] = [image.convert("RGB").resize((100,100)) for image in examples["image"]]
    return examples`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [image.convert(<span class="hljs-string">&quot;RGB&quot;</span>).resize((<span class="hljs-number">100</span>,<span class="hljs-number">100</span>)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`}}),ws=new k({props:{code:`dataset = dataset.map(transforms, remove_columns=["image"], batched=True)
dataset[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(transforms, remove_columns=[<span class="hljs-string">&quot;image&quot;</span>], batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">6</span>,
 <span class="hljs-string">&#x27;pixel_values&#x27;</span>: &lt;PIL.PngImagePlugin.PngImageFile image mode=RGB size=100x100 at <span class="hljs-number">0x7F058237BB10</span>&gt;}`}}),Es=new ga({}),ks=new ga({}),ls=new Jr({props:{$$slots:{default:[Vr]},$$scope:{ctx:da}}}),Ds=new k({props:{code:`from torchvision.transforms import Compose, ColorJitter, ToTensor

jitter = Compose(
    [
         ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.7),
         ToTensor(),
    ]
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> Compose, ColorJitter, ToTensor

<span class="hljs-meta">&gt;&gt;&gt; </span>jitter = Compose(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>         ColorJitter(brightness=<span class="hljs-number">0.25</span>, contrast=<span class="hljs-number">0.25</span>, saturation=<span class="hljs-number">0.25</span>, hue=<span class="hljs-number">0.7</span>),
<span class="hljs-meta">... </span>         ToTensor(),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)`}}),Ps=new k({props:{code:`def transforms(examples):
    examples["pixel_values"] = [jitter(image.convert("RGB")) for image in examples["image"]]
    return examples`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [jitter(image.convert(<span class="hljs-string">&quot;RGB&quot;</span>)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`}}),As=new k({props:{code:"dataset.set_transform(transforms)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_transform(transforms)'}}),Ts=new k({props:{code:`import numpy as np
import matplotlib.pyplot as plt

img = dataset[0]["pixel_values"]
plt.imshow(img.permute(1, 2, 0))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

<span class="hljs-meta">&gt;&gt;&gt; </span>img = dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;pixel_values&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>plt.imshow(img.permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>))`}}),zs=new ga({}),qs=new k({props:{code:"pip install -U albumentations opencv-python",highlighted:'pip <span class="hljs-keyword">install</span> -U albumentations opencv-python'}}),Gs=new k({props:{code:`
ds = load_dataset("cppe-5")
example = ds['train'][0]
example`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;cppe-5&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>example = ds[<span class="hljs-string">&#x27;train&#x27;</span>][<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>example
{<span class="hljs-string">&#x27;height&#x27;</span>: <span class="hljs-number">663</span>,
 <span class="hljs-string">&#x27;image&#x27;</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=943x663 at <span class="hljs-number">0x7FC3DC756250</span>&gt;,
 <span class="hljs-string">&#x27;image_id&#x27;</span>: <span class="hljs-number">15</span>,
 <span class="hljs-string">&#x27;objects&#x27;</span>: {<span class="hljs-string">&#x27;area&#x27;</span>: [<span class="hljs-number">3796</span>, <span class="hljs-number">1596</span>, <span class="hljs-number">152768</span>, <span class="hljs-number">81002</span>],
  <span class="hljs-string">&#x27;bbox&#x27;</span>: [[<span class="hljs-number">302.0</span>, <span class="hljs-number">109.0</span>, <span class="hljs-number">73.0</span>, <span class="hljs-number">52.0</span>],
   [<span class="hljs-number">810.0</span>, <span class="hljs-number">100.0</span>, <span class="hljs-number">57.0</span>, <span class="hljs-number">28.0</span>],
   [<span class="hljs-number">160.0</span>, <span class="hljs-number">31.0</span>, <span class="hljs-number">248.0</span>, <span class="hljs-number">616.0</span>],
   [<span class="hljs-number">741.0</span>, <span class="hljs-number">68.0</span>, <span class="hljs-number">202.0</span>, <span class="hljs-number">401.0</span>]],
  <span class="hljs-string">&#x27;category&#x27;</span>: [<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
  <span class="hljs-string">&#x27;id&#x27;</span>: [<span class="hljs-number">114</span>, <span class="hljs-number">115</span>, <span class="hljs-number">116</span>, <span class="hljs-number">117</span>]},
 <span class="hljs-string">&#x27;width&#x27;</span>: <span class="hljs-number">943</span>}`}}),Fs=new k({props:{code:`import torch
from torchvision.ops import box_convert
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import pil_to_tensor, to_pil_image

categories = ds['train'].features['objects'].feature['category']

boxes_xywh = torch.tensor(example['objects']['bbox'])
boxes_xyxy = box_convert(boxes_xywh, 'xywh', 'xyxy')
labels = [categories.int2str(x) for x in example['objects']['category']]
to_pil_image(
    draw_bounding_boxes(
        pil_to_tensor(example['image']),
        boxes_xyxy,
        colors="red",
        labels=labels,
    )
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.ops <span class="hljs-keyword">import</span> box_convert
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.utils <span class="hljs-keyword">import</span> draw_bounding_boxes
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms.functional <span class="hljs-keyword">import</span> pil_to_tensor, to_pil_image

<span class="hljs-meta">&gt;&gt;&gt; </span>categories = ds[<span class="hljs-string">&#x27;train&#x27;</span>].features[<span class="hljs-string">&#x27;objects&#x27;</span>].feature[<span class="hljs-string">&#x27;category&#x27;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>boxes_xywh = torch.tensor(example[<span class="hljs-string">&#x27;objects&#x27;</span>][<span class="hljs-string">&#x27;bbox&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes_xyxy = box_convert(boxes_xywh, <span class="hljs-string">&#x27;xywh&#x27;</span>, <span class="hljs-string">&#x27;xyxy&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = [categories.int2str(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example[<span class="hljs-string">&#x27;objects&#x27;</span>][<span class="hljs-string">&#x27;category&#x27;</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>to_pil_image(
<span class="hljs-meta">... </span>    draw_bounding_boxes(
<span class="hljs-meta">... </span>        pil_to_tensor(example[<span class="hljs-string">&#x27;image&#x27;</span>]),
<span class="hljs-meta">... </span>        boxes_xyxy,
<span class="hljs-meta">... </span>        colors=<span class="hljs-string">&quot;red&quot;</span>,
<span class="hljs-meta">... </span>        labels=labels,
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>)`}}),Us=new k({props:{code:`import albumentations as A
import numpy as np

transform = A.Compose([
    A.Resize(480, 480),
    A.HorizontalFlip(p=1.0),
    A.RandomBrightnessContrast(p=1.0),
], bbox_params=A.BboxParams(format='coco',  label_fields=['category']))

# RGB PIL Image -> BGR Numpy array
image = np.flip(np.array(example['image']), -1)
out = transform(
    image=image,
    bboxes=example['objects']['bbox'],
    category=example['objects']['category'],
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> albumentations <span class="hljs-keyword">as</span> A
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-meta">&gt;&gt;&gt; </span>transform = A.Compose([
<span class="hljs-meta">... </span>    A.Resize(<span class="hljs-number">480</span>, <span class="hljs-number">480</span>),
<span class="hljs-meta">... </span>    A.HorizontalFlip(p=<span class="hljs-number">1.0</span>),
<span class="hljs-meta">... </span>    A.RandomBrightnessContrast(p=<span class="hljs-number">1.0</span>),
<span class="hljs-meta">... </span>], bbox_params=A.BboxParams(<span class="hljs-built_in">format</span>=<span class="hljs-string">&#x27;coco&#x27;</span>,  label_fields=[<span class="hljs-string">&#x27;category&#x27;</span>]))

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># RGB PIL Image -&gt; BGR Numpy array</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = np.flip(np.array(example[<span class="hljs-string">&#x27;image&#x27;</span>]), -<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>out = transform(
<span class="hljs-meta">... </span>    image=image,
<span class="hljs-meta">... </span>    bboxes=example[<span class="hljs-string">&#x27;objects&#x27;</span>][<span class="hljs-string">&#x27;bbox&#x27;</span>],
<span class="hljs-meta">... </span>    category=example[<span class="hljs-string">&#x27;objects&#x27;</span>][<span class="hljs-string">&#x27;category&#x27;</span>],
<span class="hljs-meta">... </span>)`}}),Ms=new k({props:{code:`image = torch.tensor(out['image']).flip(-1).permute(2, 0, 1)
boxes_xywh = torch.stack([torch.tensor(x) for x in out['bboxes']])
boxes_xyxy = box_convert(boxes_xywh, 'xywh', 'xyxy')
labels = [categories.int2str(x) for x in out['category']]
to_pil_image(
    draw_bounding_boxes(
        image,
        boxes_xyxy,
        colors='red',
        labels=labels
    )
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>image = torch.tensor(out[<span class="hljs-string">&#x27;image&#x27;</span>]).flip(-<span class="hljs-number">1</span>).permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes_xywh = torch.stack([torch.tensor(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> out[<span class="hljs-string">&#x27;bboxes&#x27;</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes_xyxy = box_convert(boxes_xywh, <span class="hljs-string">&#x27;xywh&#x27;</span>, <span class="hljs-string">&#x27;xyxy&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = [categories.int2str(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> out[<span class="hljs-string">&#x27;category&#x27;</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>to_pil_image(
<span class="hljs-meta">... </span>    draw_bounding_boxes(
<span class="hljs-meta">... </span>        image,
<span class="hljs-meta">... </span>        boxes_xyxy,
<span class="hljs-meta">... </span>        colors=<span class="hljs-string">&#x27;red&#x27;</span>,
<span class="hljs-meta">... </span>        labels=labels
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>)`}}),Vs=new k({props:{code:`def transforms(examples):
    images, bboxes, categories = [], [], []
    for image, objects in zip(examples['image'], examples['objects']):
        image = np.array(image.convert("RGB"))[:, :, ::-1]
        out = transform(
            image=image,
            bboxes=objects['bbox'],
            category=objects['category']
        )
        images.append(torch.tensor(out['image']).flip(-1).permute(2, 0, 1))
        bboxes.append(torch.tensor(out['bboxes']))
        categories.append(out['category'])
    return {'image': images, 'bbox': bboxes, 'category': categories}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    images, bboxes, categories = [], [], []
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> image, objects <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(examples[<span class="hljs-string">&#x27;image&#x27;</span>], examples[<span class="hljs-string">&#x27;objects&#x27;</span>]):
<span class="hljs-meta">... </span>        image = np.array(image.convert(<span class="hljs-string">&quot;RGB&quot;</span>))[:, :, ::-<span class="hljs-number">1</span>]
<span class="hljs-meta">... </span>        out = transform(
<span class="hljs-meta">... </span>            image=image,
<span class="hljs-meta">... </span>            bboxes=objects[<span class="hljs-string">&#x27;bbox&#x27;</span>],
<span class="hljs-meta">... </span>            category=objects[<span class="hljs-string">&#x27;category&#x27;</span>]
<span class="hljs-meta">... </span>        )
<span class="hljs-meta">... </span>        images.append(torch.tensor(out[<span class="hljs-string">&#x27;image&#x27;</span>]).flip(-<span class="hljs-number">1</span>).permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">... </span>        bboxes.append(torch.tensor(out[<span class="hljs-string">&#x27;bboxes&#x27;</span>]))
<span class="hljs-meta">... </span>        categories.append(out[<span class="hljs-string">&#x27;category&#x27;</span>])
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> {<span class="hljs-string">&#x27;image&#x27;</span>: images, <span class="hljs-string">&#x27;bbox&#x27;</span>: bboxes, <span class="hljs-string">&#x27;category&#x27;</span>: categories}`}}),Hs=new k({props:{code:"ds['train'].set_transform(transforms)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>ds[<span class="hljs-string">&#x27;train&#x27;</span>].set_transform(transforms)'}}),Ys=new k({props:{code:`example = ds['train'][10]
to_pil_image(
    draw_bounding_boxes(
        example['image'],
        box_convert(example['bbox'], 'xywh', 'xyxy'),
        colors='red',
        labels=[categories.int2str(x) for x in example['category']]
    )
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>example = ds[<span class="hljs-string">&#x27;train&#x27;</span>][<span class="hljs-number">10</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>to_pil_image(
<span class="hljs-meta">... </span>    draw_bounding_boxes(
<span class="hljs-meta">... </span>        example[<span class="hljs-string">&#x27;image&#x27;</span>],
<span class="hljs-meta">... </span>        box_convert(example[<span class="hljs-string">&#x27;bbox&#x27;</span>], <span class="hljs-string">&#x27;xywh&#x27;</span>, <span class="hljs-string">&#x27;xyxy&#x27;</span>),
<span class="hljs-meta">... </span>        colors=<span class="hljs-string">&#x27;red&#x27;</span>,
<span class="hljs-meta">... </span>        labels=[categories.int2str(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example[<span class="hljs-string">&#x27;category&#x27;</span>]]
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>)`}}),{c(){f=t("meta"),V=c(),j=t("h1"),C=t("a"),G=t("span"),u(v.$$.fragment),bs=c(),N=t("span"),D=n("Process image data"),H=c(),T=t("p"),R=n("This guide shows specific methods for processing image datasets. Learn how to:"),y=c(),P=t("ul"),z=t("li"),Ws=n("Use "),Xs=t("a"),Nt=n("map()"),Rt=n(" with image dataset."),Ft=c(),xs=t("li"),St=n("Apply data augmentations to your dataset with "),Zs=t("a"),Ut=n("set_transform()"),Mt=n("."),ba=c(),Y=t("p"),Jt=n("For a guide on how to process any type of dataset, take a look at the "),_s=t("a"),Vt=n("general process guide"),Ht=n("."),xa=c(),F=t("h2"),K=t("a"),Ce=t("span"),u(js.$$.fragment),Yt=c(),Ie=t("span"),Kt=n("Map"),_a=c(),Q=t("p"),Qt=n("The "),se=t("a"),Wt=n("map()"),Xt=n(" function can apply transforms over an entire dataset."),ja=c(),W=t("p"),Zt=n("For example, create a basic "),vs=t("a"),De=t("code"),sl=n("Resize"),el=n(" function:"),va=c(),u(ys.$$.fragment),ya=c(),$=t("p"),al=n("Now use the "),ee=t("a"),tl=n("map()"),ll=n(" function to resize the entire dataset, and set "),Pe=t("code"),nl=n("batched=True"),ol=n(" to speed up the process by accepting batches of examples. The transform returns "),Ae=t("code"),rl=n("pixel_values"),pl=n(" as a cacheable "),Te=t("code"),il=n("PIL.Image"),cl=n(" object:"),wa=c(),u(ws.$$.fragment),$a=c(),X=t("p"),ml=n("The cache file saves time because you don\u2019t have to execute the same transform twice. The "),ae=t("a"),hl=n("map()"),fl=n(" function is best for operations you only run once per training - like resizing an image - instead of using it for operations executed for each epoch, like data augmentations."),Ea=c(),$s=t("p"),te=t("a"),ul=n("map()"),gl=n(" takes up some memory, but you can reduce its memory requirements with the following parameters:"),ka=c(),Z=t("ul"),le=t("li"),ne=t("a"),ze=t("code"),dl=n("batch_size"),bl=n(" determines the number of examples that are processed in one call to the transform function."),xl=c(),oe=t("li"),re=t("a"),Oe=t("code"),_l=n("writer_batch_size"),jl=n(" determines the number of processed examples that are kept in memory before they are stored away."),Ca=c(),ss=t("p"),vl=n("Both parameter values default to 1000, which can be expensive if you are storing images. Lower these values to use less memory when you use "),pe=t("a"),yl=n("map()"),wl=n("."),Ia=c(),S=t("h2"),es=t("a"),Le=t("span"),u(Es.$$.fragment),$l=c(),qe=t("span"),El=n("Data augmentation"),Da=c(),ie=t("p"),kl=n("\u{1F917} Datasets can apply data augmentations from any library or package to your dataset."),Pa=c(),U=t("h3"),as=t("a"),Be=t("span"),u(ks.$$.fragment),Cl=c(),Ge=t("span"),Il=n("Image Classification"),Aa=c(),ts=t("p"),Dl=n("First let\u2019s see how you can transform image classification datasets. This guide will use the transforms from "),Cs=t("a"),Pl=n("torchvision"),Al=n("."),Ta=c(),u(ls.$$.fragment),za=c(),ns=t("p"),Tl=n("As an example, try to apply a "),Is=t("a"),Ne=t("code"),zl=n("ColorJitter"),Ol=n(" transform to change the color properties of the image randomly:"),Oa=c(),u(Ds.$$.fragment),La=c(),os=t("p"),Ll=n("Create a function to apply the "),Re=t("code"),ql=n("ColorJitter"),Bl=n(" transform to an image:"),qa=c(),u(Ps.$$.fragment),Ba=c(),rs=t("p"),Gl=n("Use the "),ce=t("a"),Nl=n("set_transform()"),Rl=n(" function to apply the transform on-the-fly which consumes less disk space. This function is useful if you only need to access the examples once:"),Ga=c(),u(As.$$.fragment),Na=c(),ps=t("p"),Fl=n("Now you can take a look at the augmented image by indexing into the "),Fe=t("code"),Sl=n("pixel_values"),Ul=n(":"),Ra=c(),u(Ts.$$.fragment),Fa=c(),M=t("div"),me=t("img"),Ml=c(),he=t("img"),Sa=c(),J=t("h3"),is=t("a"),Se=t("span"),u(zs.$$.fragment),Jl=c(),Ue=t("span"),Vl=n("Object Detection"),Ua=c(),O=t("p"),Hl=n("Next, let\u2019s see how to apply transformations to object detection datasets. For this we\u2019ll use "),Os=t("a"),Yl=n("Albumentations"),Kl=n(", following their object detection "),Ls=t("a"),Ql=n("tutorial"),Wl=n("."),Ma=c(),L=t("p"),Xl=n("To run these examples, make sure you have up to date versions of "),Me=t("code"),Zl=n("albumentations"),sn=n(" and "),Je=t("code"),en=n("cv2"),an=n(" installed:"),Ja=c(),u(qs.$$.fragment),Va=c(),cs=t("p"),tn=n("In this example, the "),Bs=t("a"),Ve=t("code"),ln=n("cppe-5"),nn=n(" dataset is used, which is a dataset for identifying medical personal protective equipments (PPEs) in the context of the COVID-19 pandemic."),Ha=c(),fe=t("p"),on=n("You can load the dataset and take a look at an example:"),Ya=c(),u(Gs.$$.fragment),Ka=c(),ue=t("p"),rn=n("The dataset has the following fields:"),Qa=c(),E=t("ul"),ge=t("li"),He=t("code"),pn=n("image"),cn=n(": PIL.Image.Image object containing the image."),mn=c(),de=t("li"),Ye=t("code"),hn=n("image_id"),fn=n(": The image ID."),un=c(),be=t("li"),Ke=t("code"),gn=n("height"),dn=n(": The image height."),bn=c(),xe=t("li"),Qe=t("code"),xn=n("width"),_n=n(": The image width."),jn=c(),Ns=t("li"),We=t("code"),vn=n("objects"),yn=n(": a dictionary containing bounding box metadata for the objects present on the image"),A=t("ul"),_e=t("li"),Xe=t("code"),wn=n("id"),$n=n(": the annotation id"),En=c(),je=t("li"),Ze=t("code"),kn=n("area"),Cn=n(": the area of the bounding box"),In=c(),ms=t("li"),sa=t("code"),Dn=n("bbox"),Pn=n(": the object\u2019s bounding box (in the "),Rs=t("a"),An=n("coco"),Tn=n(" format)"),zn=c(),w=t("li"),ea=t("code"),On=n("category"),Ln=n(": the object\u2019s category, with possible values including "),aa=t("code"),qn=n("Coverall (0)"),Bn=n(", "),ta=t("code"),Gn=n("Face_Shield (1)"),Nn=n(", "),la=t("code"),Rn=n("Gloves (2)"),Fn=n(", "),na=t("code"),Sn=n("Goggles (3)"),Un=n(" and "),oa=t("code"),Mn=n("Mask (4)"),Wa=c(),hs=t("p"),Jn=n("You can visualize the bboxes on the image using some internal torch utilities. But, to do that, you will need to reference the "),ra=t("code"),Vn=n("datasets.ClassLabel"),Hn=n(" feature associated with the category IDs so you can look up the string labels."),Xa=c(),u(Fs.$$.fragment),Za=c(),Ss=t("div"),pa=t("img"),st=c(),fs=t("p"),Yn=n("Using "),ia=t("code"),Kn=n("albumentations"),Qn=n(", we can apply transforms that will effect the Image while also updating the bounding boxes accordingly. In this case, the image is resized to (480, 480), flipped horizontally, and brightened."),et=c(),us=t("p"),Wn=n("Note that "),ca=t("code"),Xn=n("albumentations"),Zn=n(" is expecting the image to be in BGR format, not RGB, so we\u2019ll have to convert our image first before applying the transform."),at=c(),u(Us.$$.fragment),tt=c(),ve=t("p"),so=n("Now when we visualize the result, the image should be flipped but the boxes should still be in the right places."),lt=c(),u(Ms.$$.fragment),nt=c(),Js=t("div"),ma=t("img"),ot=c(),ye=t("p"),eo=n("Create a function to apply the transform to a batch of examples:"),rt=c(),u(Vs.$$.fragment),pt=c(),gs=t("p"),ao=n("Use the "),we=t("a"),to=n("set_transform()"),lo=n(" function to apply the transform on-the-fly which consumes less disk space. Note that the randomness of data augmentation may return a different image if you access the same example twice. It is especially useful when training a model with several epochs."),it=c(),u(Hs.$$.fragment),ct=c(),$e=t("p"),no=n("Verify the transform is working by visualizing the 10th example:"),mt=c(),u(Ys.$$.fragment),ht=c(),Ks=t("div"),ha=t("img"),this.h()},l(s){const p=Ur('[data-svelte="svelte-1phssyn"]',document.head);f=l(p,"META",{name:!0,content:!0}),p.forEach(a),V=m(s),j=l(s,"H1",{class:!0});var Qs=r(j);C=l(Qs,"A",{id:!0,class:!0,href:!0});var vo=r(C);G=l(vo,"SPAN",{});var yo=r(G);g(v.$$.fragment,yo),yo.forEach(a),vo.forEach(a),bs=m(Qs),N=l(Qs,"SPAN",{});var wo=r(N);D=o(wo,"Process image data"),wo.forEach(a),Qs.forEach(a),H=m(s),T=l(s,"P",{});var $o=r(T);R=o($o,"This guide shows specific methods for processing image datasets. Learn how to:"),$o.forEach(a),y=m(s),P=l(s,"UL",{});var ut=r(P);z=l(ut,"LI",{});var gt=r(z);Ws=o(gt,"Use "),Xs=l(gt,"A",{href:!0});var Eo=r(Xs);Nt=o(Eo,"map()"),Eo.forEach(a),Rt=o(gt," with image dataset."),gt.forEach(a),Ft=m(ut),xs=l(ut,"LI",{});var dt=r(xs);St=o(dt,"Apply data augmentations to your dataset with "),Zs=l(dt,"A",{href:!0});var ko=r(Zs);Ut=o(ko,"set_transform()"),ko.forEach(a),Mt=o(dt,"."),dt.forEach(a),ut.forEach(a),ba=m(s),Y=l(s,"P",{});var bt=r(Y);Jt=o(bt,"For a guide on how to process any type of dataset, take a look at the "),_s=l(bt,"A",{class:!0,href:!0});var Co=r(_s);Vt=o(Co,"general process guide"),Co.forEach(a),Ht=o(bt,"."),bt.forEach(a),xa=m(s),F=l(s,"H2",{class:!0});var xt=r(F);K=l(xt,"A",{id:!0,class:!0,href:!0});var Io=r(K);Ce=l(Io,"SPAN",{});var Do=r(Ce);g(js.$$.fragment,Do),Do.forEach(a),Io.forEach(a),Yt=m(xt),Ie=l(xt,"SPAN",{});var Po=r(Ie);Kt=o(Po,"Map"),Po.forEach(a),xt.forEach(a),_a=m(s),Q=l(s,"P",{});var _t=r(Q);Qt=o(_t,"The "),se=l(_t,"A",{href:!0});var Ao=r(se);Wt=o(Ao,"map()"),Ao.forEach(a),Xt=o(_t," function can apply transforms over an entire dataset."),_t.forEach(a),ja=m(s),W=l(s,"P",{});var jt=r(W);Zt=o(jt,"For example, create a basic "),vs=l(jt,"A",{href:!0,rel:!0});var To=r(vs);De=l(To,"CODE",{});var zo=r(De);sl=o(zo,"Resize"),zo.forEach(a),To.forEach(a),el=o(jt," function:"),jt.forEach(a),va=m(s),g(ys.$$.fragment,s),ya=m(s),$=l(s,"P",{});var q=r($);al=o(q,"Now use the "),ee=l(q,"A",{href:!0});var Oo=r(ee);tl=o(Oo,"map()"),Oo.forEach(a),ll=o(q," function to resize the entire dataset, and set "),Pe=l(q,"CODE",{});var Lo=r(Pe);nl=o(Lo,"batched=True"),Lo.forEach(a),ol=o(q," to speed up the process by accepting batches of examples. The transform returns "),Ae=l(q,"CODE",{});var qo=r(Ae);rl=o(qo,"pixel_values"),qo.forEach(a),pl=o(q," as a cacheable "),Te=l(q,"CODE",{});var Bo=r(Te);il=o(Bo,"PIL.Image"),Bo.forEach(a),cl=o(q," object:"),q.forEach(a),wa=m(s),g(ws.$$.fragment,s),$a=m(s),X=l(s,"P",{});var vt=r(X);ml=o(vt,"The cache file saves time because you don\u2019t have to execute the same transform twice. The "),ae=l(vt,"A",{href:!0});var Go=r(ae);hl=o(Go,"map()"),Go.forEach(a),fl=o(vt," function is best for operations you only run once per training - like resizing an image - instead of using it for operations executed for each epoch, like data augmentations."),vt.forEach(a),Ea=m(s),$s=l(s,"P",{});var oo=r($s);te=l(oo,"A",{href:!0});var No=r(te);ul=o(No,"map()"),No.forEach(a),gl=o(oo," takes up some memory, but you can reduce its memory requirements with the following parameters:"),oo.forEach(a),ka=m(s),Z=l(s,"UL",{});var yt=r(Z);le=l(yt,"LI",{});var ro=r(le);ne=l(ro,"A",{href:!0});var Ro=r(ne);ze=l(Ro,"CODE",{});var Fo=r(ze);dl=o(Fo,"batch_size"),Fo.forEach(a),Ro.forEach(a),bl=o(ro," determines the number of examples that are processed in one call to the transform function."),ro.forEach(a),xl=m(yt),oe=l(yt,"LI",{});var po=r(oe);re=l(po,"A",{href:!0});var So=r(re);Oe=l(So,"CODE",{});var Uo=r(Oe);_l=o(Uo,"writer_batch_size"),Uo.forEach(a),So.forEach(a),jl=o(po," determines the number of processed examples that are kept in memory before they are stored away."),po.forEach(a),yt.forEach(a),Ca=m(s),ss=l(s,"P",{});var wt=r(ss);vl=o(wt,"Both parameter values default to 1000, which can be expensive if you are storing images. Lower these values to use less memory when you use "),pe=l(wt,"A",{href:!0});var Mo=r(pe);yl=o(Mo,"map()"),Mo.forEach(a),wl=o(wt,"."),wt.forEach(a),Ia=m(s),S=l(s,"H2",{class:!0});var $t=r(S);es=l($t,"A",{id:!0,class:!0,href:!0});var Jo=r(es);Le=l(Jo,"SPAN",{});var Vo=r(Le);g(Es.$$.fragment,Vo),Vo.forEach(a),Jo.forEach(a),$l=m($t),qe=l($t,"SPAN",{});var Ho=r(qe);El=o(Ho,"Data augmentation"),Ho.forEach(a),$t.forEach(a),Da=m(s),ie=l(s,"P",{});var Yo=r(ie);kl=o(Yo,"\u{1F917} Datasets can apply data augmentations from any library or package to your dataset."),Yo.forEach(a),Pa=m(s),U=l(s,"H3",{class:!0});var Et=r(U);as=l(Et,"A",{id:!0,class:!0,href:!0});var Ko=r(as);Be=l(Ko,"SPAN",{});var Qo=r(Be);g(ks.$$.fragment,Qo),Qo.forEach(a),Ko.forEach(a),Cl=m(Et),Ge=l(Et,"SPAN",{});var Wo=r(Ge);Il=o(Wo,"Image Classification"),Wo.forEach(a),Et.forEach(a),Aa=m(s),ts=l(s,"P",{});var kt=r(ts);Dl=o(kt,"First let\u2019s see how you can transform image classification datasets. This guide will use the transforms from "),Cs=l(kt,"A",{href:!0,rel:!0});var Xo=r(Cs);Pl=o(Xo,"torchvision"),Xo.forEach(a),Al=o(kt,"."),kt.forEach(a),Ta=m(s),g(ls.$$.fragment,s),za=m(s),ns=l(s,"P",{});var Ct=r(ns);Tl=o(Ct,"As an example, try to apply a "),Is=l(Ct,"A",{href:!0,rel:!0});var Zo=r(Is);Ne=l(Zo,"CODE",{});var sr=r(Ne);zl=o(sr,"ColorJitter"),sr.forEach(a),Zo.forEach(a),Ol=o(Ct," transform to change the color properties of the image randomly:"),Ct.forEach(a),Oa=m(s),g(Ds.$$.fragment,s),La=m(s),os=l(s,"P",{});var It=r(os);Ll=o(It,"Create a function to apply the "),Re=l(It,"CODE",{});var er=r(Re);ql=o(er,"ColorJitter"),er.forEach(a),Bl=o(It," transform to an image:"),It.forEach(a),qa=m(s),g(Ps.$$.fragment,s),Ba=m(s),rs=l(s,"P",{});var Dt=r(rs);Gl=o(Dt,"Use the "),ce=l(Dt,"A",{href:!0});var ar=r(ce);Nl=o(ar,"set_transform()"),ar.forEach(a),Rl=o(Dt," function to apply the transform on-the-fly which consumes less disk space. This function is useful if you only need to access the examples once:"),Dt.forEach(a),Ga=m(s),g(As.$$.fragment,s),Na=m(s),ps=l(s,"P",{});var Pt=r(ps);Fl=o(Pt,"Now you can take a look at the augmented image by indexing into the "),Fe=l(Pt,"CODE",{});var tr=r(Fe);Sl=o(tr,"pixel_values"),tr.forEach(a),Ul=o(Pt,":"),Pt.forEach(a),Ra=m(s),g(Ts.$$.fragment,s),Fa=m(s),M=l(s,"DIV",{class:!0});var At=r(M);me=l(At,"IMG",{class:!0,src:!0}),Ml=m(At),he=l(At,"IMG",{class:!0,src:!0}),At.forEach(a),Sa=m(s),J=l(s,"H3",{class:!0});var Tt=r(J);is=l(Tt,"A",{id:!0,class:!0,href:!0});var lr=r(is);Se=l(lr,"SPAN",{});var nr=r(Se);g(zs.$$.fragment,nr),nr.forEach(a),lr.forEach(a),Jl=m(Tt),Ue=l(Tt,"SPAN",{});var or=r(Ue);Vl=o(or,"Object Detection"),or.forEach(a),Tt.forEach(a),Ua=m(s),O=l(s,"P",{});var Ee=r(O);Hl=o(Ee,"Next, let\u2019s see how to apply transformations to object detection datasets. For this we\u2019ll use "),Os=l(Ee,"A",{href:!0,rel:!0});var rr=r(Os);Yl=o(rr,"Albumentations"),rr.forEach(a),Kl=o(Ee,", following their object detection "),Ls=l(Ee,"A",{href:!0,rel:!0});var pr=r(Ls);Ql=o(pr,"tutorial"),pr.forEach(a),Wl=o(Ee,"."),Ee.forEach(a),Ma=m(s),L=l(s,"P",{});var ke=r(L);Xl=o(ke,"To run these examples, make sure you have up to date versions of "),Me=l(ke,"CODE",{});var ir=r(Me);Zl=o(ir,"albumentations"),ir.forEach(a),sn=o(ke," and "),Je=l(ke,"CODE",{});var cr=r(Je);en=o(cr,"cv2"),cr.forEach(a),an=o(ke," installed:"),ke.forEach(a),Ja=m(s),g(qs.$$.fragment,s),Va=m(s),cs=l(s,"P",{});var zt=r(cs);tn=o(zt,"In this example, the "),Bs=l(zt,"A",{href:!0,rel:!0});var mr=r(Bs);Ve=l(mr,"CODE",{});var hr=r(Ve);ln=o(hr,"cppe-5"),hr.forEach(a),mr.forEach(a),nn=o(zt," dataset is used, which is a dataset for identifying medical personal protective equipments (PPEs) in the context of the COVID-19 pandemic."),zt.forEach(a),Ha=m(s),fe=l(s,"P",{});var fr=r(fe);on=o(fr,"You can load the dataset and take a look at an example:"),fr.forEach(a),Ya=m(s),g(Gs.$$.fragment,s),Ka=m(s),ue=l(s,"P",{});var ur=r(ue);rn=o(ur,"The dataset has the following fields:"),ur.forEach(a),Qa=m(s),E=l(s,"UL",{});var B=r(E);ge=l(B,"LI",{});var io=r(ge);He=l(io,"CODE",{});var gr=r(He);pn=o(gr,"image"),gr.forEach(a),cn=o(io,": PIL.Image.Image object containing the image."),io.forEach(a),mn=m(B),de=l(B,"LI",{});var co=r(de);Ye=l(co,"CODE",{});var dr=r(Ye);hn=o(dr,"image_id"),dr.forEach(a),fn=o(co,": The image ID."),co.forEach(a),un=m(B),be=l(B,"LI",{});var mo=r(be);Ke=l(mo,"CODE",{});var br=r(Ke);gn=o(br,"height"),br.forEach(a),dn=o(mo,": The image height."),mo.forEach(a),bn=m(B),xe=l(B,"LI",{});var ho=r(xe);Qe=l(ho,"CODE",{});var xr=r(Qe);xn=o(xr,"width"),xr.forEach(a),_n=o(ho,": The image width."),ho.forEach(a),jn=m(B),Ns=l(B,"LI",{});var Ot=r(Ns);We=l(Ot,"CODE",{});var _r=r(We);vn=o(_r,"objects"),_r.forEach(a),yn=o(Ot,": a dictionary containing bounding box metadata for the objects present on the image"),A=l(Ot,"UL",{});var ds=r(A);_e=l(ds,"LI",{});var fo=r(_e);Xe=l(fo,"CODE",{});var jr=r(Xe);wn=o(jr,"id"),jr.forEach(a),$n=o(fo,": the annotation id"),fo.forEach(a),En=m(ds),je=l(ds,"LI",{});var uo=r(je);Ze=l(uo,"CODE",{});var vr=r(Ze);kn=o(vr,"area"),vr.forEach(a),Cn=o(uo,": the area of the bounding box"),uo.forEach(a),In=m(ds),ms=l(ds,"LI",{});var fa=r(ms);sa=l(fa,"CODE",{});var yr=r(sa);Dn=o(yr,"bbox"),yr.forEach(a),Pn=o(fa,": the object\u2019s bounding box (in the "),Rs=l(fa,"A",{href:!0,rel:!0});var wr=r(Rs);An=o(wr,"coco"),wr.forEach(a),Tn=o(fa," format)"),fa.forEach(a),zn=m(ds),w=l(ds,"LI",{});var I=r(w);ea=l(I,"CODE",{});var $r=r(ea);On=o($r,"category"),$r.forEach(a),Ln=o(I,": the object\u2019s category, with possible values including "),aa=l(I,"CODE",{});var Er=r(aa);qn=o(Er,"Coverall (0)"),Er.forEach(a),Bn=o(I,", "),ta=l(I,"CODE",{});var kr=r(ta);Gn=o(kr,"Face_Shield (1)"),kr.forEach(a),Nn=o(I,", "),la=l(I,"CODE",{});var Cr=r(la);Rn=o(Cr,"Gloves (2)"),Cr.forEach(a),Fn=o(I,", "),na=l(I,"CODE",{});var Ir=r(na);Sn=o(Ir,"Goggles (3)"),Ir.forEach(a),Un=o(I," and "),oa=l(I,"CODE",{});var Dr=r(oa);Mn=o(Dr,"Mask (4)"),Dr.forEach(a),I.forEach(a),ds.forEach(a),Ot.forEach(a),B.forEach(a),Wa=m(s),hs=l(s,"P",{});var Lt=r(hs);Jn=o(Lt,"You can visualize the bboxes on the image using some internal torch utilities. But, to do that, you will need to reference the "),ra=l(Lt,"CODE",{});var Pr=r(ra);Vn=o(Pr,"datasets.ClassLabel"),Pr.forEach(a),Hn=o(Lt," feature associated with the category IDs so you can look up the string labels."),Lt.forEach(a),Xa=m(s),g(Fs.$$.fragment,s),Za=m(s),Ss=l(s,"DIV",{class:!0});var Ar=r(Ss);pa=l(Ar,"IMG",{src:!0}),Ar.forEach(a),st=m(s),fs=l(s,"P",{});var qt=r(fs);Yn=o(qt,"Using "),ia=l(qt,"CODE",{});var Tr=r(ia);Kn=o(Tr,"albumentations"),Tr.forEach(a),Qn=o(qt,", we can apply transforms that will effect the Image while also updating the bounding boxes accordingly. In this case, the image is resized to (480, 480), flipped horizontally, and brightened."),qt.forEach(a),et=m(s),us=l(s,"P",{});var Bt=r(us);Wn=o(Bt,"Note that "),ca=l(Bt,"CODE",{});var zr=r(ca);Xn=o(zr,"albumentations"),zr.forEach(a),Zn=o(Bt," is expecting the image to be in BGR format, not RGB, so we\u2019ll have to convert our image first before applying the transform."),Bt.forEach(a),at=m(s),g(Us.$$.fragment,s),tt=m(s),ve=l(s,"P",{});var Or=r(ve);so=o(Or,"Now when we visualize the result, the image should be flipped but the boxes should still be in the right places."),Or.forEach(a),lt=m(s),g(Ms.$$.fragment,s),nt=m(s),Js=l(s,"DIV",{class:!0});var Lr=r(Js);ma=l(Lr,"IMG",{src:!0}),Lr.forEach(a),ot=m(s),ye=l(s,"P",{});var qr=r(ye);eo=o(qr,"Create a function to apply the transform to a batch of examples:"),qr.forEach(a),rt=m(s),g(Vs.$$.fragment,s),pt=m(s),gs=l(s,"P",{});var Gt=r(gs);ao=o(Gt,"Use the "),we=l(Gt,"A",{href:!0});var Br=r(we);to=o(Br,"set_transform()"),Br.forEach(a),lo=o(Gt," function to apply the transform on-the-fly which consumes less disk space. Note that the randomness of data augmentation may return a different image if you access the same example twice. It is especially useful when training a model with several epochs."),Gt.forEach(a),it=m(s),g(Hs.$$.fragment,s),ct=m(s),$e=l(s,"P",{});var Gr=r($e);no=o(Gr,"Verify the transform is working by visualizing the 10th example:"),Gr.forEach(a),mt=m(s),g(Ys.$$.fragment,s),ht=m(s),Ks=l(s,"DIV",{class:!0});var Nr=r(Ks);ha=l(Nr,"IMG",{src:!0}),Nr.forEach(a),this.h()},h(){h(f,"name","hf:doc:metadata"),h(f,"content",JSON.stringify(Yr)),h(C,"id","process-image-data"),h(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(C,"href","#process-image-data"),h(j,"class","relative group"),h(Xs,"href","/docs/datasets/pr_4710/en/package_reference/main_classes#datasets.Dataset.map"),h(Zs,"href","/docs/datasets/pr_4710/en/package_reference/main_classes#datasets.Dataset.set_transform"),h(_s,"class","underline decoration-sky-400 decoration-2 font-semibold"),h(_s,"href","./process"),h(K,"id","map"),h(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(K,"href","#map"),h(F,"class","relative group"),h(se,"href","/docs/datasets/pr_4710/en/package_reference/main_classes#datasets.Dataset.map"),h(vs,"href","https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html"),h(vs,"rel","nofollow"),h(ee,"href","/docs/datasets/pr_4710/en/package_reference/main_classes#datasets.Dataset.map"),h(ae,"href","/docs/datasets/pr_4710/en/package_reference/main_classes#datasets.Dataset.map"),h(te,"href","/docs/datasets/pr_4710/en/package_reference/main_classes#datasets.Dataset.map"),h(ne,"href","./package_reference/main_classes#datasets.DatasetDict.map.batch_size"),h(re,"href","./package_reference/main_classes#datasets.DatasetDict.map.writer_batch_size"),h(pe,"href","/docs/datasets/pr_4710/en/package_reference/main_classes#datasets.Dataset.map"),h(es,"id","data-augmentation"),h(es,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(es,"href","#data-augmentation"),h(S,"class","relative group"),h(as,"id","image-classification"),h(as,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(as,"href","#image-classification"),h(U,"class","relative group"),h(Cs,"href","https://pytorch.org/vision/stable/transforms.html"),h(Cs,"rel","nofollow"),h(Is,"href","https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ColorJitter"),h(Is,"rel","nofollow"),h(ce,"href","/docs/datasets/pr_4710/en/package_reference/main_classes#datasets.Dataset.set_transform"),h(me,"class","block dark:hidden"),ua(me.src,go="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/image_process_jitter.png")||h(me,"src",go),h(he,"class","hidden dark:block"),ua(he.src,bo="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/image_process_jitter.png")||h(he,"src",bo),h(M,"class","flex justify-center"),h(is,"id","object-detection"),h(is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(is,"href","#object-detection"),h(J,"class","relative group"),h(Os,"href","https://albumentations.ai/docs/"),h(Os,"rel","nofollow"),h(Ls,"href","https://albumentations.ai/docs/examples/example_bboxes/"),h(Ls,"rel","nofollow"),h(Bs,"href","https://huggingface.co/datasets/cppe-5"),h(Bs,"rel","nofollow"),h(Rs,"href","https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/#coco"),h(Rs,"rel","nofollow"),ua(pa.src,xo="https://huggingface.co/datasets/nateraw/documentation-images/resolve/main/visualize_detection_example.png")||h(pa,"src",xo),h(Ss,"class","flex justify-center"),ua(ma.src,_o="https://huggingface.co/datasets/nateraw/documentation-images/resolve/main/visualize_detection_example_transformed.png")||h(ma,"src",_o),h(Js,"class","flex justify-center"),h(we,"href","/docs/datasets/pr_4710/en/package_reference/main_classes#datasets.Dataset.set_transform"),ua(ha.src,jo="https://huggingface.co/datasets/nateraw/documentation-images/resolve/main/visualize_detection_example_transformed_2.png")||h(ha,"src",jo),h(Ks,"class","flex justify-center")},m(s,p){e(document.head,f),i(s,V,p),i(s,j,p),e(j,C),e(C,G),d(v,G,null),e(j,bs),e(j,N),e(N,D),i(s,H,p),i(s,T,p),e(T,R),i(s,y,p),i(s,P,p),e(P,z),e(z,Ws),e(z,Xs),e(Xs,Nt),e(z,Rt),e(P,Ft),e(P,xs),e(xs,St),e(xs,Zs),e(Zs,Ut),e(xs,Mt),i(s,ba,p),i(s,Y,p),e(Y,Jt),e(Y,_s),e(_s,Vt),e(Y,Ht),i(s,xa,p),i(s,F,p),e(F,K),e(K,Ce),d(js,Ce,null),e(F,Yt),e(F,Ie),e(Ie,Kt),i(s,_a,p),i(s,Q,p),e(Q,Qt),e(Q,se),e(se,Wt),e(Q,Xt),i(s,ja,p),i(s,W,p),e(W,Zt),e(W,vs),e(vs,De),e(De,sl),e(W,el),i(s,va,p),d(ys,s,p),i(s,ya,p),i(s,$,p),e($,al),e($,ee),e(ee,tl),e($,ll),e($,Pe),e(Pe,nl),e($,ol),e($,Ae),e(Ae,rl),e($,pl),e($,Te),e(Te,il),e($,cl),i(s,wa,p),d(ws,s,p),i(s,$a,p),i(s,X,p),e(X,ml),e(X,ae),e(ae,hl),e(X,fl),i(s,Ea,p),i(s,$s,p),e($s,te),e(te,ul),e($s,gl),i(s,ka,p),i(s,Z,p),e(Z,le),e(le,ne),e(ne,ze),e(ze,dl),e(le,bl),e(Z,xl),e(Z,oe),e(oe,re),e(re,Oe),e(Oe,_l),e(oe,jl),i(s,Ca,p),i(s,ss,p),e(ss,vl),e(ss,pe),e(pe,yl),e(ss,wl),i(s,Ia,p),i(s,S,p),e(S,es),e(es,Le),d(Es,Le,null),e(S,$l),e(S,qe),e(qe,El),i(s,Da,p),i(s,ie,p),e(ie,kl),i(s,Pa,p),i(s,U,p),e(U,as),e(as,Be),d(ks,Be,null),e(U,Cl),e(U,Ge),e(Ge,Il),i(s,Aa,p),i(s,ts,p),e(ts,Dl),e(ts,Cs),e(Cs,Pl),e(ts,Al),i(s,Ta,p),d(ls,s,p),i(s,za,p),i(s,ns,p),e(ns,Tl),e(ns,Is),e(Is,Ne),e(Ne,zl),e(ns,Ol),i(s,Oa,p),d(Ds,s,p),i(s,La,p),i(s,os,p),e(os,Ll),e(os,Re),e(Re,ql),e(os,Bl),i(s,qa,p),d(Ps,s,p),i(s,Ba,p),i(s,rs,p),e(rs,Gl),e(rs,ce),e(ce,Nl),e(rs,Rl),i(s,Ga,p),d(As,s,p),i(s,Na,p),i(s,ps,p),e(ps,Fl),e(ps,Fe),e(Fe,Sl),e(ps,Ul),i(s,Ra,p),d(Ts,s,p),i(s,Fa,p),i(s,M,p),e(M,me),e(M,Ml),e(M,he),i(s,Sa,p),i(s,J,p),e(J,is),e(is,Se),d(zs,Se,null),e(J,Jl),e(J,Ue),e(Ue,Vl),i(s,Ua,p),i(s,O,p),e(O,Hl),e(O,Os),e(Os,Yl),e(O,Kl),e(O,Ls),e(Ls,Ql),e(O,Wl),i(s,Ma,p),i(s,L,p),e(L,Xl),e(L,Me),e(Me,Zl),e(L,sn),e(L,Je),e(Je,en),e(L,an),i(s,Ja,p),d(qs,s,p),i(s,Va,p),i(s,cs,p),e(cs,tn),e(cs,Bs),e(Bs,Ve),e(Ve,ln),e(cs,nn),i(s,Ha,p),i(s,fe,p),e(fe,on),i(s,Ya,p),d(Gs,s,p),i(s,Ka,p),i(s,ue,p),e(ue,rn),i(s,Qa,p),i(s,E,p),e(E,ge),e(ge,He),e(He,pn),e(ge,cn),e(E,mn),e(E,de),e(de,Ye),e(Ye,hn),e(de,fn),e(E,un),e(E,be),e(be,Ke),e(Ke,gn),e(be,dn),e(E,bn),e(E,xe),e(xe,Qe),e(Qe,xn),e(xe,_n),e(E,jn),e(E,Ns),e(Ns,We),e(We,vn),e(Ns,yn),e(Ns,A),e(A,_e),e(_e,Xe),e(Xe,wn),e(_e,$n),e(A,En),e(A,je),e(je,Ze),e(Ze,kn),e(je,Cn),e(A,In),e(A,ms),e(ms,sa),e(sa,Dn),e(ms,Pn),e(ms,Rs),e(Rs,An),e(ms,Tn),e(A,zn),e(A,w),e(w,ea),e(ea,On),e(w,Ln),e(w,aa),e(aa,qn),e(w,Bn),e(w,ta),e(ta,Gn),e(w,Nn),e(w,la),e(la,Rn),e(w,Fn),e(w,na),e(na,Sn),e(w,Un),e(w,oa),e(oa,Mn),i(s,Wa,p),i(s,hs,p),e(hs,Jn),e(hs,ra),e(ra,Vn),e(hs,Hn),i(s,Xa,p),d(Fs,s,p),i(s,Za,p),i(s,Ss,p),e(Ss,pa),i(s,st,p),i(s,fs,p),e(fs,Yn),e(fs,ia),e(ia,Kn),e(fs,Qn),i(s,et,p),i(s,us,p),e(us,Wn),e(us,ca),e(ca,Xn),e(us,Zn),i(s,at,p),d(Us,s,p),i(s,tt,p),i(s,ve,p),e(ve,so),i(s,lt,p),d(Ms,s,p),i(s,nt,p),i(s,Js,p),e(Js,ma),i(s,ot,p),i(s,ye,p),e(ye,eo),i(s,rt,p),d(Vs,s,p),i(s,pt,p),i(s,gs,p),e(gs,ao),e(gs,we),e(we,to),e(gs,lo),i(s,it,p),d(Hs,s,p),i(s,ct,p),i(s,$e,p),e($e,no),i(s,mt,p),d(Ys,s,p),i(s,ht,p),i(s,Ks,p),e(Ks,ha),ft=!0},p(s,[p]){const Qs={};p&2&&(Qs.$$scope={dirty:p,ctx:s}),ls.$set(Qs)},i(s){ft||(b(v.$$.fragment,s),b(js.$$.fragment,s),b(ys.$$.fragment,s),b(ws.$$.fragment,s),b(Es.$$.fragment,s),b(ks.$$.fragment,s),b(ls.$$.fragment,s),b(Ds.$$.fragment,s),b(Ps.$$.fragment,s),b(As.$$.fragment,s),b(Ts.$$.fragment,s),b(zs.$$.fragment,s),b(qs.$$.fragment,s),b(Gs.$$.fragment,s),b(Fs.$$.fragment,s),b(Us.$$.fragment,s),b(Ms.$$.fragment,s),b(Vs.$$.fragment,s),b(Hs.$$.fragment,s),b(Ys.$$.fragment,s),ft=!0)},o(s){x(v.$$.fragment,s),x(js.$$.fragment,s),x(ys.$$.fragment,s),x(ws.$$.fragment,s),x(Es.$$.fragment,s),x(ks.$$.fragment,s),x(ls.$$.fragment,s),x(Ds.$$.fragment,s),x(Ps.$$.fragment,s),x(As.$$.fragment,s),x(Ts.$$.fragment,s),x(zs.$$.fragment,s),x(qs.$$.fragment,s),x(Gs.$$.fragment,s),x(Fs.$$.fragment,s),x(Us.$$.fragment,s),x(Ms.$$.fragment,s),x(Vs.$$.fragment,s),x(Hs.$$.fragment,s),x(Ys.$$.fragment,s),ft=!1},d(s){a(f),s&&a(V),s&&a(j),_(v),s&&a(H),s&&a(T),s&&a(y),s&&a(P),s&&a(ba),s&&a(Y),s&&a(xa),s&&a(F),_(js),s&&a(_a),s&&a(Q),s&&a(ja),s&&a(W),s&&a(va),_(ys,s),s&&a(ya),s&&a($),s&&a(wa),_(ws,s),s&&a($a),s&&a(X),s&&a(Ea),s&&a($s),s&&a(ka),s&&a(Z),s&&a(Ca),s&&a(ss),s&&a(Ia),s&&a(S),_(Es),s&&a(Da),s&&a(ie),s&&a(Pa),s&&a(U),_(ks),s&&a(Aa),s&&a(ts),s&&a(Ta),_(ls,s),s&&a(za),s&&a(ns),s&&a(Oa),_(Ds,s),s&&a(La),s&&a(os),s&&a(qa),_(Ps,s),s&&a(Ba),s&&a(rs),s&&a(Ga),_(As,s),s&&a(Na),s&&a(ps),s&&a(Ra),_(Ts,s),s&&a(Fa),s&&a(M),s&&a(Sa),s&&a(J),_(zs),s&&a(Ua),s&&a(O),s&&a(Ma),s&&a(L),s&&a(Ja),_(qs,s),s&&a(Va),s&&a(cs),s&&a(Ha),s&&a(fe),s&&a(Ya),_(Gs,s),s&&a(Ka),s&&a(ue),s&&a(Qa),s&&a(E),s&&a(Wa),s&&a(hs),s&&a(Xa),_(Fs,s),s&&a(Za),s&&a(Ss),s&&a(st),s&&a(fs),s&&a(et),s&&a(us),s&&a(at),_(Us,s),s&&a(tt),s&&a(ve),s&&a(lt),_(Ms,s),s&&a(nt),s&&a(Js),s&&a(ot),s&&a(ye),s&&a(rt),_(Vs,s),s&&a(pt),s&&a(gs),s&&a(it),_(Hs,s),s&&a(ct),s&&a($e),s&&a(mt),_(Ys,s),s&&a(ht),s&&a(Ks)}}}const Yr={local:"process-image-data",sections:[{local:"map",title:"Map"},{local:"data-augmentation",sections:[{local:"image-classification",title:"Image Classification"},{local:"object-detection",title:"Object Detection"}],title:"Data augmentation"}],title:"Process image data"};function Kr(da){return Mr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class sp extends Rr{constructor(f){super();Fr(this,f,Kr,Hr,Sr,{})}}export{sp as default,Yr as metadata};
