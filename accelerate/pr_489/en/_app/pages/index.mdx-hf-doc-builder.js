import{S as Pa,i as ba,s as ka,e as i,k as n,w as d,t as c,M as Ea,c as l,d as t,m as p,a as o,x as y,h,b as u,G as a,g as s,y as v,L as Aa,q as _,o as w,B as $,v as Ua}from"../chunks/vendor-hf-doc-builder.js";import{I as be}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as Z}from"../chunks/CodeBlock-hf-doc-builder.js";function ja(Mt){let g,ke,P,U,ee,N,tt,te,at,Ee,j,rt,ae,it,lt,Ae,b,x,re,C,ot,ie,st,Ue,S,le,oe,nt,pt,se,ne,ct,je,k,L,pe,G,ht,ce,ut,xe,B,ft,Se,q,Le,Y,mt,Te,F,Ie,K,dt,ze,Q,yt,Ne,M,Ce,E,T,he,H,vt,ue,_t,Ge,I,wt,fe,$t,gt,qe,V,Pt,Fe,O,Me,W,bt,He,R,Oe,X,kt,Re,D,De,A,z,me,J,Et,de,At,Je,f,ye,Ut,jt,ve,xt,St,_e,Lt,Tt,we,It,zt,$e,Nt,Ct,ge,Gt,qt,Pe,Ft,Be;return N=new be({}),C=new be({}),G=new be({}),q=new Z({props:{code:"",highlighted:`my_model.to(device)

<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> my_training_dataloader:
    my_optimizer.zero_grad()
    inputs, targets = batch
    inputs = inputs.to(device)
    targets = targets.to(device)
    outputs = my_model(inputs)
    loss = my_loss_function(outputs, targets)
    loss.backward()
    my_optimizer.step()`}}),F=new Z({props:{code:`
`,highlighted:`<span class="hljs-addition">+ from accelerate import Accelerator</span>

<span class="hljs-addition">+ accelerator = Accelerator()</span>
  # Use the device given by the *accelerator* object.
<span class="hljs-addition">+ device = accelerator.device</span>
  my_model.to(device)
  # Pass every important object (model, optimizer, dataloader) to *accelerator.prepare*
<span class="hljs-addition">+ my_model, my_optimizer, my_training_dataloader = accelerator.prepare(</span>
<span class="hljs-addition">+     my_model, my_optimizer, my_training_dataloader</span>
<span class="hljs-addition">+ )</span>

  for batch in my_training_dataloader:
      my_optimizer.zero_grad()
      inputs, targets = batch
      inputs = inputs.to(device)
      targets = targets.to(device)
      outputs = my_model(inputs)
      loss = my_loss_function(outputs, targets)
      # Just a small change for the backward instruction
<span class="hljs-deletion">-     loss.backward()</span>
<span class="hljs-addition">+     accelerator.backward(loss)</span>
      my_optimizer.step()`}}),M=new Z({props:{code:`
`,highlighted:`<span class="hljs-addition">+ from accelerate import Accelerator</span>

<span class="hljs-addition">+ accelerator = Accelerator()</span>
<span class="hljs-deletion">- my_model.to(device)</span>
  # Pass every important object (model, optimizer, dataloader) to *accelerator.prepare*
<span class="hljs-addition">+ my_model, my_optimizer, my_training_dataloader = accelerate.prepare(</span>
<span class="hljs-addition">+     my_model, my_optimizer, my_training_dataloader</span>
<span class="hljs-addition">+ )</span>

  for batch in my_training_dataloader:
      my_optimizer.zero_grad()
      inputs, targets = batch
<span class="hljs-deletion">-     inputs = inputs.to(device)</span>
<span class="hljs-deletion">-     targets = targets.to(device)</span>
      outputs = my_model(inputs)
      loss = my_loss_function(outputs, targets)
      # Just a small change for the backward instruction
<span class="hljs-deletion">-     loss.backward()</span>
<span class="hljs-addition">+     accelerator.backward(loss)</span>
      my_optimizer.step()`}}),H=new be({}),O=new Z({props:{code:"accelerate config",highlighted:"accelerate config"}}),R=new Z({props:{code:"accelerate launch my_script.py --args_to_my_script",highlighted:"accelerate launch my_script.py --args_to_my_script"}}),D=new Z({props:{code:"accelerate launch examples/nlp_example.py",highlighted:"accelerate launch examples/nlp_example.py"}}),J=new be({}),{c(){g=i("meta"),ke=n(),P=i("h1"),U=i("a"),ee=i("span"),d(N.$$.fragment),tt=n(),te=i("span"),at=c("Accelerate"),Ee=n(),j=i("p"),rt=c("Run your "),ae=i("em"),it=c("raw"),lt=c(" PyTorch training script on any kind of device."),Ae=n(),b=i("h2"),x=i("a"),re=i("span"),d(C.$$.fragment),ot=n(),ie=i("span"),st=c("Features"),Ue=n(),S=i("ul"),le=i("li"),oe=i("p"),nt=c(`\u{1F917} Accelerate provides an easy API to make your scripts run with mixed precision and in any kind of distributed
setting (multi-GPUs, TPUs etc.) while still letting you write your own training loop. The same code can then run
seamlessly on your local machine for debugging or your training environment.`),pt=n(),se=i("li"),ne=i("p"),ct=c(`\u{1F917} Accelerate also provides a CLI tool that allows you to quickly configure and test your training environment and
then launch the scripts.`),je=n(),k=i("h2"),L=i("a"),pe=i("span"),d(G.$$.fragment),ht=n(),ce=i("span"),ut=c("Easy to integrate"),xe=n(),B=i("p"),ft=c("A traditional training loop in PyTorch looks like this:"),Se=n(),d(q.$$.fragment),Le=n(),Y=i("p"),mt=c("Changing it to work with accelerate is really easy and only adds a few lines of code:"),Te=n(),d(F.$$.fragment),Ie=n(),K=i("p"),dt=c("and with this, your script can now run in a distributed environment (multi-GPU, TPU)."),ze=n(),Q=i("p"),yt=c(`You can even simplify your script a bit by letting \u{1F917} Accelerate handle the device placement for you (which is safer,
especially for TPU training):`),Ne=n(),d(M.$$.fragment),Ce=n(),E=i("h2"),T=i("a"),he=i("span"),d(H.$$.fragment),vt=n(),ue=i("span"),_t=c("Script launcher"),Ge=n(),I=i("p"),wt=c("No need to remember how to use "),fe=i("code"),$t=c("torch.distributed.launch"),gt=c(` or to write a specific launcher for TPU training! \u{1F917}
Accelerate comes with a CLI tool that will make your life easier when launching distributed scripts.`),qe=n(),V=i("p"),Pt=c("On your machine(s) just run:"),Fe=n(),d(O.$$.fragment),Me=n(),W=i("p"),bt=c(`and answer the questions asked. This will generate a config file that will be used automatically to properly set the
default options when doing`),He=n(),d(R.$$.fragment),Oe=n(),X=i("p"),kt=c("For instance, here is how you would run the NLP example (from the root of the repo):"),Re=n(),d(D.$$.fragment),De=n(),A=i("h2"),z=i("a"),me=i("span"),d(J.$$.fragment),Et=n(),de=i("span"),At=c("Supported integrations"),Je=n(),f=i("ul"),ye=i("li"),Ut=c("CPU only"),jt=n(),ve=i("li"),xt=c("single GPU"),St=n(),_e=i("li"),Lt=c("multi-GPU on one node (machine)"),Tt=n(),we=i("li"),It=c("multi-GPU on several nodes (machines)"),zt=n(),$e=i("li"),Nt=c("TPU"),Ct=n(),ge=i("li"),Gt=c("FP16 with native AMP (apex on the roadmap)"),qt=n(),Pe=i("li"),Ft=c("DeepSpeed (experimental support)"),this.h()},l(e){const r=Ea('[data-svelte="svelte-1phssyn"]',document.head);g=l(r,"META",{name:!0,content:!0}),r.forEach(t),ke=p(e),P=l(e,"H1",{class:!0});var Ye=o(P);U=l(Ye,"A",{id:!0,class:!0,href:!0});var Ht=o(U);ee=l(Ht,"SPAN",{});var Ot=o(ee);y(N.$$.fragment,Ot),Ot.forEach(t),Ht.forEach(t),tt=p(Ye),te=l(Ye,"SPAN",{});var Rt=o(te);at=h(Rt,"Accelerate"),Rt.forEach(t),Ye.forEach(t),Ee=p(e),j=l(e,"P",{});var Ke=o(j);rt=h(Ke,"Run your "),ae=l(Ke,"EM",{});var Dt=o(ae);it=h(Dt,"raw"),Dt.forEach(t),lt=h(Ke," PyTorch training script on any kind of device."),Ke.forEach(t),Ae=p(e),b=l(e,"H2",{class:!0});var Qe=o(b);x=l(Qe,"A",{id:!0,class:!0,href:!0});var Jt=o(x);re=l(Jt,"SPAN",{});var Bt=o(re);y(C.$$.fragment,Bt),Bt.forEach(t),Jt.forEach(t),ot=p(Qe),ie=l(Qe,"SPAN",{});var Yt=o(ie);st=h(Yt,"Features"),Yt.forEach(t),Qe.forEach(t),Ue=p(e),S=l(e,"UL",{});var Ve=o(S);le=l(Ve,"LI",{});var Kt=o(le);oe=l(Kt,"P",{});var Qt=o(oe);nt=h(Qt,`\u{1F917} Accelerate provides an easy API to make your scripts run with mixed precision and in any kind of distributed
setting (multi-GPUs, TPUs etc.) while still letting you write your own training loop. The same code can then run
seamlessly on your local machine for debugging or your training environment.`),Qt.forEach(t),Kt.forEach(t),pt=p(Ve),se=l(Ve,"LI",{});var Vt=o(se);ne=l(Vt,"P",{});var Wt=o(ne);ct=h(Wt,`\u{1F917} Accelerate also provides a CLI tool that allows you to quickly configure and test your training environment and
then launch the scripts.`),Wt.forEach(t),Vt.forEach(t),Ve.forEach(t),je=p(e),k=l(e,"H2",{class:!0});var We=o(k);L=l(We,"A",{id:!0,class:!0,href:!0});var Xt=o(L);pe=l(Xt,"SPAN",{});var Zt=o(pe);y(G.$$.fragment,Zt),Zt.forEach(t),Xt.forEach(t),ht=p(We),ce=l(We,"SPAN",{});var ea=o(ce);ut=h(ea,"Easy to integrate"),ea.forEach(t),We.forEach(t),xe=p(e),B=l(e,"P",{});var ta=o(B);ft=h(ta,"A traditional training loop in PyTorch looks like this:"),ta.forEach(t),Se=p(e),y(q.$$.fragment,e),Le=p(e),Y=l(e,"P",{});var aa=o(Y);mt=h(aa,"Changing it to work with accelerate is really easy and only adds a few lines of code:"),aa.forEach(t),Te=p(e),y(F.$$.fragment,e),Ie=p(e),K=l(e,"P",{});var ra=o(K);dt=h(ra,"and with this, your script can now run in a distributed environment (multi-GPU, TPU)."),ra.forEach(t),ze=p(e),Q=l(e,"P",{});var ia=o(Q);yt=h(ia,`You can even simplify your script a bit by letting \u{1F917} Accelerate handle the device placement for you (which is safer,
especially for TPU training):`),ia.forEach(t),Ne=p(e),y(M.$$.fragment,e),Ce=p(e),E=l(e,"H2",{class:!0});var Xe=o(E);T=l(Xe,"A",{id:!0,class:!0,href:!0});var la=o(T);he=l(la,"SPAN",{});var oa=o(he);y(H.$$.fragment,oa),oa.forEach(t),la.forEach(t),vt=p(Xe),ue=l(Xe,"SPAN",{});var sa=o(ue);_t=h(sa,"Script launcher"),sa.forEach(t),Xe.forEach(t),Ge=p(e),I=l(e,"P",{});var Ze=o(I);wt=h(Ze,"No need to remember how to use "),fe=l(Ze,"CODE",{});var na=o(fe);$t=h(na,"torch.distributed.launch"),na.forEach(t),gt=h(Ze,` or to write a specific launcher for TPU training! \u{1F917}
Accelerate comes with a CLI tool that will make your life easier when launching distributed scripts.`),Ze.forEach(t),qe=p(e),V=l(e,"P",{});var pa=o(V);Pt=h(pa,"On your machine(s) just run:"),pa.forEach(t),Fe=p(e),y(O.$$.fragment,e),Me=p(e),W=l(e,"P",{});var ca=o(W);bt=h(ca,`and answer the questions asked. This will generate a config file that will be used automatically to properly set the
default options when doing`),ca.forEach(t),He=p(e),y(R.$$.fragment,e),Oe=p(e),X=l(e,"P",{});var ha=o(X);kt=h(ha,"For instance, here is how you would run the NLP example (from the root of the repo):"),ha.forEach(t),Re=p(e),y(D.$$.fragment,e),De=p(e),A=l(e,"H2",{class:!0});var et=o(A);z=l(et,"A",{id:!0,class:!0,href:!0});var ua=o(z);me=l(ua,"SPAN",{});var fa=o(me);y(J.$$.fragment,fa),fa.forEach(t),ua.forEach(t),Et=p(et),de=l(et,"SPAN",{});var ma=o(de);At=h(ma,"Supported integrations"),ma.forEach(t),et.forEach(t),Je=p(e),f=l(e,"UL",{});var m=o(f);ye=l(m,"LI",{});var da=o(ye);Ut=h(da,"CPU only"),da.forEach(t),jt=p(m),ve=l(m,"LI",{});var ya=o(ve);xt=h(ya,"single GPU"),ya.forEach(t),St=p(m),_e=l(m,"LI",{});var va=o(_e);Lt=h(va,"multi-GPU on one node (machine)"),va.forEach(t),Tt=p(m),we=l(m,"LI",{});var _a=o(we);It=h(_a,"multi-GPU on several nodes (machines)"),_a.forEach(t),zt=p(m),$e=l(m,"LI",{});var wa=o($e);Nt=h(wa,"TPU"),wa.forEach(t),Ct=p(m),ge=l(m,"LI",{});var $a=o(ge);Gt=h($a,"FP16 with native AMP (apex on the roadmap)"),$a.forEach(t),qt=p(m),Pe=l(m,"LI",{});var ga=o(Pe);Ft=h(ga,"DeepSpeed (experimental support)"),ga.forEach(t),m.forEach(t),this.h()},h(){u(g,"name","hf:doc:metadata"),u(g,"content",JSON.stringify(xa)),u(U,"id","accelerate"),u(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(U,"href","#accelerate"),u(P,"class","relative group"),u(x,"id","features"),u(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(x,"href","#features"),u(b,"class","relative group"),u(L,"id","easy-to-integrate"),u(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(L,"href","#easy-to-integrate"),u(k,"class","relative group"),u(T,"id","script-launcher"),u(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(T,"href","#script-launcher"),u(E,"class","relative group"),u(z,"id","supported-integrations"),u(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(z,"href","#supported-integrations"),u(A,"class","relative group")},m(e,r){a(document.head,g),s(e,ke,r),s(e,P,r),a(P,U),a(U,ee),v(N,ee,null),a(P,tt),a(P,te),a(te,at),s(e,Ee,r),s(e,j,r),a(j,rt),a(j,ae),a(ae,it),a(j,lt),s(e,Ae,r),s(e,b,r),a(b,x),a(x,re),v(C,re,null),a(b,ot),a(b,ie),a(ie,st),s(e,Ue,r),s(e,S,r),a(S,le),a(le,oe),a(oe,nt),a(S,pt),a(S,se),a(se,ne),a(ne,ct),s(e,je,r),s(e,k,r),a(k,L),a(L,pe),v(G,pe,null),a(k,ht),a(k,ce),a(ce,ut),s(e,xe,r),s(e,B,r),a(B,ft),s(e,Se,r),v(q,e,r),s(e,Le,r),s(e,Y,r),a(Y,mt),s(e,Te,r),v(F,e,r),s(e,Ie,r),s(e,K,r),a(K,dt),s(e,ze,r),s(e,Q,r),a(Q,yt),s(e,Ne,r),v(M,e,r),s(e,Ce,r),s(e,E,r),a(E,T),a(T,he),v(H,he,null),a(E,vt),a(E,ue),a(ue,_t),s(e,Ge,r),s(e,I,r),a(I,wt),a(I,fe),a(fe,$t),a(I,gt),s(e,qe,r),s(e,V,r),a(V,Pt),s(e,Fe,r),v(O,e,r),s(e,Me,r),s(e,W,r),a(W,bt),s(e,He,r),v(R,e,r),s(e,Oe,r),s(e,X,r),a(X,kt),s(e,Re,r),v(D,e,r),s(e,De,r),s(e,A,r),a(A,z),a(z,me),v(J,me,null),a(A,Et),a(A,de),a(de,At),s(e,Je,r),s(e,f,r),a(f,ye),a(ye,Ut),a(f,jt),a(f,ve),a(ve,xt),a(f,St),a(f,_e),a(_e,Lt),a(f,Tt),a(f,we),a(we,It),a(f,zt),a(f,$e),a($e,Nt),a(f,Ct),a(f,ge),a(ge,Gt),a(f,qt),a(f,Pe),a(Pe,Ft),Be=!0},p:Aa,i(e){Be||(_(N.$$.fragment,e),_(C.$$.fragment,e),_(G.$$.fragment,e),_(q.$$.fragment,e),_(F.$$.fragment,e),_(M.$$.fragment,e),_(H.$$.fragment,e),_(O.$$.fragment,e),_(R.$$.fragment,e),_(D.$$.fragment,e),_(J.$$.fragment,e),Be=!0)},o(e){w(N.$$.fragment,e),w(C.$$.fragment,e),w(G.$$.fragment,e),w(q.$$.fragment,e),w(F.$$.fragment,e),w(M.$$.fragment,e),w(H.$$.fragment,e),w(O.$$.fragment,e),w(R.$$.fragment,e),w(D.$$.fragment,e),w(J.$$.fragment,e),Be=!1},d(e){t(g),e&&t(ke),e&&t(P),$(N),e&&t(Ee),e&&t(j),e&&t(Ae),e&&t(b),$(C),e&&t(Ue),e&&t(S),e&&t(je),e&&t(k),$(G),e&&t(xe),e&&t(B),e&&t(Se),$(q,e),e&&t(Le),e&&t(Y),e&&t(Te),$(F,e),e&&t(Ie),e&&t(K),e&&t(ze),e&&t(Q),e&&t(Ne),$(M,e),e&&t(Ce),e&&t(E),$(H),e&&t(Ge),e&&t(I),e&&t(qe),e&&t(V),e&&t(Fe),$(O,e),e&&t(Me),e&&t(W),e&&t(He),$(R,e),e&&t(Oe),e&&t(X),e&&t(Re),$(D,e),e&&t(De),e&&t(A),$(J),e&&t(Je),e&&t(f)}}}const xa={local:"accelerate",sections:[{local:"features",title:"Features"},{local:"easy-to-integrate",title:"Easy to integrate"},{local:"script-launcher",title:"Script launcher"},{local:"supported-integrations",title:"Supported integrations"}],title:"Accelerate"};function Sa(Mt){return Ua(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class za extends Pa{constructor(g){super();ba(this,g,Sa,ja,ka,{})}}export{za as default,xa as metadata};
