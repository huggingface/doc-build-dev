import{S as mn,i as _n,s as bn,e as s,k as h,w,t as o,M as gn,c as i,d as a,m as f,a as l,x as v,h as n,b as u,G as t,g as c,y as $,q as E,o as k,B as T,v as yn}from"../../chunks/vendor-hf-doc-builder.js";import{T as $o}from"../../chunks/Tip-hf-doc-builder.js";import{I as Lt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ae}from"../../chunks/CodeBlock-hf-doc-builder.js";function wn(L){let p,g,d,m,x,_,P,C;return{c(){p=s("p"),g=o("This code snippet is based off the one from the "),d=s("code"),m=o("simple_nlp_example"),x=o(" notebook found "),_=s("a"),P=o("here"),C=o(` with slight
modifications for the sake of simplicity`),this.h()},l(N){p=i(N,"P",{});var j=l(p);g=n(j,"This code snippet is based off the one from the "),d=i(j,"CODE",{});var B=l(d);m=n(B,"simple_nlp_example"),B.forEach(a),x=n(j," notebook found "),_=i(j,"A",{href:!0,rel:!0});var be=l(_);P=n(be,"here"),be.forEach(a),C=n(j,` with slight
modifications for the sake of simplicity`),j.forEach(a),this.h()},h(){u(_,"href","https://github.com/huggingface/notebooks/blob/main/examples/accelerate/simple_nlp_example.ipynb"),u(_,"rel","nofollow")},m(N,j){c(N,p,j),t(p,g),t(p,d),t(d,m),t(p,x),t(p,_),t(_,P),t(p,C)},d(N){N&&a(p)}}}function vn(L){let p,g,d,m,x;return{c(){p=s("p"),g=o("The "),d=s("code"),m=o("notebook_launcher"),x=o(" will default to 8 processes if \u{1F917} Accelerate has been configured for a TPU")},l(_){p=i(_,"P",{});var P=l(p);g=n(P,"The "),d=i(P,"CODE",{});var C=l(d);m=n(C,"notebook_launcher"),C.forEach(a),x=n(P," will default to 8 processes if \u{1F917} Accelerate has been configured for a TPU"),P.forEach(a)},m(_,P){c(_,p,P),t(p,g),t(p,d),t(d,m),t(p,x)},d(_){_&&a(p)}}}function $n(L){let p,g;return{c(){p=s("p"),g=o("Just because the memory is allocated does not mean it will be used or that the batch size will increase when going back to your training dataloader.")},l(d){p=i(d,"P",{});var m=l(p);g=n(m,"Just because the memory is allocated does not mean it will be used or that the batch size will increase when going back to your training dataloader."),m.forEach(a)},m(d,m){c(d,p,m),t(p,g)},d(d){d&&a(p)}}}function En(L){let p,g,d,m,x,_,P,C,N,j,B,be,ct,F,R,Pe,oe,Rt,Ae,Vt,pt,U,Ht,ze,Xt,Kt,ne,Yt,Qt,xe,Zt,ea,ht,A,ta,je,aa,oa,Ce,na,ra,Ue,sa,ia,Oe,la,ca,ft,q,pa,qe,ha,fa,Se,da,ua,dt,V,ma,De,_a,ba,ut,H,mt,re,_t,se,bt,X,gt,K,ga,Ie,ya,wa,yt,ie,wt,S,va,Me,$a,Ea,Ge,ka,Ta,vt,le,$t,ce,Et,ge,Pa,kt,pe,Tt,W,Y,Ne,he,Aa,Be,za,Pt,D,xa,fe,ja,Ca,Fe,Ua,Oa,At,Q,qa,We,Sa,Da,zt,I,Ia,Je,Ma,Ga,Le,Na,Ba,xt,de,jt,y,Fa,Re,Wa,Ja,Ve,La,Ra,He,Va,Ha,Xe,Xa,Ka,Ke,Ya,Qa,Ct,b,Za,Ye,eo,to,Qe,ao,oo,Ze,no,ro,et,so,io,tt,lo,co,at,po,ho,Ut,M,fo,ot,uo,mo,nt,_o,bo,Ot,ue,qt,J,Z,rt,me,go,st,yo,St,ye,wo,Dt,we,vo,It,ee,Mt;return _=new Lt({}),oe=new Lt({}),H=new $o({props:{$$slots:{default:[wn]},$$scope:{ctx:L}}}),re=new ae({props:{code:`def training_function():
    # Initialize accelerator
    accelerator = Accelerator()
    model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2)
    train_dataloader, eval_dataloader = create_dataloaders(
        train_batch_size=hyperparameters["train_batch_size"], eval_batch_size=hyperparameters["eval_batch_size"]
    )

    # Instantiate optimizer
    optimizer = AdamW(params=model.parameters(), lr=hyperparameters["learning_rate"])

    # Prepare everything
    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the
    # prepare method.
    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader
    )

    num_epochs = hyperparameters["num_epochs"]
    # Now we train the model
    for epoch in range(num_epochs):
        model.train()
        for step, batch in enumerate(train_dataloader):
            outputs = model(**batch)
            loss = outputs.loss
            accelerator.backward(loss)

            optimizer.step()
            optimizer.zero_grad()`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">training_function</span>():
    <span class="hljs-comment"># Initialize accelerator</span>
    accelerator = Accelerator()
    model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="hljs-number">2</span>)
    train_dataloader, eval_dataloader = create_dataloaders(
        train_batch_size=hyperparameters[<span class="hljs-string">&quot;train_batch_size&quot;</span>], eval_batch_size=hyperparameters[<span class="hljs-string">&quot;eval_batch_size&quot;</span>]
    )

    <span class="hljs-comment"># Instantiate optimizer</span>
    optimizer = AdamW(params=model.parameters(), lr=hyperparameters[<span class="hljs-string">&quot;learning_rate&quot;</span>])

    <span class="hljs-comment"># Prepare everything</span>
    <span class="hljs-comment"># There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the</span>
    <span class="hljs-comment"># prepare method.</span>
    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader
    )

    num_epochs = hyperparameters[<span class="hljs-string">&quot;num_epochs&quot;</span>]
    <span class="hljs-comment"># Now we train the model</span>
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):
        model.train()
        <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dataloader):
            outputs = model(**batch)
            loss = outputs.loss
            accelerator.backward(loss)

            optimizer.step()
            optimizer.zero_grad()`}}),se=new ae({props:{code:`from accelerate import notebook_launcher

notebook_launcher(training_function)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> notebook_launcher

notebook_launcher(training_function)`}}),X=new $o({props:{$$slots:{default:[vn]},$$scope:{ctx:L}}}),ie=new ae({props:{code:"ProcessExitedException: process 0 terminated with signal SIGSEGV",highlighted:'<span class="hljs-attribute">ProcessExitedException</span>: process <span class="hljs-number">0</span> terminated <span class="hljs-keyword">with</span> <span class="hljs-keyword">signal</span><span class="hljs-string"> SIGSEGV</span>'}}),le=new ae({props:{code:`# In another Jupyter cell
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2)`,highlighted:`<span class="hljs-comment"># In another Jupyter cell</span>
model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),ce=new ae({props:{code:`+ def training_function(model):
      # Initialize accelerator
      accelerator = Accelerator()
-     model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=2)
      train_dataloader, eval_dataloader = create_dataloaders(
          train_batch_size=hyperparameters["train_batch_size"], eval_batch_size=hyperparameters["eval_batch_size"]
      )
  ...`,highlighted:`<span class="hljs-addition">+ def training_function(model):</span>
      # Initialize accelerator
      accelerator = Accelerator()
<span class="hljs-deletion">-     model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=2)</span>
      train_dataloader, eval_dataloader = create_dataloaders(
          train_batch_size=hyperparameters[&quot;train_batch_size&quot;], eval_batch_size=hyperparameters[&quot;eval_batch_size&quot;]
      )
  ...`}}),pe=new ae({props:{code:`  from accelerate import notebook_launcher
- notebook_launcher(training_function)
+ notebook_launcher(training_function, (model,))`,highlighted:`  from accelerate import notebook_launcher
<span class="hljs-deletion">- notebook_launcher(training_function)</span>
<span class="hljs-addition">+ notebook_launcher(training_function, (model,))</span>`}}),he=new Lt({}),de=new ae({props:{code:'accelerator = Accelerator(mixed_precision="bf16")',highlighted:'accelerator = Accelerator(mixed_precision=<span class="hljs-string">&quot;bf16&quot;</span>)'}}),ue=new ae({props:{code:'accelerator = Accelerator(mixed_precision="bf16", downcast_bf16=True)',highlighted:'accelerator = Accelerator(mixed_precision=<span class="hljs-string">&quot;bf16&quot;</span>, downcast_bf16=<span class="hljs-literal">True</span>)'}}),me=new Lt({}),ee=new $o({props:{$$slots:{default:[$n]},$$scope:{ctx:L}}}),{c(){p=s("meta"),g=h(),d=s("h1"),m=s("a"),x=s("span"),w(_.$$.fragment),P=h(),C=s("span"),N=o("Training on TPUs with \u{1F917} Accelerate"),j=h(),B=s("p"),be=o(`Training on TPUs can be slightly different than training on multi-gpu, even with \u{1F917} Accelerate. This guide aims to show you
where you should be careful and why, as well as the best practices in general.`),ct=h(),F=s("h2"),R=s("a"),Pe=s("span"),w(oe.$$.fragment),Rt=h(),Ae=s("span"),Vt=o("Training in a Notebook"),pt=h(),U=s("p"),Ht=o("The main carepoint when training on TPUs comes from the "),ze=s("code"),Xt=o("notebook_launcher"),Kt=o(". As mentioned in the "),ne=s("a"),Yt=o("notebook tutorial"),Qt=o(`, you need to
restructure your training code into a function that can get passed to the `),xe=s("code"),Zt=o("notebook_launcher"),ea=o(" function and be careful about not declaring any tensors on the GPU."),ht=h(),A=s("p"),ta=o("While on a TPU that last part is not as important, a critical part to understand is that when you launch code from a notebook you do so through a process called "),je=s("strong"),aa=o("forking"),oa=o(`.
When launching from the command-line, you perform `),Ce=s("strong"),na=o("spawning"),ra=o(", where a python process is not currently running and you "),Ue=s("em"),sa=o("spawn"),ia=o(` a new process in. Since your Jupyter notebook is already
utilizing a python process, you need to `),Oe=s("em"),la=o("fork"),ca=o(" a new process from it to launch your code."),ft=h(),q=s("p"),pa=o("Where this becomes important is in regards to declaring your model. On forked TPU processes, it is recommended that you instantiate your model "),qe=s("em"),ha=o("once"),fa=o(` and pass this into your
training function. This is different than training on GPUs where you create `),Se=s("code"),da=o("n"),ua=o(` models that have their gradients synced and back-propagated at certain moments. Instead one
model instance is shared between all the nodes and it is passed back and forth. This is important especially when training on low-resource TPUs such as those provided in Kaggle kernels or
on Google Colaboratory.`),dt=h(),V=s("p"),ma=o("Below is an example of a training function passed to the "),De=s("code"),_a=o("notebook_launcher"),ba=o(" if training on CPUs or GPUs:"),ut=h(),w(H.$$.fragment),mt=h(),w(re.$$.fragment),_t=h(),w(se.$$.fragment),bt=h(),w(X.$$.fragment),gt=h(),K=s("p"),ga=o("If you use this example and declare the model "),Ie=s("em"),ya=o("inside"),wa=o(` the training loop, then on a low-resource system you will potentially see an error
like:`),yt=h(),w(ie.$$.fragment),wt=h(),S=s("p"),va=o("This error is "),Me=s("em"),$a=o("extremely"),Ea=o(` cryptic but the basic explaination is you ran out of system RAM. You can avoid this entirely by reconfiguring the training function to
accept a single `),Ge=s("code"),ka=o("model"),Ta=o(" argument, and declare it in an outside cell:"),vt=h(),w(le.$$.fragment),$t=h(),w(ce.$$.fragment),Et=h(),ge=s("p"),Pa=o("And finally calling the training function with:"),kt=h(),w(pe.$$.fragment),Tt=h(),W=s("h2"),Y=s("a"),Ne=s("span"),w(he.$$.fragment),Aa=h(),Be=s("span"),za=o("Mixed Precision and Global Variables"),Pt=h(),D=s("p"),xa=o("As mentioned in the "),fe=s("a"),ja=o("mixed precision tutorial"),Ca=o(`, \u{1F917} Accelerate supports fp16 and bf16, both of which can be used on TPUs.
That being said, ideally `),Fe=s("code"),Ua=o("bf16"),Oa=o(" should be utilized as it is extremely efficient to use."),At=h(),Q=s("p"),qa=o("There are two \u201Clayers\u201D when using "),We=s("code"),Sa=o("bf16"),Da=o(" and \u{1F917} Accelerate on TPUs, at the base level and at the operation level."),zt=h(),I=s("p"),Ia=o("At the base level, this is enabled when passing "),Je=s("code"),Ma=o('mixed_precision="bf16"'),Ga=o(" to "),Le=s("code"),Na=o("Accelerator"),Ba=o(", such as:"),xt=h(),w(de.$$.fragment),jt=h(),y=s("p"),Fa=o("By default this will cast "),Re=s("code"),Wa=o("torch.float"),Ja=o(" and "),Ve=s("code"),La=o("torch.double"),Ra=o(" to "),He=s("code"),Va=o("bfloat16"),Ha=o(` on TPUs.
The specific configuration being set is an environmental variable of `),Xe=s("code"),Xa=o("XLA_USE_BF16"),Ka=o(" is set to "),Ke=s("code"),Ya=o("1"),Qa=o("."),Ct=h(),b=s("p"),Za=o("There is a futher configuration you can perform which is setting the "),Ye=s("code"),eo=o("XLA_DOWNCAST_BF16"),to=o(" environmental variable. If set to "),Qe=s("code"),ao=o("1"),oo=o(`, then
`),Ze=s("code"),no=o("torch.float"),ro=o(" is "),et=s("code"),so=o("bfloat16"),io=o(" and "),tt=s("code"),lo=o("torch.double"),co=o(" is "),at=s("code"),po=o("float32"),ho=o("."),Ut=h(),M=s("p"),fo=o("This is performed in the "),ot=s("code"),uo=o("Accelerator"),mo=o(" object when passing "),nt=s("code"),_o=o("downcast_bf16=True"),bo=o(":"),Ot=h(),w(ue.$$.fragment),qt=h(),J=s("h2"),Z=s("a"),rt=s("span"),w(me.$$.fragment),go=h(),st=s("span"),yo=o("Training Times on TPUs"),St=h(),ye=s("p"),wo=o(`As you launch your script, you may notice that training seems exceptionally slow at first. This is because TPUs
first run through a few batches of data to see how much memory to allocate before finally utilizing this configured
memory allocation extremely efficiently.`),Dt=h(),we=s("p"),vo=o(`If you notice that your evaluation code to calculate the metrics of your model takes longer due to a larger batch size being used,
it is recommended to keep the batch size the same as the training data if it is too slow. Otherwise the memory will reallocate to this
new batch size after the first few iterations.`),It=h(),w(ee.$$.fragment),this.h()},l(e){const r=gn('[data-svelte="svelte-1phssyn"]',document.head);p=i(r,"META",{name:!0,content:!0}),r.forEach(a),g=f(e),d=i(e,"H1",{class:!0});var _e=l(d);m=i(_e,"A",{id:!0,class:!0,href:!0});var it=l(m);x=i(it,"SPAN",{});var lt=l(x);v(_.$$.fragment,lt),lt.forEach(a),it.forEach(a),P=f(_e),C=i(_e,"SPAN",{});var Eo=l(C);N=n(Eo,"Training on TPUs with \u{1F917} Accelerate"),Eo.forEach(a),_e.forEach(a),j=f(e),B=i(e,"P",{});var ko=l(B);be=n(ko,`Training on TPUs can be slightly different than training on multi-gpu, even with \u{1F917} Accelerate. This guide aims to show you
where you should be careful and why, as well as the best practices in general.`),ko.forEach(a),ct=f(e),F=i(e,"H2",{class:!0});var Gt=l(F);R=i(Gt,"A",{id:!0,class:!0,href:!0});var To=l(R);Pe=i(To,"SPAN",{});var Po=l(Pe);v(oe.$$.fragment,Po),Po.forEach(a),To.forEach(a),Rt=f(Gt),Ae=i(Gt,"SPAN",{});var Ao=l(Ae);Vt=n(Ao,"Training in a Notebook"),Ao.forEach(a),Gt.forEach(a),pt=f(e),U=i(e,"P",{});var te=l(U);Ht=n(te,"The main carepoint when training on TPUs comes from the "),ze=i(te,"CODE",{});var zo=l(ze);Xt=n(zo,"notebook_launcher"),zo.forEach(a),Kt=n(te,". As mentioned in the "),ne=i(te,"A",{href:!0,rel:!0});var xo=l(ne);Yt=n(xo,"notebook tutorial"),xo.forEach(a),Qt=n(te,`, you need to
restructure your training code into a function that can get passed to the `),xe=i(te,"CODE",{});var jo=l(xe);Zt=n(jo,"notebook_launcher"),jo.forEach(a),ea=n(te," function and be careful about not declaring any tensors on the GPU."),te.forEach(a),ht=f(e),A=i(e,"P",{});var G=l(A);ta=n(G,"While on a TPU that last part is not as important, a critical part to understand is that when you launch code from a notebook you do so through a process called "),je=i(G,"STRONG",{});var Co=l(je);aa=n(Co,"forking"),Co.forEach(a),oa=n(G,`.
When launching from the command-line, you perform `),Ce=i(G,"STRONG",{});var Uo=l(Ce);na=n(Uo,"spawning"),Uo.forEach(a),ra=n(G,", where a python process is not currently running and you "),Ue=i(G,"EM",{});var Oo=l(Ue);sa=n(Oo,"spawn"),Oo.forEach(a),ia=n(G,` a new process in. Since your Jupyter notebook is already
utilizing a python process, you need to `),Oe=i(G,"EM",{});var qo=l(Oe);la=n(qo,"fork"),qo.forEach(a),ca=n(G," a new process from it to launch your code."),G.forEach(a),ft=f(e),q=i(e,"P",{});var ve=l(q);pa=n(ve,"Where this becomes important is in regards to declaring your model. On forked TPU processes, it is recommended that you instantiate your model "),qe=i(ve,"EM",{});var So=l(qe);ha=n(So,"once"),So.forEach(a),fa=n(ve,` and pass this into your
training function. This is different than training on GPUs where you create `),Se=i(ve,"CODE",{});var Do=l(Se);da=n(Do,"n"),Do.forEach(a),ua=n(ve,` models that have their gradients synced and back-propagated at certain moments. Instead one
model instance is shared between all the nodes and it is passed back and forth. This is important especially when training on low-resource TPUs such as those provided in Kaggle kernels or
on Google Colaboratory.`),ve.forEach(a),dt=f(e),V=i(e,"P",{});var Nt=l(V);ma=n(Nt,"Below is an example of a training function passed to the "),De=i(Nt,"CODE",{});var Io=l(De);_a=n(Io,"notebook_launcher"),Io.forEach(a),ba=n(Nt," if training on CPUs or GPUs:"),Nt.forEach(a),ut=f(e),v(H.$$.fragment,e),mt=f(e),v(re.$$.fragment,e),_t=f(e),v(se.$$.fragment,e),bt=f(e),v(X.$$.fragment,e),gt=f(e),K=i(e,"P",{});var Bt=l(K);ga=n(Bt,"If you use this example and declare the model "),Ie=i(Bt,"EM",{});var Mo=l(Ie);ya=n(Mo,"inside"),Mo.forEach(a),wa=n(Bt,` the training loop, then on a low-resource system you will potentially see an error
like:`),Bt.forEach(a),yt=f(e),v(ie.$$.fragment,e),wt=f(e),S=i(e,"P",{});var $e=l(S);va=n($e,"This error is "),Me=i($e,"EM",{});var Go=l(Me);$a=n(Go,"extremely"),Go.forEach(a),Ea=n($e,` cryptic but the basic explaination is you ran out of system RAM. You can avoid this entirely by reconfiguring the training function to
accept a single `),Ge=i($e,"CODE",{});var No=l(Ge);ka=n(No,"model"),No.forEach(a),Ta=n($e," argument, and declare it in an outside cell:"),$e.forEach(a),vt=f(e),v(le.$$.fragment,e),$t=f(e),v(ce.$$.fragment,e),Et=f(e),ge=i(e,"P",{});var Bo=l(ge);Pa=n(Bo,"And finally calling the training function with:"),Bo.forEach(a),kt=f(e),v(pe.$$.fragment,e),Tt=f(e),W=i(e,"H2",{class:!0});var Ft=l(W);Y=i(Ft,"A",{id:!0,class:!0,href:!0});var Fo=l(Y);Ne=i(Fo,"SPAN",{});var Wo=l(Ne);v(he.$$.fragment,Wo),Wo.forEach(a),Fo.forEach(a),Aa=f(Ft),Be=i(Ft,"SPAN",{});var Jo=l(Be);za=n(Jo,"Mixed Precision and Global Variables"),Jo.forEach(a),Ft.forEach(a),Pt=f(e),D=i(e,"P",{});var Ee=l(D);xa=n(Ee,"As mentioned in the "),fe=i(Ee,"A",{href:!0,rel:!0});var Lo=l(fe);ja=n(Lo,"mixed precision tutorial"),Lo.forEach(a),Ca=n(Ee,`, \u{1F917} Accelerate supports fp16 and bf16, both of which can be used on TPUs.
That being said, ideally `),Fe=i(Ee,"CODE",{});var Ro=l(Fe);Ua=n(Ro,"bf16"),Ro.forEach(a),Oa=n(Ee," should be utilized as it is extremely efficient to use."),Ee.forEach(a),At=f(e),Q=i(e,"P",{});var Wt=l(Q);qa=n(Wt,"There are two \u201Clayers\u201D when using "),We=i(Wt,"CODE",{});var Vo=l(We);Sa=n(Vo,"bf16"),Vo.forEach(a),Da=n(Wt," and \u{1F917} Accelerate on TPUs, at the base level and at the operation level."),Wt.forEach(a),zt=f(e),I=i(e,"P",{});var ke=l(I);Ia=n(ke,"At the base level, this is enabled when passing "),Je=i(ke,"CODE",{});var Ho=l(Je);Ma=n(Ho,'mixed_precision="bf16"'),Ho.forEach(a),Ga=n(ke," to "),Le=i(ke,"CODE",{});var Xo=l(Le);Na=n(Xo,"Accelerator"),Xo.forEach(a),Ba=n(ke,", such as:"),ke.forEach(a),xt=f(e),v(de.$$.fragment,e),jt=f(e),y=i(e,"P",{});var O=l(y);Fa=n(O,"By default this will cast "),Re=i(O,"CODE",{});var Ko=l(Re);Wa=n(Ko,"torch.float"),Ko.forEach(a),Ja=n(O," and "),Ve=i(O,"CODE",{});var Yo=l(Ve);La=n(Yo,"torch.double"),Yo.forEach(a),Ra=n(O," to "),He=i(O,"CODE",{});var Qo=l(He);Va=n(Qo,"bfloat16"),Qo.forEach(a),Ha=n(O,` on TPUs.
The specific configuration being set is an environmental variable of `),Xe=i(O,"CODE",{});var Zo=l(Xe);Xa=n(Zo,"XLA_USE_BF16"),Zo.forEach(a),Ka=n(O," is set to "),Ke=i(O,"CODE",{});var en=l(Ke);Ya=n(en,"1"),en.forEach(a),Qa=n(O,"."),O.forEach(a),Ct=f(e),b=i(e,"P",{});var z=l(b);Za=n(z,"There is a futher configuration you can perform which is setting the "),Ye=i(z,"CODE",{});var tn=l(Ye);eo=n(tn,"XLA_DOWNCAST_BF16"),tn.forEach(a),to=n(z," environmental variable. If set to "),Qe=i(z,"CODE",{});var an=l(Qe);ao=n(an,"1"),an.forEach(a),oo=n(z,`, then
`),Ze=i(z,"CODE",{});var on=l(Ze);no=n(on,"torch.float"),on.forEach(a),ro=n(z," is "),et=i(z,"CODE",{});var nn=l(et);so=n(nn,"bfloat16"),nn.forEach(a),io=n(z," and "),tt=i(z,"CODE",{});var rn=l(tt);lo=n(rn,"torch.double"),rn.forEach(a),co=n(z," is "),at=i(z,"CODE",{});var sn=l(at);po=n(sn,"float32"),sn.forEach(a),ho=n(z,"."),z.forEach(a),Ut=f(e),M=i(e,"P",{});var Te=l(M);fo=n(Te,"This is performed in the "),ot=i(Te,"CODE",{});var ln=l(ot);uo=n(ln,"Accelerator"),ln.forEach(a),mo=n(Te," object when passing "),nt=i(Te,"CODE",{});var cn=l(nt);_o=n(cn,"downcast_bf16=True"),cn.forEach(a),bo=n(Te,":"),Te.forEach(a),Ot=f(e),v(ue.$$.fragment,e),qt=f(e),J=i(e,"H2",{class:!0});var Jt=l(J);Z=i(Jt,"A",{id:!0,class:!0,href:!0});var pn=l(Z);rt=i(pn,"SPAN",{});var hn=l(rt);v(me.$$.fragment,hn),hn.forEach(a),pn.forEach(a),go=f(Jt),st=i(Jt,"SPAN",{});var fn=l(st);yo=n(fn,"Training Times on TPUs"),fn.forEach(a),Jt.forEach(a),St=f(e),ye=i(e,"P",{});var dn=l(ye);wo=n(dn,`As you launch your script, you may notice that training seems exceptionally slow at first. This is because TPUs
first run through a few batches of data to see how much memory to allocate before finally utilizing this configured
memory allocation extremely efficiently.`),dn.forEach(a),Dt=f(e),we=i(e,"P",{});var un=l(we);vo=n(un,`If you notice that your evaluation code to calculate the metrics of your model takes longer due to a larger batch size being used,
it is recommended to keep the batch size the same as the training data if it is too slow. Otherwise the memory will reallocate to this
new batch size after the first few iterations.`),un.forEach(a),It=f(e),v(ee.$$.fragment,e),this.h()},h(){u(p,"name","hf:doc:metadata"),u(p,"content",JSON.stringify(kn)),u(m,"id","training-on-tpus-with-accelerate"),u(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(m,"href","#training-on-tpus-with-accelerate"),u(d,"class","relative group"),u(R,"id","training-in-a-notebook"),u(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(R,"href","#training-in-a-notebook"),u(F,"class","relative group"),u(ne,"href","https://huggingface.co/docs/accelerate/basic_tutorials/notebook"),u(ne,"rel","nofollow"),u(Y,"id","mixed-precision-and-global-variables"),u(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Y,"href","#mixed-precision-and-global-variables"),u(W,"class","relative group"),u(fe,"href","https://huggingface.co/docs/accelerate/usage_guides/mixed_precision"),u(fe,"rel","nofollow"),u(Z,"id","training-times-on-tpus"),u(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Z,"href","#training-times-on-tpus"),u(J,"class","relative group")},m(e,r){t(document.head,p),c(e,g,r),c(e,d,r),t(d,m),t(m,x),$(_,x,null),t(d,P),t(d,C),t(C,N),c(e,j,r),c(e,B,r),t(B,be),c(e,ct,r),c(e,F,r),t(F,R),t(R,Pe),$(oe,Pe,null),t(F,Rt),t(F,Ae),t(Ae,Vt),c(e,pt,r),c(e,U,r),t(U,Ht),t(U,ze),t(ze,Xt),t(U,Kt),t(U,ne),t(ne,Yt),t(U,Qt),t(U,xe),t(xe,Zt),t(U,ea),c(e,ht,r),c(e,A,r),t(A,ta),t(A,je),t(je,aa),t(A,oa),t(A,Ce),t(Ce,na),t(A,ra),t(A,Ue),t(Ue,sa),t(A,ia),t(A,Oe),t(Oe,la),t(A,ca),c(e,ft,r),c(e,q,r),t(q,pa),t(q,qe),t(qe,ha),t(q,fa),t(q,Se),t(Se,da),t(q,ua),c(e,dt,r),c(e,V,r),t(V,ma),t(V,De),t(De,_a),t(V,ba),c(e,ut,r),$(H,e,r),c(e,mt,r),$(re,e,r),c(e,_t,r),$(se,e,r),c(e,bt,r),$(X,e,r),c(e,gt,r),c(e,K,r),t(K,ga),t(K,Ie),t(Ie,ya),t(K,wa),c(e,yt,r),$(ie,e,r),c(e,wt,r),c(e,S,r),t(S,va),t(S,Me),t(Me,$a),t(S,Ea),t(S,Ge),t(Ge,ka),t(S,Ta),c(e,vt,r),$(le,e,r),c(e,$t,r),$(ce,e,r),c(e,Et,r),c(e,ge,r),t(ge,Pa),c(e,kt,r),$(pe,e,r),c(e,Tt,r),c(e,W,r),t(W,Y),t(Y,Ne),$(he,Ne,null),t(W,Aa),t(W,Be),t(Be,za),c(e,Pt,r),c(e,D,r),t(D,xa),t(D,fe),t(fe,ja),t(D,Ca),t(D,Fe),t(Fe,Ua),t(D,Oa),c(e,At,r),c(e,Q,r),t(Q,qa),t(Q,We),t(We,Sa),t(Q,Da),c(e,zt,r),c(e,I,r),t(I,Ia),t(I,Je),t(Je,Ma),t(I,Ga),t(I,Le),t(Le,Na),t(I,Ba),c(e,xt,r),$(de,e,r),c(e,jt,r),c(e,y,r),t(y,Fa),t(y,Re),t(Re,Wa),t(y,Ja),t(y,Ve),t(Ve,La),t(y,Ra),t(y,He),t(He,Va),t(y,Ha),t(y,Xe),t(Xe,Xa),t(y,Ka),t(y,Ke),t(Ke,Ya),t(y,Qa),c(e,Ct,r),c(e,b,r),t(b,Za),t(b,Ye),t(Ye,eo),t(b,to),t(b,Qe),t(Qe,ao),t(b,oo),t(b,Ze),t(Ze,no),t(b,ro),t(b,et),t(et,so),t(b,io),t(b,tt),t(tt,lo),t(b,co),t(b,at),t(at,po),t(b,ho),c(e,Ut,r),c(e,M,r),t(M,fo),t(M,ot),t(ot,uo),t(M,mo),t(M,nt),t(nt,_o),t(M,bo),c(e,Ot,r),$(ue,e,r),c(e,qt,r),c(e,J,r),t(J,Z),t(Z,rt),$(me,rt,null),t(J,go),t(J,st),t(st,yo),c(e,St,r),c(e,ye,r),t(ye,wo),c(e,Dt,r),c(e,we,r),t(we,vo),c(e,It,r),$(ee,e,r),Mt=!0},p(e,[r]){const _e={};r&2&&(_e.$$scope={dirty:r,ctx:e}),H.$set(_e);const it={};r&2&&(it.$$scope={dirty:r,ctx:e}),X.$set(it);const lt={};r&2&&(lt.$$scope={dirty:r,ctx:e}),ee.$set(lt)},i(e){Mt||(E(_.$$.fragment,e),E(oe.$$.fragment,e),E(H.$$.fragment,e),E(re.$$.fragment,e),E(se.$$.fragment,e),E(X.$$.fragment,e),E(ie.$$.fragment,e),E(le.$$.fragment,e),E(ce.$$.fragment,e),E(pe.$$.fragment,e),E(he.$$.fragment,e),E(de.$$.fragment,e),E(ue.$$.fragment,e),E(me.$$.fragment,e),E(ee.$$.fragment,e),Mt=!0)},o(e){k(_.$$.fragment,e),k(oe.$$.fragment,e),k(H.$$.fragment,e),k(re.$$.fragment,e),k(se.$$.fragment,e),k(X.$$.fragment,e),k(ie.$$.fragment,e),k(le.$$.fragment,e),k(ce.$$.fragment,e),k(pe.$$.fragment,e),k(he.$$.fragment,e),k(de.$$.fragment,e),k(ue.$$.fragment,e),k(me.$$.fragment,e),k(ee.$$.fragment,e),Mt=!1},d(e){a(p),e&&a(g),e&&a(d),T(_),e&&a(j),e&&a(B),e&&a(ct),e&&a(F),T(oe),e&&a(pt),e&&a(U),e&&a(ht),e&&a(A),e&&a(ft),e&&a(q),e&&a(dt),e&&a(V),e&&a(ut),T(H,e),e&&a(mt),T(re,e),e&&a(_t),T(se,e),e&&a(bt),T(X,e),e&&a(gt),e&&a(K),e&&a(yt),T(ie,e),e&&a(wt),e&&a(S),e&&a(vt),T(le,e),e&&a($t),T(ce,e),e&&a(Et),e&&a(ge),e&&a(kt),T(pe,e),e&&a(Tt),e&&a(W),T(he),e&&a(Pt),e&&a(D),e&&a(At),e&&a(Q),e&&a(zt),e&&a(I),e&&a(xt),T(de,e),e&&a(jt),e&&a(y),e&&a(Ct),e&&a(b),e&&a(Ut),e&&a(M),e&&a(Ot),T(ue,e),e&&a(qt),e&&a(J),T(me),e&&a(St),e&&a(ye),e&&a(Dt),e&&a(we),e&&a(It),T(ee,e)}}}const kn={local:"training-on-tpus-with-accelerate",sections:[{local:"training-in-a-notebook",title:"Training in a Notebook"},{local:"mixed-precision-and-global-variables",title:"Mixed Precision and Global Variables "},{local:"training-times-on-tpus",title:"Training Times on TPUs"}],title:"Training on TPUs with \u{1F917} Accelerate"};function Tn(L){return yn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class jn extends mn{constructor(p){super();_n(this,p,Tn,En,bn,{})}}export{jn as default,kn as metadata};
