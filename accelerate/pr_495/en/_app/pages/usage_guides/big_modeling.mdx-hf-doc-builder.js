import{S as Qi,i as Xi,s as Zi,e as o,k as p,w as _,t as l,M as en,c as i,d as t,m as d,a as n,x as w,h as r,b as c,G as s,g as h,y as v,q as g,o as y,B as $,v as tn}from"../../chunks/vendor-hf-doc-builder.js";import{T as Bo}from"../../chunks/Tip-hf-doc-builder.js";import{I as Ve}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as A}from"../../chunks/CodeBlock-hf-doc-builder.js";function sn(N){let f,j;return{c(){f=o("p"),j=l("This API is quite new and still in its experimental stage. While we strive to provide a stable API, it\u2019s possible some small parts of the public API will change in the future.")},l(m){f=i(m,"P",{});var u=n(f);j=r(u,"This API is quite new and still in its experimental stage. While we strive to provide a stable API, it\u2019s possible some small parts of the public API will change in the future."),u.forEach(t)},m(m,u){h(m,f,u),s(f,j)},d(m){m&&t(f)}}}function an(N){let f,j;return{c(){f=o("p"),j=l("You can\u2019t move a model initialized like this on CPU or another device directly, since it doesn\u2019t have any data. It\u2019s also very likely that a forward pass with that empty model will fail, as not all operations are supported on the meta device.")},l(m){f=i(m,"P",{});var u=n(f);j=r(u,"You can\u2019t move a model initialized like this on CPU or another device directly, since it doesn\u2019t have any data. It\u2019s also very likely that a forward pass with that empty model will fail, as not all operations are supported on the meta device."),u.forEach(t)},m(m,u){h(m,f,u),s(f,j)},d(m){m&&t(f)}}}function on(N){let f,j,m,u,L;return{c(){f=o("p"),j=l("This only supports inference of your model, not training. Most of the computation happens behind "),m=o("code"),u=l("torch.no_grad()"),L=l(" context managers to avoid spending some GPU memory with intermediate activations.")},l(E){f=i(E,"P",{});var C=n(f);j=r(C,"This only supports inference of your model, not training. Most of the computation happens behind "),m=i(C,"CODE",{});var H=n(m);u=r(H,"torch.no_grad()"),H.forEach(t),L=r(C," context managers to avoid spending some GPU memory with intermediate activations."),C.forEach(t)},m(E,C){h(E,f,C),s(f,j),s(f,m),s(m,u),s(f,L)},d(E){E&&t(f)}}}function nn(N){let f,j,m,u,L,E,C,H,Ns,St,Ee,Hs,Dt,ie,Ot,xe,Ws,Nt,U,Ke,Js,Fs,Qe,Ys,Vs,Xe,Ks,Ht,Ae,Qs,Wt,W,Jt,z,J,Ze,ne,Xs,et,Zs,Ft,F,ea,Ce,ta,sa,Yt,le,Vt,Ue,aa,Kt,re,Qt,Ie,oa,Xt,Y,Zt,B,V,tt,he,ia,st,na,es,Me,la,ts,Ge,ra,ss,pe,as,Te,ha,os,de,is,k,pa,at,da,ca,ot,fa,ma,it,ua,_a,nt,wa,va,lt,ga,ya,rt,$a,ns,R,K,ht,ce,ba,pt,ka,ls,Q,ja,qe,Pa,Ea,rs,X,xa,fe,Aa,Ca,hs,me,ps,Le,Ua,ds,ue,cs,ze,Ia,fs,_e,ms,Z,Ma,dt,Ga,Ta,us,I,ct,qa,La,ft,za,Ba,mt,Ra,_s,S,ut,Sa,Da,_t,Oa,Na,ws,M,Ha,wt,Wa,Ja,vt,Fa,Ya,vs,we,gs,ve,ys,ee,Va,gt,Ka,Qa,$s,ge,bs,D,te,yt,ye,Xa,$t,Za,ks,Be,eo,js,$e,Ps,Re,to,Es,G,bt,so,ao,kt,oo,io,jt,no,xs,Se,lo,As,se,Cs,O,ae,Pt,be,ro,Et,ho,Us,De,po,Is,b,xt,co,fo,T,At,mo,uo,Ct,_o,wo,Oe,vo,go,yo,q,Ut,$o,bo,It,ko,jo,Ne,Po,Eo,xo,oe,He,Ao,Co,Mt,Uo,Io,Mo,Gt,Go,To,Tt,qo,Lo,qt,zo,Ms;return E=new Ve({}),ie=new A({props:{code:`import torch

my_model = ModelClass(...)
state_dict = torch.load(checkpoint_file)
my_model.load_state_dict(state_dict)`,highlighted:`<span class="hljs-keyword">import</span> torch

my_model = ModelClass(...)
state_dict = torch.load(checkpoint_file)
my_model.load_state_dict(state_dict)`}}),W=new Bo({props:{warning:!0,$$slots:{default:[sn]},$$scope:{ctx:N}}}),ne=new Ve({}),le=new A({props:{code:`from accelerate import init_empty_weights

with init_empty_weights():
    my_model = ModelClass(...)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> init_empty_weights

<span class="hljs-keyword">with</span> init_empty_weights():
    my_model = ModelClass(...)`}}),re=new A({props:{code:`with init_empty_weights():
    model = nn.Sequential(*[nn.Linear(10000, 10000) for _ in range(1000)])`,highlighted:`<span class="hljs-keyword">with</span> init_empty_weights():
    model = nn.Sequential(*[nn.Linear(<span class="hljs-number">10000</span>, <span class="hljs-number">10000</span>) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>)])`}}),Y=new Bo({props:{warning:!0,$$slots:{default:[an]},$$scope:{ctx:N}}}),he=new Ve({}),pe=new A({props:{code:`first_state_dict.bin
index.json
second_state_dict.bin`,highlighted:`first_state_dict.bin
index.json
second_state_dict.bin`}}),de=new A({props:{code:`{
  "linear1.weight": "first_state_dict.bin",
  "linear1.bias": "first_state_dict.bin",
  "linear2.weight": "second_state_dict.bin",
  "linear2.bias": "second_state_dict.bin"
}`,highlighted:`<span class="hljs-punctuation">{</span>
  <span class="hljs-attr">&quot;linear1.weight&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;first_state_dict.bin&quot;</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;linear1.bias&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;first_state_dict.bin&quot;</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;linear2.weight&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;second_state_dict.bin&quot;</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;linear2.bias&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;second_state_dict.bin&quot;</span>
<span class="hljs-punctuation">}</span>`}}),ce=new Ve({}),me=new A({props:{code:`git clone https://huggingface.co/sgugger/sharded-gpt-j-6B
cd sharded-gpt-j-6B
git-lfs install
git pull`,highlighted:`git <span class="hljs-built_in">clone</span> https://huggingface.co/sgugger/sharded-gpt-j-6B
<span class="hljs-built_in">cd</span> sharded-gpt-j-6B
git-lfs install
git pull`}}),ue=new A({props:{code:`from accelerate import init_empty_weights
from transformers import AutoConfig, AutoModelForCausalLM

checkpoint = "EleutherAI/gpt-j-6B"
config = AutoConfig.from_pretrained(checkpoint)

with init_empty_weights():
    model = AutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> init_empty_weights
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

checkpoint = <span class="hljs-string">&quot;EleutherAI/gpt-j-6B&quot;</span>
config = AutoConfig.from_pretrained(checkpoint)

<span class="hljs-keyword">with</span> init_empty_weights():
    model = AutoModelForCausalLM.from_config(config)`}}),_e=new A({props:{code:`from accelerate import load_checkpoint_and_dispatch

model = load_checkpoint_and_dispatch(
    model, "sharded-gpt-j-6B", device_map="auto", no_split_module_classes=["GPTJBlock"]
)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> load_checkpoint_and_dispatch

model = load_checkpoint_and_dispatch(
    model, <span class="hljs-string">&quot;sharded-gpt-j-6B&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, no_split_module_classes=[<span class="hljs-string">&quot;GPTJBlock&quot;</span>]
)`}}),we=new A({props:{code:"model.hf_device_map",highlighted:"model.hf_device_map"}}),ve=new A({props:{code:`{'transformer.wte': 0,
 'transformer.drop': 0,
 'transformer.h.0': 0,
 'transformer.h.1': 0,
 'transformer.h.2': 0,
 'transformer.h.3': 0,
 'transformer.h.4': 0,
 'transformer.h.5': 0,
 'transformer.h.6': 0,
 'transformer.h.7': 0,
 'transformer.h.8': 0,
 'transformer.h.9': 0,
 'transformer.h.10': 0,
 'transformer.h.11': 0,
 'transformer.h.12': 0,
 'transformer.h.13': 0,
 'transformer.h.14': 0,
 'transformer.h.15': 0,
 'transformer.h.16': 0,
 'transformer.h.17': 0,
 'transformer.h.18': 0,
 'transformer.h.19': 0,
 'transformer.h.20': 0,
 'transformer.h.21': 0,
 'transformer.h.22': 0,
 'transformer.h.23': 0,
 'transformer.h.24': 1,
 'transformer.h.25': 1,
 'transformer.h.26': 1,
 'transformer.h.27': 1,
 'transformer.ln_f': 1,
 'lm_head': 1}`,highlighted:`{<span class="hljs-string">&#x27;transformer.wte&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.drop&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.0&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.1&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.2&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.3&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.4&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.5&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.6&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.7&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.8&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.9&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.10&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.11&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.12&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.13&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.14&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.15&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.16&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.17&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.18&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.19&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.20&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.21&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.22&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.23&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;transformer.h.24&#x27;</span>: <span class="hljs-number">1</span>,
 <span class="hljs-string">&#x27;transformer.h.25&#x27;</span>: <span class="hljs-number">1</span>,
 <span class="hljs-string">&#x27;transformer.h.26&#x27;</span>: <span class="hljs-number">1</span>,
 <span class="hljs-string">&#x27;transformer.h.27&#x27;</span>: <span class="hljs-number">1</span>,
 <span class="hljs-string">&#x27;transformer.ln_f&#x27;</span>: <span class="hljs-number">1</span>,
 <span class="hljs-string">&#x27;lm_head&#x27;</span>: <span class="hljs-number">1</span>}`}}),ge=new A({props:{code:'model = load_checkpoint_and_dispatch(model, "sharded-gpt-j-6B", device_map=my_device_map)',highlighted:'model = load_checkpoint_and_dispatch(model, <span class="hljs-string">&quot;sharded-gpt-j-6B&quot;</span>, device_map=my_device_map)'}}),ye=new Ve({}),$e=new A({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer("Hello, my name is", return_tensors="pt")
inputs = inputs.to(0)
output = model.generate(inputs["input_ids"])
tokenizer.decode(output[0].tolist())`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(<span class="hljs-string">&quot;Hello, my name is&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
inputs = inputs.to(<span class="hljs-number">0</span>)
output = model.generate(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])
tokenizer.decode(output[<span class="hljs-number">0</span>].tolist())`}}),se=new Bo({props:{warning:!0,$$slots:{default:[on]},$$scope:{ctx:N}}}),be=new Ve({}),{c(){f=o("meta"),j=p(),m=o("h1"),u=o("a"),L=o("span"),_(E.$$.fragment),C=p(),H=o("span"),Ns=l("Handling big models"),St=p(),Ee=o("p"),Hs=l("When loading a pretrained model in PyTorch, the usual workflow looks like this:"),Dt=p(),_(ie.$$.fragment),Ot=p(),xe=o("p"),Ws=l("In plain English, those steps are:"),Nt=p(),U=o("ol"),Ke=o("li"),Js=l("Create the model with randomly initialized weights"),Fs=p(),Qe=o("li"),Ys=l("Load the model weights (in a dictionary usually called a state dict) from the disk"),Vs=p(),Xe=o("li"),Ks=l("Load those weights inside the model"),Ht=p(),Ae=o("p"),Qs=l("While this works very well for regularly sized models, this workflow has some clear limitations when we deal with a huge model: in step 1, we load a full version of the model in RAM, and spend some time randomly initializing the weights (which will be discarded in step 3). In step 2, we load another full version of the model in RAM, with the pretrained weights. If you\u2019re loading a model with 6 billions parameters, this means you will need 24GB of RAM for each copy of the model, so 48GB in total (half of it to load the model in FP16)."),Wt=p(),_(W.$$.fragment),Jt=p(),z=o("h2"),J=o("a"),Ze=o("span"),_(ne.$$.fragment),Xs=p(),et=o("span"),Zs=l("Instantiating an empty model"),Ft=p(),F=o("p"),ea=l("The first tool \u{1F917} Accelerate introduces to help with big models is a context manager "),Ce=o("a"),ta=l("init_empty_weights()"),sa=l(" that helps you initialize a model without using any RAM, so that step 1 can be done on models of any size. Here is how it works:"),Yt=p(),_(le.$$.fragment),Vt=p(),Ue=o("p"),aa=l("For instance:"),Kt=p(),_(re.$$.fragment),Qt=p(),Ie=o("p"),oa=l("initializes an empty model with a bit more than 100B parameters. Behind the scenes, this relies on the meta device introduced in PyTorch 1.9. During the initialization under the context manager, each time a parameter is created, it is instantly moved on that device."),Xt=p(),_(Y.$$.fragment),Zt=p(),B=o("h2"),V=o("a"),tt=o("span"),_(he.$$.fragment),ia=p(),st=o("span"),na=l("Sharded checkpoints"),es=p(),Me=o("p"),la=l("It\u2019s possible your model is so big that even a single copy won\u2019t fit in RAM. That doesn\u2019t mean it can\u2019t be loaded: if you have one or several GPUs, this is more memory available to store your model. In this case, it\u2019s better if your checkpoint is split in several smaller files that we call checkpoint shards."),ts=p(),Ge=o("p"),ra=l("\u{1F917} Accelerate will handle sharded checkpoints as long as you follow the following format: your checkpoint should be in a folder, with several files containing the partial state dicts, and there should be an index in the JSON format that contains a dictionary mapping parameter names to the file containing their weights. For instance we could have a folder containing:"),ss=p(),_(pe.$$.fragment),as=p(),Te=o("p"),ha=l("with index.json being the following file:"),os=p(),_(de.$$.fragment),is=p(),k=o("p"),pa=l("and "),at=o("code"),da=l("first_state_dict.bin"),ca=l(" containing the weights for "),ot=o("code"),fa=l('"linear1.weight"'),ma=l(" and "),it=o("code"),ua=l('"linear1.bias"'),_a=l(", "),nt=o("code"),wa=l("second_state_dict.bin"),va=l(" the ones for "),lt=o("code"),ga=l('"linear2.weight"'),ya=l(" and "),rt=o("code"),$a=l('"linear2.bias"'),ns=p(),R=o("h2"),K=o("a"),ht=o("span"),_(ce.$$.fragment),ba=p(),pt=o("span"),ka=l("Loading weights"),ls=p(),Q=o("p"),ja=l("The second tool \u{1F917} Accelerate introduces is a function "),qe=o("a"),Pa=l("load_checkpoint_and_dispatch()"),Ea=l(", that will allow you to load a checkpoint inside your empty model. This supports full checkpoints (a single file containing the whole state dict) as well as sharded checkpoints. It will also automatically dispatch those weights across the devices you have available (GPUs, CPU RAM), so if you are loading a sharded checkpoint, the maximum RAM usage will be the size of the biggest shard."),rs=p(),X=o("p"),xa=l("Here is how we can use this to load the "),fe=o("a"),Aa=l("GPT-J-6B"),Ca=l(" model. You clone the sharded version of this model with:"),hs=p(),_(me.$$.fragment),ps=p(),Le=o("p"),Ua=l("then we can initialize the model with"),ds=p(),_(ue.$$.fragment),cs=p(),ze=o("p"),Ia=l("and load the checkpoint we just downloaded with:"),fs=p(),_(_e.$$.fragment),ms=p(),Z=o("p"),Ma=l("By passing "),dt=o("code"),Ga=l('device_map="auto"'),Ta=l(", we tell \u{1F917} Accelerate to determine automatically where to put each layer of the model depending on the available resources:"),us=p(),I=o("ul"),ct=o("li"),qa=l("first we use the maximum space available on the GPU(s)"),La=p(),ft=o("li"),za=l("if we still need space, we store the remaining weights on the CPU"),Ba=p(),mt=o("li"),Ra=l("if there is not enough RAM, we store the remaining weights on the hard drive as memory-mapped tensors"),_s=p(),S=o("p"),ut=o("code"),Sa=l('no_split_module_classes=["GPTJBlock"]'),Da=l(" indicates that the modules that are "),_t=o("code"),Oa=l("GPTJBlock"),Na=l(" should not be split on different devices. You should set here all blocks that include a residual connection of some kind."),ws=p(),M=o("p"),Ha=l("You can see the "),wt=o("code"),Wa=l("device_map"),Ja=l(" that \u{1F917} Accelerate picked by accessing the "),vt=o("code"),Fa=l("hf_device_map"),Ya=l(" attribute of your model:"),vs=p(),_(we.$$.fragment),gs=p(),_(ve.$$.fragment),ys=p(),ee=o("p"),Va=l("You can also design your "),gt=o("code"),Ka=l("device_map"),Qa=l(" yourself, if you prefer to explicitly decide where each layer should be. In this case, the command above becomes:"),$s=p(),_(ge.$$.fragment),bs=p(),D=o("h2"),te=o("a"),yt=o("span"),_(ye.$$.fragment),Xa=p(),$t=o("span"),Za=l("Run the model"),ks=p(),Be=o("p"),eo=l("Now that we have done this, our model lies across several devices, and maybe the hard drive. But it can still be used as a regular PyTorch model:"),js=p(),_($e.$$.fragment),Ps=p(),Re=o("p"),to=l("Behind the scenes, \u{1F917} Accelerate added hooks to the model, so that:"),Es=p(),G=o("ul"),bt=o("li"),so=l("at each layer, the inputs are put on the right device (so even if your model is spread across several GPUs, it works)"),ao=p(),kt=o("li"),oo=l("for the weights offloaded on the CPU, they are put on a GPU just before the forward pass, and cleaned up just after"),io=p(),jt=o("li"),no=l("for the weights offloaded on the hard drive, they are loaded in RAM then put on a GPU just before the forward pass, and cleaned up just after"),xs=p(),Se=o("p"),lo=l("This way, you model can run for inference even if it doesn\u2019t fit on one of the GPUs or the CPU RAM!"),As=p(),_(se.$$.fragment),Cs=p(),O=o("h2"),ae=o("a"),Pt=o("span"),_(be.$$.fragment),ro=p(),Et=o("span"),ho=l("Limits and further development"),Us=p(),De=o("p"),po=l("We are aware of the current limitations in the API:"),Is=p(),b=o("ul"),xt=o("li"),co=l("While this could theoretically work on just one CPU with potential disk offload, you need at least one GPU to run this API. This will be fixed in further development."),fo=p(),T=o("li"),At=o("code"),mo=l("infer_auto_device_map()"),uo=l(" (or "),Ct=o("code"),_o=l('device_map="auto"'),wo=l(" in "),Oe=o("a"),vo=l("load_checkpoint_and_dispatch()"),go=l(") tries to maximize GPU and CPU RAM it sees available when you execute it. While PyTorch is very good at managing GPU RAM efficiently (and giving it back when not needed), it\u2019s not entirely true with Python and CPU RAM. Therefore, an automatically computed device map might be too intense on the CPU. Move a few modules to the disk device if you get crashes due to lack of RAM."),yo=p(),q=o("li"),Ut=o("code"),$o=l("infer_auto_device_map()"),bo=l(" (or "),It=o("code"),ko=l('device_map="auto"'),jo=l(" in "),Ne=o("a"),Po=l("load_checkpoint_and_dispatch()"),Eo=l(") attributes devices sequentially (to avoid moving things back and forth) so if your first layer is bigger than the size of the GPU you have, it will end up with everything on the CPU/Disk."),xo=p(),oe=o("li"),He=o("a"),Ao=l("load_checkpoint_and_dispatch()"),Co=l(" and "),Mt=o("code"),Uo=l("load_checkpoint_in_model()"),Io=l(" do not perform any check on the correctness of your state dict compared to your model at the moment (this will be fixed in a future version), so you may get some weird errors if trying to load a checkpoint with mismatched or missing keys."),Mo=p(),Gt=o("li"),Go=l("The model parallelism used when your model is split on several GPUs is naive and not optimized, meaning that only one GPU works at a given time and the other sits idle."),To=p(),Tt=o("li"),qo=l("When weights are offloaded on the CPU/hard drive, there is no pre-fetching (yet, we will work on this for future versions) which means the weights are put on the GPU when they are needed and not before."),Lo=p(),qt=o("li"),zo=l("Hard-drive offloading might be very slow if the hardware you run on does not have fast communication between disk and CPU (like NVMes)."),this.h()},l(e){const a=en('[data-svelte="svelte-1phssyn"]',document.head);f=i(a,"META",{name:!0,content:!0}),a.forEach(t),j=d(e),m=i(e,"H1",{class:!0});var ke=n(m);u=i(ke,"A",{id:!0,class:!0,href:!0});var Lt=n(u);L=i(Lt,"SPAN",{});var zt=n(L);w(E.$$.fragment,zt),zt.forEach(t),Lt.forEach(t),C=d(ke),H=i(ke,"SPAN",{});var Ro=n(H);Ns=r(Ro,"Handling big models"),Ro.forEach(t),ke.forEach(t),St=d(e),Ee=i(e,"P",{});var So=n(Ee);Hs=r(So,"When loading a pretrained model in PyTorch, the usual workflow looks like this:"),So.forEach(t),Dt=d(e),w(ie.$$.fragment,e),Ot=d(e),xe=i(e,"P",{});var Do=n(xe);Ws=r(Do,"In plain English, those steps are:"),Do.forEach(t),Nt=d(e),U=i(e,"OL",{});var We=n(U);Ke=i(We,"LI",{});var Oo=n(Ke);Js=r(Oo,"Create the model with randomly initialized weights"),Oo.forEach(t),Fs=d(We),Qe=i(We,"LI",{});var No=n(Qe);Ys=r(No,"Load the model weights (in a dictionary usually called a state dict) from the disk"),No.forEach(t),Vs=d(We),Xe=i(We,"LI",{});var Ho=n(Xe);Ks=r(Ho,"Load those weights inside the model"),Ho.forEach(t),We.forEach(t),Ht=d(e),Ae=i(e,"P",{});var Wo=n(Ae);Qs=r(Wo,"While this works very well for regularly sized models, this workflow has some clear limitations when we deal with a huge model: in step 1, we load a full version of the model in RAM, and spend some time randomly initializing the weights (which will be discarded in step 3). In step 2, we load another full version of the model in RAM, with the pretrained weights. If you\u2019re loading a model with 6 billions parameters, this means you will need 24GB of RAM for each copy of the model, so 48GB in total (half of it to load the model in FP16)."),Wo.forEach(t),Wt=d(e),w(W.$$.fragment,e),Jt=d(e),z=i(e,"H2",{class:!0});var Gs=n(z);J=i(Gs,"A",{id:!0,class:!0,href:!0});var Jo=n(J);Ze=i(Jo,"SPAN",{});var Fo=n(Ze);w(ne.$$.fragment,Fo),Fo.forEach(t),Jo.forEach(t),Xs=d(Gs),et=i(Gs,"SPAN",{});var Yo=n(et);Zs=r(Yo,"Instantiating an empty model"),Yo.forEach(t),Gs.forEach(t),Ft=d(e),F=i(e,"P",{});var Ts=n(F);ea=r(Ts,"The first tool \u{1F917} Accelerate introduces to help with big models is a context manager "),Ce=i(Ts,"A",{href:!0});var Vo=n(Ce);ta=r(Vo,"init_empty_weights()"),Vo.forEach(t),sa=r(Ts," that helps you initialize a model without using any RAM, so that step 1 can be done on models of any size. Here is how it works:"),Ts.forEach(t),Yt=d(e),w(le.$$.fragment,e),Vt=d(e),Ue=i(e,"P",{});var Ko=n(Ue);aa=r(Ko,"For instance:"),Ko.forEach(t),Kt=d(e),w(re.$$.fragment,e),Qt=d(e),Ie=i(e,"P",{});var Qo=n(Ie);oa=r(Qo,"initializes an empty model with a bit more than 100B parameters. Behind the scenes, this relies on the meta device introduced in PyTorch 1.9. During the initialization under the context manager, each time a parameter is created, it is instantly moved on that device."),Qo.forEach(t),Xt=d(e),w(Y.$$.fragment,e),Zt=d(e),B=i(e,"H2",{class:!0});var qs=n(B);V=i(qs,"A",{id:!0,class:!0,href:!0});var Xo=n(V);tt=i(Xo,"SPAN",{});var Zo=n(tt);w(he.$$.fragment,Zo),Zo.forEach(t),Xo.forEach(t),ia=d(qs),st=i(qs,"SPAN",{});var ei=n(st);na=r(ei,"Sharded checkpoints"),ei.forEach(t),qs.forEach(t),es=d(e),Me=i(e,"P",{});var ti=n(Me);la=r(ti,"It\u2019s possible your model is so big that even a single copy won\u2019t fit in RAM. That doesn\u2019t mean it can\u2019t be loaded: if you have one or several GPUs, this is more memory available to store your model. In this case, it\u2019s better if your checkpoint is split in several smaller files that we call checkpoint shards."),ti.forEach(t),ts=d(e),Ge=i(e,"P",{});var si=n(Ge);ra=r(si,"\u{1F917} Accelerate will handle sharded checkpoints as long as you follow the following format: your checkpoint should be in a folder, with several files containing the partial state dicts, and there should be an index in the JSON format that contains a dictionary mapping parameter names to the file containing their weights. For instance we could have a folder containing:"),si.forEach(t),ss=d(e),w(pe.$$.fragment,e),as=d(e),Te=i(e,"P",{});var ai=n(Te);ha=r(ai,"with index.json being the following file:"),ai.forEach(t),os=d(e),w(de.$$.fragment,e),is=d(e),k=i(e,"P",{});var x=n(k);pa=r(x,"and "),at=i(x,"CODE",{});var oi=n(at);da=r(oi,"first_state_dict.bin"),oi.forEach(t),ca=r(x," containing the weights for "),ot=i(x,"CODE",{});var ii=n(ot);fa=r(ii,'"linear1.weight"'),ii.forEach(t),ma=r(x," and "),it=i(x,"CODE",{});var ni=n(it);ua=r(ni,'"linear1.bias"'),ni.forEach(t),_a=r(x,", "),nt=i(x,"CODE",{});var li=n(nt);wa=r(li,"second_state_dict.bin"),li.forEach(t),va=r(x," the ones for "),lt=i(x,"CODE",{});var ri=n(lt);ga=r(ri,'"linear2.weight"'),ri.forEach(t),ya=r(x," and "),rt=i(x,"CODE",{});var hi=n(rt);$a=r(hi,'"linear2.bias"'),hi.forEach(t),x.forEach(t),ns=d(e),R=i(e,"H2",{class:!0});var Ls=n(R);K=i(Ls,"A",{id:!0,class:!0,href:!0});var pi=n(K);ht=i(pi,"SPAN",{});var di=n(ht);w(ce.$$.fragment,di),di.forEach(t),pi.forEach(t),ba=d(Ls),pt=i(Ls,"SPAN",{});var ci=n(pt);ka=r(ci,"Loading weights"),ci.forEach(t),Ls.forEach(t),ls=d(e),Q=i(e,"P",{});var zs=n(Q);ja=r(zs,"The second tool \u{1F917} Accelerate introduces is a function "),qe=i(zs,"A",{href:!0});var fi=n(qe);Pa=r(fi,"load_checkpoint_and_dispatch()"),fi.forEach(t),Ea=r(zs,", that will allow you to load a checkpoint inside your empty model. This supports full checkpoints (a single file containing the whole state dict) as well as sharded checkpoints. It will also automatically dispatch those weights across the devices you have available (GPUs, CPU RAM), so if you are loading a sharded checkpoint, the maximum RAM usage will be the size of the biggest shard."),zs.forEach(t),rs=d(e),X=i(e,"P",{});var Bs=n(X);xa=r(Bs,"Here is how we can use this to load the "),fe=i(Bs,"A",{href:!0,rel:!0});var mi=n(fe);Aa=r(mi,"GPT-J-6B"),mi.forEach(t),Ca=r(Bs," model. You clone the sharded version of this model with:"),Bs.forEach(t),hs=d(e),w(me.$$.fragment,e),ps=d(e),Le=i(e,"P",{});var ui=n(Le);Ua=r(ui,"then we can initialize the model with"),ui.forEach(t),ds=d(e),w(ue.$$.fragment,e),cs=d(e),ze=i(e,"P",{});var _i=n(ze);Ia=r(_i,"and load the checkpoint we just downloaded with:"),_i.forEach(t),fs=d(e),w(_e.$$.fragment,e),ms=d(e),Z=i(e,"P",{});var Rs=n(Z);Ma=r(Rs,"By passing "),dt=i(Rs,"CODE",{});var wi=n(dt);Ga=r(wi,'device_map="auto"'),wi.forEach(t),Ta=r(Rs,", we tell \u{1F917} Accelerate to determine automatically where to put each layer of the model depending on the available resources:"),Rs.forEach(t),us=d(e),I=i(e,"UL",{});var Je=n(I);ct=i(Je,"LI",{});var vi=n(ct);qa=r(vi,"first we use the maximum space available on the GPU(s)"),vi.forEach(t),La=d(Je),ft=i(Je,"LI",{});var gi=n(ft);za=r(gi,"if we still need space, we store the remaining weights on the CPU"),gi.forEach(t),Ba=d(Je),mt=i(Je,"LI",{});var yi=n(mt);Ra=r(yi,"if there is not enough RAM, we store the remaining weights on the hard drive as memory-mapped tensors"),yi.forEach(t),Je.forEach(t),_s=d(e),S=i(e,"P",{});var Bt=n(S);ut=i(Bt,"CODE",{});var $i=n(ut);Sa=r($i,'no_split_module_classes=["GPTJBlock"]'),$i.forEach(t),Da=r(Bt," indicates that the modules that are "),_t=i(Bt,"CODE",{});var bi=n(_t);Oa=r(bi,"GPTJBlock"),bi.forEach(t),Na=r(Bt," should not be split on different devices. You should set here all blocks that include a residual connection of some kind."),Bt.forEach(t),ws=d(e),M=i(e,"P",{});var Fe=n(M);Ha=r(Fe,"You can see the "),wt=i(Fe,"CODE",{});var ki=n(wt);Wa=r(ki,"device_map"),ki.forEach(t),Ja=r(Fe," that \u{1F917} Accelerate picked by accessing the "),vt=i(Fe,"CODE",{});var ji=n(vt);Fa=r(ji,"hf_device_map"),ji.forEach(t),Ya=r(Fe," attribute of your model:"),Fe.forEach(t),vs=d(e),w(we.$$.fragment,e),gs=d(e),w(ve.$$.fragment,e),ys=d(e),ee=i(e,"P",{});var Ss=n(ee);Va=r(Ss,"You can also design your "),gt=i(Ss,"CODE",{});var Pi=n(gt);Ka=r(Pi,"device_map"),Pi.forEach(t),Qa=r(Ss," yourself, if you prefer to explicitly decide where each layer should be. In this case, the command above becomes:"),Ss.forEach(t),$s=d(e),w(ge.$$.fragment,e),bs=d(e),D=i(e,"H2",{class:!0});var Ds=n(D);te=i(Ds,"A",{id:!0,class:!0,href:!0});var Ei=n(te);yt=i(Ei,"SPAN",{});var xi=n(yt);w(ye.$$.fragment,xi),xi.forEach(t),Ei.forEach(t),Xa=d(Ds),$t=i(Ds,"SPAN",{});var Ai=n($t);Za=r(Ai,"Run the model"),Ai.forEach(t),Ds.forEach(t),ks=d(e),Be=i(e,"P",{});var Ci=n(Be);eo=r(Ci,"Now that we have done this, our model lies across several devices, and maybe the hard drive. But it can still be used as a regular PyTorch model:"),Ci.forEach(t),js=d(e),w($e.$$.fragment,e),Ps=d(e),Re=i(e,"P",{});var Ui=n(Re);to=r(Ui,"Behind the scenes, \u{1F917} Accelerate added hooks to the model, so that:"),Ui.forEach(t),Es=d(e),G=i(e,"UL",{});var Ye=n(G);bt=i(Ye,"LI",{});var Ii=n(bt);so=r(Ii,"at each layer, the inputs are put on the right device (so even if your model is spread across several GPUs, it works)"),Ii.forEach(t),ao=d(Ye),kt=i(Ye,"LI",{});var Mi=n(kt);oo=r(Mi,"for the weights offloaded on the CPU, they are put on a GPU just before the forward pass, and cleaned up just after"),Mi.forEach(t),io=d(Ye),jt=i(Ye,"LI",{});var Gi=n(jt);no=r(Gi,"for the weights offloaded on the hard drive, they are loaded in RAM then put on a GPU just before the forward pass, and cleaned up just after"),Gi.forEach(t),Ye.forEach(t),xs=d(e),Se=i(e,"P",{});var Ti=n(Se);lo=r(Ti,"This way, you model can run for inference even if it doesn\u2019t fit on one of the GPUs or the CPU RAM!"),Ti.forEach(t),As=d(e),w(se.$$.fragment,e),Cs=d(e),O=i(e,"H2",{class:!0});var Os=n(O);ae=i(Os,"A",{id:!0,class:!0,href:!0});var qi=n(ae);Pt=i(qi,"SPAN",{});var Li=n(Pt);w(be.$$.fragment,Li),Li.forEach(t),qi.forEach(t),ro=d(Os),Et=i(Os,"SPAN",{});var zi=n(Et);ho=r(zi,"Limits and further development"),zi.forEach(t),Os.forEach(t),Us=d(e),De=i(e,"P",{});var Bi=n(De);po=r(Bi,"We are aware of the current limitations in the API:"),Bi.forEach(t),Is=d(e),b=i(e,"UL",{});var P=n(b);xt=i(P,"LI",{});var Ri=n(xt);co=r(Ri,"While this could theoretically work on just one CPU with potential disk offload, you need at least one GPU to run this API. This will be fixed in further development."),Ri.forEach(t),fo=d(P),T=i(P,"LI",{});var je=n(T);At=i(je,"CODE",{});var Si=n(At);mo=r(Si,"infer_auto_device_map()"),Si.forEach(t),uo=r(je," (or "),Ct=i(je,"CODE",{});var Di=n(Ct);_o=r(Di,'device_map="auto"'),Di.forEach(t),wo=r(je," in "),Oe=i(je,"A",{href:!0});var Oi=n(Oe);vo=r(Oi,"load_checkpoint_and_dispatch()"),Oi.forEach(t),go=r(je,") tries to maximize GPU and CPU RAM it sees available when you execute it. While PyTorch is very good at managing GPU RAM efficiently (and giving it back when not needed), it\u2019s not entirely true with Python and CPU RAM. Therefore, an automatically computed device map might be too intense on the CPU. Move a few modules to the disk device if you get crashes due to lack of RAM."),je.forEach(t),yo=d(P),q=i(P,"LI",{});var Pe=n(q);Ut=i(Pe,"CODE",{});var Ni=n(Ut);$o=r(Ni,"infer_auto_device_map()"),Ni.forEach(t),bo=r(Pe," (or "),It=i(Pe,"CODE",{});var Hi=n(It);ko=r(Hi,'device_map="auto"'),Hi.forEach(t),jo=r(Pe," in "),Ne=i(Pe,"A",{href:!0});var Wi=n(Ne);Po=r(Wi,"load_checkpoint_and_dispatch()"),Wi.forEach(t),Eo=r(Pe,") attributes devices sequentially (to avoid moving things back and forth) so if your first layer is bigger than the size of the GPU you have, it will end up with everything on the CPU/Disk."),Pe.forEach(t),xo=d(P),oe=i(P,"LI",{});var Rt=n(oe);He=i(Rt,"A",{href:!0});var Ji=n(He);Ao=r(Ji,"load_checkpoint_and_dispatch()"),Ji.forEach(t),Co=r(Rt," and "),Mt=i(Rt,"CODE",{});var Fi=n(Mt);Uo=r(Fi,"load_checkpoint_in_model()"),Fi.forEach(t),Io=r(Rt," do not perform any check on the correctness of your state dict compared to your model at the moment (this will be fixed in a future version), so you may get some weird errors if trying to load a checkpoint with mismatched or missing keys."),Rt.forEach(t),Mo=d(P),Gt=i(P,"LI",{});var Yi=n(Gt);Go=r(Yi,"The model parallelism used when your model is split on several GPUs is naive and not optimized, meaning that only one GPU works at a given time and the other sits idle."),Yi.forEach(t),To=d(P),Tt=i(P,"LI",{});var Vi=n(Tt);qo=r(Vi,"When weights are offloaded on the CPU/hard drive, there is no pre-fetching (yet, we will work on this for future versions) which means the weights are put on the GPU when they are needed and not before."),Vi.forEach(t),Lo=d(P),qt=i(P,"LI",{});var Ki=n(qt);zo=r(Ki,"Hard-drive offloading might be very slow if the hardware you run on does not have fast communication between disk and CPU (like NVMes)."),Ki.forEach(t),P.forEach(t),this.h()},h(){c(f,"name","hf:doc:metadata"),c(f,"content",JSON.stringify(ln)),c(u,"id","handling-big-models"),c(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(u,"href","#handling-big-models"),c(m,"class","relative group"),c(J,"id","instantiating-an-empty-model"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#instantiating-an-empty-model"),c(z,"class","relative group"),c(Ce,"href","/docs/accelerate/pr_495/en/package_reference/big_modeling#accelerate.init_empty_weights"),c(V,"id","sharded-checkpoints"),c(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(V,"href","#sharded-checkpoints"),c(B,"class","relative group"),c(K,"id","loading-weights"),c(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K,"href","#loading-weights"),c(R,"class","relative group"),c(qe,"href","/docs/accelerate/pr_495/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch"),c(fe,"href","https://huggingface.co/EleutherAI/gpt-j-6B"),c(fe,"rel","nofollow"),c(te,"id","run-the-model"),c(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(te,"href","#run-the-model"),c(D,"class","relative group"),c(ae,"id","limits-and-further-development"),c(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ae,"href","#limits-and-further-development"),c(O,"class","relative group"),c(Oe,"href","/docs/accelerate/pr_495/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch"),c(Ne,"href","/docs/accelerate/pr_495/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch"),c(He,"href","/docs/accelerate/pr_495/en/package_reference/big_modeling#accelerate.load_checkpoint_and_dispatch")},m(e,a){s(document.head,f),h(e,j,a),h(e,m,a),s(m,u),s(u,L),v(E,L,null),s(m,C),s(m,H),s(H,Ns),h(e,St,a),h(e,Ee,a),s(Ee,Hs),h(e,Dt,a),v(ie,e,a),h(e,Ot,a),h(e,xe,a),s(xe,Ws),h(e,Nt,a),h(e,U,a),s(U,Ke),s(Ke,Js),s(U,Fs),s(U,Qe),s(Qe,Ys),s(U,Vs),s(U,Xe),s(Xe,Ks),h(e,Ht,a),h(e,Ae,a),s(Ae,Qs),h(e,Wt,a),v(W,e,a),h(e,Jt,a),h(e,z,a),s(z,J),s(J,Ze),v(ne,Ze,null),s(z,Xs),s(z,et),s(et,Zs),h(e,Ft,a),h(e,F,a),s(F,ea),s(F,Ce),s(Ce,ta),s(F,sa),h(e,Yt,a),v(le,e,a),h(e,Vt,a),h(e,Ue,a),s(Ue,aa),h(e,Kt,a),v(re,e,a),h(e,Qt,a),h(e,Ie,a),s(Ie,oa),h(e,Xt,a),v(Y,e,a),h(e,Zt,a),h(e,B,a),s(B,V),s(V,tt),v(he,tt,null),s(B,ia),s(B,st),s(st,na),h(e,es,a),h(e,Me,a),s(Me,la),h(e,ts,a),h(e,Ge,a),s(Ge,ra),h(e,ss,a),v(pe,e,a),h(e,as,a),h(e,Te,a),s(Te,ha),h(e,os,a),v(de,e,a),h(e,is,a),h(e,k,a),s(k,pa),s(k,at),s(at,da),s(k,ca),s(k,ot),s(ot,fa),s(k,ma),s(k,it),s(it,ua),s(k,_a),s(k,nt),s(nt,wa),s(k,va),s(k,lt),s(lt,ga),s(k,ya),s(k,rt),s(rt,$a),h(e,ns,a),h(e,R,a),s(R,K),s(K,ht),v(ce,ht,null),s(R,ba),s(R,pt),s(pt,ka),h(e,ls,a),h(e,Q,a),s(Q,ja),s(Q,qe),s(qe,Pa),s(Q,Ea),h(e,rs,a),h(e,X,a),s(X,xa),s(X,fe),s(fe,Aa),s(X,Ca),h(e,hs,a),v(me,e,a),h(e,ps,a),h(e,Le,a),s(Le,Ua),h(e,ds,a),v(ue,e,a),h(e,cs,a),h(e,ze,a),s(ze,Ia),h(e,fs,a),v(_e,e,a),h(e,ms,a),h(e,Z,a),s(Z,Ma),s(Z,dt),s(dt,Ga),s(Z,Ta),h(e,us,a),h(e,I,a),s(I,ct),s(ct,qa),s(I,La),s(I,ft),s(ft,za),s(I,Ba),s(I,mt),s(mt,Ra),h(e,_s,a),h(e,S,a),s(S,ut),s(ut,Sa),s(S,Da),s(S,_t),s(_t,Oa),s(S,Na),h(e,ws,a),h(e,M,a),s(M,Ha),s(M,wt),s(wt,Wa),s(M,Ja),s(M,vt),s(vt,Fa),s(M,Ya),h(e,vs,a),v(we,e,a),h(e,gs,a),v(ve,e,a),h(e,ys,a),h(e,ee,a),s(ee,Va),s(ee,gt),s(gt,Ka),s(ee,Qa),h(e,$s,a),v(ge,e,a),h(e,bs,a),h(e,D,a),s(D,te),s(te,yt),v(ye,yt,null),s(D,Xa),s(D,$t),s($t,Za),h(e,ks,a),h(e,Be,a),s(Be,eo),h(e,js,a),v($e,e,a),h(e,Ps,a),h(e,Re,a),s(Re,to),h(e,Es,a),h(e,G,a),s(G,bt),s(bt,so),s(G,ao),s(G,kt),s(kt,oo),s(G,io),s(G,jt),s(jt,no),h(e,xs,a),h(e,Se,a),s(Se,lo),h(e,As,a),v(se,e,a),h(e,Cs,a),h(e,O,a),s(O,ae),s(ae,Pt),v(be,Pt,null),s(O,ro),s(O,Et),s(Et,ho),h(e,Us,a),h(e,De,a),s(De,po),h(e,Is,a),h(e,b,a),s(b,xt),s(xt,co),s(b,fo),s(b,T),s(T,At),s(At,mo),s(T,uo),s(T,Ct),s(Ct,_o),s(T,wo),s(T,Oe),s(Oe,vo),s(T,go),s(b,yo),s(b,q),s(q,Ut),s(Ut,$o),s(q,bo),s(q,It),s(It,ko),s(q,jo),s(q,Ne),s(Ne,Po),s(q,Eo),s(b,xo),s(b,oe),s(oe,He),s(He,Ao),s(oe,Co),s(oe,Mt),s(Mt,Uo),s(oe,Io),s(b,Mo),s(b,Gt),s(Gt,Go),s(b,To),s(b,Tt),s(Tt,qo),s(b,Lo),s(b,qt),s(qt,zo),Ms=!0},p(e,[a]){const ke={};a&2&&(ke.$$scope={dirty:a,ctx:e}),W.$set(ke);const Lt={};a&2&&(Lt.$$scope={dirty:a,ctx:e}),Y.$set(Lt);const zt={};a&2&&(zt.$$scope={dirty:a,ctx:e}),se.$set(zt)},i(e){Ms||(g(E.$$.fragment,e),g(ie.$$.fragment,e),g(W.$$.fragment,e),g(ne.$$.fragment,e),g(le.$$.fragment,e),g(re.$$.fragment,e),g(Y.$$.fragment,e),g(he.$$.fragment,e),g(pe.$$.fragment,e),g(de.$$.fragment,e),g(ce.$$.fragment,e),g(me.$$.fragment,e),g(ue.$$.fragment,e),g(_e.$$.fragment,e),g(we.$$.fragment,e),g(ve.$$.fragment,e),g(ge.$$.fragment,e),g(ye.$$.fragment,e),g($e.$$.fragment,e),g(se.$$.fragment,e),g(be.$$.fragment,e),Ms=!0)},o(e){y(E.$$.fragment,e),y(ie.$$.fragment,e),y(W.$$.fragment,e),y(ne.$$.fragment,e),y(le.$$.fragment,e),y(re.$$.fragment,e),y(Y.$$.fragment,e),y(he.$$.fragment,e),y(pe.$$.fragment,e),y(de.$$.fragment,e),y(ce.$$.fragment,e),y(me.$$.fragment,e),y(ue.$$.fragment,e),y(_e.$$.fragment,e),y(we.$$.fragment,e),y(ve.$$.fragment,e),y(ge.$$.fragment,e),y(ye.$$.fragment,e),y($e.$$.fragment,e),y(se.$$.fragment,e),y(be.$$.fragment,e),Ms=!1},d(e){t(f),e&&t(j),e&&t(m),$(E),e&&t(St),e&&t(Ee),e&&t(Dt),$(ie,e),e&&t(Ot),e&&t(xe),e&&t(Nt),e&&t(U),e&&t(Ht),e&&t(Ae),e&&t(Wt),$(W,e),e&&t(Jt),e&&t(z),$(ne),e&&t(Ft),e&&t(F),e&&t(Yt),$(le,e),e&&t(Vt),e&&t(Ue),e&&t(Kt),$(re,e),e&&t(Qt),e&&t(Ie),e&&t(Xt),$(Y,e),e&&t(Zt),e&&t(B),$(he),e&&t(es),e&&t(Me),e&&t(ts),e&&t(Ge),e&&t(ss),$(pe,e),e&&t(as),e&&t(Te),e&&t(os),$(de,e),e&&t(is),e&&t(k),e&&t(ns),e&&t(R),$(ce),e&&t(ls),e&&t(Q),e&&t(rs),e&&t(X),e&&t(hs),$(me,e),e&&t(ps),e&&t(Le),e&&t(ds),$(ue,e),e&&t(cs),e&&t(ze),e&&t(fs),$(_e,e),e&&t(ms),e&&t(Z),e&&t(us),e&&t(I),e&&t(_s),e&&t(S),e&&t(ws),e&&t(M),e&&t(vs),$(we,e),e&&t(gs),$(ve,e),e&&t(ys),e&&t(ee),e&&t($s),$(ge,e),e&&t(bs),e&&t(D),$(ye),e&&t(ks),e&&t(Be),e&&t(js),$($e,e),e&&t(Ps),e&&t(Re),e&&t(Es),e&&t(G),e&&t(xs),e&&t(Se),e&&t(As),$(se,e),e&&t(Cs),e&&t(O),$(be),e&&t(Us),e&&t(De),e&&t(Is),e&&t(b)}}}const ln={local:"handling-big-models",sections:[{local:"instantiating-an-empty-model",title:"Instantiating an empty model"},{local:"sharded-checkpoints",title:"Sharded checkpoints"},{local:"loading-weights",title:"Loading weights"}],title:"Handling big models"};function rn(N){return tn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class fn extends Qi{constructor(f){super();Xi(this,f,rn,nn,Zi,{})}}export{fn as default,ln as metadata};
