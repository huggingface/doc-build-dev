import{S as us,i as ms,s as vs,e as o,k as d,w as x,t as s,M as ys,c as r,d as t,m as c,a as i,x as M,h as n,b as h,$ as ps,G as a,g as f,y as G,q as L,o as H,B as q,v as ws}from"../chunks/vendor-hf-doc-builder.js";import{T as gs}from"../chunks/Tip-hf-doc-builder.js";import{I as ae}from"../chunks/IconCopyLink-hf-doc-builder.js";function bs(Ht){let u;return{c(){u=s("Training and evaluating on the same split can misrepresent your results! If you overfit on your training data the evaluation results on that split will look great but the model will perform poorly on new data.")},l(g){u=n(g,"Training and evaluating on the same split can misrepresent your results! If you overfit on your training data the evaluation results on that split will look great but the model will perform poorly on new data.")},m(g,w){f(g,u,w)},d(g){g&&t(u)}}}function _s(Ht){let u,g,w,C,et,oe,Da,tt,Ua,qt,D,Ra,at,Wa,za,Ct,U,Ba,ot,Fa,ja,Dt,P,R,rt,re,Ja,it,Ka,Ut,Se,Qa,Rt,b,Oe,st,Va,Xa,Ya,Te,nt,Za,eo,to,Ne,lt,ao,oo,Wt,p,ro,ft,io,so,ht,no,lo,dt,fo,ho,ct,co,po,pt,uo,mo,zt,xe,vo,Bt,Me,yo,Ft,W,jt,z,wo,ie,go,bo,Jt,A,B,ut,se,_o,mt,Eo,Kt,_,$o,ne,ko,Po,vt,Ao,Io,Qt,Ge,So,Vt,m,Oo,yt,le,To,No,wt,fe,xo,Mo,gt,he,Go,Lo,Xt,F,Ho,de,qo,Co,Yt,Le,He,Fr,Zt,j,Do,ce,Uo,Ro,ea,qe,Ce,jr,ta,De,Wo,aa,I,J,bt,pe,zo,_t,Bo,oa,Ue,Fo,ra,ue,Et,jo,Jo,ia,me,$t,Ko,Qo,sa,Re,Vo,na,S,K,kt,ve,Xo,Pt,Yo,la,We,Zo,fa,ze,er,ha,O,Q,At,ye,tr,It,ar,da,E,or,St,rr,ir,Ot,sr,nr,ca,V,lr,we,fr,hr,pa,$,dr,ge,cr,pr,be,ur,mr,ua,Be,vr,ma,T,X,Tt,_e,yr,Nt,wr,va,k,gr,xt,br,_r,Mt,Er,$r,ya,Fe,kr,wa,je,Pr,ga,Je,Ar,ba,N,Y,Gt,Ee,Ir,Lt,Sr,_a,Z,Or,$e,Tr,Nr,Ea,v,xr,ke,Mr,Gr,Pe,Lr,Hr,Ae,qr,Cr,$a,Ke,Dr,ka;return oe=new ae({}),re=new ae({}),W=new gs({props:{warning:!0,$$slots:{default:[bs]},$$scope:{ctx:Ht}}}),se=new ae({}),pe=new ae({}),ve=new ae({}),ye=new ae({}),_e=new ae({}),Ee=new ae({}),{c(){u=o("meta"),g=d(),w=o("h1"),C=o("a"),et=o("span"),x(oe.$$.fragment),Da=d(),tt=o("span"),Ua=s("Considerations for model evaluation"),qt=d(),D=o("p"),Ra=s("Developing an ML model is rarely a one-shot deal: it often involves multiple stages of defining the model architecture and tuning hyper-parameters before converging on a final set. Responsible model evaluation is a key part of this process, and the \u{1F917}  "),at=o("code"),Wa=s("evaluate"),za=s(" package is here to help!"),Ct=d(),U=o("p"),Ba=s("Here are some things to keep in mind when evaluating your model using the \u{1F917}  "),ot=o("code"),Fa=s("evaluate"),ja=s(" package:"),Dt=d(),P=o("h2"),R=o("a"),rt=o("span"),x(re.$$.fragment),Ja=d(),it=o("span"),Ka=s("Properly splitting your data"),Ut=d(),Se=o("p"),Qa=s("Good evaluation generally requires three splits of your dataset:"),Rt=d(),b=o("ul"),Oe=o("li"),st=o("strong"),Va=s("train"),Xa=s(": this is used for training your model."),Ya=d(),Te=o("li"),nt=o("strong"),Za=s("validation"),eo=s(": this is used for validating the model hyperparameters."),to=d(),Ne=o("li"),lt=o("strong"),ao=s("test"),oo=s(": this is used for evaluating your model."),Wt=d(),p=o("p"),ro=s("Many of the datasets on the \u{1F917} Hub are separated into 2 splits: "),ft=o("code"),io=s("train"),so=s(" and "),ht=o("code"),no=s("validation"),lo=s("; others are split into 3 splits ("),dt=o("code"),fo=s("train"),ho=s(", "),ct=o("code"),co=s("validation"),po=s(" and "),pt=o("code"),uo=s("test"),mo=s(") \u2014 make sure to use the right split for the right purpose!"),zt=d(),xe=o("p"),vo=s("Some datasets on the \u{1F917} Hub are already separated into these three splits. However, there are also many that only have a train/validation or only train split."),Bt=d(),Me=o("p"),yo=s("If the dataset you\u2019re using doesn\u2019t have a predefined train-test split, it is up to you to define which part of the dataset you want to use for training your model and  which you want to use for hyperparameter tuning or final evaluation."),Ft=d(),x(W.$$.fragment),jt=d(),z=o("p"),wo=s("Depending on the size of the dataset, you can keep anywhere from 10-30% for evaluation and the rest for testing, while aiming to set up the test set to reflect the production data as close as possible. Check out "),ie=o("a"),go=s("this thread"),bo=s(" for a more in-depth discussion of dataset splitting!"),Jt=d(),A=o("h2"),B=o("a"),ut=o("span"),x(se.$$.fragment),_o=d(),mt=o("span"),Eo=s("The impact of class imbalance"),Kt=d(),_=o("p"),$o=s("While many academic datasets, such as the "),ne=o("a"),ko=s("IMDb dataset"),Po=s(" of movie reviews, are perfectly balanced (as all things should be), most real-world datasets are not. In machine learning a "),vt=o("em"),Ao=s("balanced dataset"),Io=s(" corresponds to a datasets where all labels are represented equally. In the case of the IMDb dataset this means that there are as many positive as negative reviews in the dataset. In an imbalanced dataset this is not the case: in fraud detection for example there are usually many more non-fraud cases than fraud cases in the dataset."),Qt=d(),Ge=o("p"),So=s("Having an imbalanced dataset can skew the results of your metrics. Imagine a dataset with 99 \u201Cnon-fraud\u201D cases and 1 \u201Cfraud\u201D case. A simple model that always predicts \u201Cnon-fraud\u201D cases would give yield a 99% accuracy which might sound good at first until you realize that you will never catch a fraud case."),Vt=d(),m=o("p"),Oo=s("Often, using more than one metric can help get a better idea of your model\u2019s performance from different points of view. For instance, metrics like "),yt=o("strong"),le=o("a"),To=s("recall"),No=s(" and "),wt=o("strong"),fe=o("a"),xo=s("precision"),Mo=s(" can be used together, and the "),gt=o("strong"),he=o("a"),Go=s("f1 score"),Lo=s(" is actually the harmonic mean of the two."),Xt=d(),F=o("p"),Ho=s("In cases where a dataset is balanced, using "),de=o("a"),qo=s("accuracy"),Co=s(" can be reflect the overall model performance:"),Yt=d(),Le=o("p"),He=o("img"),Zt=d(),j=o("p"),Do=s("In cases where there is an imbalance, using "),ce=o("a"),Uo=s("F1 score"),Ro=s("can be a better representation of performance, given that it encompasses both precision and recall."),ea=d(),qe=o("p"),Ce=o("img"),ta=d(),De=o("p"),Wo=s("Using accuracy in an imbalanced setting is less ideal, since it is not sensitive to minority classes and will not faithfully reflect model performance on them."),aa=d(),I=o("h2"),J=o("a"),bt=o("span"),x(pe.$$.fragment),zo=d(),_t=o("span"),Bo=s("Offline vs. online model evaluation"),oa=d(),Ue=o("p"),Fo=s("There are multiple ways to evaluate models, and an important distinction is offline versus online evaluation:"),ra=d(),ue=o("p"),Et=o("strong"),jo=s("Offline evaluation"),Jo=s(" is done before deploying a model or using insights generated from a model, using static datasets and metrics."),ia=d(),me=o("p"),$t=o("strong"),Ko=s("Online evaluation"),Qo=s(" means evaluating how a model is performing after deployment and during its use in production."),sa=d(),Re=o("p"),Vo=s("These two types of evaluation can use different metrics and measure different aspects of model performance. For example, offline evaluation can compare a model to other models based on their performance on common benchmarks, whereas online evaluation will evaluate aspects such as latency and accuracy of the model based on production data (for example, the number of user queries that it was able to address)."),na=d(),S=o("h2"),K=o("a"),kt=o("span"),x(ve.$$.fragment),Xo=d(),Pt=o("span"),Yo=s("Trade-offs in model evaluation"),la=d(),We=o("p"),Zo=s("When evaluating models in practice, there are often trade-offs that have to be made between different aspects of model performance: for instance, choosing a model that is slightly less accurate but that has a faster inference time, compared to a high-accuracy that has a higher memory footprint and requires access to more GPUs."),fa=d(),ze=o("p"),er=s("Here are other aspects of model performance to consider during evaluation:"),ha=d(),O=o("h3"),Q=o("a"),At=o("span"),x(ye.$$.fragment),tr=d(),It=o("span"),ar=s("Interpretability"),da=d(),E=o("p"),or=s("When evaluating models, "),St=o("strong"),rr=s("interpretability"),ir=s(" (i.e. the ability to "),Ot=o("em"),sr=s("interpret"),nr=s(" results)  can be very important, especially when deploying models in production."),ca=d(),V=o("p"),lr=s("For instance, metrics such as "),we=o("a"),fr=s("exact match"),hr=s(" have a set range (between 0 and 1, or 0% and 100%) and are easily understandable to users: for a pair of strings, the exact match score is 1 if the two strings are the exact same, and 0 otherwise."),pa=d(),$=o("p"),dr=s("Other metrics, such as "),ge=o("a"),cr=s("BLEU"),pr=s(" are harder to interpret: while they also range between 0 and 1, it can vary greatly depending on which parameters are used to generate the scores, especially when different tokenization and normalization techniques are used (see the "),be=o("a"),ur=s("metric card"),mr=s(" for more information about BLEU limitations). This means that it is difficult to interpret a BLEU score without having more information procedure used for obtaining it."),ua=d(),Be=o("p"),vr=s("Interpretability can be more or less important depending on the evaluation use case, but it is a useful aspect of model evaluation to keep in mind, since communicating and comparing model evaluations is an important part of responsible machine learning."),ma=d(),T=o("h3"),X=o("a"),Tt=o("span"),x(_e.$$.fragment),yr=d(),Nt=o("span"),wr=s("Inference speed and memory footprint"),va=d(),k=o("p"),gr=s("While recent years have seen increasingly large ML models achieve high performance on a large variety of tasks and benchmarks, deploying these multi-billion parameter models in practice can be a challenge in itself, and many organizations lack the resources for this. This is why considering the "),xt=o("strong"),br=s("inference speed"),_r=s(" and "),Mt=o("strong"),Er=s("memory footprint"),$r=s(" of models is important, especially when doing online model evaluation."),ya=d(),Fe=o("p"),kr=s("Inference speed refers to the time that it takes for a model to make a prediction \u2014 this will vary depending on the hardware used and the way in which models are queried, e.g. in real time via an API or in batch jobs that run once a day."),wa=d(),je=o("p"),Pr=s("Memory footprint refers to the size of the model weights and how much hardware memory they occupy. If a model is too large to fit on a single GPU or CPU, then it has to be split over multiple ones, which can be more or less difficult depending on the model architecture and the deployment method."),ga=d(),Je=o("p"),Ar=s("When doing online model evaluation, there is often a trade-off to be done between inference speed and accuracy or precision, whereas this is less the case for offline evaluation."),ba=d(),N=o("h2"),Y=o("a"),Gt=o("span"),x(Ee.$$.fragment),Ir=d(),Lt=o("span"),Sr=s("Limitations and bias"),_a=d(),Z=o("p"),Or=s("All models and all metrics have their limitations and biases, which depend on the way in which they were trained, the data that was used, and their intended uses. It is important to measure and communicate these limitations clearly to prevent misuse and unintended impacts, for instance via "),$e=o("a"),Tr=s("model cards"),Nr=s(" which document the training and evaluation process."),Ea=d(),v=o("p"),xr=s("Measuring biases can be done by evaluating models on datasets such as "),ke=o("a"),Mr=s("Wino Bias"),Gr=s(" of "),Pe=o("a"),Lr=s("MD Gender Bias"),Hr=s(", and by doing "),Ae=o("a"),qr=s("Interactive Error Analyis"),Cr=s(" to try to identify which subsets of the evaluation dataset a model performs poorly on."),$a=d(),Ke=o("p"),Dr=s("We are currently working on additional measurements that can be used to quantify different dimensions of bias in both models and datasets \u2014 stay tuned for more documentation on this topic!"),this.h()},l(e){const l=ys('[data-svelte="svelte-1phssyn"]',document.head);u=r(l,"META",{name:!0,content:!0}),l.forEach(t),g=c(e),w=r(e,"H1",{class:!0});var Ie=i(w);C=r(Ie,"A",{id:!0,class:!0,href:!0});var Jr=i(C);et=r(Jr,"SPAN",{});var Kr=i(et);M(oe.$$.fragment,Kr),Kr.forEach(t),Jr.forEach(t),Da=c(Ie),tt=r(Ie,"SPAN",{});var Qr=i(tt);Ua=n(Qr,"Considerations for model evaluation"),Qr.forEach(t),Ie.forEach(t),qt=c(e),D=r(e,"P",{});var Pa=i(D);Ra=n(Pa,"Developing an ML model is rarely a one-shot deal: it often involves multiple stages of defining the model architecture and tuning hyper-parameters before converging on a final set. Responsible model evaluation is a key part of this process, and the \u{1F917}  "),at=r(Pa,"CODE",{});var Vr=i(at);Wa=n(Vr,"evaluate"),Vr.forEach(t),za=n(Pa," package is here to help!"),Pa.forEach(t),Ct=c(e),U=r(e,"P",{});var Aa=i(U);Ba=n(Aa,"Here are some things to keep in mind when evaluating your model using the \u{1F917}  "),ot=r(Aa,"CODE",{});var Xr=i(ot);Fa=n(Xr,"evaluate"),Xr.forEach(t),ja=n(Aa," package:"),Aa.forEach(t),Dt=c(e),P=r(e,"H2",{class:!0});var Ia=i(P);R=r(Ia,"A",{id:!0,class:!0,href:!0});var Yr=i(R);rt=r(Yr,"SPAN",{});var Zr=i(rt);M(re.$$.fragment,Zr),Zr.forEach(t),Yr.forEach(t),Ja=c(Ia),it=r(Ia,"SPAN",{});var ei=i(it);Ka=n(ei,"Properly splitting your data"),ei.forEach(t),Ia.forEach(t),Ut=c(e),Se=r(e,"P",{});var ti=i(Se);Qa=n(ti,"Good evaluation generally requires three splits of your dataset:"),ti.forEach(t),Rt=c(e),b=r(e,"UL",{});var Qe=i(b);Oe=r(Qe,"LI",{});var Ur=i(Oe);st=r(Ur,"STRONG",{});var ai=i(st);Va=n(ai,"train"),ai.forEach(t),Xa=n(Ur,": this is used for training your model."),Ur.forEach(t),Ya=c(Qe),Te=r(Qe,"LI",{});var Rr=i(Te);nt=r(Rr,"STRONG",{});var oi=i(nt);Za=n(oi,"validation"),oi.forEach(t),eo=n(Rr,": this is used for validating the model hyperparameters."),Rr.forEach(t),to=c(Qe),Ne=r(Qe,"LI",{});var Wr=i(Ne);lt=r(Wr,"STRONG",{});var ri=i(lt);ao=n(ri,"test"),ri.forEach(t),oo=n(Wr,": this is used for evaluating your model."),Wr.forEach(t),Qe.forEach(t),Wt=c(e),p=r(e,"P",{});var y=i(p);ro=n(y,"Many of the datasets on the \u{1F917} Hub are separated into 2 splits: "),ft=r(y,"CODE",{});var ii=i(ft);io=n(ii,"train"),ii.forEach(t),so=n(y," and "),ht=r(y,"CODE",{});var si=i(ht);no=n(si,"validation"),si.forEach(t),lo=n(y,"; others are split into 3 splits ("),dt=r(y,"CODE",{});var ni=i(dt);fo=n(ni,"train"),ni.forEach(t),ho=n(y,", "),ct=r(y,"CODE",{});var li=i(ct);co=n(li,"validation"),li.forEach(t),po=n(y," and "),pt=r(y,"CODE",{});var fi=i(pt);uo=n(fi,"test"),fi.forEach(t),mo=n(y,") \u2014 make sure to use the right split for the right purpose!"),y.forEach(t),zt=c(e),xe=r(e,"P",{});var hi=i(xe);vo=n(hi,"Some datasets on the \u{1F917} Hub are already separated into these three splits. However, there are also many that only have a train/validation or only train split."),hi.forEach(t),Bt=c(e),Me=r(e,"P",{});var di=i(Me);yo=n(di,"If the dataset you\u2019re using doesn\u2019t have a predefined train-test split, it is up to you to define which part of the dataset you want to use for training your model and  which you want to use for hyperparameter tuning or final evaluation."),di.forEach(t),Ft=c(e),M(W.$$.fragment,e),jt=c(e),z=r(e,"P",{});var Sa=i(z);wo=n(Sa,"Depending on the size of the dataset, you can keep anywhere from 10-30% for evaluation and the rest for testing, while aiming to set up the test set to reflect the production data as close as possible. Check out "),ie=r(Sa,"A",{href:!0,rel:!0});var ci=i(ie);go=n(ci,"this thread"),ci.forEach(t),bo=n(Sa," for a more in-depth discussion of dataset splitting!"),Sa.forEach(t),Jt=c(e),A=r(e,"H2",{class:!0});var Oa=i(A);B=r(Oa,"A",{id:!0,class:!0,href:!0});var pi=i(B);ut=r(pi,"SPAN",{});var ui=i(ut);M(se.$$.fragment,ui),ui.forEach(t),pi.forEach(t),_o=c(Oa),mt=r(Oa,"SPAN",{});var mi=i(mt);Eo=n(mi,"The impact of class imbalance"),mi.forEach(t),Oa.forEach(t),Kt=c(e),_=r(e,"P",{});var Ve=i(_);$o=n(Ve,"While many academic datasets, such as the "),ne=r(Ve,"A",{href:!0,rel:!0});var vi=i(ne);ko=n(vi,"IMDb dataset"),vi.forEach(t),Po=n(Ve," of movie reviews, are perfectly balanced (as all things should be), most real-world datasets are not. In machine learning a "),vt=r(Ve,"EM",{});var yi=i(vt);Ao=n(yi,"balanced dataset"),yi.forEach(t),Io=n(Ve," corresponds to a datasets where all labels are represented equally. In the case of the IMDb dataset this means that there are as many positive as negative reviews in the dataset. In an imbalanced dataset this is not the case: in fraud detection for example there are usually many more non-fraud cases than fraud cases in the dataset."),Ve.forEach(t),Qt=c(e),Ge=r(e,"P",{});var wi=i(Ge);So=n(wi,"Having an imbalanced dataset can skew the results of your metrics. Imagine a dataset with 99 \u201Cnon-fraud\u201D cases and 1 \u201Cfraud\u201D case. A simple model that always predicts \u201Cnon-fraud\u201D cases would give yield a 99% accuracy which might sound good at first until you realize that you will never catch a fraud case."),wi.forEach(t),Vt=c(e),m=r(e,"P",{});var ee=i(m);Oo=n(ee,"Often, using more than one metric can help get a better idea of your model\u2019s performance from different points of view. For instance, metrics like "),yt=r(ee,"STRONG",{});var gi=i(yt);le=r(gi,"A",{href:!0,rel:!0});var bi=i(le);To=n(bi,"recall"),bi.forEach(t),gi.forEach(t),No=n(ee," and "),wt=r(ee,"STRONG",{});var _i=i(wt);fe=r(_i,"A",{href:!0,rel:!0});var Ei=i(fe);xo=n(Ei,"precision"),Ei.forEach(t),_i.forEach(t),Mo=n(ee," can be used together, and the "),gt=r(ee,"STRONG",{});var $i=i(gt);he=r($i,"A",{href:!0,rel:!0});var ki=i(he);Go=n(ki,"f1 score"),ki.forEach(t),$i.forEach(t),Lo=n(ee," is actually the harmonic mean of the two."),ee.forEach(t),Xt=c(e),F=r(e,"P",{});var Ta=i(F);Ho=n(Ta,"In cases where a dataset is balanced, using "),de=r(Ta,"A",{href:!0,rel:!0});var Pi=i(de);qo=n(Pi,"accuracy"),Pi.forEach(t),Co=n(Ta," can be reflect the overall model performance:"),Ta.forEach(t),Yt=c(e),Le=r(e,"P",{});var Ai=i(Le);He=r(Ai,"IMG",{src:!0,alt:!0}),Ai.forEach(t),Zt=c(e),j=r(e,"P",{});var Na=i(j);Do=n(Na,"In cases where there is an imbalance, using "),ce=r(Na,"A",{href:!0,rel:!0});var Ii=i(ce);Uo=n(Ii,"F1 score"),Ii.forEach(t),Ro=n(Na,"can be a better representation of performance, given that it encompasses both precision and recall."),Na.forEach(t),ea=c(e),qe=r(e,"P",{});var Si=i(qe);Ce=r(Si,"IMG",{src:!0,alt:!0}),Si.forEach(t),ta=c(e),De=r(e,"P",{});var Oi=i(De);Wo=n(Oi,"Using accuracy in an imbalanced setting is less ideal, since it is not sensitive to minority classes and will not faithfully reflect model performance on them."),Oi.forEach(t),aa=c(e),I=r(e,"H2",{class:!0});var xa=i(I);J=r(xa,"A",{id:!0,class:!0,href:!0});var Ti=i(J);bt=r(Ti,"SPAN",{});var Ni=i(bt);M(pe.$$.fragment,Ni),Ni.forEach(t),Ti.forEach(t),zo=c(xa),_t=r(xa,"SPAN",{});var xi=i(_t);Bo=n(xi,"Offline vs. online model evaluation"),xi.forEach(t),xa.forEach(t),oa=c(e),Ue=r(e,"P",{});var Mi=i(Ue);Fo=n(Mi,"There are multiple ways to evaluate models, and an important distinction is offline versus online evaluation:"),Mi.forEach(t),ra=c(e),ue=r(e,"P",{});var zr=i(ue);Et=r(zr,"STRONG",{});var Gi=i(Et);jo=n(Gi,"Offline evaluation"),Gi.forEach(t),Jo=n(zr," is done before deploying a model or using insights generated from a model, using static datasets and metrics."),zr.forEach(t),ia=c(e),me=r(e,"P",{});var Br=i(me);$t=r(Br,"STRONG",{});var Li=i($t);Ko=n(Li,"Online evaluation"),Li.forEach(t),Qo=n(Br," means evaluating how a model is performing after deployment and during its use in production."),Br.forEach(t),sa=c(e),Re=r(e,"P",{});var Hi=i(Re);Vo=n(Hi,"These two types of evaluation can use different metrics and measure different aspects of model performance. For example, offline evaluation can compare a model to other models based on their performance on common benchmarks, whereas online evaluation will evaluate aspects such as latency and accuracy of the model based on production data (for example, the number of user queries that it was able to address)."),Hi.forEach(t),na=c(e),S=r(e,"H2",{class:!0});var Ma=i(S);K=r(Ma,"A",{id:!0,class:!0,href:!0});var qi=i(K);kt=r(qi,"SPAN",{});var Ci=i(kt);M(ve.$$.fragment,Ci),Ci.forEach(t),qi.forEach(t),Xo=c(Ma),Pt=r(Ma,"SPAN",{});var Di=i(Pt);Yo=n(Di,"Trade-offs in model evaluation"),Di.forEach(t),Ma.forEach(t),la=c(e),We=r(e,"P",{});var Ui=i(We);Zo=n(Ui,"When evaluating models in practice, there are often trade-offs that have to be made between different aspects of model performance: for instance, choosing a model that is slightly less accurate but that has a faster inference time, compared to a high-accuracy that has a higher memory footprint and requires access to more GPUs."),Ui.forEach(t),fa=c(e),ze=r(e,"P",{});var Ri=i(ze);er=n(Ri,"Here are other aspects of model performance to consider during evaluation:"),Ri.forEach(t),ha=c(e),O=r(e,"H3",{class:!0});var Ga=i(O);Q=r(Ga,"A",{id:!0,class:!0,href:!0});var Wi=i(Q);At=r(Wi,"SPAN",{});var zi=i(At);M(ye.$$.fragment,zi),zi.forEach(t),Wi.forEach(t),tr=c(Ga),It=r(Ga,"SPAN",{});var Bi=i(It);ar=n(Bi,"Interpretability"),Bi.forEach(t),Ga.forEach(t),da=c(e),E=r(e,"P",{});var Xe=i(E);or=n(Xe,"When evaluating models, "),St=r(Xe,"STRONG",{});var Fi=i(St);rr=n(Fi,"interpretability"),Fi.forEach(t),ir=n(Xe," (i.e. the ability to "),Ot=r(Xe,"EM",{});var ji=i(Ot);sr=n(ji,"interpret"),ji.forEach(t),nr=n(Xe," results)  can be very important, especially when deploying models in production."),Xe.forEach(t),ca=c(e),V=r(e,"P",{});var La=i(V);lr=n(La,"For instance, metrics such as "),we=r(La,"A",{href:!0,rel:!0});var Ji=i(we);fr=n(Ji,"exact match"),Ji.forEach(t),hr=n(La," have a set range (between 0 and 1, or 0% and 100%) and are easily understandable to users: for a pair of strings, the exact match score is 1 if the two strings are the exact same, and 0 otherwise."),La.forEach(t),pa=c(e),$=r(e,"P",{});var Ye=i($);dr=n(Ye,"Other metrics, such as "),ge=r(Ye,"A",{href:!0,rel:!0});var Ki=i(ge);cr=n(Ki,"BLEU"),Ki.forEach(t),pr=n(Ye," are harder to interpret: while they also range between 0 and 1, it can vary greatly depending on which parameters are used to generate the scores, especially when different tokenization and normalization techniques are used (see the "),be=r(Ye,"A",{href:!0,rel:!0});var Qi=i(be);ur=n(Qi,"metric card"),Qi.forEach(t),mr=n(Ye," for more information about BLEU limitations). This means that it is difficult to interpret a BLEU score without having more information procedure used for obtaining it."),Ye.forEach(t),ua=c(e),Be=r(e,"P",{});var Vi=i(Be);vr=n(Vi,"Interpretability can be more or less important depending on the evaluation use case, but it is a useful aspect of model evaluation to keep in mind, since communicating and comparing model evaluations is an important part of responsible machine learning."),Vi.forEach(t),ma=c(e),T=r(e,"H3",{class:!0});var Ha=i(T);X=r(Ha,"A",{id:!0,class:!0,href:!0});var Xi=i(X);Tt=r(Xi,"SPAN",{});var Yi=i(Tt);M(_e.$$.fragment,Yi),Yi.forEach(t),Xi.forEach(t),yr=c(Ha),Nt=r(Ha,"SPAN",{});var Zi=i(Nt);wr=n(Zi,"Inference speed and memory footprint"),Zi.forEach(t),Ha.forEach(t),va=c(e),k=r(e,"P",{});var Ze=i(k);gr=n(Ze,"While recent years have seen increasingly large ML models achieve high performance on a large variety of tasks and benchmarks, deploying these multi-billion parameter models in practice can be a challenge in itself, and many organizations lack the resources for this. This is why considering the "),xt=r(Ze,"STRONG",{});var es=i(xt);br=n(es,"inference speed"),es.forEach(t),_r=n(Ze," and "),Mt=r(Ze,"STRONG",{});var ts=i(Mt);Er=n(ts,"memory footprint"),ts.forEach(t),$r=n(Ze," of models is important, especially when doing online model evaluation."),Ze.forEach(t),ya=c(e),Fe=r(e,"P",{});var as=i(Fe);kr=n(as,"Inference speed refers to the time that it takes for a model to make a prediction \u2014 this will vary depending on the hardware used and the way in which models are queried, e.g. in real time via an API or in batch jobs that run once a day."),as.forEach(t),wa=c(e),je=r(e,"P",{});var os=i(je);Pr=n(os,"Memory footprint refers to the size of the model weights and how much hardware memory they occupy. If a model is too large to fit on a single GPU or CPU, then it has to be split over multiple ones, which can be more or less difficult depending on the model architecture and the deployment method."),os.forEach(t),ga=c(e),Je=r(e,"P",{});var rs=i(Je);Ar=n(rs,"When doing online model evaluation, there is often a trade-off to be done between inference speed and accuracy or precision, whereas this is less the case for offline evaluation."),rs.forEach(t),ba=c(e),N=r(e,"H2",{class:!0});var qa=i(N);Y=r(qa,"A",{id:!0,class:!0,href:!0});var is=i(Y);Gt=r(is,"SPAN",{});var ss=i(Gt);M(Ee.$$.fragment,ss),ss.forEach(t),is.forEach(t),Ir=c(qa),Lt=r(qa,"SPAN",{});var ns=i(Lt);Sr=n(ns,"Limitations and bias"),ns.forEach(t),qa.forEach(t),_a=c(e),Z=r(e,"P",{});var Ca=i(Z);Or=n(Ca,"All models and all metrics have their limitations and biases, which depend on the way in which they were trained, the data that was used, and their intended uses. It is important to measure and communicate these limitations clearly to prevent misuse and unintended impacts, for instance via "),$e=r(Ca,"A",{href:!0,rel:!0});var ls=i($e);Tr=n(ls,"model cards"),ls.forEach(t),Nr=n(Ca," which document the training and evaluation process."),Ca.forEach(t),Ea=c(e),v=r(e,"P",{});var te=i(v);xr=n(te,"Measuring biases can be done by evaluating models on datasets such as "),ke=r(te,"A",{href:!0,rel:!0});var fs=i(ke);Mr=n(fs,"Wino Bias"),fs.forEach(t),Gr=n(te," of "),Pe=r(te,"A",{href:!0,rel:!0});var hs=i(Pe);Lr=n(hs,"MD Gender Bias"),hs.forEach(t),Hr=n(te,", and by doing "),Ae=r(te,"A",{href:!0,rel:!0});var ds=i(Ae);qr=n(ds,"Interactive Error Analyis"),ds.forEach(t),Cr=n(te," to try to identify which subsets of the evaluation dataset a model performs poorly on."),te.forEach(t),$a=c(e),Ke=r(e,"P",{});var cs=i(Ke);Dr=n(cs,"We are currently working on additional measurements that can be used to quantify different dimensions of bias in both models and datasets \u2014 stay tuned for more documentation on this topic!"),cs.forEach(t),this.h()},h(){h(u,"name","hf:doc:metadata"),h(u,"content",JSON.stringify(Es)),h(C,"id","considerations-for-model-evaluation"),h(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(C,"href","#considerations-for-model-evaluation"),h(w,"class","relative group"),h(R,"id","properly-splitting-your-data"),h(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(R,"href","#properly-splitting-your-data"),h(P,"class","relative group"),h(ie,"href","https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090"),h(ie,"rel","nofollow"),h(B,"id","the-impact-of-class-imbalance"),h(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(B,"href","#the-impact-of-class-imbalance"),h(A,"class","relative group"),h(ne,"href","https://huggingface.co/datasets/imdb"),h(ne,"rel","nofollow"),h(le,"href","https://huggingface.co/metrics/recall"),h(le,"rel","nofollow"),h(fe,"href","https://huggingface.co/metrics/precision"),h(fe,"rel","nofollow"),h(he,"href","https://huggingface.co/metrics/f1"),h(he,"rel","nofollow"),h(de,"href","https://huggingface.co/metrics/accuracy"),h(de,"rel","nofollow"),ps(He.src,Fr="https://huggingface.co/datasets/evaluate/media/resolve/main/balanced-classes.png")||h(He,"src",Fr),h(He,"alt","Balanced Labels"),h(ce,"href","https://huggingface.co/metrics/f1"),h(ce,"rel","nofollow"),ps(Ce.src,jr="https://huggingface.co/datasets/evaluate/media/resolve/main/imbalanced-classes.png")||h(Ce,"src",jr),h(Ce,"alt","Imbalanced Labels"),h(J,"id","offline-vs-online-model-evaluation"),h(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(J,"href","#offline-vs-online-model-evaluation"),h(I,"class","relative group"),h(K,"id","tradeoffs-in-model-evaluation"),h(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(K,"href","#tradeoffs-in-model-evaluation"),h(S,"class","relative group"),h(Q,"id","interpretability"),h(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Q,"href","#interpretability"),h(O,"class","relative group"),h(we,"href","https://huggingface.co/metrics/exact_match"),h(we,"rel","nofollow"),h(ge,"href","https://huggingface.co/metrics/bleu"),h(ge,"rel","nofollow"),h(be,"href","https://github.com/huggingface/evaluate/tree/main/metrics/bleu"),h(be,"rel","nofollow"),h(X,"id","inference-speed-and-memory-footprint"),h(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(X,"href","#inference-speed-and-memory-footprint"),h(T,"class","relative group"),h(Y,"id","limitations-and-bias"),h(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Y,"href","#limitations-and-bias"),h(N,"class","relative group"),h($e,"href","https://huggingface.co/course/chapter4/4?fw=pt"),h($e,"rel","nofollow"),h(ke,"href","https://huggingface.co/datasets/wino_bias"),h(ke,"rel","nofollow"),h(Pe,"href","https://huggingface.co/datasets/md_gender_bias"),h(Pe,"rel","nofollow"),h(Ae,"href","https://huggingface.co/spaces/nazneen/error-analysis"),h(Ae,"rel","nofollow")},m(e,l){a(document.head,u),f(e,g,l),f(e,w,l),a(w,C),a(C,et),G(oe,et,null),a(w,Da),a(w,tt),a(tt,Ua),f(e,qt,l),f(e,D,l),a(D,Ra),a(D,at),a(at,Wa),a(D,za),f(e,Ct,l),f(e,U,l),a(U,Ba),a(U,ot),a(ot,Fa),a(U,ja),f(e,Dt,l),f(e,P,l),a(P,R),a(R,rt),G(re,rt,null),a(P,Ja),a(P,it),a(it,Ka),f(e,Ut,l),f(e,Se,l),a(Se,Qa),f(e,Rt,l),f(e,b,l),a(b,Oe),a(Oe,st),a(st,Va),a(Oe,Xa),a(b,Ya),a(b,Te),a(Te,nt),a(nt,Za),a(Te,eo),a(b,to),a(b,Ne),a(Ne,lt),a(lt,ao),a(Ne,oo),f(e,Wt,l),f(e,p,l),a(p,ro),a(p,ft),a(ft,io),a(p,so),a(p,ht),a(ht,no),a(p,lo),a(p,dt),a(dt,fo),a(p,ho),a(p,ct),a(ct,co),a(p,po),a(p,pt),a(pt,uo),a(p,mo),f(e,zt,l),f(e,xe,l),a(xe,vo),f(e,Bt,l),f(e,Me,l),a(Me,yo),f(e,Ft,l),G(W,e,l),f(e,jt,l),f(e,z,l),a(z,wo),a(z,ie),a(ie,go),a(z,bo),f(e,Jt,l),f(e,A,l),a(A,B),a(B,ut),G(se,ut,null),a(A,_o),a(A,mt),a(mt,Eo),f(e,Kt,l),f(e,_,l),a(_,$o),a(_,ne),a(ne,ko),a(_,Po),a(_,vt),a(vt,Ao),a(_,Io),f(e,Qt,l),f(e,Ge,l),a(Ge,So),f(e,Vt,l),f(e,m,l),a(m,Oo),a(m,yt),a(yt,le),a(le,To),a(m,No),a(m,wt),a(wt,fe),a(fe,xo),a(m,Mo),a(m,gt),a(gt,he),a(he,Go),a(m,Lo),f(e,Xt,l),f(e,F,l),a(F,Ho),a(F,de),a(de,qo),a(F,Co),f(e,Yt,l),f(e,Le,l),a(Le,He),f(e,Zt,l),f(e,j,l),a(j,Do),a(j,ce),a(ce,Uo),a(j,Ro),f(e,ea,l),f(e,qe,l),a(qe,Ce),f(e,ta,l),f(e,De,l),a(De,Wo),f(e,aa,l),f(e,I,l),a(I,J),a(J,bt),G(pe,bt,null),a(I,zo),a(I,_t),a(_t,Bo),f(e,oa,l),f(e,Ue,l),a(Ue,Fo),f(e,ra,l),f(e,ue,l),a(ue,Et),a(Et,jo),a(ue,Jo),f(e,ia,l),f(e,me,l),a(me,$t),a($t,Ko),a(me,Qo),f(e,sa,l),f(e,Re,l),a(Re,Vo),f(e,na,l),f(e,S,l),a(S,K),a(K,kt),G(ve,kt,null),a(S,Xo),a(S,Pt),a(Pt,Yo),f(e,la,l),f(e,We,l),a(We,Zo),f(e,fa,l),f(e,ze,l),a(ze,er),f(e,ha,l),f(e,O,l),a(O,Q),a(Q,At),G(ye,At,null),a(O,tr),a(O,It),a(It,ar),f(e,da,l),f(e,E,l),a(E,or),a(E,St),a(St,rr),a(E,ir),a(E,Ot),a(Ot,sr),a(E,nr),f(e,ca,l),f(e,V,l),a(V,lr),a(V,we),a(we,fr),a(V,hr),f(e,pa,l),f(e,$,l),a($,dr),a($,ge),a(ge,cr),a($,pr),a($,be),a(be,ur),a($,mr),f(e,ua,l),f(e,Be,l),a(Be,vr),f(e,ma,l),f(e,T,l),a(T,X),a(X,Tt),G(_e,Tt,null),a(T,yr),a(T,Nt),a(Nt,wr),f(e,va,l),f(e,k,l),a(k,gr),a(k,xt),a(xt,br),a(k,_r),a(k,Mt),a(Mt,Er),a(k,$r),f(e,ya,l),f(e,Fe,l),a(Fe,kr),f(e,wa,l),f(e,je,l),a(je,Pr),f(e,ga,l),f(e,Je,l),a(Je,Ar),f(e,ba,l),f(e,N,l),a(N,Y),a(Y,Gt),G(Ee,Gt,null),a(N,Ir),a(N,Lt),a(Lt,Sr),f(e,_a,l),f(e,Z,l),a(Z,Or),a(Z,$e),a($e,Tr),a(Z,Nr),f(e,Ea,l),f(e,v,l),a(v,xr),a(v,ke),a(ke,Mr),a(v,Gr),a(v,Pe),a(Pe,Lr),a(v,Hr),a(v,Ae),a(Ae,qr),a(v,Cr),f(e,$a,l),f(e,Ke,l),a(Ke,Dr),ka=!0},p(e,[l]){const Ie={};l&2&&(Ie.$$scope={dirty:l,ctx:e}),W.$set(Ie)},i(e){ka||(L(oe.$$.fragment,e),L(re.$$.fragment,e),L(W.$$.fragment,e),L(se.$$.fragment,e),L(pe.$$.fragment,e),L(ve.$$.fragment,e),L(ye.$$.fragment,e),L(_e.$$.fragment,e),L(Ee.$$.fragment,e),ka=!0)},o(e){H(oe.$$.fragment,e),H(re.$$.fragment,e),H(W.$$.fragment,e),H(se.$$.fragment,e),H(pe.$$.fragment,e),H(ve.$$.fragment,e),H(ye.$$.fragment,e),H(_e.$$.fragment,e),H(Ee.$$.fragment,e),ka=!1},d(e){t(u),e&&t(g),e&&t(w),q(oe),e&&t(qt),e&&t(D),e&&t(Ct),e&&t(U),e&&t(Dt),e&&t(P),q(re),e&&t(Ut),e&&t(Se),e&&t(Rt),e&&t(b),e&&t(Wt),e&&t(p),e&&t(zt),e&&t(xe),e&&t(Bt),e&&t(Me),e&&t(Ft),q(W,e),e&&t(jt),e&&t(z),e&&t(Jt),e&&t(A),q(se),e&&t(Kt),e&&t(_),e&&t(Qt),e&&t(Ge),e&&t(Vt),e&&t(m),e&&t(Xt),e&&t(F),e&&t(Yt),e&&t(Le),e&&t(Zt),e&&t(j),e&&t(ea),e&&t(qe),e&&t(ta),e&&t(De),e&&t(aa),e&&t(I),q(pe),e&&t(oa),e&&t(Ue),e&&t(ra),e&&t(ue),e&&t(ia),e&&t(me),e&&t(sa),e&&t(Re),e&&t(na),e&&t(S),q(ve),e&&t(la),e&&t(We),e&&t(fa),e&&t(ze),e&&t(ha),e&&t(O),q(ye),e&&t(da),e&&t(E),e&&t(ca),e&&t(V),e&&t(pa),e&&t($),e&&t(ua),e&&t(Be),e&&t(ma),e&&t(T),q(_e),e&&t(va),e&&t(k),e&&t(ya),e&&t(Fe),e&&t(wa),e&&t(je),e&&t(ga),e&&t(Je),e&&t(ba),e&&t(N),q(Ee),e&&t(_a),e&&t(Z),e&&t(Ea),e&&t(v),e&&t($a),e&&t(Ke)}}}const Es={local:"considerations-for-model-evaluation",sections:[{local:"properly-splitting-your-data",title:"Properly splitting your data"},{local:"the-impact-of-class-imbalance",title:"The impact of class imbalance"},{local:"offline-vs-online-model-evaluation",title:"Offline vs. online model evaluation"},{local:"tradeoffs-in-model-evaluation",sections:[{local:"interpretability",title:"Interpretability"},{local:"inference-speed-and-memory-footprint",title:"Inference speed and memory footprint"}],title:"Trade-offs in model evaluation"},{local:"limitations-and-bias",title:"Limitations and bias"}],title:"Considerations for model evaluation"};function $s(Ht){return ws(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Is extends us{constructor(u){super();ms(this,u,$s,_s,vs,{})}}export{Is as default,Es as metadata};
