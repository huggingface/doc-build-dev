import{S as mt,i as ft,s as ht,e as s,k as p,w as $,t as l,M as gt,c as o,d as t,m as d,a as r,x as y,h as i,b as h,G as a,g as m,y as E,q as w,o as x,B as j,v as vt,L as dt}from"../../chunks/vendor-hf-doc-builder.js";import{D as H}from"../../chunks/Docstring-hf-doc-builder.js";import{C as ut}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as ct}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as pt}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function _t(te){let f,k,g,u,b;return u=new ut({props:{code:`from evaluate import evaluator
# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){f=s("p"),k=l("Examples:"),g=p(),$(u.$$.fragment)},l(n){f=o(n,"P",{});var v=r(f);k=i(v,"Examples:"),v.forEach(t),g=d(n),y(u.$$.fragment,n)},m(n,v){m(n,f,v),a(f,k),m(n,g,v),E(u,n,v),b=!0},p:dt,i(n){b||(w(u.$$.fragment,n),b=!0)},o(n){x(u.$$.fragment,n),b=!1},d(n){n&&t(f),n&&t(g),j(u,n)}}}function bt(te){let f,k,g,u,b;return u=new ut({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("text-classification")
data =  Dataset.from_dict(load_dataset("imdb")["test"][:2])
results = e.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    input_column="text",
    label_column="label",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)
e = evaluator("image-classification")
data =  Dataset.from_dict(load_dataset("beans")["test"][:2])
results = e.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    metric="accuracy",
    input_column="image",
    label_column="labels",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data =  Dataset.from_dict(load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)[<span class="hljs-string">&quot;test&quot;</span>][:<span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;text&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;label&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;image-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data =  Dataset.from_dict(load_dataset(<span class="hljs-string">&quot;beans&quot;</span>)[<span class="hljs-string">&quot;test&quot;</span>][:<span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;nateraw/vit-base-beans&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;image&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;labels&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&#x27;angular_leaf_spot&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;bean_rust&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>: <span class="hljs-number">2</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){f=s("p"),k=l("Examples:"),g=p(),$(u.$$.fragment)},l(n){f=o(n,"P",{});var v=r(f);k=i(v,"Examples:"),v.forEach(t),g=d(n),y(u.$$.fragment,n)},m(n,v){m(n,f,v),a(f,k),m(n,g,v),E(u,n,v),b=!0},p:dt,i(n){b||(w(u.$$.fragment,n),b=!0)},o(n){x(u.$$.fragment,n),b=!1},d(n){n&&t(f),n&&t(g),j(u,n)}}}function $t(te){let f,k,g,u,b,n,v,ue,Ve,Te,se,He,Pe,M,A,me,G,Ge,fe,Je,De,oe,We,Ce,P,J,Ke,D,Qe,re,Xe,Ye,he,Ze,ea,ge,aa,ta,sa,O,Ne,ne,oa,Ie,_,W,ra,ve,na,la,N,K,ia,_e,ca,pa,S,da,z,Q,ua,be,ma,fa,R,X,ha,$e,ga,va,U,Y,_a,ye,ba,Me,le,$a,Le,L,Z,ya,q,Ea,ie,wa,xa,Ee,ja,qa,we,ka,Ta,xe,Pa,Da,Fe,F,ee,Ca,C,Na,ce,Ia,Ma,je,La,Fa,qe,Aa,Oa,Ae;return n=new ct({}),G=new ct({}),J=new H({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:<ul>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/pr_178/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
<li><code>&quot;image-classification&quot;</code>: will return a <a href="/docs/evaluate/pr_178/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator">ImageClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/vr_178/src/evaluate/evaluator/__init__.py#L73",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/pr_178/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),O=new pt({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[_t]},$$scope:{ctx:te}}}),W=new H({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_178/src/evaluate/evaluator/base.py#L57"}}),K=new H({props:{name:"compute",anchor:"evaluate.Evaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"preprocessor",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), ForwardRef('FeatureExtractionMixin'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'text'"},{name:"label_column",val:": str = 'label'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"evaluate.Evaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.Evaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"},{anchor:"evaluate.Evaluator.compute.preprocessor",description:`<strong>preprocessor</strong> (<code>str</code> or <code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"preprocessor"},{anchor:"evaluate.Evaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.Evaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.Evaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.Evaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.Evaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_178/src/evaluate/evaluator/base.py#L227",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),S=new pt({props:{anchor:"evaluate.Evaluator.compute.example",$$slots:{default:[bt]},$$scope:{ctx:te}}}),Q=new H({props:{name:"prepare_data",anchor:"evaluate.Evaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"input_column",val:": str"},{name:"label_column",val:": str"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_data.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.Evaluator.prepare_data.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.prepare_data.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_178/src/evaluate/evaluator/base.py#L131",returnDescription:`
<p>Loaded <code>datasets.Dataset</code>.</p>
`}}),X=new H({props:{name:"prepare_metric",anchor:"evaluate.Evaluator.prepare_metric",parameters:[{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule]"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_metric.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"}],source:"https://github.com/huggingface/evaluate/blob/vr_178/src/evaluate/evaluator/base.py#L203",returnDescription:`
<p>The loaded metric.</p>
`}}),Y=new H({props:{name:"prepare_pipeline",anchor:"evaluate.Evaluator.prepare_pipeline",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"preprocessor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_pipeline.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.prepare_pipeline.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.Evaluator.prepare_pipeline.preprocessor",description:`<strong>preprocessor</strong> (<code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"preprocessor"}],source:"https://github.com/huggingface/evaluate/blob/vr_178/src/evaluate/evaluator/base.py#L162",returnDescription:`
<p>The initialized pipeline.</p>
`}}),Z=new H({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_178/src/evaluate/evaluator/text_classification.py#L28"}}),ee=new H({props:{name:"class evaluate.ImageClassificationEvaluator",anchor:"evaluate.ImageClassificationEvaluator",parameters:[{name:"task",val:" = 'image-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_178/src/evaluate/evaluator/image_classification.py#L28"}}),{c(){f=s("meta"),k=p(),g=s("h1"),u=s("a"),b=s("span"),$(n.$$.fragment),v=p(),ue=s("span"),Ve=l("Evaluator"),Te=p(),se=s("p"),He=l("The evaluator classes for automatic evaluation."),Pe=p(),M=s("h2"),A=s("a"),me=s("span"),$(G.$$.fragment),Ge=p(),fe=s("span"),Je=l("Evaluator classes"),De=p(),oe=s("p"),We=l("The main entry point for using the evaluator:"),Ce=p(),P=s("div"),$(J.$$.fragment),Ke=p(),D=s("p"),Qe=l("Utility factory method to build an "),re=s("a"),Xe=l("Evaluator"),Ye=l(`.
Evaluators encapsulate a task and a default metric name. They leverage `),he=s("code"),Ze=l("pipeline"),ea=l(" functionalify from "),ge=s("code"),aa=l("transformers"),ta=l(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),sa=p(),$(O.$$.fragment),Ne=p(),ne=s("p"),oa=l("The base class for all evaluator classes:"),Ie=p(),_=s("div"),$(W.$$.fragment),ra=p(),ve=s("p"),na=l(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),la=p(),N=s("div"),$(K.$$.fragment),ia=p(),_e=s("p"),ca=l("Compute the metric for a given pipeline and dataset combination."),pa=p(),$(S.$$.fragment),da=p(),z=s("div"),$(Q.$$.fragment),ua=p(),be=s("p"),ma=l("Prepare data."),fa=p(),R=s("div"),$(X.$$.fragment),ha=p(),$e=s("p"),ga=l("Prepare metric."),va=p(),U=s("div"),$(Y.$$.fragment),_a=p(),ye=s("p"),ba=l("Prepare pipeline."),Me=p(),le=s("p"),$a=l("The class for text classification evaluation:"),Le=p(),L=s("div"),$(Z.$$.fragment),ya=p(),q=s("p"),Ea=l(`Text classification evaluator.
This text classification evaluator can currently be loaded from `),ie=s("a"),wa=l("evaluator()"),xa=l(` using the default task name
`),Ee=s("code"),ja=l("text-classification"),qa=l(" or with a "),we=s("code"),ka=l('"sentiment-analysis"'),Ta=l(` alias.
Methods in this class assume a data format compatible with the `),xe=s("code"),Pa=l("TextClassificationPipeline"),Da=l(` - a single textual
feature as input and a categorical label as output.`),Fe=p(),F=s("div"),$(ee.$$.fragment),Ca=p(),C=s("p"),Na=l(`Image classification evaluator.
This image classification evaluator can currently be loaded from `),ce=s("a"),Ia=l("evaluator()"),Ma=l(` using the default task name
`),je=s("code"),La=l("image-classification"),Fa=l(`.
Methods in this class assume a data format compatible with the `),qe=s("code"),Aa=l("ImageClassificationPipeline"),Oa=l("."),this.h()},l(e){const c=gt('[data-svelte="svelte-1phssyn"]',document.head);f=o(c,"META",{name:!0,content:!0}),c.forEach(t),k=d(e),g=o(e,"H1",{class:!0});var ae=r(g);u=o(ae,"A",{id:!0,class:!0,href:!0});var ke=r(u);b=o(ke,"SPAN",{});var Sa=r(b);y(n.$$.fragment,Sa),Sa.forEach(t),ke.forEach(t),v=d(ae),ue=o(ae,"SPAN",{});var za=r(ue);Ve=i(za,"Evaluator"),za.forEach(t),ae.forEach(t),Te=d(e),se=o(e,"P",{});var Ra=r(se);He=i(Ra,"The evaluator classes for automatic evaluation."),Ra.forEach(t),Pe=d(e),M=o(e,"H2",{class:!0});var Oe=r(M);A=o(Oe,"A",{id:!0,class:!0,href:!0});var Ua=r(A);me=o(Ua,"SPAN",{});var Ba=r(me);y(G.$$.fragment,Ba),Ba.forEach(t),Ua.forEach(t),Ge=d(Oe),fe=o(Oe,"SPAN",{});var Va=r(fe);Je=i(Va,"Evaluator classes"),Va.forEach(t),Oe.forEach(t),De=d(e),oe=o(e,"P",{});var Ha=r(oe);We=i(Ha,"The main entry point for using the evaluator:"),Ha.forEach(t),Ce=d(e),P=o(e,"DIV",{class:!0});var pe=r(P);y(J.$$.fragment,pe),Ke=d(pe),D=o(pe,"P",{});var B=r(D);Qe=i(B,"Utility factory method to build an "),re=o(B,"A",{href:!0});var Ga=r(re);Xe=i(Ga,"Evaluator"),Ga.forEach(t),Ye=i(B,`.
Evaluators encapsulate a task and a default metric name. They leverage `),he=o(B,"CODE",{});var Ja=r(he);Ze=i(Ja,"pipeline"),Ja.forEach(t),ea=i(B," functionalify from "),ge=o(B,"CODE",{});var Wa=r(ge);aa=i(Wa,"transformers"),Wa.forEach(t),ta=i(B,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),B.forEach(t),sa=d(pe),y(O.$$.fragment,pe),pe.forEach(t),Ne=d(e),ne=o(e,"P",{});var Ka=r(ne);oa=i(Ka,"The base class for all evaluator classes:"),Ka.forEach(t),Ie=d(e),_=o(e,"DIV",{class:!0});var T=r(_);y(W.$$.fragment,T),ra=d(T),ve=o(T,"P",{});var Qa=r(ve);na=i(Qa,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),Qa.forEach(t),la=d(T),N=o(T,"DIV",{class:!0});var de=r(N);y(K.$$.fragment,de),ia=d(de),_e=o(de,"P",{});var Xa=r(_e);ca=i(Xa,"Compute the metric for a given pipeline and dataset combination."),Xa.forEach(t),pa=d(de),y(S.$$.fragment,de),de.forEach(t),da=d(T),z=o(T,"DIV",{class:!0});var Se=r(z);y(Q.$$.fragment,Se),ua=d(Se),be=o(Se,"P",{});var Ya=r(be);ma=i(Ya,"Prepare data."),Ya.forEach(t),Se.forEach(t),fa=d(T),R=o(T,"DIV",{class:!0});var ze=r(R);y(X.$$.fragment,ze),ha=d(ze),$e=o(ze,"P",{});var Za=r($e);ga=i(Za,"Prepare metric."),Za.forEach(t),ze.forEach(t),va=d(T),U=o(T,"DIV",{class:!0});var Re=r(U);y(Y.$$.fragment,Re),_a=d(Re),ye=o(Re,"P",{});var et=r(ye);ba=i(et,"Prepare pipeline."),et.forEach(t),Re.forEach(t),T.forEach(t),Me=d(e),le=o(e,"P",{});var at=r(le);$a=i(at,"The class for text classification evaluation:"),at.forEach(t),Le=d(e),L=o(e,"DIV",{class:!0});var Ue=r(L);y(Z.$$.fragment,Ue),ya=d(Ue),q=o(Ue,"P",{});var I=r(q);Ea=i(I,`Text classification evaluator.
This text classification evaluator can currently be loaded from `),ie=o(I,"A",{href:!0});var tt=r(ie);wa=i(tt,"evaluator()"),tt.forEach(t),xa=i(I,` using the default task name
`),Ee=o(I,"CODE",{});var st=r(Ee);ja=i(st,"text-classification"),st.forEach(t),qa=i(I," or with a "),we=o(I,"CODE",{});var ot=r(we);ka=i(ot,'"sentiment-analysis"'),ot.forEach(t),Ta=i(I,` alias.
Methods in this class assume a data format compatible with the `),xe=o(I,"CODE",{});var rt=r(xe);Pa=i(rt,"TextClassificationPipeline"),rt.forEach(t),Da=i(I,` - a single textual
feature as input and a categorical label as output.`),I.forEach(t),Ue.forEach(t),Fe=d(e),F=o(e,"DIV",{class:!0});var Be=r(F);y(ee.$$.fragment,Be),Ca=d(Be),C=o(Be,"P",{});var V=r(C);Na=i(V,`Image classification evaluator.
This image classification evaluator can currently be loaded from `),ce=o(V,"A",{href:!0});var nt=r(ce);Ia=i(nt,"evaluator()"),nt.forEach(t),Ma=i(V,` using the default task name
`),je=o(V,"CODE",{});var lt=r(je);La=i(lt,"image-classification"),lt.forEach(t),Fa=i(V,`.
Methods in this class assume a data format compatible with the `),qe=o(V,"CODE",{});var it=r(qe);Aa=i(it,"ImageClassificationPipeline"),it.forEach(t),Oa=i(V,"."),V.forEach(t),Be.forEach(t),this.h()},h(){h(f,"name","hf:doc:metadata"),h(f,"content",JSON.stringify(yt)),h(u,"id","evaluator"),h(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(u,"href","#evaluator"),h(g,"class","relative group"),h(A,"id","evaluate.evaluator"),h(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(A,"href","#evaluate.evaluator"),h(M,"class","relative group"),h(re,"href","/docs/evaluate/pr_178/en/package_reference/evaluator_classes#evaluate.Evaluator"),h(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(_,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(ie,"href","/docs/evaluate/pr_178/en/package_reference/evaluator_classes#evaluate.evaluator"),h(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(ce,"href","/docs/evaluate/pr_178/en/package_reference/evaluator_classes#evaluate.evaluator"),h(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,c){a(document.head,f),m(e,k,c),m(e,g,c),a(g,u),a(u,b),E(n,b,null),a(g,v),a(g,ue),a(ue,Ve),m(e,Te,c),m(e,se,c),a(se,He),m(e,Pe,c),m(e,M,c),a(M,A),a(A,me),E(G,me,null),a(M,Ge),a(M,fe),a(fe,Je),m(e,De,c),m(e,oe,c),a(oe,We),m(e,Ce,c),m(e,P,c),E(J,P,null),a(P,Ke),a(P,D),a(D,Qe),a(D,re),a(re,Xe),a(D,Ye),a(D,he),a(he,Ze),a(D,ea),a(D,ge),a(ge,aa),a(D,ta),a(P,sa),E(O,P,null),m(e,Ne,c),m(e,ne,c),a(ne,oa),m(e,Ie,c),m(e,_,c),E(W,_,null),a(_,ra),a(_,ve),a(ve,na),a(_,la),a(_,N),E(K,N,null),a(N,ia),a(N,_e),a(_e,ca),a(N,pa),E(S,N,null),a(_,da),a(_,z),E(Q,z,null),a(z,ua),a(z,be),a(be,ma),a(_,fa),a(_,R),E(X,R,null),a(R,ha),a(R,$e),a($e,ga),a(_,va),a(_,U),E(Y,U,null),a(U,_a),a(U,ye),a(ye,ba),m(e,Me,c),m(e,le,c),a(le,$a),m(e,Le,c),m(e,L,c),E(Z,L,null),a(L,ya),a(L,q),a(q,Ea),a(q,ie),a(ie,wa),a(q,xa),a(q,Ee),a(Ee,ja),a(q,qa),a(q,we),a(we,ka),a(q,Ta),a(q,xe),a(xe,Pa),a(q,Da),m(e,Fe,c),m(e,F,c),E(ee,F,null),a(F,Ca),a(F,C),a(C,Na),a(C,ce),a(ce,Ia),a(C,Ma),a(C,je),a(je,La),a(C,Fa),a(C,qe),a(qe,Aa),a(C,Oa),Ae=!0},p(e,[c]){const ae={};c&2&&(ae.$$scope={dirty:c,ctx:e}),O.$set(ae);const ke={};c&2&&(ke.$$scope={dirty:c,ctx:e}),S.$set(ke)},i(e){Ae||(w(n.$$.fragment,e),w(G.$$.fragment,e),w(J.$$.fragment,e),w(O.$$.fragment,e),w(W.$$.fragment,e),w(K.$$.fragment,e),w(S.$$.fragment,e),w(Q.$$.fragment,e),w(X.$$.fragment,e),w(Y.$$.fragment,e),w(Z.$$.fragment,e),w(ee.$$.fragment,e),Ae=!0)},o(e){x(n.$$.fragment,e),x(G.$$.fragment,e),x(J.$$.fragment,e),x(O.$$.fragment,e),x(W.$$.fragment,e),x(K.$$.fragment,e),x(S.$$.fragment,e),x(Q.$$.fragment,e),x(X.$$.fragment,e),x(Y.$$.fragment,e),x(Z.$$.fragment,e),x(ee.$$.fragment,e),Ae=!1},d(e){t(f),e&&t(k),e&&t(g),j(n),e&&t(Te),e&&t(se),e&&t(Pe),e&&t(M),j(G),e&&t(De),e&&t(oe),e&&t(Ce),e&&t(P),j(J),j(O),e&&t(Ne),e&&t(ne),e&&t(Ie),e&&t(_),j(W),j(K),j(S),j(Q),j(X),j(Y),e&&t(Me),e&&t(le),e&&t(Le),e&&t(L),j(Z),e&&t(Fe),e&&t(F),j(ee)}}}const yt={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"}],title:"Evaluator"};function Et(te){return vt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Tt extends mt{constructor(f){super();ft(this,f,Et,$t,ht,{})}}export{Tt as default,yt as metadata};
