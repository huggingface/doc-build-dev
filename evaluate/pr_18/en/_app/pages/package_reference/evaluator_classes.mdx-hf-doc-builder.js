import{S as As,i as Ds,s as Is,e as o,k as d,w as _,t as l,M as Ns,c as s,d as a,m as p,a as n,x as b,h as i,b as u,G as e,g as v,y as $,q as w,o as E,B as y,v as Ms,L as It}from"../../chunks/vendor-hf-doc-builder.js";import{T as Qs}from"../../chunks/Tip-hf-doc-builder.js";import{D}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Nt}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Ye}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Dt}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function zs(T){let c,x,f,m,q;return m=new Nt({props:{code:`from evaluate import evaluator
# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){c=o("p"),x=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var g=n(c);x=i(g,"Examples:"),g.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,g){v(r,c,g),e(c,x),v(r,f,g),$(m,r,g),q=!0},p:It,i(r){q||(w(m.$$.fragment,r),q=!0)},o(r){E(m.$$.fragment,r),q=!1},d(r){r&&a(c),r&&a(f),y(m,r)}}}function Ls(T){let c,x,f,m,q;return m=new Nt({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
task_evaluator = evaluator("image-classification")
data = load_dataset("beans", split="test[:2]")
results = task_evaluator.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    metric="accuracy",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;image-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;beans&quot;</span>, split=<span class="hljs-string">&quot;test[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;nateraw/vit-base-beans&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&#x27;angular_leaf_spot&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;bean_rust&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>: <span class="hljs-number">2</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){c=o("p"),x=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var g=n(c);x=i(g,"Examples:"),g.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,g){v(r,c,g),e(c,x),v(r,f,g),$(m,r,g),q=!0},p:It,i(r){q||(w(m.$$.fragment,r),q=!0)},o(r){E(m.$$.fragment,r),q=!1},d(r){r&&a(c),r&&a(f),y(m,r)}}}function Ss(T){let c,x,f,m,q;return m=new Nt({props:{code:`from evaluate import evaluator
from datasets import load_dataset
e = evaluator("question-answering")
data = load_dataset("squad", split="validation[:2]")
results = e.compute(
    model_or_pipeline="sshleifer/tiny-distilbert-base-cased-distilled-squad",
    data=data,
    metric="squad",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;sshleifer/tiny-distilbert-base-cased-distilled-squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){c=o("p"),x=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var g=n(c);x=i(g,"Examples:"),g.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,g){v(r,c,g),e(c,x),v(r,f,g),$(m,r,g),q=!0},p:It,i(r){q||(w(m.$$.fragment,r),q=!0)},o(r){E(m.$$.fragment,r),q=!1},d(r){r&&a(c),r&&a(f),y(m,r)}}}function Fs(T){let c,x,f,m,q;return{c(){c=o("p"),x=l(`Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. If using transformers pipeline
with models trained on this type of data, make sure to pass `),f=o("code"),m=l("handle_impossible_answer=True"),q=l(" as an argument to the pipeline.")},l(r){c=s(r,"P",{});var g=n(c);x=i(g,`Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. If using transformers pipeline
with models trained on this type of data, make sure to pass `),f=s(g,"CODE",{});var J=n(f);m=i(J,"handle_impossible_answer=True"),J.forEach(a),q=i(g," as an argument to the pipeline."),g.forEach(a)},m(r,g){v(r,c,g),e(c,x),e(c,f),e(f,m),e(c,q)},d(r){r&&a(c)}}}function Os(T){let c,x;return c=new Nt({props:{code:`from evaluate import evaluator
from datasets import load_dataset
from transformers import pipeline
task_evaluator = evaluator("question-answering")
data = load_dataset("squad_v2", split="validation[:2]")
pipe = pipeline(
    task="question-answering",
    model="sshleifer/mrm8488/bert-tiny-finetuned-squadv2",
    handle_impossible_answer=True
)
results = task_evaluator.compute(
    model_or_pipeline=pipe,
    data=data,
    metric="squad_v2",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad_v2&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(
<span class="hljs-meta">&gt;&gt;&gt; </span>    task=<span class="hljs-string">&quot;question-answering&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    model=<span class="hljs-string">&quot;sshleifer/mrm8488/bert-tiny-finetuned-squadv2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    handle_impossible_answer=<span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=pipe,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad_v2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){_(c.$$.fragment)},l(f){b(c.$$.fragment,f)},m(f,m){$(c,f,m),x=!0},p:It,i(f){x||(w(c.$$.fragment,f),x=!0)},o(f){E(c.$$.fragment,f),x=!1},d(f){y(c,f)}}}function Rs(T){let c,x,f,m,q;return m=new Nt({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
task_evaluator = evaluator("text-classification")
data = load_dataset("imdb", split="test[:2]")
results = task_evaluator.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>, split=<span class="hljs-string">&quot;test[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){c=o("p"),x=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var g=n(c);x=i(g,"Examples:"),g.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,g){v(r,c,g),e(c,x),v(r,f,g),$(m,r,g),q=!0},p:It,i(r){q||(w(m.$$.fragment,r),q=!0)},o(r){E(m.$$.fragment,r),q=!1},d(r){r&&a(c),r&&a(f),y(m,r)}}}function Us(T){let c,x,f,m,q,r,g,J,la,Mt,Se,ia,Qt,U,K,Ze,he,ca,et,da,zt,Fe,pa,Lt,I,ge,ua,N,ma,Oe,fa,ha,tt,ga,va,at,_a,ba,$a,X,St,Re,wa,Ft,k,ve,Ea,ot,ya,xa,Y,_e,qa,st,ka,ja,Z,be,Ta,$e,Pa,nt,Ca,Aa,Da,ee,we,Ia,rt,Na,Ma,te,Ee,Qa,lt,za,La,ae,ye,Sa,it,Fa,Ot,V,oe,ct,xe,Oa,dt,Ra,Rt,B,se,pt,qe,Ua,ut,Va,Ut,M,ke,Ba,Q,Ha,Ue,Ga,Wa,mt,Ja,Ka,ft,Xa,Ya,Za,L,je,eo,ht,to,ao,ne,Vt,H,re,gt,Te,oo,vt,so,Bt,j,Pe,no,Ce,ro,le,_t,lo,io,co,po,G,uo,Ve,mo,fo,bt,ho,go,vo,Ae,_o,De,$t,bo,$o,wo,P,Ie,Eo,wt,yo,xo,ie,qo,ce,ko,de,Ht,W,pe,Et,Ne,jo,yt,To,Gt,z,Me,Po,A,Co,Be,Ao,Do,xt,Io,No,qt,Mo,Qo,kt,zo,Lo,So,S,Qe,Fo,jt,Oo,Ro,ue,Wt;return r=new Ye({}),he=new Ye({}),ge=new D({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:<ul>
<li><code>&quot;image-classification&quot;</code>: will return a <a href="/docs/evaluate/pr_18/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator">ImageClassificationEvaluator</a>.</li>
<li><code>&quot;question-answering&quot;</code>: will return a <a href="/docs/evaluate/pr_18/en/package_reference/evaluator_classes#evaluate.QuestionAnsweringEvaluator">QuestionAnsweringEvaluator</a>.</li>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/pr_18/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/vr_18/src/evaluate/evaluator/__init__.py#L79",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/pr_18/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),X=new Dt({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[zs]},$$scope:{ctx:T}}}),ve=new D({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_18/src/evaluate/evaluator/base.py#L50"}}),_e=new D({props:{name:"compute_metric",anchor:"evaluate.Evaluator.compute_metric",parameters:[{name:"metric",val:": EvaluationModule"},{name:"metric_inputs",val:": typing.Dict"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_18/src/evaluate/evaluator/base.py#L281"}}),be=new D({props:{name:"predictions_processor",anchor:"evaluate.Evaluator.predictions_processor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/evaluate/blob/vr_18/src/evaluate/evaluator/base.py#L121"}}),we=new D({props:{name:"prepare_data",anchor:"evaluate.Evaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"input_column",val:": str"},{name:"label_column",val:": str"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_data.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.Evaluator.prepare_data.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.prepare_data.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_18/src/evaluate/evaluator/base.py#L176",returnDescription:`
<p>metric inputs.
<code>list</code>:  pipeline inputs.</p>
`,returnType:`
<p><code>dict</code></p>
`}}),Ee=new D({props:{name:"prepare_metric",anchor:"evaluate.Evaluator.prepare_metric",parameters:[{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule]"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_metric.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"}],source:"https://github.com/huggingface/evaluate/blob/vr_18/src/evaluate/evaluator/base.py#L251",returnDescription:`
<p>The loaded metric.</p>
`}}),ye=new D({props:{name:"prepare_pipeline",anchor:"evaluate.Evaluator.prepare_pipeline",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"tokenizer",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"},{name:"feature_extractor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_pipeline.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.prepare_pipeline.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.Evaluator.prepare_pipeline.preprocessor",description:`<strong>preprocessor</strong> (<code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"preprocessor"}],source:"https://github.com/huggingface/evaluate/blob/vr_18/src/evaluate/evaluator/base.py#L208",returnDescription:`
<p>The initialized pipeline.</p>
`}}),xe=new Ye({}),qe=new Ye({}),ke=new D({props:{name:"class evaluate.ImageClassificationEvaluator",anchor:"evaluate.ImageClassificationEvaluator",parameters:[{name:"task",val:" = 'image-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_18/src/evaluate/evaluator/image_classification.py#L20"}}),je=new D({props:{name:"compute",anchor:"evaluate.ImageClassificationEvaluator.compute",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"evaluate.ImageClassificationEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>image-classification</code>. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.ImageClassificationEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.ImageClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
feature_extractor &#x2014; (<code>str</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default feature extractor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.ImageClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.ImageClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.ImageClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.ImageClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;image&quot;</code>) &#x2014;
the name of the column containing the images as PIL ImageFile in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;labels&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_18/src/evaluate/evaluator/image_classification.py#L37",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ne=new Dt({props:{anchor:"evaluate.ImageClassificationEvaluator.compute.example",$$slots:{default:[Ls]},$$scope:{ctx:T}}}),Te=new Ye({}),Pe=new D({props:{name:"class evaluate.QuestionAnsweringEvaluator",anchor:"evaluate.QuestionAnsweringEvaluator",parameters:[{name:"task",val:" = 'question-answering'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_18/src/evaluate/evaluator/question_answering.py#L38"}}),Ie=new D({props:{name:"compute",anchor:"evaluate.QuestionAnsweringEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"question_column",val:": str = 'question'"},{name:"context_column",val:": str = 'context'"},{name:"id_column",val:": str = 'id'"},{name:"label_column",val:": str = 'answers'"}],parametersDescription:[{anchor:"evaluate.QuestionAnsweringEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>question-answering</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.tokenizer",description:`<strong>tokenizer</strong> (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"tokenizer"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.strategy",description:`<strong>strategy</strong> (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;) &#x2014;
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"strategy"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.question_column",description:`<strong>question_column</strong> (<code>str</code>, defaults to <code>&quot;question&quot;</code>) &#x2014;
the name of the column containing the question in the dataset specified by <code>data</code>.`,name:"question_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.context_column",description:`<strong>context_column</strong> (<code>str</code>, defaults to <code>&quot;context&quot;</code>) &#x2014;
the name of the column containing the context in the dataset specified by <code>data</code>.`,name:"context_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.id_column",description:`<strong>id_column</strong> (<code>str</code>, defaults to <code>&quot;id&quot;</code>) &#x2014;
the name of the column cointaing the identification field of the question and answer pair in the
dataset specified by <code>data</code>.`,name:"id_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;answers&quot;</code>) &#x2014;
the name of the column containing the answers in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_18/src/evaluate/evaluator/question_answering.py#L115",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ie=new Dt({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example",$$slots:{default:[Ss]},$$scope:{ctx:T}}}),ce=new Qs({props:{$$slots:{default:[Fs]},$$scope:{ctx:T}}}),de=new Dt({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example-2",$$slots:{default:[Os]},$$scope:{ctx:T}}}),Ne=new Ye({}),Me=new D({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_18/src/evaluate/evaluator/text_classification.py#L20"}}),Qe=new D({props:{name:"compute",anchor:"evaluate.TextClassificationEvaluator.compute",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"evaluate.TextClassificationEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.TextClassificationEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.TextClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.TextClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TextClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TextClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TextClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_18/src/evaluate/evaluator/text_classification.py#L41",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ue=new Dt({props:{anchor:"evaluate.TextClassificationEvaluator.compute.example",$$slots:{default:[Rs]},$$scope:{ctx:T}}}),{c(){c=o("meta"),x=d(),f=o("h1"),m=o("a"),q=o("span"),_(r.$$.fragment),g=d(),J=o("span"),la=l("Evaluator"),Mt=d(),Se=o("p"),ia=l("The evaluator classes for automatic evaluation."),Qt=d(),U=o("h2"),K=o("a"),Ze=o("span"),_(he.$$.fragment),ca=d(),et=o("span"),da=l("Evaluator classes"),zt=d(),Fe=o("p"),pa=l("The main entry point for using the evaluator:"),Lt=d(),I=o("div"),_(ge.$$.fragment),ua=d(),N=o("p"),ma=l("Utility factory method to build an "),Oe=o("a"),fa=l("Evaluator"),ha=l(`.
Evaluators encapsulate a task and a default metric name. They leverage `),tt=o("code"),ga=l("pipeline"),va=l(" functionalify from "),at=o("code"),_a=l("transformers"),ba=l(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),$a=d(),_(X.$$.fragment),St=d(),Re=o("p"),wa=l("The base class for all evaluator classes:"),Ft=d(),k=o("div"),_(ve.$$.fragment),Ea=d(),ot=o("p"),ya=l(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),xa=d(),Y=o("div"),_(_e.$$.fragment),qa=d(),st=o("p"),ka=l("Compute and return metrics."),ja=d(),Z=o("div"),_(be.$$.fragment),Ta=d(),$e=o("p"),Pa=l("A core method of the "),nt=o("code"),Ca=l("Evaluator"),Aa=l(" class, which processes the pipeline outputs for compatibility with the metric."),Da=d(),ee=o("div"),_(we.$$.fragment),Ia=d(),rt=o("p"),Na=l("Prepare data."),Ma=d(),te=o("div"),_(Ee.$$.fragment),Qa=d(),lt=o("p"),za=l("Prepare metric."),La=d(),ae=o("div"),_(ye.$$.fragment),Sa=d(),it=o("p"),Fa=l("Prepare pipeline."),Ot=d(),V=o("h2"),oe=o("a"),ct=o("span"),_(xe.$$.fragment),Oa=d(),dt=o("span"),Ra=l("The task specific evaluators"),Rt=d(),B=o("h3"),se=o("a"),pt=o("span"),_(qe.$$.fragment),Ua=d(),ut=o("span"),Va=l("ImageClassificationEvaluator"),Ut=d(),M=o("div"),_(ke.$$.fragment),Ba=d(),Q=o("p"),Ha=l(`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Ue=o("a"),Ga=l("evaluator()"),Wa=l(` using the default task name
`),mt=o("code"),Ja=l("image-classification"),Ka=l(`.
Methods in this class assume a data format compatible with the `),ft=o("code"),Xa=l("ImageClassificationPipeline"),Ya=l("."),Za=d(),L=o("div"),_(je.$$.fragment),eo=d(),ht=o("p"),to=l("Compute the metric for a given pipeline and dataset combination."),ao=d(),_(ne.$$.fragment),Vt=d(),H=o("h3"),re=o("a"),gt=o("span"),_(Te.$$.fragment),oo=d(),vt=o("span"),so=l("QuestionAnsweringEvaluator"),Bt=d(),j=o("div"),_(Pe.$$.fragment),no=d(),Ce=o("p"),ro=l(`Question answering evaluator. This evaluator handles
`),le=o("a"),_t=o("strong"),lo=l("extractive"),io=l(" question answering"),co=l(`,
where the answer to the question is extracted from a context.`),po=d(),G=o("p"),uo=l("This question answering evaluator can currently be loaded from "),Ve=o("a"),mo=l("evaluator()"),fo=l(` using the default task name
`),bt=o("code"),ho=l("question-answering"),go=l("."),vo=d(),Ae=o("p"),_o=l(`Methods in this class assume a data format compatible with the
`),De=o("a"),$t=o("code"),bo=l("QuestionAnsweringPipeline"),$o=l("."),wo=d(),P=o("div"),_(Ie.$$.fragment),Eo=d(),wt=o("p"),yo=l("Compute the metric for a given pipeline and dataset combination."),xo=d(),_(ie.$$.fragment),qo=d(),_(ce.$$.fragment),ko=d(),_(de.$$.fragment),Ht=d(),W=o("h3"),pe=o("a"),Et=o("span"),_(Ne.$$.fragment),jo=d(),yt=o("span"),To=l("TextClassificationEvaluator"),Gt=d(),z=o("div"),_(Me.$$.fragment),Po=d(),A=o("p"),Co=l(`Text classification evaluator.
This text classification evaluator can currently be loaded from `),Be=o("a"),Ao=l("evaluator()"),Do=l(` using the default task name
`),xt=o("code"),Io=l("text-classification"),No=l(" or with a "),qt=o("code"),Mo=l('"sentiment-analysis"'),Qo=l(` alias.
Methods in this class assume a data format compatible with the `),kt=o("code"),zo=l("TextClassificationPipeline"),Lo=l(` - a single textual
feature as input and a categorical label as output.`),So=d(),S=o("div"),_(Qe.$$.fragment),Fo=d(),jt=o("p"),Oo=l("Compute the metric for a given pipeline and dataset combination."),Ro=d(),_(ue.$$.fragment),this.h()},l(t){const h=Ns('[data-svelte="svelte-1phssyn"]',document.head);c=s(h,"META",{name:!0,content:!0}),h.forEach(a),x=p(t),f=s(t,"H1",{class:!0});var ze=n(f);m=s(ze,"A",{id:!0,class:!0,href:!0});var Tt=n(m);q=s(Tt,"SPAN",{});var Pt=n(q);b(r.$$.fragment,Pt),Pt.forEach(a),Tt.forEach(a),g=p(ze),J=s(ze,"SPAN",{});var Ct=n(J);la=i(Ct,"Evaluator"),Ct.forEach(a),ze.forEach(a),Mt=p(t),Se=s(t,"P",{});var At=n(Se);ia=i(At,"The evaluator classes for automatic evaluation."),At.forEach(a),Qt=p(t),U=s(t,"H2",{class:!0});var Le=n(U);K=s(Le,"A",{id:!0,class:!0,href:!0});var Vo=n(K);Ze=s(Vo,"SPAN",{});var Bo=n(Ze);b(he.$$.fragment,Bo),Bo.forEach(a),Vo.forEach(a),ca=p(Le),et=s(Le,"SPAN",{});var Ho=n(et);da=i(Ho,"Evaluator classes"),Ho.forEach(a),Le.forEach(a),zt=p(t),Fe=s(t,"P",{});var Go=n(Fe);pa=i(Go,"The main entry point for using the evaluator:"),Go.forEach(a),Lt=p(t),I=s(t,"DIV",{class:!0});var He=n(I);b(ge.$$.fragment,He),ua=p(He),N=s(He,"P",{});var me=n(N);ma=i(me,"Utility factory method to build an "),Oe=s(me,"A",{href:!0});var Wo=n(Oe);fa=i(Wo,"Evaluator"),Wo.forEach(a),ha=i(me,`.
Evaluators encapsulate a task and a default metric name. They leverage `),tt=s(me,"CODE",{});var Jo=n(tt);ga=i(Jo,"pipeline"),Jo.forEach(a),va=i(me," functionalify from "),at=s(me,"CODE",{});var Ko=n(at);_a=i(Ko,"transformers"),Ko.forEach(a),ba=i(me,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),me.forEach(a),$a=p(He),b(X.$$.fragment,He),He.forEach(a),St=p(t),Re=s(t,"P",{});var Xo=n(Re);wa=i(Xo,"The base class for all evaluator classes:"),Xo.forEach(a),Ft=p(t),k=s(t,"DIV",{class:!0});var C=n(k);b(ve.$$.fragment,C),Ea=p(C),ot=s(C,"P",{});var Yo=n(ot);ya=i(Yo,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),Yo.forEach(a),xa=p(C),Y=s(C,"DIV",{class:!0});var Jt=n(Y);b(_e.$$.fragment,Jt),qa=p(Jt),st=s(Jt,"P",{});var Zo=n(st);ka=i(Zo,"Compute and return metrics."),Zo.forEach(a),Jt.forEach(a),ja=p(C),Z=s(C,"DIV",{class:!0});var Kt=n(Z);b(be.$$.fragment,Kt),Ta=p(Kt),$e=s(Kt,"P",{});var Xt=n($e);Pa=i(Xt,"A core method of the "),nt=s(Xt,"CODE",{});var es=n(nt);Ca=i(es,"Evaluator"),es.forEach(a),Aa=i(Xt," class, which processes the pipeline outputs for compatibility with the metric."),Xt.forEach(a),Kt.forEach(a),Da=p(C),ee=s(C,"DIV",{class:!0});var Yt=n(ee);b(we.$$.fragment,Yt),Ia=p(Yt),rt=s(Yt,"P",{});var ts=n(rt);Na=i(ts,"Prepare data."),ts.forEach(a),Yt.forEach(a),Ma=p(C),te=s(C,"DIV",{class:!0});var Zt=n(te);b(Ee.$$.fragment,Zt),Qa=p(Zt),lt=s(Zt,"P",{});var as=n(lt);za=i(as,"Prepare metric."),as.forEach(a),Zt.forEach(a),La=p(C),ae=s(C,"DIV",{class:!0});var ea=n(ae);b(ye.$$.fragment,ea),Sa=p(ea),it=s(ea,"P",{});var os=n(it);Fa=i(os,"Prepare pipeline."),os.forEach(a),ea.forEach(a),C.forEach(a),Ot=p(t),V=s(t,"H2",{class:!0});var ta=n(V);oe=s(ta,"A",{id:!0,class:!0,href:!0});var ss=n(oe);ct=s(ss,"SPAN",{});var ns=n(ct);b(xe.$$.fragment,ns),ns.forEach(a),ss.forEach(a),Oa=p(ta),dt=s(ta,"SPAN",{});var rs=n(dt);Ra=i(rs,"The task specific evaluators"),rs.forEach(a),ta.forEach(a),Rt=p(t),B=s(t,"H3",{class:!0});var aa=n(B);se=s(aa,"A",{id:!0,class:!0,href:!0});var ls=n(se);pt=s(ls,"SPAN",{});var is=n(pt);b(qe.$$.fragment,is),is.forEach(a),ls.forEach(a),Ua=p(aa),ut=s(aa,"SPAN",{});var cs=n(ut);Va=i(cs,"ImageClassificationEvaluator"),cs.forEach(a),aa.forEach(a),Ut=p(t),M=s(t,"DIV",{class:!0});var Ge=n(M);b(ke.$$.fragment,Ge),Ba=p(Ge),Q=s(Ge,"P",{});var fe=n(Q);Ha=i(fe,`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Ue=s(fe,"A",{href:!0});var ds=n(Ue);Ga=i(ds,"evaluator()"),ds.forEach(a),Wa=i(fe,` using the default task name
`),mt=s(fe,"CODE",{});var ps=n(mt);Ja=i(ps,"image-classification"),ps.forEach(a),Ka=i(fe,`.
Methods in this class assume a data format compatible with the `),ft=s(fe,"CODE",{});var us=n(ft);Xa=i(us,"ImageClassificationPipeline"),us.forEach(a),Ya=i(fe,"."),fe.forEach(a),Za=p(Ge),L=s(Ge,"DIV",{class:!0});var We=n(L);b(je.$$.fragment,We),eo=p(We),ht=s(We,"P",{});var ms=n(ht);to=i(ms,"Compute the metric for a given pipeline and dataset combination."),ms.forEach(a),ao=p(We),b(ne.$$.fragment,We),We.forEach(a),Ge.forEach(a),Vt=p(t),H=s(t,"H3",{class:!0});var oa=n(H);re=s(oa,"A",{id:!0,class:!0,href:!0});var fs=n(re);gt=s(fs,"SPAN",{});var hs=n(gt);b(Te.$$.fragment,hs),hs.forEach(a),fs.forEach(a),oo=p(oa),vt=s(oa,"SPAN",{});var gs=n(vt);so=i(gs,"QuestionAnsweringEvaluator"),gs.forEach(a),oa.forEach(a),Bt=p(t),j=s(t,"DIV",{class:!0});var F=n(j);b(Pe.$$.fragment,F),no=p(F),Ce=s(F,"P",{});var sa=n(Ce);ro=i(sa,`Question answering evaluator. This evaluator handles
`),le=s(sa,"A",{href:!0,rel:!0});var Uo=n(le);_t=s(Uo,"STRONG",{});var vs=n(_t);lo=i(vs,"extractive"),vs.forEach(a),io=i(Uo," question answering"),Uo.forEach(a),co=i(sa,`,
where the answer to the question is extracted from a context.`),sa.forEach(a),po=p(F),G=s(F,"P",{});var Je=n(G);uo=i(Je,"This question answering evaluator can currently be loaded from "),Ve=s(Je,"A",{href:!0});var _s=n(Ve);mo=i(_s,"evaluator()"),_s.forEach(a),fo=i(Je,` using the default task name
`),bt=s(Je,"CODE",{});var bs=n(bt);ho=i(bs,"question-answering"),bs.forEach(a),go=i(Je,"."),Je.forEach(a),vo=p(F),Ae=s(F,"P",{});var na=n(Ae);_o=i(na,`Methods in this class assume a data format compatible with the
`),De=s(na,"A",{href:!0,rel:!0});var $s=n(De);$t=s($s,"CODE",{});var ws=n($t);bo=i(ws,"QuestionAnsweringPipeline"),ws.forEach(a),$s.forEach(a),$o=i(na,"."),na.forEach(a),wo=p(F),P=s(F,"DIV",{class:!0});var O=n(P);b(Ie.$$.fragment,O),Eo=p(O),wt=s(O,"P",{});var Es=n(wt);yo=i(Es,"Compute the metric for a given pipeline and dataset combination."),Es.forEach(a),xo=p(O),b(ie.$$.fragment,O),qo=p(O),b(ce.$$.fragment,O),ko=p(O),b(de.$$.fragment,O),O.forEach(a),F.forEach(a),Ht=p(t),W=s(t,"H3",{class:!0});var ra=n(W);pe=s(ra,"A",{id:!0,class:!0,href:!0});var ys=n(pe);Et=s(ys,"SPAN",{});var xs=n(Et);b(Ne.$$.fragment,xs),xs.forEach(a),ys.forEach(a),jo=p(ra),yt=s(ra,"SPAN",{});var qs=n(yt);To=i(qs,"TextClassificationEvaluator"),qs.forEach(a),ra.forEach(a),Gt=p(t),z=s(t,"DIV",{class:!0});var Ke=n(z);b(Me.$$.fragment,Ke),Po=p(Ke),A=s(Ke,"P",{});var R=n(A);Co=i(R,`Text classification evaluator.
This text classification evaluator can currently be loaded from `),Be=s(R,"A",{href:!0});var ks=n(Be);Ao=i(ks,"evaluator()"),ks.forEach(a),Do=i(R,` using the default task name
`),xt=s(R,"CODE",{});var js=n(xt);Io=i(js,"text-classification"),js.forEach(a),No=i(R," or with a "),qt=s(R,"CODE",{});var Ts=n(qt);Mo=i(Ts,'"sentiment-analysis"'),Ts.forEach(a),Qo=i(R,` alias.
Methods in this class assume a data format compatible with the `),kt=s(R,"CODE",{});var Ps=n(kt);zo=i(Ps,"TextClassificationPipeline"),Ps.forEach(a),Lo=i(R,` - a single textual
feature as input and a categorical label as output.`),R.forEach(a),So=p(Ke),S=s(Ke,"DIV",{class:!0});var Xe=n(S);b(Qe.$$.fragment,Xe),Fo=p(Xe),jt=s(Xe,"P",{});var Cs=n(jt);Oo=i(Cs,"Compute the metric for a given pipeline and dataset combination."),Cs.forEach(a),Ro=p(Xe),b(ue.$$.fragment,Xe),Xe.forEach(a),Ke.forEach(a),this.h()},h(){u(c,"name","hf:doc:metadata"),u(c,"content",JSON.stringify(Vs)),u(m,"id","evaluator"),u(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(m,"href","#evaluator"),u(f,"class","relative group"),u(K,"id","evaluate.evaluator"),u(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(K,"href","#evaluate.evaluator"),u(U,"class","relative group"),u(Oe,"href","/docs/evaluate/pr_18/en/package_reference/evaluator_classes#evaluate.Evaluator"),u(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(oe,"id","the-task-specific-evaluators"),u(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(oe,"href","#the-task-specific-evaluators"),u(V,"class","relative group"),u(se,"id","evaluate.ImageClassificationEvaluator"),u(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(se,"href","#evaluate.ImageClassificationEvaluator"),u(B,"class","relative group"),u(Ue,"href","/docs/evaluate/pr_18/en/package_reference/evaluator_classes#evaluate.evaluator"),u(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(re,"id","evaluate.QuestionAnsweringEvaluator"),u(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(re,"href","#evaluate.QuestionAnsweringEvaluator"),u(H,"class","relative group"),u(le,"href","https://huggingface.co/docs/transformers/task_summary#extractive-question-answering"),u(le,"rel","nofollow"),u(Ve,"href","/docs/evaluate/pr_18/en/package_reference/evaluator_classes#evaluate.evaluator"),u(De,"href","https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline"),u(De,"rel","nofollow"),u(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(pe,"id","evaluate.TextClassificationEvaluator"),u(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(pe,"href","#evaluate.TextClassificationEvaluator"),u(W,"class","relative group"),u(Be,"href","/docs/evaluate/pr_18/en/package_reference/evaluator_classes#evaluate.evaluator"),u(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,h){e(document.head,c),v(t,x,h),v(t,f,h),e(f,m),e(m,q),$(r,q,null),e(f,g),e(f,J),e(J,la),v(t,Mt,h),v(t,Se,h),e(Se,ia),v(t,Qt,h),v(t,U,h),e(U,K),e(K,Ze),$(he,Ze,null),e(U,ca),e(U,et),e(et,da),v(t,zt,h),v(t,Fe,h),e(Fe,pa),v(t,Lt,h),v(t,I,h),$(ge,I,null),e(I,ua),e(I,N),e(N,ma),e(N,Oe),e(Oe,fa),e(N,ha),e(N,tt),e(tt,ga),e(N,va),e(N,at),e(at,_a),e(N,ba),e(I,$a),$(X,I,null),v(t,St,h),v(t,Re,h),e(Re,wa),v(t,Ft,h),v(t,k,h),$(ve,k,null),e(k,Ea),e(k,ot),e(ot,ya),e(k,xa),e(k,Y),$(_e,Y,null),e(Y,qa),e(Y,st),e(st,ka),e(k,ja),e(k,Z),$(be,Z,null),e(Z,Ta),e(Z,$e),e($e,Pa),e($e,nt),e(nt,Ca),e($e,Aa),e(k,Da),e(k,ee),$(we,ee,null),e(ee,Ia),e(ee,rt),e(rt,Na),e(k,Ma),e(k,te),$(Ee,te,null),e(te,Qa),e(te,lt),e(lt,za),e(k,La),e(k,ae),$(ye,ae,null),e(ae,Sa),e(ae,it),e(it,Fa),v(t,Ot,h),v(t,V,h),e(V,oe),e(oe,ct),$(xe,ct,null),e(V,Oa),e(V,dt),e(dt,Ra),v(t,Rt,h),v(t,B,h),e(B,se),e(se,pt),$(qe,pt,null),e(B,Ua),e(B,ut),e(ut,Va),v(t,Ut,h),v(t,M,h),$(ke,M,null),e(M,Ba),e(M,Q),e(Q,Ha),e(Q,Ue),e(Ue,Ga),e(Q,Wa),e(Q,mt),e(mt,Ja),e(Q,Ka),e(Q,ft),e(ft,Xa),e(Q,Ya),e(M,Za),e(M,L),$(je,L,null),e(L,eo),e(L,ht),e(ht,to),e(L,ao),$(ne,L,null),v(t,Vt,h),v(t,H,h),e(H,re),e(re,gt),$(Te,gt,null),e(H,oo),e(H,vt),e(vt,so),v(t,Bt,h),v(t,j,h),$(Pe,j,null),e(j,no),e(j,Ce),e(Ce,ro),e(Ce,le),e(le,_t),e(_t,lo),e(le,io),e(Ce,co),e(j,po),e(j,G),e(G,uo),e(G,Ve),e(Ve,mo),e(G,fo),e(G,bt),e(bt,ho),e(G,go),e(j,vo),e(j,Ae),e(Ae,_o),e(Ae,De),e(De,$t),e($t,bo),e(Ae,$o),e(j,wo),e(j,P),$(Ie,P,null),e(P,Eo),e(P,wt),e(wt,yo),e(P,xo),$(ie,P,null),e(P,qo),$(ce,P,null),e(P,ko),$(de,P,null),v(t,Ht,h),v(t,W,h),e(W,pe),e(pe,Et),$(Ne,Et,null),e(W,jo),e(W,yt),e(yt,To),v(t,Gt,h),v(t,z,h),$(Me,z,null),e(z,Po),e(z,A),e(A,Co),e(A,Be),e(Be,Ao),e(A,Do),e(A,xt),e(xt,Io),e(A,No),e(A,qt),e(qt,Mo),e(A,Qo),e(A,kt),e(kt,zo),e(A,Lo),e(z,So),e(z,S),$(Qe,S,null),e(S,Fo),e(S,jt),e(jt,Oo),e(S,Ro),$(ue,S,null),Wt=!0},p(t,[h]){const ze={};h&2&&(ze.$$scope={dirty:h,ctx:t}),X.$set(ze);const Tt={};h&2&&(Tt.$$scope={dirty:h,ctx:t}),ne.$set(Tt);const Pt={};h&2&&(Pt.$$scope={dirty:h,ctx:t}),ie.$set(Pt);const Ct={};h&2&&(Ct.$$scope={dirty:h,ctx:t}),ce.$set(Ct);const At={};h&2&&(At.$$scope={dirty:h,ctx:t}),de.$set(At);const Le={};h&2&&(Le.$$scope={dirty:h,ctx:t}),ue.$set(Le)},i(t){Wt||(w(r.$$.fragment,t),w(he.$$.fragment,t),w(ge.$$.fragment,t),w(X.$$.fragment,t),w(ve.$$.fragment,t),w(_e.$$.fragment,t),w(be.$$.fragment,t),w(we.$$.fragment,t),w(Ee.$$.fragment,t),w(ye.$$.fragment,t),w(xe.$$.fragment,t),w(qe.$$.fragment,t),w(ke.$$.fragment,t),w(je.$$.fragment,t),w(ne.$$.fragment,t),w(Te.$$.fragment,t),w(Pe.$$.fragment,t),w(Ie.$$.fragment,t),w(ie.$$.fragment,t),w(ce.$$.fragment,t),w(de.$$.fragment,t),w(Ne.$$.fragment,t),w(Me.$$.fragment,t),w(Qe.$$.fragment,t),w(ue.$$.fragment,t),Wt=!0)},o(t){E(r.$$.fragment,t),E(he.$$.fragment,t),E(ge.$$.fragment,t),E(X.$$.fragment,t),E(ve.$$.fragment,t),E(_e.$$.fragment,t),E(be.$$.fragment,t),E(we.$$.fragment,t),E(Ee.$$.fragment,t),E(ye.$$.fragment,t),E(xe.$$.fragment,t),E(qe.$$.fragment,t),E(ke.$$.fragment,t),E(je.$$.fragment,t),E(ne.$$.fragment,t),E(Te.$$.fragment,t),E(Pe.$$.fragment,t),E(Ie.$$.fragment,t),E(ie.$$.fragment,t),E(ce.$$.fragment,t),E(de.$$.fragment,t),E(Ne.$$.fragment,t),E(Me.$$.fragment,t),E(Qe.$$.fragment,t),E(ue.$$.fragment,t),Wt=!1},d(t){a(c),t&&a(x),t&&a(f),y(r),t&&a(Mt),t&&a(Se),t&&a(Qt),t&&a(U),y(he),t&&a(zt),t&&a(Fe),t&&a(Lt),t&&a(I),y(ge),y(X),t&&a(St),t&&a(Re),t&&a(Ft),t&&a(k),y(ve),y(_e),y(be),y(we),y(Ee),y(ye),t&&a(Ot),t&&a(V),y(xe),t&&a(Rt),t&&a(B),y(qe),t&&a(Ut),t&&a(M),y(ke),y(je),y(ne),t&&a(Vt),t&&a(H),y(Te),t&&a(Bt),t&&a(j),y(Pe),y(Ie),y(ie),y(ce),y(de),t&&a(Ht),t&&a(W),y(Ne),t&&a(Gt),t&&a(z),y(Me),y(Qe),y(ue)}}}const Vs={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"},{local:"the-task-specific-evaluators",sections:[{local:"evaluate.ImageClassificationEvaluator",title:"ImageClassificationEvaluator"},{local:"evaluate.QuestionAnsweringEvaluator",title:"QuestionAnsweringEvaluator"},{local:"evaluate.TextClassificationEvaluator",title:"TextClassificationEvaluator"}],title:"The task specific evaluators"}],title:"Evaluator"};function Bs(T){return Ms(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ys extends As{constructor(c){super();Ds(this,c,Bs,Us,Is,{})}}export{Ys as default,Vs as metadata};
