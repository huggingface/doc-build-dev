import{S as Ar,i as Lr,s as Pr,e as o,k as f,w as B,t as s,M as jr,c as i,d as a,m as p,a as l,x as F,h as r,b as n,G as t,g as h,y as Q,q as H,o as W,B as Y,v as Sr}from"../chunks/vendor-hf-doc-builder.js";import{T as Tr}from"../chunks/Tip-hf-doc-builder.js";import{I as lt}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as xr}from"../chunks/CodeBlock-hf-doc-builder.js";function qr(nt){let m;return{c(){m=s(`\u{1F4A1}
GLUE is actually a collection of different subsets on different tasks, so first you need to choose the one that corresponds to the NLI task, such as mnli, which is described as \u201Ccrowdsourced collection of sentence pairs with textual entailment annotations\u201D`)},l(k){m=r(k,`\u{1F4A1}
GLUE is actually a collection of different subsets on different tasks, so first you need to choose the one that corresponds to the NLI task, such as mnli, which is described as \u201Ccrowdsourced collection of sentence pairs with textual entailment annotations\u201D`)},m(k,w){h(k,m,w)},d(k){k&&a(m)}}}function Gr(nt){let m,k,w,q,De,z,Ct,Oe,Mt,ct,xe,Re,Dt,ht,Ae,Ot,ft,L,G,Ie,V,Rt,Be,It,pt,Le,Bt,ut,b,Pe,Fe,Ft,Qt,Ht,v,Qe,Wt,Yt,J,zt,Vt,K,Jt,Kt,X,Xt,Zt,ea,$,He,ta,aa,Z,sa,ra,ee,oa,ia,dt,je,la,mt,P,U,We,te,na,Ye,ca,gt,Se,ha,vt,_,fa,ae,pa,ua,se,da,ma,re,ga,va,_t,N,_a,oe,ya,Ea,yt,ie,Et,j,C,ze,le,wa,Ve,ka,wt,d,ba,ne,$a,xa,ce,Aa,La,he,Pa,ja,fe,Sa,Ta,pe,qa,Ga,kt,Te,Ua,bt,y,qe,Ge,Na,ue,Ca,Ma,Da,M,Je,Oa,Ra,de,Ia,Ba,Fa,x,Ke,Qa,Ha,me,Wa,Ya,ge,za,Va,Ja,Ue,Xe,Ka,Xa,$t,S,D,Ze,ve,Za,et,es,xt,A,ts,_e,as,ss,ye,rs,os,At,O,Lt,u,is,Ee,ls,ns,tt,cs,hs,at,fs,ps,st,us,ds,rt,ms,gs,ot,vs,_s,Pt,we,jt,Ne,ys,St;return z=new lt({}),V=new lt({}),te=new lt({}),ie=new xr({props:{code:`precision_metric = evaluate.load("precision")
results = precision_metric.compute(references=[0, 1], predictions=[0, 1])
print(results)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">precision_metric = evaluate.load(<span class="hljs-string">&quot;precision&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">results = precision_metric.compute(references=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], predictions=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-built_in">print</span>(results)</span>
{&#x27;precision&#x27;: 1.0}`}}),le=new lt({}),ve=new lt({}),O=new Tr({props:{warning:!0,$$slots:{default:[qr]},$$scope:{ctx:nt}}}),we=new xr({props:{code:`from evaluate import load
squad_metric = load("squad")
predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]
references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]
results = squad_metric.compute(predictions=predictions, references=references)
results`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> load</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">squad_metric = load(<span class="hljs-string">&quot;squad&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">predictions = [{<span class="hljs-string">&#x27;prediction_text&#x27;</span>: <span class="hljs-string">&#x27;1976&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;56e10a3be3433e1400422b22&#x27;</span>}]</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">references = [{<span class="hljs-string">&#x27;answers&#x27;</span>: {<span class="hljs-string">&#x27;answer_start&#x27;</span>: [<span class="hljs-number">97</span>], <span class="hljs-string">&#x27;text&#x27;</span>: [<span class="hljs-string">&#x27;1976&#x27;</span>]}, <span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;56e10a3be3433e1400422b22&#x27;</span>}]</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">results = squad_metric.compute(predictions=predictions, references=references)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">results</span>
{&#x27;exact_match&#x27;: 100.0, &#x27;f1&#x27;: 100.0}`}}),{c(){m=o("meta"),k=f(),w=o("h1"),q=o("a"),De=o("span"),B(z.$$.fragment),Ct=f(),Oe=o("span"),Mt=s("Choosing a metric for your task"),ct=f(),xe=o("p"),Re=o("strong"),Dt=s("So you\u2019ve trained your model and want to see how well it\u2019s doing on a dataset of your choice. Where do you start?"),ht=f(),Ae=o("p"),Ot=s("There is no \u201Cone size fits all\u201D approach to choosing an evaluation metric, but some good guidelines to keep in mind are:"),ft=f(),L=o("h2"),G=o("a"),Ie=o("span"),B(V.$$.fragment),Rt=f(),Be=o("span"),It=s("Categories of metrics"),pt=f(),Le=o("p"),Bt=s("There are 3 high-level categories of metrics:"),ut=f(),b=o("ol"),Pe=o("li"),Fe=o("em"),Ft=s("Generic metrics"),Qt=s(", which can be applied to a variety of situations and datasets, such as precision and accuracy."),Ht=f(),v=o("li"),Qe=o("em"),Wt=s("Task-specific metrics"),Yt=s(", which are limited to a given task, such as Machine Translation (often evaluated using metrics "),J=o("a"),zt=s("BLEU"),Vt=s(" or "),K=o("a"),Jt=s("ROUGE"),Kt=s(") or Named Entity Recognition (often evaluated with "),X=o("a"),Xt=s("seqeval"),Zt=s(")."),ea=f(),$=o("li"),He=o("em"),ta=s("Dataset-specific metrics"),aa=s(", which aim to measure model performance on specific benchmarks: for instance, the "),Z=o("a"),sa=s("GLUE benchmark"),ra=s(" has a dedicated "),ee=o("a"),oa=s("evaluation metric"),ia=s("."),dt=f(),je=o("p"),la=s("Let\u2019s look at each of these three cases:"),mt=f(),P=o("h3"),U=o("a"),We=o("span"),B(te.$$.fragment),na=f(),Ye=o("span"),ca=s("Generic metrics"),gt=f(),Se=o("p"),ha=s("Many of the metrics used in the Machine Learning community are quite generic and can be applied in a variety of tasks and datasets."),vt=f(),_=o("p"),fa=s("This is the case for metrics like "),ae=o("a"),pa=s("accuracy"),ua=s(" and "),se=o("a"),da=s("precision"),ma=s(", which can be used for evaluating labeled (supervised) datasets, as well as "),re=o("a"),ga=s("perplexity"),va=s(", which can be used for evaluating different kinds of (unsupervised) generative tasks."),_t=f(),N=o("p"),_a=s("To see the input structure of a given metric, you can look at its metric card. For example, in the case of "),oe=o("a"),ya=s("precision"),Ea=s(", the format is:"),yt=f(),B(ie.$$.fragment),Et=f(),j=o("h3"),C=o("a"),ze=o("span"),B(le.$$.fragment),wa=f(),Ve=o("span"),ka=s("Task-specific metrics"),wt=f(),d=o("p"),ba=s("Popular ML tasks like Machine Translation and Named Entity Recognition have specific metrics that can be used to compare models. For example, a series of different metrics have been proposed for text generation, ranging from "),ne=o("a"),$a=s("BLEU"),xa=s(" and its derivatives such as "),ce=o("a"),Aa=s("GoogleBLEU"),La=s(" and "),he=o("a"),Pa=s("GLEU"),ja=s(", but also "),fe=o("a"),Sa=s("ROUGE"),Ta=s(", "),pe=o("a"),qa=s("MAUVE"),Ga=s(", etc."),kt=f(),Te=o("p"),Ua=s("You can find the right metric for your task by:"),bt=f(),y=o("ul"),qe=o("li"),Ge=o("strong"),Na=s("Looking at the "),ue=o("a"),Ca=s("Task pages"),Ma=s(" to see what metrics can be used for evaluating models for a given task."),Da=f(),M=o("li"),Je=o("strong"),Oa=s("Checking out leaderboards"),Ra=s(" on sites like "),de=o("a"),Ia=s("Papers With Code"),Ba=s(" (you can search by task and by dataset)."),Fa=f(),x=o("li"),Ke=o("strong"),Qa=s("Reading the metric cards"),Ha=s(" for the relevant metrics and see which ones are a good fit for your use case. For example, see the "),me=o("a"),Wa=s("BLEU metric card"),Ya=s(" or "),ge=o("a"),za=s("SQuaD metric card"),Va=s("."),Ja=f(),Ue=o("li"),Xe=o("strong"),Ka=s("Looking at papers and blog posts"),Xa=s(" published on the topic and see what metrics they report. This can change over time, so try to pick papers from the last couple of years!"),$t=f(),S=o("h3"),D=o("a"),Ze=o("span"),B(ve.$$.fragment),Za=f(),et=o("span"),es=s("Dataset-specific metrics"),xt=f(),A=o("p"),ts=s("Some datasets have specific metrics associated with them \u2014 this is especially in the case of popular benchmarks like "),_e=o("a"),as=s("GLUE"),ss=s(" and "),ye=o("a"),rs=s("SQuAD"),os=s("."),At=f(),B(O.$$.fragment),Lt=f(),u=o("p"),is=s("If you are evaluating your model on a benchmark dataset like the ones mentioned above, you can use its dedicated evaluation metric. Make sure you respect the format that they require. For example, to evaluate your model on the "),Ee=o("a"),ls=s("SQuAD"),ns=s(" dataset, you need to feed the "),tt=o("code"),cs=s("question"),hs=s(" and "),at=o("code"),fs=s("context"),ps=s(" into your model and return the "),st=o("code"),us=s("prediction_text"),ds=s(", which should be compared with the "),rt=o("code"),ms=s("references"),gs=s(" (based on matching the "),ot=o("code"),vs=s("id"),_s=s(" of the question) :"),Pt=f(),B(we.$$.fragment),jt=f(),Ne=o("p"),ys=s("You can find examples of dataset structures by consulting the \u201CDataset Preview\u201D function or the dataset card for a given dataset, and you can see how to use its dedicated evaluation function based on the metric card."),this.h()},l(e){const c=jr('[data-svelte="svelte-1phssyn"]',document.head);m=i(c,"META",{name:!0,content:!0}),c.forEach(a),k=p(e),w=i(e,"H1",{class:!0});var ke=l(w);q=i(ke,"A",{id:!0,class:!0,href:!0});var $s=l(q);De=i($s,"SPAN",{});var xs=l(De);F(z.$$.fragment,xs),xs.forEach(a),$s.forEach(a),Ct=p(ke),Oe=i(ke,"SPAN",{});var As=l(Oe);Mt=r(As,"Choosing a metric for your task"),As.forEach(a),ke.forEach(a),ct=p(e),xe=i(e,"P",{});var Ls=l(xe);Re=i(Ls,"STRONG",{});var Ps=l(Re);Dt=r(Ps,"So you\u2019ve trained your model and want to see how well it\u2019s doing on a dataset of your choice. Where do you start?"),Ps.forEach(a),Ls.forEach(a),ht=p(e),Ae=i(e,"P",{});var js=l(Ae);Ot=r(js,"There is no \u201Cone size fits all\u201D approach to choosing an evaluation metric, but some good guidelines to keep in mind are:"),js.forEach(a),ft=p(e),L=i(e,"H2",{class:!0});var Tt=l(L);G=i(Tt,"A",{id:!0,class:!0,href:!0});var Ss=l(G);Ie=i(Ss,"SPAN",{});var Ts=l(Ie);F(V.$$.fragment,Ts),Ts.forEach(a),Ss.forEach(a),Rt=p(Tt),Be=i(Tt,"SPAN",{});var qs=l(Be);It=r(qs,"Categories of metrics"),qs.forEach(a),Tt.forEach(a),pt=p(e),Le=i(e,"P",{});var Gs=l(Le);Bt=r(Gs,"There are 3 high-level categories of metrics:"),Gs.forEach(a),ut=p(e),b=i(e,"OL",{});var Ce=l(b);Pe=i(Ce,"LI",{});var Es=l(Pe);Fe=i(Es,"EM",{});var Us=l(Fe);Ft=r(Us,"Generic metrics"),Us.forEach(a),Qt=r(Es,", which can be applied to a variety of situations and datasets, such as precision and accuracy."),Es.forEach(a),Ht=p(Ce),v=i(Ce,"LI",{});var T=l(v);Qe=i(T,"EM",{});var Ns=l(Qe);Wt=r(Ns,"Task-specific metrics"),Ns.forEach(a),Yt=r(T,", which are limited to a given task, such as Machine Translation (often evaluated using metrics "),J=i(T,"A",{href:!0,rel:!0});var Cs=l(J);zt=r(Cs,"BLEU"),Cs.forEach(a),Vt=r(T," or "),K=i(T,"A",{href:!0,rel:!0});var Ms=l(K);Jt=r(Ms,"ROUGE"),Ms.forEach(a),Kt=r(T,") or Named Entity Recognition (often evaluated with "),X=i(T,"A",{href:!0,rel:!0});var Ds=l(X);Xt=r(Ds,"seqeval"),Ds.forEach(a),Zt=r(T,")."),T.forEach(a),ea=p(Ce),$=i(Ce,"LI",{});var be=l($);He=i(be,"EM",{});var Os=l(He);ta=r(Os,"Dataset-specific metrics"),Os.forEach(a),aa=r(be,", which aim to measure model performance on specific benchmarks: for instance, the "),Z=i(be,"A",{href:!0,rel:!0});var Rs=l(Z);sa=r(Rs,"GLUE benchmark"),Rs.forEach(a),ra=r(be," has a dedicated "),ee=i(be,"A",{href:!0,rel:!0});var Is=l(ee);oa=r(Is,"evaluation metric"),Is.forEach(a),ia=r(be,"."),be.forEach(a),Ce.forEach(a),dt=p(e),je=i(e,"P",{});var Bs=l(je);la=r(Bs,"Let\u2019s look at each of these three cases:"),Bs.forEach(a),mt=p(e),P=i(e,"H3",{class:!0});var qt=l(P);U=i(qt,"A",{id:!0,class:!0,href:!0});var Fs=l(U);We=i(Fs,"SPAN",{});var Qs=l(We);F(te.$$.fragment,Qs),Qs.forEach(a),Fs.forEach(a),na=p(qt),Ye=i(qt,"SPAN",{});var Hs=l(Ye);ca=r(Hs,"Generic metrics"),Hs.forEach(a),qt.forEach(a),gt=p(e),Se=i(e,"P",{});var Ws=l(Se);ha=r(Ws,"Many of the metrics used in the Machine Learning community are quite generic and can be applied in a variety of tasks and datasets."),Ws.forEach(a),vt=p(e),_=i(e,"P",{});var R=l(_);fa=r(R,"This is the case for metrics like "),ae=i(R,"A",{href:!0,rel:!0});var Ys=l(ae);pa=r(Ys,"accuracy"),Ys.forEach(a),ua=r(R," and "),se=i(R,"A",{href:!0,rel:!0});var zs=l(se);da=r(zs,"precision"),zs.forEach(a),ma=r(R,", which can be used for evaluating labeled (supervised) datasets, as well as "),re=i(R,"A",{href:!0,rel:!0});var Vs=l(re);ga=r(Vs,"perplexity"),Vs.forEach(a),va=r(R,", which can be used for evaluating different kinds of (unsupervised) generative tasks."),R.forEach(a),_t=p(e),N=i(e,"P",{});var Gt=l(N);_a=r(Gt,"To see the input structure of a given metric, you can look at its metric card. For example, in the case of "),oe=i(Gt,"A",{href:!0,rel:!0});var Js=l(oe);ya=r(Js,"precision"),Js.forEach(a),Ea=r(Gt,", the format is:"),Gt.forEach(a),yt=p(e),F(ie.$$.fragment,e),Et=p(e),j=i(e,"H3",{class:!0});var Ut=l(j);C=i(Ut,"A",{id:!0,class:!0,href:!0});var Ks=l(C);ze=i(Ks,"SPAN",{});var Xs=l(ze);F(le.$$.fragment,Xs),Xs.forEach(a),Ks.forEach(a),wa=p(Ut),Ve=i(Ut,"SPAN",{});var Zs=l(Ve);ka=r(Zs,"Task-specific metrics"),Zs.forEach(a),Ut.forEach(a),wt=p(e),d=i(e,"P",{});var E=l(d);ba=r(E,"Popular ML tasks like Machine Translation and Named Entity Recognition have specific metrics that can be used to compare models. For example, a series of different metrics have been proposed for text generation, ranging from "),ne=i(E,"A",{href:!0,rel:!0});var er=l(ne);$a=r(er,"BLEU"),er.forEach(a),xa=r(E," and its derivatives such as "),ce=i(E,"A",{href:!0,rel:!0});var tr=l(ce);Aa=r(tr,"GoogleBLEU"),tr.forEach(a),La=r(E," and "),he=i(E,"A",{href:!0,rel:!0});var ar=l(he);Pa=r(ar,"GLEU"),ar.forEach(a),ja=r(E,", but also "),fe=i(E,"A",{href:!0,rel:!0});var sr=l(fe);Sa=r(sr,"ROUGE"),sr.forEach(a),Ta=r(E,", "),pe=i(E,"A",{href:!0,rel:!0});var rr=l(pe);qa=r(rr,"MAUVE"),rr.forEach(a),Ga=r(E,", etc."),E.forEach(a),kt=p(e),Te=i(e,"P",{});var or=l(Te);Ua=r(or,"You can find the right metric for your task by:"),or.forEach(a),bt=p(e),y=i(e,"UL",{});var I=l(y);qe=i(I,"LI",{});var ws=l(qe);Ge=i(ws,"STRONG",{});var ks=l(Ge);Na=r(ks,"Looking at the "),ue=i(ks,"A",{href:!0,rel:!0});var ir=l(ue);Ca=r(ir,"Task pages"),ir.forEach(a),ks.forEach(a),Ma=r(ws," to see what metrics can be used for evaluating models for a given task."),ws.forEach(a),Da=p(I),M=i(I,"LI",{});var it=l(M);Je=i(it,"STRONG",{});var lr=l(Je);Oa=r(lr,"Checking out leaderboards"),lr.forEach(a),Ra=r(it," on sites like "),de=i(it,"A",{href:!0,rel:!0});var nr=l(de);Ia=r(nr,"Papers With Code"),nr.forEach(a),Ba=r(it," (you can search by task and by dataset)."),it.forEach(a),Fa=p(I),x=i(I,"LI",{});var $e=l(x);Ke=i($e,"STRONG",{});var cr=l(Ke);Qa=r(cr,"Reading the metric cards"),cr.forEach(a),Ha=r($e," for the relevant metrics and see which ones are a good fit for your use case. For example, see the "),me=i($e,"A",{href:!0,rel:!0});var hr=l(me);Wa=r(hr,"BLEU metric card"),hr.forEach(a),Ya=r($e," or "),ge=i($e,"A",{href:!0,rel:!0});var fr=l(ge);za=r(fr,"SQuaD metric card"),fr.forEach(a),Va=r($e,"."),$e.forEach(a),Ja=p(I),Ue=i(I,"LI",{});var bs=l(Ue);Xe=i(bs,"STRONG",{});var pr=l(Xe);Ka=r(pr,"Looking at papers and blog posts"),pr.forEach(a),Xa=r(bs," published on the topic and see what metrics they report. This can change over time, so try to pick papers from the last couple of years!"),bs.forEach(a),I.forEach(a),$t=p(e),S=i(e,"H3",{class:!0});var Nt=l(S);D=i(Nt,"A",{id:!0,class:!0,href:!0});var ur=l(D);Ze=i(ur,"SPAN",{});var dr=l(Ze);F(ve.$$.fragment,dr),dr.forEach(a),ur.forEach(a),Za=p(Nt),et=i(Nt,"SPAN",{});var mr=l(et);es=r(mr,"Dataset-specific metrics"),mr.forEach(a),Nt.forEach(a),xt=p(e),A=i(e,"P",{});var Me=l(A);ts=r(Me,"Some datasets have specific metrics associated with them \u2014 this is especially in the case of popular benchmarks like "),_e=i(Me,"A",{href:!0,rel:!0});var gr=l(_e);as=r(gr,"GLUE"),gr.forEach(a),ss=r(Me," and "),ye=i(Me,"A",{href:!0,rel:!0});var vr=l(ye);rs=r(vr,"SQuAD"),vr.forEach(a),os=r(Me,"."),Me.forEach(a),At=p(e),F(O.$$.fragment,e),Lt=p(e),u=i(e,"P",{});var g=l(u);is=r(g,"If you are evaluating your model on a benchmark dataset like the ones mentioned above, you can use its dedicated evaluation metric. Make sure you respect the format that they require. For example, to evaluate your model on the "),Ee=i(g,"A",{href:!0,rel:!0});var _r=l(Ee);ls=r(_r,"SQuAD"),_r.forEach(a),ns=r(g," dataset, you need to feed the "),tt=i(g,"CODE",{});var yr=l(tt);cs=r(yr,"question"),yr.forEach(a),hs=r(g," and "),at=i(g,"CODE",{});var Er=l(at);fs=r(Er,"context"),Er.forEach(a),ps=r(g," into your model and return the "),st=i(g,"CODE",{});var wr=l(st);us=r(wr,"prediction_text"),wr.forEach(a),ds=r(g,", which should be compared with the "),rt=i(g,"CODE",{});var kr=l(rt);ms=r(kr,"references"),kr.forEach(a),gs=r(g," (based on matching the "),ot=i(g,"CODE",{});var br=l(ot);vs=r(br,"id"),br.forEach(a),_s=r(g," of the question) :"),g.forEach(a),Pt=p(e),F(we.$$.fragment,e),jt=p(e),Ne=i(e,"P",{});var $r=l(Ne);ys=r($r,"You can find examples of dataset structures by consulting the \u201CDataset Preview\u201D function or the dataset card for a given dataset, and you can see how to use its dedicated evaluation function based on the metric card."),$r.forEach(a),this.h()},h(){n(m,"name","hf:doc:metadata"),n(m,"content",JSON.stringify(Ur)),n(q,"id","choosing-a-metric-for-your-task"),n(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(q,"href","#choosing-a-metric-for-your-task"),n(w,"class","relative group"),n(G,"id","categories-of-metrics"),n(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(G,"href","#categories-of-metrics"),n(L,"class","relative group"),n(J,"href","https://huggingface.co/metrics/bleu"),n(J,"rel","nofollow"),n(K,"href","https://huggingface.co/metrics/rouge"),n(K,"rel","nofollow"),n(X,"href","https://huggingface.co/metrics/seqeval"),n(X,"rel","nofollow"),n(Z,"href","https://huggingface.co/datasets/glue"),n(Z,"rel","nofollow"),n(ee,"href","https://huggingface.co/metrics/glue"),n(ee,"rel","nofollow"),n(U,"id","generic-metrics"),n(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(U,"href","#generic-metrics"),n(P,"class","relative group"),n(ae,"href","https://huggingface.co/metrics/accuracy"),n(ae,"rel","nofollow"),n(se,"href","https://huggingface.co/metrics/precision"),n(se,"rel","nofollow"),n(re,"href","https://huggingface.co/metrics/perplexity"),n(re,"rel","nofollow"),n(oe,"href","https://huggingface.co/metrics/precision"),n(oe,"rel","nofollow"),n(C,"id","taskspecific-metrics"),n(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(C,"href","#taskspecific-metrics"),n(j,"class","relative group"),n(ne,"href","https://huggingface.co/metrics/bleu"),n(ne,"rel","nofollow"),n(ce,"href","https://huggingface.co/metrics/google_bleu"),n(ce,"rel","nofollow"),n(he,"href","https://huggingface.co/metrics/gleu"),n(he,"rel","nofollow"),n(fe,"href","https://huggingface.co/metrics/rouge"),n(fe,"rel","nofollow"),n(pe,"href","https://huggingface.co/metrics/mauve"),n(pe,"rel","nofollow"),n(ue,"href","https://huggingface.co/tasks"),n(ue,"rel","nofollow"),n(de,"href","https://paperswithcode.com/"),n(de,"rel","nofollow"),n(me,"href","https://github.com/huggingface/evaluate/tree/main/metrics/bleu"),n(me,"rel","nofollow"),n(ge,"href","https://github.com/huggingface/evaluate/tree/main/metrics/squad"),n(ge,"rel","nofollow"),n(D,"id","datasetspecific-metrics"),n(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(D,"href","#datasetspecific-metrics"),n(S,"class","relative group"),n(_e,"href","https://huggingface.co/metrics/glue"),n(_e,"rel","nofollow"),n(ye,"href","https://huggingface.co/metrics/squad"),n(ye,"rel","nofollow"),n(Ee,"href","https://huggingface.co/datasets/squad"),n(Ee,"rel","nofollow")},m(e,c){t(document.head,m),h(e,k,c),h(e,w,c),t(w,q),t(q,De),Q(z,De,null),t(w,Ct),t(w,Oe),t(Oe,Mt),h(e,ct,c),h(e,xe,c),t(xe,Re),t(Re,Dt),h(e,ht,c),h(e,Ae,c),t(Ae,Ot),h(e,ft,c),h(e,L,c),t(L,G),t(G,Ie),Q(V,Ie,null),t(L,Rt),t(L,Be),t(Be,It),h(e,pt,c),h(e,Le,c),t(Le,Bt),h(e,ut,c),h(e,b,c),t(b,Pe),t(Pe,Fe),t(Fe,Ft),t(Pe,Qt),t(b,Ht),t(b,v),t(v,Qe),t(Qe,Wt),t(v,Yt),t(v,J),t(J,zt),t(v,Vt),t(v,K),t(K,Jt),t(v,Kt),t(v,X),t(X,Xt),t(v,Zt),t(b,ea),t(b,$),t($,He),t(He,ta),t($,aa),t($,Z),t(Z,sa),t($,ra),t($,ee),t(ee,oa),t($,ia),h(e,dt,c),h(e,je,c),t(je,la),h(e,mt,c),h(e,P,c),t(P,U),t(U,We),Q(te,We,null),t(P,na),t(P,Ye),t(Ye,ca),h(e,gt,c),h(e,Se,c),t(Se,ha),h(e,vt,c),h(e,_,c),t(_,fa),t(_,ae),t(ae,pa),t(_,ua),t(_,se),t(se,da),t(_,ma),t(_,re),t(re,ga),t(_,va),h(e,_t,c),h(e,N,c),t(N,_a),t(N,oe),t(oe,ya),t(N,Ea),h(e,yt,c),Q(ie,e,c),h(e,Et,c),h(e,j,c),t(j,C),t(C,ze),Q(le,ze,null),t(j,wa),t(j,Ve),t(Ve,ka),h(e,wt,c),h(e,d,c),t(d,ba),t(d,ne),t(ne,$a),t(d,xa),t(d,ce),t(ce,Aa),t(d,La),t(d,he),t(he,Pa),t(d,ja),t(d,fe),t(fe,Sa),t(d,Ta),t(d,pe),t(pe,qa),t(d,Ga),h(e,kt,c),h(e,Te,c),t(Te,Ua),h(e,bt,c),h(e,y,c),t(y,qe),t(qe,Ge),t(Ge,Na),t(Ge,ue),t(ue,Ca),t(qe,Ma),t(y,Da),t(y,M),t(M,Je),t(Je,Oa),t(M,Ra),t(M,de),t(de,Ia),t(M,Ba),t(y,Fa),t(y,x),t(x,Ke),t(Ke,Qa),t(x,Ha),t(x,me),t(me,Wa),t(x,Ya),t(x,ge),t(ge,za),t(x,Va),t(y,Ja),t(y,Ue),t(Ue,Xe),t(Xe,Ka),t(Ue,Xa),h(e,$t,c),h(e,S,c),t(S,D),t(D,Ze),Q(ve,Ze,null),t(S,Za),t(S,et),t(et,es),h(e,xt,c),h(e,A,c),t(A,ts),t(A,_e),t(_e,as),t(A,ss),t(A,ye),t(ye,rs),t(A,os),h(e,At,c),Q(O,e,c),h(e,Lt,c),h(e,u,c),t(u,is),t(u,Ee),t(Ee,ls),t(u,ns),t(u,tt),t(tt,cs),t(u,hs),t(u,at),t(at,fs),t(u,ps),t(u,st),t(st,us),t(u,ds),t(u,rt),t(rt,ms),t(u,gs),t(u,ot),t(ot,vs),t(u,_s),h(e,Pt,c),Q(we,e,c),h(e,jt,c),h(e,Ne,c),t(Ne,ys),St=!0},p(e,[c]){const ke={};c&2&&(ke.$$scope={dirty:c,ctx:e}),O.$set(ke)},i(e){St||(H(z.$$.fragment,e),H(V.$$.fragment,e),H(te.$$.fragment,e),H(ie.$$.fragment,e),H(le.$$.fragment,e),H(ve.$$.fragment,e),H(O.$$.fragment,e),H(we.$$.fragment,e),St=!0)},o(e){W(z.$$.fragment,e),W(V.$$.fragment,e),W(te.$$.fragment,e),W(ie.$$.fragment,e),W(le.$$.fragment,e),W(ve.$$.fragment,e),W(O.$$.fragment,e),W(we.$$.fragment,e),St=!1},d(e){a(m),e&&a(k),e&&a(w),Y(z),e&&a(ct),e&&a(xe),e&&a(ht),e&&a(Ae),e&&a(ft),e&&a(L),Y(V),e&&a(pt),e&&a(Le),e&&a(ut),e&&a(b),e&&a(dt),e&&a(je),e&&a(mt),e&&a(P),Y(te),e&&a(gt),e&&a(Se),e&&a(vt),e&&a(_),e&&a(_t),e&&a(N),e&&a(yt),Y(ie,e),e&&a(Et),e&&a(j),Y(le),e&&a(wt),e&&a(d),e&&a(kt),e&&a(Te),e&&a(bt),e&&a(y),e&&a($t),e&&a(S),Y(ve),e&&a(xt),e&&a(A),e&&a(At),Y(O,e),e&&a(Lt),e&&a(u),e&&a(Pt),Y(we,e),e&&a(jt),e&&a(Ne)}}}const Ur={local:"choosing-a-metric-for-your-task",sections:[{local:"categories-of-metrics",sections:[{local:"generic-metrics",title:"Generic metrics"},{local:"taskspecific-metrics",title:"Task-specific metrics"},{local:"datasetspecific-metrics",title:"Dataset-specific metrics"}],title:"Categories of metrics"}],title:"Choosing a metric for your task"};function Nr(nt){return Sr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Rr extends Ar{constructor(m){super();Lr(this,m,Nr,Gr,Pr,{})}}export{Rr as default,Ur as metadata};
