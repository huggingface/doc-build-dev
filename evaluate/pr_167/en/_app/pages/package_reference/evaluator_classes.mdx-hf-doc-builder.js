import{S as Qn,i as Rn,s as Vn,e as o,k as u,w as b,t as c,M as Un,c as n,d as a,m,a as r,x as w,h as p,b as h,G as e,g as $,y,q,o as E,B as k,v as Bn,L as Ce}from"../../chunks/vendor-hf-doc-builder.js";import{T as Ho}from"../../chunks/Tip-hf-doc-builder.js";import{D as I}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Pe}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as lt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Te}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Hn(x){let l,v,i,d,_;return d=new Pe({props:{code:`from evaluate import evaluator
# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){l=o("p"),v=c("Examples:"),i=u(),b(d.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);v=p(f,"Examples:"),f.forEach(a),i=m(s),w(d.$$.fragment,s)},m(s,f){$(s,l,f),e(l,v),$(s,i,f),y(d,s,f),_=!0},p:Ce,i(s){_||(q(d.$$.fragment,s),_=!0)},o(s){E(d.$$.fragment,s),_=!1},d(s){s&&a(l),s&&a(i),k(d,s)}}}function Yn(x){let l,v,i,d,_;return d=new Pe({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
task_evaluator = evaluator("image-classification")
data = load_dataset("beans", split="test[:2]")
results = task_evaluator.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    metric="accuracy",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;image-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;beans&quot;</span>, split=<span class="hljs-string">&quot;test[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;nateraw/vit-base-beans&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&#x27;angular_leaf_spot&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;bean_rust&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>: <span class="hljs-number">2</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){l=o("p"),v=c("Examples:"),i=u(),b(d.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);v=p(f,"Examples:"),f.forEach(a),i=m(s),w(d.$$.fragment,s)},m(s,f){$(s,l,f),e(l,v),$(s,i,f),y(d,s,f),_=!0},p:Ce,i(s){_||(q(d.$$.fragment,s),_=!0)},o(s){E(d.$$.fragment,s),_=!1},d(s){s&&a(l),s&&a(i),k(d,s)}}}function Gn(x){let l,v,i,d,_;return d=new Pe({props:{code:`from evaluate import evaluator
from datasets import load_dataset
e = evaluator("question-answering")
data = load_dataset("squad", split="validation[:2]")
results = e.compute(
    model_or_pipeline="sshleifer/tiny-distilbert-base-cased-distilled-squad",
    data=data,
    metric="squad",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;sshleifer/tiny-distilbert-base-cased-distilled-squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){l=o("p"),v=c("Examples:"),i=u(),b(d.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);v=p(f,"Examples:"),f.forEach(a),i=m(s),w(d.$$.fragment,s)},m(s,f){$(s,l,f),e(l,v),$(s,i,f),y(d,s,f),_=!0},p:Ce,i(s){_||(q(d.$$.fragment,s),_=!0)},o(s){E(d.$$.fragment,s),_=!1},d(s){s&&a(l),s&&a(i),k(d,s)}}}function Wn(x){let l,v,i,d,_;return{c(){l=o("p"),v=c(`Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. If using transformers pipeline
with models trained on this type of data, make sure to pass `),i=o("code"),d=c("handle_impossible_answer=True"),_=c(" as an argument to the pipeline.")},l(s){l=n(s,"P",{});var f=r(l);v=p(f,`Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. If using transformers pipeline
with models trained on this type of data, make sure to pass `),i=n(f,"CODE",{});var F=r(i);d=p(F,"handle_impossible_answer=True"),F.forEach(a),_=p(f," as an argument to the pipeline."),f.forEach(a)},m(s,f){$(s,l,f),e(l,v),e(l,i),e(i,d),e(l,_)},d(s){s&&a(l)}}}function Jn(x){let l,v;return l=new Pe({props:{code:`from evaluate import evaluator
from datasets import load_dataset
from transformers import pipeline
task_evaluator = evaluator("question-answering")
data = load_dataset("squad_v2", split="validation[:2]")
pipe = pipeline(
    task="question-answering",
    model="sshleifer/mrm8488/bert-tiny-finetuned-squadv2",
    handle_impossible_answer=True
)
results = task_evaluator.compute(
    model_or_pipeline=pipe,
    data=data,
    metric="squad_v2",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad_v2&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(
<span class="hljs-meta">&gt;&gt;&gt; </span>    task=<span class="hljs-string">&quot;question-answering&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    model=<span class="hljs-string">&quot;sshleifer/mrm8488/bert-tiny-finetuned-squadv2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    handle_impossible_answer=<span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=pipe,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad_v2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){b(l.$$.fragment)},l(i){w(l.$$.fragment,i)},m(i,d){y(l,i,d),v=!0},p:Ce,i(i){v||(q(l.$$.fragment,i),v=!0)},o(i){E(l.$$.fragment,i),v=!1},d(i){k(l,i)}}}function Kn(x){let l,v,i,d,_;return d=new Pe({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
task_evaluator = evaluator("text-classification")
data = load_dataset("imdb", split="test[:2]")
results = task_evaluator.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>, split=<span class="hljs-string">&quot;test[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){l=o("p"),v=c("Examples:"),i=u(),b(d.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);v=p(f,"Examples:"),f.forEach(a),i=m(s),w(d.$$.fragment,s)},m(s,f){$(s,l,f),e(l,v),$(s,i,f),y(d,s,f),_=!0},p:Ce,i(s){_||(q(d.$$.fragment,s),_=!0)},o(s){E(d.$$.fragment,s),_=!1},d(s){s&&a(l),s&&a(i),k(d,s)}}}function Xn(x){let l,v,i,d,_;return d=new Pe({props:{code:`from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("token-classification")
data = load_dataset("conll2003", split="validation[:2]")
results = task_evaluator.compute(
    model_or_pipeline="elastic/distilbert-base-uncased-finetuned-conll03-english",
    data=data,
    metric="seqeval",
    join_by=" ",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;token-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;conll2003&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;elastic/distilbert-base-uncased-finetuned-conll03-english&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;seqeval&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    join_by=<span class="hljs-string">&quot; &quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){l=o("p"),v=c("Examples:"),i=u(),b(d.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);v=p(f,"Examples:"),f.forEach(a),i=m(s),w(d.$$.fragment,s)},m(s,f){$(s,l,f),e(l,v),$(s,i,f),y(d,s,f),_=!0},p:Ce,i(s){_||(q(d.$$.fragment,s),_=!0)},o(s){E(d.$$.fragment,s),_=!1},d(s){s&&a(l),s&&a(i),k(d,s)}}}function Zn(x){let l,v,i,d,_;return d=new Pe({props:{code:`dataset = Dataset.from_dict(
    mapping={
        "tokens": [["New", "York", "is", "a", "city", "and", "Felix", "a", "person", "."]],
        "ner_tags": [[1, 2, 0, 0, 0, 0, 3, 0, 0, 0]],
    },
    features=Features({
        "tokens": Sequence(feature=Value(dtype="string")),
        "ner_tags": Sequence(feature=ClassLabel(names=["O", "B-LOC", "I-LOC", "B-PER", "I-PER"])),
        }),
)`,highlighted:`dataset = Dataset.from_dict(
    mapping={
        <span class="hljs-string">&quot;tokens&quot;</span>: [[<span class="hljs-string">&quot;New&quot;</span>, <span class="hljs-string">&quot;York&quot;</span>, <span class="hljs-string">&quot;is&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;city&quot;</span>, <span class="hljs-string">&quot;and&quot;</span>, <span class="hljs-string">&quot;Felix&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;person&quot;</span>, <span class="hljs-string">&quot;.&quot;</span>]],
        <span class="hljs-string">&quot;ner_tags&quot;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]],
    },
    features=Features({
        <span class="hljs-string">&quot;tokens&quot;</span>: <span class="hljs-type">Sequence</span>(feature=Value(dtype=<span class="hljs-string">&quot;string&quot;</span>)),
        <span class="hljs-string">&quot;ner_tags&quot;</span>: <span class="hljs-type">Sequence</span>(feature=ClassLabel(names=[<span class="hljs-string">&quot;O&quot;</span>, <span class="hljs-string">&quot;B-LOC&quot;</span>, <span class="hljs-string">&quot;I-LOC&quot;</span>, <span class="hljs-string">&quot;B-PER&quot;</span>, <span class="hljs-string">&quot;I-PER&quot;</span>])),
        }),
)`}}),{c(){l=o("p"),v=c("For example, the following dataset format is accepted by the evaluator:"),i=u(),b(d.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);v=p(f,"For example, the following dataset format is accepted by the evaluator:"),f.forEach(a),i=m(s),w(d.$$.fragment,s)},m(s,f){$(s,l,f),e(l,v),$(s,i,f),y(d,s,f),_=!0},p:Ce,i(s){_||(q(d.$$.fragment,s),_=!0)},o(s){E(d.$$.fragment,s),_=!1},d(s){s&&a(l),s&&a(i),k(d,s)}}}function er(x){let l,v;return l=new Te({props:{anchor:"evaluate.TokenClassificationEvaluator.compute.example-2",$$slots:{default:[Zn]},$$scope:{ctx:x}}}),{c(){b(l.$$.fragment)},l(i){w(l.$$.fragment,i)},m(i,d){y(l,i,d),v=!0},p(i,d){const _={};d&2&&(_.$$scope={dirty:d,ctx:i}),l.$set(_)},i(i){v||(q(l.$$.fragment,i),v=!0)},o(i){E(l.$$.fragment,i),v=!1},d(i){k(l,i)}}}function tr(x){let l,v,i,d,_,s,f,F;return f=new Pe({props:{code:`dataset = Dataset.from_dict(
    mapping={
        "tokens": [["New York is a city and Felix a person."]],
        "starts": [[0, 23]],
        "ends": [[7, 27]],
        "ner_tags": [["LOC", "PER"]],
    },
    features=Features({
        "tokens": Value(dtype="string"),
        "starts": Sequence(feature=Value(dtype="int32")),
        "ends": Sequence(feature=Value(dtype="int32")),
        "ner_tags": Sequence(feature=Value(dtype="string")),
    }),
)`,highlighted:`dataset = Dataset.from_dict(
    mapping={
        <span class="hljs-string">&quot;tokens&quot;</span>: [[<span class="hljs-string">&quot;New York is a city and Felix a person.&quot;</span>]],
        <span class="hljs-string">&quot;starts&quot;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">23</span>]],
        <span class="hljs-string">&quot;ends&quot;</span>: [[<span class="hljs-number">7</span>, <span class="hljs-number">27</span>]],
        <span class="hljs-string">&quot;ner_tags&quot;</span>: [[<span class="hljs-string">&quot;LOC&quot;</span>, <span class="hljs-string">&quot;PER&quot;</span>]],
    },
    features=Features({
        <span class="hljs-string">&quot;tokens&quot;</span>: Value(dtype=<span class="hljs-string">&quot;string&quot;</span>),
        <span class="hljs-string">&quot;starts&quot;</span>: <span class="hljs-type">Sequence</span>(feature=Value(dtype=<span class="hljs-string">&quot;int32&quot;</span>)),
        <span class="hljs-string">&quot;ends&quot;</span>: <span class="hljs-type">Sequence</span>(feature=Value(dtype=<span class="hljs-string">&quot;int32&quot;</span>)),
        <span class="hljs-string">&quot;ner_tags&quot;</span>: <span class="hljs-type">Sequence</span>(feature=Value(dtype=<span class="hljs-string">&quot;string&quot;</span>)),
    }),
)`}}),{c(){l=o("p"),v=c("For example, the following dataset format is "),i=o("strong"),d=c("not"),_=c(" accepted by the evaluator:"),s=u(),b(f.$$.fragment)},l(j){l=n(j,"P",{});var z=r(l);v=p(z,"For example, the following dataset format is "),i=n(z,"STRONG",{});var J=r(i);d=p(J,"not"),J.forEach(a),_=p(z," accepted by the evaluator:"),z.forEach(a),s=m(j),w(f.$$.fragment,j)},m(j,z){$(j,l,z),e(l,v),e(l,i),e(i,d),e(l,_),$(j,s,z),y(f,j,z),F=!0},p:Ce,i(j){F||(q(f.$$.fragment,j),F=!0)},o(j){E(f.$$.fragment,j),F=!1},d(j){j&&a(l),j&&a(s),k(f,j)}}}function ar(x){let l,v;return l=new Te({props:{anchor:"evaluate.TokenClassificationEvaluator.compute.example-3",$$slots:{default:[tr]},$$scope:{ctx:x}}}),{c(){b(l.$$.fragment)},l(i){w(l.$$.fragment,i)},m(i,d){y(l,i,d),v=!0},p(i,d){const _={};d&2&&(_.$$scope={dirty:d,ctx:i}),l.$set(_)},i(i){v||(q(l.$$.fragment,i),v=!0)},o(i){E(l.$$.fragment,i),v=!1},d(i){k(l,i)}}}function sr(x){let l,v,i,d,_,s,f,F,j,z,J,Ma,pa,K,ne,yt,De,Qa,qt,Ra,da,it,Va,ua,L,Ie,Ua,M,Ba,ct,Ha,Ya,Et,Ga,Wa,kt,Ja,Ka,Xa,re,ma,pt,Za,fa,T,Ae,es,xt,ts,as,le,Ne,ss,jt,os,ns,ie,Fe,rs,ze,ls,Tt,is,cs,ps,ce,Se,ds,Ct,us,ms,pe,Oe,fs,Pt,hs,gs,de,Le,vs,Dt,_s,ha,X,ue,It,Me,$s,At,bs,ga,Z,me,Nt,Qe,ws,Ft,ys,va,Q,Re,qs,R,Es,dt,ks,xs,zt,js,Ts,St,Cs,Ps,Ds,U,Ve,Is,Ot,As,Ns,fe,_a,ee,he,Lt,Ue,Fs,Mt,zs,$a,P,Be,Ss,He,Os,ge,Qt,Ls,Ms,Qs,Rs,te,Vs,ut,Us,Bs,Rt,Hs,Ys,Gs,Ye,Ws,Ge,Vt,Js,Ks,Xs,A,We,Zs,Ut,eo,to,ve,ao,_e,so,$e,ba,ae,be,Bt,Je,oo,Ht,no,wa,V,Ke,ro,S,lo,mt,io,co,Yt,po,uo,Gt,mo,fo,Wt,ho,go,vo,B,Xe,_o,Jt,$o,bo,we,ya,se,ye,Kt,Ze,wo,Xt,yo,qa,D,et,qo,Zt,Eo,ko,oe,xo,ft,jo,To,ea,Co,Po,Do,tt,Io,ta,Ao,No,Fo,C,at,zo,aa,So,Oo,st,Lo,ot,Mo,Qo,Ro,qe,Vo,Ee,Uo,ke,Ea;return s=new lt({}),De=new lt({}),Ie=new I({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:<ul>
<li><code>&quot;image-classification&quot;</code>: will return a <a href="/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator">ImageClassificationEvaluator</a>.</li>
<li><code>&quot;question-answering&quot;</code>: will return a <a href="/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.QuestionAnsweringEvaluator">QuestionAnsweringEvaluator</a>.</li>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
<li><code>&quot;token-classification&quot;</code>: will return a <a href="/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.TokenClassificationEvaluator">TokenClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/__init__.py#L85",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),re=new Te({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[Hn]},$$scope:{ctx:x}}}),Ae=new I({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L50"}}),Ne=new I({props:{name:"compute_metric",anchor:"evaluate.Evaluator.compute_metric",parameters:[{name:"metric",val:": EvaluationModule"},{name:"metric_inputs",val:": typing.Dict"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L281"}}),Fe=new I({props:{name:"predictions_processor",anchor:"evaluate.Evaluator.predictions_processor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L121"}}),Se=new I({props:{name:"prepare_data",anchor:"evaluate.Evaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"input_column",val:": str"},{name:"label_column",val:": str"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_data.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.Evaluator.prepare_data.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.prepare_data.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L176",returnDescription:`
<p>metric inputs.
<code>list</code>:  pipeline inputs.</p>
`,returnType:`
<p><code>dict</code></p>
`}}),Oe=new I({props:{name:"prepare_metric",anchor:"evaluate.Evaluator.prepare_metric",parameters:[{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule]"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_metric.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L251",returnDescription:`
<p>The loaded metric.</p>
`}}),Le=new I({props:{name:"prepare_pipeline",anchor:"evaluate.Evaluator.prepare_pipeline",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"tokenizer",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"},{name:"feature_extractor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_pipeline.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.prepare_pipeline.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.Evaluator.prepare_pipeline.preprocessor",description:`<strong>preprocessor</strong> (<code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"preprocessor"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L208",returnDescription:`
<p>The initialized pipeline.</p>
`}}),Me=new lt({}),Qe=new lt({}),Re=new I({props:{name:"class evaluate.ImageClassificationEvaluator",anchor:"evaluate.ImageClassificationEvaluator",parameters:[{name:"task",val:" = 'image-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/image_classification.py#L20"}}),Ve=new I({props:{name:"compute",anchor:"evaluate.ImageClassificationEvaluator.compute",parameters:[{name:"input_column",val:": str = 'image'"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"evaluate.ImageClassificationEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>image-classification</code>. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.ImageClassificationEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.ImageClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"},{anchor:"evaluate.ImageClassificationEvaluator.compute.feature_extractor",description:`<strong>feature_extractor</strong> (<code>str</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default feature extractor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"feature_extractor"},{anchor:"evaluate.ImageClassificationEvaluator.compute.strategy",description:`<strong>strategy</strong> (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;) &#x2014;
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"strategy"},{anchor:"evaluate.ImageClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.ImageClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.ImageClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.ImageClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;image&quot;</code>) &#x2014;
the name of the column containing the images as PIL ImageFile in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/image_classification.py#L37",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),fe=new Te({props:{anchor:"evaluate.ImageClassificationEvaluator.compute.example",$$slots:{default:[Yn]},$$scope:{ctx:x}}}),Ue=new lt({}),Be=new I({props:{name:"class evaluate.QuestionAnsweringEvaluator",anchor:"evaluate.QuestionAnsweringEvaluator",parameters:[{name:"task",val:" = 'question-answering'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/question_answering.py#L38"}}),We=new I({props:{name:"compute",anchor:"evaluate.QuestionAnsweringEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"question_column",val:": str = 'question'"},{name:"context_column",val:": str = 'context'"},{name:"id_column",val:": str = 'id'"},{name:"label_column",val:": str = 'answers'"}],parametersDescription:[{anchor:"evaluate.QuestionAnsweringEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>question-answering</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.tokenizer",description:`<strong>tokenizer</strong> (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"tokenizer"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.strategy",description:`<strong>strategy</strong> (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;) &#x2014;
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"strategy"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.question_column",description:`<strong>question_column</strong> (<code>str</code>, defaults to <code>&quot;question&quot;</code>) &#x2014;
the name of the column containing the question in the dataset specified by <code>data</code>.`,name:"question_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.context_column",description:`<strong>context_column</strong> (<code>str</code>, defaults to <code>&quot;context&quot;</code>) &#x2014;
the name of the column containing the context in the dataset specified by <code>data</code>.`,name:"context_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.id_column",description:`<strong>id_column</strong> (<code>str</code>, defaults to <code>&quot;id&quot;</code>) &#x2014;
the name of the column cointaing the identification field of the question and answer pair in the
dataset specified by <code>data</code>.`,name:"id_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;answers&quot;</code>) &#x2014;
the name of the column containing the answers in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/question_answering.py#L115",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ve=new Te({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example",$$slots:{default:[Gn]},$$scope:{ctx:x}}}),_e=new Ho({props:{$$slots:{default:[Wn]},$$scope:{ctx:x}}}),$e=new Te({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example-2",$$slots:{default:[Jn]},$$scope:{ctx:x}}}),Je=new lt({}),Ke=new I({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/text_classification.py#L20"}}),Xe=new I({props:{name:"compute",anchor:"evaluate.TextClassificationEvaluator.compute",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"evaluate.TextClassificationEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.TextClassificationEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.TextClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"},{anchor:"evaluate.TextClassificationEvaluator.compute.tokenizer",description:`<strong>tokenizer</strong> (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"tokenizer"},{anchor:"evaluate.TextClassificationEvaluator.compute.strategy",description:`<strong>strategy</strong> (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;) &#x2014;
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"strategy"},{anchor:"evaluate.TextClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TextClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TextClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TextClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/text_classification.py#L41",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),we=new Te({props:{anchor:"evaluate.TextClassificationEvaluator.compute.example",$$slots:{default:[Kn]},$$scope:{ctx:x}}}),Ze=new lt({}),et=new I({props:{name:"class evaluate.TokenClassificationEvaluator",anchor:"evaluate.TokenClassificationEvaluator",parameters:[{name:"task",val:" = 'token-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/token_classification.py#L23"}}),at=new I({props:{name:"compute",anchor:"evaluate.TokenClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, ForwardRef('EvaluationModule')] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'tokens'"},{name:"label_column",val:": str = 'ner_tags'"},{name:"join_by",val:": typing.Optional[str] = ' '"}],parametersDescription:[{anchor:"evaluate.TokenClassificationEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>token-classification</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.TokenClassificationEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.TokenClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"},{anchor:"evaluate.TokenClassificationEvaluator.compute.tokenizer",description:`<strong>tokenizer</strong> (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"tokenizer"},{anchor:"evaluate.TokenClassificationEvaluator.compute.strategy",description:`<strong>strategy</strong> (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;) &#x2014;
specifies the evaluation strategy. Possible values are:</p>
<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"strategy"},{anchor:"evaluate.TokenClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TokenClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TokenClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TokenClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;tokens&quot;</code>) &#x2014;
the name of the column containing the tokens feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TokenClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TokenClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"},{anchor:"evaluate.TokenClassificationEvaluator.compute.join_by",description:`<strong>join_by</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot; &quot;</code>) &#x2014;
This evaluator supports dataset whose input column is a list of words. This parameter specifies how to join
words to generate a string input. This is especially useful for languages that do not separate words by a space.`,name:"join_by"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/token_classification.py#L144",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),qe=new Te({props:{anchor:"evaluate.TokenClassificationEvaluator.compute.example",$$slots:{default:[Xn]},$$scope:{ctx:x}}}),Ee=new Ho({props:{$$slots:{default:[er]},$$scope:{ctx:x}}}),ke=new Ho({props:{warning:!0,$$slots:{default:[ar]},$$scope:{ctx:x}}}),{c(){l=o("meta"),v=u(),i=o("h1"),d=o("a"),_=o("span"),b(s.$$.fragment),f=u(),F=o("span"),j=c("Evaluator"),z=u(),J=o("p"),Ma=c("The evaluator classes for automatic evaluation."),pa=u(),K=o("h2"),ne=o("a"),yt=o("span"),b(De.$$.fragment),Qa=u(),qt=o("span"),Ra=c("Evaluator classes"),da=u(),it=o("p"),Va=c("The main entry point for using the evaluator:"),ua=u(),L=o("div"),b(Ie.$$.fragment),Ua=u(),M=o("p"),Ba=c("Utility factory method to build an "),ct=o("a"),Ha=c("Evaluator"),Ya=c(`.
Evaluators encapsulate a task and a default metric name. They leverage `),Et=o("code"),Ga=c("pipeline"),Wa=c(" functionalify from "),kt=o("code"),Ja=c("transformers"),Ka=c(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),Xa=u(),b(re.$$.fragment),ma=u(),pt=o("p"),Za=c("The base class for all evaluator classes:"),fa=u(),T=o("div"),b(Ae.$$.fragment),es=u(),xt=o("p"),ts=c(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),as=u(),le=o("div"),b(Ne.$$.fragment),ss=u(),jt=o("p"),os=c("Compute and return metrics."),ns=u(),ie=o("div"),b(Fe.$$.fragment),rs=u(),ze=o("p"),ls=c("A core method of the "),Tt=o("code"),is=c("Evaluator"),cs=c(" class, which processes the pipeline outputs for compatibility with the metric."),ps=u(),ce=o("div"),b(Se.$$.fragment),ds=u(),Ct=o("p"),us=c("Prepare data."),ms=u(),pe=o("div"),b(Oe.$$.fragment),fs=u(),Pt=o("p"),hs=c("Prepare metric."),gs=u(),de=o("div"),b(Le.$$.fragment),vs=u(),Dt=o("p"),_s=c("Prepare pipeline."),ha=u(),X=o("h2"),ue=o("a"),It=o("span"),b(Me.$$.fragment),$s=u(),At=o("span"),bs=c("The task specific evaluators"),ga=u(),Z=o("h3"),me=o("a"),Nt=o("span"),b(Qe.$$.fragment),ws=u(),Ft=o("span"),ys=c("ImageClassificationEvaluator"),va=u(),Q=o("div"),b(Re.$$.fragment),qs=u(),R=o("p"),Es=c(`Image classification evaluator.
This image classification evaluator can currently be loaded from `),dt=o("a"),ks=c("evaluator()"),xs=c(` using the default task name
`),zt=o("code"),js=c("image-classification"),Ts=c(`.
Methods in this class assume a data format compatible with the `),St=o("code"),Cs=c("ImageClassificationPipeline"),Ps=c("."),Ds=u(),U=o("div"),b(Ve.$$.fragment),Is=u(),Ot=o("p"),As=c("Compute the metric for a given pipeline and dataset combination."),Ns=u(),b(fe.$$.fragment),_a=u(),ee=o("h3"),he=o("a"),Lt=o("span"),b(Ue.$$.fragment),Fs=u(),Mt=o("span"),zs=c("QuestionAnsweringEvaluator"),$a=u(),P=o("div"),b(Be.$$.fragment),Ss=u(),He=o("p"),Os=c(`Question answering evaluator. This evaluator handles
`),ge=o("a"),Qt=o("strong"),Ls=c("extractive"),Ms=c(" question answering"),Qs=c(`,
where the answer to the question is extracted from a context.`),Rs=u(),te=o("p"),Vs=c("This question answering evaluator can currently be loaded from "),ut=o("a"),Us=c("evaluator()"),Bs=c(` using the default task name
`),Rt=o("code"),Hs=c("question-answering"),Ys=c("."),Gs=u(),Ye=o("p"),Ws=c(`Methods in this class assume a data format compatible with the
`),Ge=o("a"),Vt=o("code"),Js=c("QuestionAnsweringPipeline"),Ks=c("."),Xs=u(),A=o("div"),b(We.$$.fragment),Zs=u(),Ut=o("p"),eo=c("Compute the metric for a given pipeline and dataset combination."),to=u(),b(ve.$$.fragment),ao=u(),b(_e.$$.fragment),so=u(),b($e.$$.fragment),ba=u(),ae=o("h3"),be=o("a"),Bt=o("span"),b(Je.$$.fragment),oo=u(),Ht=o("span"),no=c("TextClassificationEvaluator"),wa=u(),V=o("div"),b(Ke.$$.fragment),ro=u(),S=o("p"),lo=c(`Text classification evaluator.
This text classification evaluator can currently be loaded from `),mt=o("a"),io=c("evaluator()"),co=c(` using the default task name
`),Yt=o("code"),po=c("text-classification"),uo=c(" or with a "),Gt=o("code"),mo=c('"sentiment-analysis"'),fo=c(` alias.
Methods in this class assume a data format compatible with the `),Wt=o("code"),ho=c("TextClassificationPipeline"),go=c(` - a single textual
feature as input and a categorical label as output.`),vo=u(),B=o("div"),b(Xe.$$.fragment),_o=u(),Jt=o("p"),$o=c("Compute the metric for a given pipeline and dataset combination."),bo=u(),b(we.$$.fragment),ya=u(),se=o("h3"),ye=o("a"),Kt=o("span"),b(Ze.$$.fragment),wo=u(),Xt=o("span"),yo=c("TokenClassificationEvaluator"),qa=u(),D=o("div"),b(et.$$.fragment),qo=u(),Zt=o("p"),Eo=c("Token classification evaluator."),ko=u(),oe=o("p"),xo=c("This token classification evaluator can currently be loaded from "),ft=o("a"),jo=c("evaluator()"),To=c(` using the default task name
`),ea=o("code"),Co=c("token-classification"),Po=c("."),Do=u(),tt=o("p"),Io=c("Methods in this class assume a data format compatible with the "),ta=o("code"),Ao=c("TokenClassificationPipeline"),No=c("."),Fo=u(),C=o("div"),b(at.$$.fragment),zo=u(),aa=o("p"),So=c("Compute the metric for a given pipeline and dataset combination."),Oo=u(),st=o("p"),Lo=c("The dataset input and label columns are expected to be formatted as a list of words and a list of labels respectively, following "),ot=o("a"),Mo=c("conll2003 dataset"),Qo=c(". Datasets whose inputs are single strings, and labels are a list of offset are not supported."),Ro=u(),b(qe.$$.fragment),Vo=u(),b(Ee.$$.fragment),Uo=u(),b(ke.$$.fragment),this.h()},l(t){const g=Un('[data-svelte="svelte-1phssyn"]',document.head);l=n(g,"META",{name:!0,content:!0}),g.forEach(a),v=m(t),i=n(t,"H1",{class:!0});var nt=r(i);d=n(nt,"A",{id:!0,class:!0,href:!0});var sa=r(d);_=n(sa,"SPAN",{});var oa=r(_);w(s.$$.fragment,oa),oa.forEach(a),sa.forEach(a),f=m(nt),F=n(nt,"SPAN",{});var na=r(F);j=p(na,"Evaluator"),na.forEach(a),nt.forEach(a),z=m(t),J=n(t,"P",{});var ra=r(J);Ma=p(ra,"The evaluator classes for automatic evaluation."),ra.forEach(a),pa=m(t),K=n(t,"H2",{class:!0});var rt=r(K);ne=n(rt,"A",{id:!0,class:!0,href:!0});var la=r(ne);yt=n(la,"SPAN",{});var ia=r(yt);w(De.$$.fragment,ia),ia.forEach(a),la.forEach(a),Qa=m(rt),qt=n(rt,"SPAN",{});var ca=r(qt);Ra=p(ca,"Evaluator classes"),ca.forEach(a),rt.forEach(a),da=m(t),it=n(t,"P",{});var Yo=r(it);Va=p(Yo,"The main entry point for using the evaluator:"),Yo.forEach(a),ua=m(t),L=n(t,"DIV",{class:!0});var ht=r(L);w(Ie.$$.fragment,ht),Ua=m(ht),M=n(ht,"P",{});var xe=r(M);Ba=p(xe,"Utility factory method to build an "),ct=n(xe,"A",{href:!0});var Go=r(ct);Ha=p(Go,"Evaluator"),Go.forEach(a),Ya=p(xe,`.
Evaluators encapsulate a task and a default metric name. They leverage `),Et=n(xe,"CODE",{});var Wo=r(Et);Ga=p(Wo,"pipeline"),Wo.forEach(a),Wa=p(xe," functionalify from "),kt=n(xe,"CODE",{});var Jo=r(kt);Ja=p(Jo,"transformers"),Jo.forEach(a),Ka=p(xe,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),xe.forEach(a),Xa=m(ht),w(re.$$.fragment,ht),ht.forEach(a),ma=m(t),pt=n(t,"P",{});var Ko=r(pt);Za=p(Ko,"The base class for all evaluator classes:"),Ko.forEach(a),fa=m(t),T=n(t,"DIV",{class:!0});var N=r(T);w(Ae.$$.fragment,N),es=m(N),xt=n(N,"P",{});var Xo=r(xt);ts=p(Xo,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),Xo.forEach(a),as=m(N),le=n(N,"DIV",{class:!0});var ka=r(le);w(Ne.$$.fragment,ka),ss=m(ka),jt=n(ka,"P",{});var Zo=r(jt);os=p(Zo,"Compute and return metrics."),Zo.forEach(a),ka.forEach(a),ns=m(N),ie=n(N,"DIV",{class:!0});var xa=r(ie);w(Fe.$$.fragment,xa),rs=m(xa),ze=n(xa,"P",{});var ja=r(ze);ls=p(ja,"A core method of the "),Tt=n(ja,"CODE",{});var en=r(Tt);is=p(en,"Evaluator"),en.forEach(a),cs=p(ja," class, which processes the pipeline outputs for compatibility with the metric."),ja.forEach(a),xa.forEach(a),ps=m(N),ce=n(N,"DIV",{class:!0});var Ta=r(ce);w(Se.$$.fragment,Ta),ds=m(Ta),Ct=n(Ta,"P",{});var tn=r(Ct);us=p(tn,"Prepare data."),tn.forEach(a),Ta.forEach(a),ms=m(N),pe=n(N,"DIV",{class:!0});var Ca=r(pe);w(Oe.$$.fragment,Ca),fs=m(Ca),Pt=n(Ca,"P",{});var an=r(Pt);hs=p(an,"Prepare metric."),an.forEach(a),Ca.forEach(a),gs=m(N),de=n(N,"DIV",{class:!0});var Pa=r(de);w(Le.$$.fragment,Pa),vs=m(Pa),Dt=n(Pa,"P",{});var sn=r(Dt);_s=p(sn,"Prepare pipeline."),sn.forEach(a),Pa.forEach(a),N.forEach(a),ha=m(t),X=n(t,"H2",{class:!0});var Da=r(X);ue=n(Da,"A",{id:!0,class:!0,href:!0});var on=r(ue);It=n(on,"SPAN",{});var nn=r(It);w(Me.$$.fragment,nn),nn.forEach(a),on.forEach(a),$s=m(Da),At=n(Da,"SPAN",{});var rn=r(At);bs=p(rn,"The task specific evaluators"),rn.forEach(a),Da.forEach(a),ga=m(t),Z=n(t,"H3",{class:!0});var Ia=r(Z);me=n(Ia,"A",{id:!0,class:!0,href:!0});var ln=r(me);Nt=n(ln,"SPAN",{});var cn=r(Nt);w(Qe.$$.fragment,cn),cn.forEach(a),ln.forEach(a),ws=m(Ia),Ft=n(Ia,"SPAN",{});var pn=r(Ft);ys=p(pn,"ImageClassificationEvaluator"),pn.forEach(a),Ia.forEach(a),va=m(t),Q=n(t,"DIV",{class:!0});var gt=r(Q);w(Re.$$.fragment,gt),qs=m(gt),R=n(gt,"P",{});var je=r(R);Es=p(je,`Image classification evaluator.
This image classification evaluator can currently be loaded from `),dt=n(je,"A",{href:!0});var dn=r(dt);ks=p(dn,"evaluator()"),dn.forEach(a),xs=p(je,` using the default task name
`),zt=n(je,"CODE",{});var un=r(zt);js=p(un,"image-classification"),un.forEach(a),Ts=p(je,`.
Methods in this class assume a data format compatible with the `),St=n(je,"CODE",{});var mn=r(St);Cs=p(mn,"ImageClassificationPipeline"),mn.forEach(a),Ps=p(je,"."),je.forEach(a),Ds=m(gt),U=n(gt,"DIV",{class:!0});var vt=r(U);w(Ve.$$.fragment,vt),Is=m(vt),Ot=n(vt,"P",{});var fn=r(Ot);As=p(fn,"Compute the metric for a given pipeline and dataset combination."),fn.forEach(a),Ns=m(vt),w(fe.$$.fragment,vt),vt.forEach(a),gt.forEach(a),_a=m(t),ee=n(t,"H3",{class:!0});var Aa=r(ee);he=n(Aa,"A",{id:!0,class:!0,href:!0});var hn=r(he);Lt=n(hn,"SPAN",{});var gn=r(Lt);w(Ue.$$.fragment,gn),gn.forEach(a),hn.forEach(a),Fs=m(Aa),Mt=n(Aa,"SPAN",{});var vn=r(Mt);zs=p(vn,"QuestionAnsweringEvaluator"),vn.forEach(a),Aa.forEach(a),$a=m(t),P=n(t,"DIV",{class:!0});var H=r(P);w(Be.$$.fragment,H),Ss=m(H),He=n(H,"P",{});var Na=r(He);Os=p(Na,`Question answering evaluator. This evaluator handles
`),ge=n(Na,"A",{href:!0,rel:!0});var Bo=r(ge);Qt=n(Bo,"STRONG",{});var _n=r(Qt);Ls=p(_n,"extractive"),_n.forEach(a),Ms=p(Bo," question answering"),Bo.forEach(a),Qs=p(Na,`,
where the answer to the question is extracted from a context.`),Na.forEach(a),Rs=m(H),te=n(H,"P",{});var _t=r(te);Vs=p(_t,"This question answering evaluator can currently be loaded from "),ut=n(_t,"A",{href:!0});var $n=r(ut);Us=p($n,"evaluator()"),$n.forEach(a),Bs=p(_t,` using the default task name
`),Rt=n(_t,"CODE",{});var bn=r(Rt);Hs=p(bn,"question-answering"),bn.forEach(a),Ys=p(_t,"."),_t.forEach(a),Gs=m(H),Ye=n(H,"P",{});var Fa=r(Ye);Ws=p(Fa,`Methods in this class assume a data format compatible with the
`),Ge=n(Fa,"A",{href:!0,rel:!0});var wn=r(Ge);Vt=n(wn,"CODE",{});var yn=r(Vt);Js=p(yn,"QuestionAnsweringPipeline"),yn.forEach(a),wn.forEach(a),Ks=p(Fa,"."),Fa.forEach(a),Xs=m(H),A=n(H,"DIV",{class:!0});var Y=r(A);w(We.$$.fragment,Y),Zs=m(Y),Ut=n(Y,"P",{});var qn=r(Ut);eo=p(qn,"Compute the metric for a given pipeline and dataset combination."),qn.forEach(a),to=m(Y),w(ve.$$.fragment,Y),ao=m(Y),w(_e.$$.fragment,Y),so=m(Y),w($e.$$.fragment,Y),Y.forEach(a),H.forEach(a),ba=m(t),ae=n(t,"H3",{class:!0});var za=r(ae);be=n(za,"A",{id:!0,class:!0,href:!0});var En=r(be);Bt=n(En,"SPAN",{});var kn=r(Bt);w(Je.$$.fragment,kn),kn.forEach(a),En.forEach(a),oo=m(za),Ht=n(za,"SPAN",{});var xn=r(Ht);no=p(xn,"TextClassificationEvaluator"),xn.forEach(a),za.forEach(a),wa=m(t),V=n(t,"DIV",{class:!0});var $t=r(V);w(Ke.$$.fragment,$t),ro=m($t),S=n($t,"P",{});var G=r(S);lo=p(G,`Text classification evaluator.
This text classification evaluator can currently be loaded from `),mt=n(G,"A",{href:!0});var jn=r(mt);io=p(jn,"evaluator()"),jn.forEach(a),co=p(G,` using the default task name
`),Yt=n(G,"CODE",{});var Tn=r(Yt);po=p(Tn,"text-classification"),Tn.forEach(a),uo=p(G," or with a "),Gt=n(G,"CODE",{});var Cn=r(Gt);mo=p(Cn,'"sentiment-analysis"'),Cn.forEach(a),fo=p(G,` alias.
Methods in this class assume a data format compatible with the `),Wt=n(G,"CODE",{});var Pn=r(Wt);ho=p(Pn,"TextClassificationPipeline"),Pn.forEach(a),go=p(G,` - a single textual
feature as input and a categorical label as output.`),G.forEach(a),vo=m($t),B=n($t,"DIV",{class:!0});var bt=r(B);w(Xe.$$.fragment,bt),_o=m(bt),Jt=n(bt,"P",{});var Dn=r(Jt);$o=p(Dn,"Compute the metric for a given pipeline and dataset combination."),Dn.forEach(a),bo=m(bt),w(we.$$.fragment,bt),bt.forEach(a),$t.forEach(a),ya=m(t),se=n(t,"H3",{class:!0});var Sa=r(se);ye=n(Sa,"A",{id:!0,class:!0,href:!0});var In=r(ye);Kt=n(In,"SPAN",{});var An=r(Kt);w(Ze.$$.fragment,An),An.forEach(a),In.forEach(a),wo=m(Sa),Xt=n(Sa,"SPAN",{});var Nn=r(Xt);yo=p(Nn,"TokenClassificationEvaluator"),Nn.forEach(a),Sa.forEach(a),qa=m(t),D=n(t,"DIV",{class:!0});var W=r(D);w(et.$$.fragment,W),qo=m(W),Zt=n(W,"P",{});var Fn=r(Zt);Eo=p(Fn,"Token classification evaluator."),Fn.forEach(a),ko=m(W),oe=n(W,"P",{});var wt=r(oe);xo=p(wt,"This token classification evaluator can currently be loaded from "),ft=n(wt,"A",{href:!0});var zn=r(ft);jo=p(zn,"evaluator()"),zn.forEach(a),To=p(wt,` using the default task name
`),ea=n(wt,"CODE",{});var Sn=r(ea);Co=p(Sn,"token-classification"),Sn.forEach(a),Po=p(wt,"."),wt.forEach(a),Do=m(W),tt=n(W,"P",{});var Oa=r(tt);Io=p(Oa,"Methods in this class assume a data format compatible with the "),ta=n(Oa,"CODE",{});var On=r(ta);Ao=p(On,"TokenClassificationPipeline"),On.forEach(a),No=p(Oa,"."),Oa.forEach(a),Fo=m(W),C=n(W,"DIV",{class:!0});var O=r(C);w(at.$$.fragment,O),zo=m(O),aa=n(O,"P",{});var Ln=r(aa);So=p(Ln,"Compute the metric for a given pipeline and dataset combination."),Ln.forEach(a),Oo=m(O),st=n(O,"P",{});var La=r(st);Lo=p(La,"The dataset input and label columns are expected to be formatted as a list of words and a list of labels respectively, following "),ot=n(La,"A",{href:!0,rel:!0});var Mn=r(ot);Mo=p(Mn,"conll2003 dataset"),Mn.forEach(a),Qo=p(La,". Datasets whose inputs are single strings, and labels are a list of offset are not supported."),La.forEach(a),Ro=m(O),w(qe.$$.fragment,O),Vo=m(O),w(Ee.$$.fragment,O),Uo=m(O),w(ke.$$.fragment,O),O.forEach(a),W.forEach(a),this.h()},h(){h(l,"name","hf:doc:metadata"),h(l,"content",JSON.stringify(or)),h(d,"id","evaluator"),h(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(d,"href","#evaluator"),h(i,"class","relative group"),h(ne,"id","evaluate.evaluator"),h(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ne,"href","#evaluate.evaluator"),h(K,"class","relative group"),h(ct,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.Evaluator"),h(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(ue,"id","the-task-specific-evaluators"),h(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ue,"href","#the-task-specific-evaluators"),h(X,"class","relative group"),h(me,"id","evaluate.ImageClassificationEvaluator"),h(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(me,"href","#evaluate.ImageClassificationEvaluator"),h(Z,"class","relative group"),h(dt,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.evaluator"),h(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(he,"id","evaluate.QuestionAnsweringEvaluator"),h(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(he,"href","#evaluate.QuestionAnsweringEvaluator"),h(ee,"class","relative group"),h(ge,"href","https://huggingface.co/docs/transformers/task_summary#extractive-question-answering"),h(ge,"rel","nofollow"),h(ut,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.evaluator"),h(Ge,"href","https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline"),h(Ge,"rel","nofollow"),h(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(be,"id","evaluate.TextClassificationEvaluator"),h(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(be,"href","#evaluate.TextClassificationEvaluator"),h(ae,"class","relative group"),h(mt,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.evaluator"),h(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(ye,"id","evaluate.TokenClassificationEvaluator"),h(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ye,"href","#evaluate.TokenClassificationEvaluator"),h(se,"class","relative group"),h(ft,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.evaluator"),h(ot,"href","https://huggingface.co/datasets/conll2003"),h(ot,"rel","nofollow"),h(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,g){e(document.head,l),$(t,v,g),$(t,i,g),e(i,d),e(d,_),y(s,_,null),e(i,f),e(i,F),e(F,j),$(t,z,g),$(t,J,g),e(J,Ma),$(t,pa,g),$(t,K,g),e(K,ne),e(ne,yt),y(De,yt,null),e(K,Qa),e(K,qt),e(qt,Ra),$(t,da,g),$(t,it,g),e(it,Va),$(t,ua,g),$(t,L,g),y(Ie,L,null),e(L,Ua),e(L,M),e(M,Ba),e(M,ct),e(ct,Ha),e(M,Ya),e(M,Et),e(Et,Ga),e(M,Wa),e(M,kt),e(kt,Ja),e(M,Ka),e(L,Xa),y(re,L,null),$(t,ma,g),$(t,pt,g),e(pt,Za),$(t,fa,g),$(t,T,g),y(Ae,T,null),e(T,es),e(T,xt),e(xt,ts),e(T,as),e(T,le),y(Ne,le,null),e(le,ss),e(le,jt),e(jt,os),e(T,ns),e(T,ie),y(Fe,ie,null),e(ie,rs),e(ie,ze),e(ze,ls),e(ze,Tt),e(Tt,is),e(ze,cs),e(T,ps),e(T,ce),y(Se,ce,null),e(ce,ds),e(ce,Ct),e(Ct,us),e(T,ms),e(T,pe),y(Oe,pe,null),e(pe,fs),e(pe,Pt),e(Pt,hs),e(T,gs),e(T,de),y(Le,de,null),e(de,vs),e(de,Dt),e(Dt,_s),$(t,ha,g),$(t,X,g),e(X,ue),e(ue,It),y(Me,It,null),e(X,$s),e(X,At),e(At,bs),$(t,ga,g),$(t,Z,g),e(Z,me),e(me,Nt),y(Qe,Nt,null),e(Z,ws),e(Z,Ft),e(Ft,ys),$(t,va,g),$(t,Q,g),y(Re,Q,null),e(Q,qs),e(Q,R),e(R,Es),e(R,dt),e(dt,ks),e(R,xs),e(R,zt),e(zt,js),e(R,Ts),e(R,St),e(St,Cs),e(R,Ps),e(Q,Ds),e(Q,U),y(Ve,U,null),e(U,Is),e(U,Ot),e(Ot,As),e(U,Ns),y(fe,U,null),$(t,_a,g),$(t,ee,g),e(ee,he),e(he,Lt),y(Ue,Lt,null),e(ee,Fs),e(ee,Mt),e(Mt,zs),$(t,$a,g),$(t,P,g),y(Be,P,null),e(P,Ss),e(P,He),e(He,Os),e(He,ge),e(ge,Qt),e(Qt,Ls),e(ge,Ms),e(He,Qs),e(P,Rs),e(P,te),e(te,Vs),e(te,ut),e(ut,Us),e(te,Bs),e(te,Rt),e(Rt,Hs),e(te,Ys),e(P,Gs),e(P,Ye),e(Ye,Ws),e(Ye,Ge),e(Ge,Vt),e(Vt,Js),e(Ye,Ks),e(P,Xs),e(P,A),y(We,A,null),e(A,Zs),e(A,Ut),e(Ut,eo),e(A,to),y(ve,A,null),e(A,ao),y(_e,A,null),e(A,so),y($e,A,null),$(t,ba,g),$(t,ae,g),e(ae,be),e(be,Bt),y(Je,Bt,null),e(ae,oo),e(ae,Ht),e(Ht,no),$(t,wa,g),$(t,V,g),y(Ke,V,null),e(V,ro),e(V,S),e(S,lo),e(S,mt),e(mt,io),e(S,co),e(S,Yt),e(Yt,po),e(S,uo),e(S,Gt),e(Gt,mo),e(S,fo),e(S,Wt),e(Wt,ho),e(S,go),e(V,vo),e(V,B),y(Xe,B,null),e(B,_o),e(B,Jt),e(Jt,$o),e(B,bo),y(we,B,null),$(t,ya,g),$(t,se,g),e(se,ye),e(ye,Kt),y(Ze,Kt,null),e(se,wo),e(se,Xt),e(Xt,yo),$(t,qa,g),$(t,D,g),y(et,D,null),e(D,qo),e(D,Zt),e(Zt,Eo),e(D,ko),e(D,oe),e(oe,xo),e(oe,ft),e(ft,jo),e(oe,To),e(oe,ea),e(ea,Co),e(oe,Po),e(D,Do),e(D,tt),e(tt,Io),e(tt,ta),e(ta,Ao),e(tt,No),e(D,Fo),e(D,C),y(at,C,null),e(C,zo),e(C,aa),e(aa,So),e(C,Oo),e(C,st),e(st,Lo),e(st,ot),e(ot,Mo),e(st,Qo),e(C,Ro),y(qe,C,null),e(C,Vo),y(Ee,C,null),e(C,Uo),y(ke,C,null),Ea=!0},p(t,[g]){const nt={};g&2&&(nt.$$scope={dirty:g,ctx:t}),re.$set(nt);const sa={};g&2&&(sa.$$scope={dirty:g,ctx:t}),fe.$set(sa);const oa={};g&2&&(oa.$$scope={dirty:g,ctx:t}),ve.$set(oa);const na={};g&2&&(na.$$scope={dirty:g,ctx:t}),_e.$set(na);const ra={};g&2&&(ra.$$scope={dirty:g,ctx:t}),$e.$set(ra);const rt={};g&2&&(rt.$$scope={dirty:g,ctx:t}),we.$set(rt);const la={};g&2&&(la.$$scope={dirty:g,ctx:t}),qe.$set(la);const ia={};g&2&&(ia.$$scope={dirty:g,ctx:t}),Ee.$set(ia);const ca={};g&2&&(ca.$$scope={dirty:g,ctx:t}),ke.$set(ca)},i(t){Ea||(q(s.$$.fragment,t),q(De.$$.fragment,t),q(Ie.$$.fragment,t),q(re.$$.fragment,t),q(Ae.$$.fragment,t),q(Ne.$$.fragment,t),q(Fe.$$.fragment,t),q(Se.$$.fragment,t),q(Oe.$$.fragment,t),q(Le.$$.fragment,t),q(Me.$$.fragment,t),q(Qe.$$.fragment,t),q(Re.$$.fragment,t),q(Ve.$$.fragment,t),q(fe.$$.fragment,t),q(Ue.$$.fragment,t),q(Be.$$.fragment,t),q(We.$$.fragment,t),q(ve.$$.fragment,t),q(_e.$$.fragment,t),q($e.$$.fragment,t),q(Je.$$.fragment,t),q(Ke.$$.fragment,t),q(Xe.$$.fragment,t),q(we.$$.fragment,t),q(Ze.$$.fragment,t),q(et.$$.fragment,t),q(at.$$.fragment,t),q(qe.$$.fragment,t),q(Ee.$$.fragment,t),q(ke.$$.fragment,t),Ea=!0)},o(t){E(s.$$.fragment,t),E(De.$$.fragment,t),E(Ie.$$.fragment,t),E(re.$$.fragment,t),E(Ae.$$.fragment,t),E(Ne.$$.fragment,t),E(Fe.$$.fragment,t),E(Se.$$.fragment,t),E(Oe.$$.fragment,t),E(Le.$$.fragment,t),E(Me.$$.fragment,t),E(Qe.$$.fragment,t),E(Re.$$.fragment,t),E(Ve.$$.fragment,t),E(fe.$$.fragment,t),E(Ue.$$.fragment,t),E(Be.$$.fragment,t),E(We.$$.fragment,t),E(ve.$$.fragment,t),E(_e.$$.fragment,t),E($e.$$.fragment,t),E(Je.$$.fragment,t),E(Ke.$$.fragment,t),E(Xe.$$.fragment,t),E(we.$$.fragment,t),E(Ze.$$.fragment,t),E(et.$$.fragment,t),E(at.$$.fragment,t),E(qe.$$.fragment,t),E(Ee.$$.fragment,t),E(ke.$$.fragment,t),Ea=!1},d(t){a(l),t&&a(v),t&&a(i),k(s),t&&a(z),t&&a(J),t&&a(pa),t&&a(K),k(De),t&&a(da),t&&a(it),t&&a(ua),t&&a(L),k(Ie),k(re),t&&a(ma),t&&a(pt),t&&a(fa),t&&a(T),k(Ae),k(Ne),k(Fe),k(Se),k(Oe),k(Le),t&&a(ha),t&&a(X),k(Me),t&&a(ga),t&&a(Z),k(Qe),t&&a(va),t&&a(Q),k(Re),k(Ve),k(fe),t&&a(_a),t&&a(ee),k(Ue),t&&a($a),t&&a(P),k(Be),k(We),k(ve),k(_e),k($e),t&&a(ba),t&&a(ae),k(Je),t&&a(wa),t&&a(V),k(Ke),k(Xe),k(we),t&&a(ya),t&&a(se),k(Ze),t&&a(qa),t&&a(D),k(et),k(at),k(qe),k(Ee),k(ke)}}}const or={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"},{local:"the-task-specific-evaluators",sections:[{local:"evaluate.ImageClassificationEvaluator",title:"ImageClassificationEvaluator"},{local:"evaluate.QuestionAnsweringEvaluator",title:"QuestionAnsweringEvaluator"},{local:"evaluate.TextClassificationEvaluator",title:"TextClassificationEvaluator"},{local:"evaluate.TokenClassificationEvaluator",title:"TokenClassificationEvaluator"}],title:"The task specific evaluators"}],title:"Evaluator"};function nr(x){return Bn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ur extends Qn{constructor(l){super();Rn(this,l,nr,sr,Vn,{})}}export{ur as default,or as metadata};
