import{S as fo,i as ho,s as vo,e as o,k as c,w as k,t as n,M as go,c as s,d as a,m as d,a as l,x as T,h as r,b as h,G as e,g as u,y as x,q,o as C,B as P,v as _o,L as uo}from"../../chunks/vendor-hf-doc-builder.js";import{D as ie}from"../../chunks/Docstring-hf-doc-builder.js";import{C as mo}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as co}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as po}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function bo(ce){let f,D,v,m,b;return m=new mo({props:{code:`from evaluate import evaluator

# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){f=o("p"),D=n("Examples:"),v=c(),k(m.$$.fragment)},l(i){f=s(i,"P",{});var _=l(f);D=r(_,"Examples:"),_.forEach(a),v=d(i),T(m.$$.fragment,i)},m(i,_){u(i,f,_),e(f,D),u(i,v,_),x(m,i,_),b=!0},p:uo,i(i){b||(q(m.$$.fragment,i),b=!0)},o(i){C(m.$$.fragment,i),b=!1},d(i){i&&a(f),i&&a(v),P(m,i)}}}function yo(ce){let f,D,v,m,b;return m=new mo({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset

e = evaluator("text-classification")
data =  Dataset.from_dict(load_dataset("imdb")["test"][:2])

results = e.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    input_column="text",
    label_column="label",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data =  Dataset.from_dict(load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)[<span class="hljs-string">&quot;test&quot;</span>][:<span class="hljs-number">2</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;text&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;label&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){f=o("p"),D=n("Examples:"),v=c(),k(m.$$.fragment)},l(i){f=s(i,"P",{});var _=l(f);D=r(_,"Examples:"),_.forEach(a),v=d(i),T(m.$$.fragment,i)},m(i,_){u(i,f,_),e(f,D),u(i,v,_),x(m,i,_),b=!0},p:uo,i(i){b||(q(m.$$.fragment,i),b=!0)},o(i){C(m.$$.fragment,i),b=!1},d(i){i&&a(f),i&&a(v),P(m,i)}}}function Eo(ce){let f,D,v,m,b,i,_,$e,dt,He,de,pt,We,I,O,ke,G,ut,Te,mt,Ge,pe,ft,Je,w,J,ht,K,vt,ue,gt,_t,bt,z,yt,xe,Et,wt,qe,$t,kt,Tt,U,Ke,me,xt,Qe,$,Q,qt,Ce,Ct,Pt,Pe,Dt,jt,R,X,Nt,A,Lt,De,It,zt,je,At,Mt,Xe,fe,Ft,Ye,y,Y,Ot,Ne,Ut,Rt,j,St,he,Bt,Vt,Le,Ht,Wt,Ie,Gt,Jt,Kt,Z,Qt,ze,Xt,Yt,Zt,N,ee,ea,Ae,ta,aa,S,Ze,ve,oa,et,g,te,sa,Me,na,la,M,ra,ge,ia,ca,Fe,da,pa,ua,ae,ma,Oe,fa,ha,va,Ue,ga,_a,F,oe,ba,Re,ya,Ea,wa,se,$a,ne,ka,Ta,xa,Se,qa,Ca,B,le,Pa,Be,Da,tt;return i=new co({}),G=new co({}),J=new ie({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:</p>
<ul>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/__init__.py#L75",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),U=new po({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[bo]},$$scope:{ctx:ce}}}),Q=new ie({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L47"}}),X=new ie({props:{name:"compute",anchor:"evaluate.Evaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"**compute_parameters",val:": typing.Dict"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L105"}}),Y=new ie({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/text_classification.py#L41"}}),ee=new ie({props:{name:"compute",anchor:"evaluate.TextClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'text'"},{name:"label_column",val:": str = 'label'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"}],parametersDescription:[{anchor:"evaluate.TextClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.TextClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.TextClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.TextClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>&#x201D; &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:</p>
<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.TextClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TextClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TextClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TextClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/text_classification.py#L62",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),S=new po({props:{anchor:"evaluate.TextClassificationEvaluator.compute.example",$$slots:{default:[yo]},$$scope:{ctx:ce}}}),te=new ie({props:{name:"class evaluate.TokenClassificationEvaluator",anchor:"evaluate.TokenClassificationEvaluator",parameters:[{name:"task",val:" = 'token-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/token_classification.py#L41"}}),le=new ie({props:{name:"compute",anchor:"evaluate.TokenClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'tokens'"},{name:"label_column",val:": str = 'ner_tags'"},{name:"label_mapping",val:": typing.Optional[typing.Dict] = None"},{name:"join_by",val:": typing.Optional[str] = ' '"}],parametersDescription:[{anchor:"evaluate.TokenClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.TokenClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>token-classification</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.TokenClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.TokenClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>&#x201D; &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:</p>
<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.TokenClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TokenClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TokenClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TokenClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;tokens&quot;</code>) &#x2014;
the name of the column containing the tokens feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TokenClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TokenClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"},{anchor:"evaluate.TokenClassificationEvaluator.compute.join_by",description:`<strong>join_by</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot; &quot;</code>) &#x2014;
This evaluator supports dataset whose input column is a list of words. This parameter specifies how to join
words to generate a string input. This is especially useful for languages that do not separate words by a space.`,name:"join_by"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/token_classification.py#L95",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),{c(){f=o("meta"),D=c(),v=o("h1"),m=o("a"),b=o("span"),k(i.$$.fragment),_=c(),$e=o("span"),dt=n("Evaluator"),He=c(),de=o("p"),pt=n("The evaluator classes for automatic evaluation."),We=c(),I=o("h2"),O=o("a"),ke=o("span"),k(G.$$.fragment),ut=c(),Te=o("span"),mt=n("Evaluator classes"),Ge=c(),pe=o("p"),ft=n("The main entry point for using the evaluator:"),Je=c(),w=o("div"),k(J.$$.fragment),ht=c(),K=o("p"),vt=n("Utility factory method to build an "),ue=o("a"),gt=n("Evaluator"),_t=n("."),bt=c(),z=o("p"),yt=n("Evaluators encapsulate a task and a default metric name. They leverate "),xe=o("code"),Et=n("pipeline"),wt=n(" functionalify from "),qe=o("code"),$t=n("transformers"),kt=n(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),Tt=c(),k(U.$$.fragment),Ke=c(),me=o("p"),xt=n("The base class for all evaluator classes:"),Qe=c(),$=o("div"),k(Q.$$.fragment),qt=c(),Ce=o("p"),Ct=n(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.`),Pt=c(),Pe=o("p"),Dt=n("Base class implementing evaluator operations."),jt=c(),R=o("div"),k(X.$$.fragment),Nt=c(),A=o("p"),Lt=n("A core method of the "),De=o("code"),It=n("Evaluator"),zt=n(` class, computes the metric value for a pipeline and dataset compatible
with the task specified by the `),je=o("code"),At=n("Evaluator"),Mt=n("."),Xe=c(),fe=o("p"),Ft=n("The class for text classification evaluation:"),Ye=c(),y=o("div"),k(Y.$$.fragment),Ot=c(),Ne=o("p"),Ut=n("Text classification evaluator."),Rt=c(),j=o("p"),St=n("This text classification evaluator can currently be loaded from "),he=o("a"),Bt=n("evaluator()"),Vt=n(` using the default task name
`),Le=o("code"),Ht=n("text-classification"),Wt=n(" or with a "),Ie=o("code"),Gt=n('"sentiment-analysis"'),Jt=n(" alias."),Kt=c(),Z=o("p"),Qt=n("Methods in this class assume a data format compatible with the "),ze=o("code"),Xt=n("TextClassificationPipeline"),Yt=n(` - a single textual
feature as input and a categorical label as output.`),Zt=c(),N=o("div"),k(ee.$$.fragment),ea=c(),Ae=o("p"),ta=n("Compute the metric for a given pipeline and dataset combination."),aa=c(),k(S.$$.fragment),Ze=c(),ve=o("p"),oa=n("The class for token classification evaluation:"),et=c(),g=o("div"),k(te.$$.fragment),sa=c(),Me=o("p"),na=n("Token classification evaluator."),la=c(),M=o("p"),ra=n("This token classification evaluator can currently be loaded from "),ge=o("a"),ia=n("evaluator()"),ca=n(` using the default task name
`),Fe=o("code"),da=n("token-classification"),pa=n("."),ua=c(),ae=o("p"),ma=n("Methods in this class assume a data format compatible with the "),Oe=o("code"),fa=n("TokenClassificationPipeline"),ha=n("."),va=c(),Ue=o("p"),ga=n("In particular, the following cases are not handled:"),_a=c(),F=o("ul"),oe=o("li"),ba=n("models not splitting tokens by space (e.g. where "),Re=o("code"),ya=n('"he oh"'),Ea=n(" can be a token)."),wa=c(),se=o("li"),$a=n("datasets as "),ne=o("a"),ka=n("https://huggingface.co/datasets/msra_ner"),Ta=n(" , where tokens are provided ideogram by ideogram, and where a tokenizer may map several ideograms to a single token."),xa=c(),Se=o("li"),qa=n("datasets not providing the input and reference columns as a list of \u201Cwords\u201D as the conll2003 dataset."),Ca=c(),B=o("div"),k(le.$$.fragment),Pa=c(),Be=o("p"),Da=n("Compute the metric for a given pipeline and dataset combination."),this.h()},l(t){const p=go('[data-svelte="svelte-1phssyn"]',document.head);f=s(p,"META",{name:!0,content:!0}),p.forEach(a),D=d(t),v=s(t,"H1",{class:!0});var re=l(v);m=s(re,"A",{id:!0,class:!0,href:!0});var Ve=l(m);b=s(Ve,"SPAN",{});var ja=l(b);T(i.$$.fragment,ja),ja.forEach(a),Ve.forEach(a),_=d(re),$e=s(re,"SPAN",{});var Na=l($e);dt=r(Na,"Evaluator"),Na.forEach(a),re.forEach(a),He=d(t),de=s(t,"P",{});var La=l(de);pt=r(La,"The evaluator classes for automatic evaluation."),La.forEach(a),We=d(t),I=s(t,"H2",{class:!0});var at=l(I);O=s(at,"A",{id:!0,class:!0,href:!0});var Ia=l(O);ke=s(Ia,"SPAN",{});var za=l(ke);T(G.$$.fragment,za),za.forEach(a),Ia.forEach(a),ut=d(at),Te=s(at,"SPAN",{});var Aa=l(Te);mt=r(Aa,"Evaluator classes"),Aa.forEach(a),at.forEach(a),Ge=d(t),pe=s(t,"P",{});var Ma=l(pe);ft=r(Ma,"The main entry point for using the evaluator:"),Ma.forEach(a),Je=d(t),w=s(t,"DIV",{class:!0});var V=l(w);T(J.$$.fragment,V),ht=d(V),K=s(V,"P",{});var ot=l(K);vt=r(ot,"Utility factory method to build an "),ue=s(ot,"A",{href:!0});var Fa=l(ue);gt=r(Fa,"Evaluator"),Fa.forEach(a),_t=r(ot,"."),ot.forEach(a),bt=d(V),z=s(V,"P",{});var _e=l(z);yt=r(_e,"Evaluators encapsulate a task and a default metric name. They leverate "),xe=s(_e,"CODE",{});var Oa=l(xe);Et=r(Oa,"pipeline"),Oa.forEach(a),wt=r(_e," functionalify from "),qe=s(_e,"CODE",{});var Ua=l(qe);$t=r(Ua,"transformers"),Ua.forEach(a),kt=r(_e,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),_e.forEach(a),Tt=d(V),T(U.$$.fragment,V),V.forEach(a),Ke=d(t),me=s(t,"P",{});var Ra=l(me);xt=r(Ra,"The base class for all evaluator classes:"),Ra.forEach(a),Qe=d(t),$=s(t,"DIV",{class:!0});var H=l($);T(Q.$$.fragment,H),qt=d(H),Ce=s(H,"P",{});var Sa=l(Ce);Ct=r(Sa,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.`),Sa.forEach(a),Pt=d(H),Pe=s(H,"P",{});var Ba=l(Pe);Dt=r(Ba,"Base class implementing evaluator operations."),Ba.forEach(a),jt=d(H),R=s(H,"DIV",{class:!0});var st=l(R);T(X.$$.fragment,st),Nt=d(st),A=s(st,"P",{});var be=l(A);Lt=r(be,"A core method of the "),De=s(be,"CODE",{});var Va=l(De);It=r(Va,"Evaluator"),Va.forEach(a),zt=r(be,` class, computes the metric value for a pipeline and dataset compatible
with the task specified by the `),je=s(be,"CODE",{});var Ha=l(je);At=r(Ha,"Evaluator"),Ha.forEach(a),Mt=r(be,"."),be.forEach(a),st.forEach(a),H.forEach(a),Xe=d(t),fe=s(t,"P",{});var Wa=l(fe);Ft=r(Wa,"The class for text classification evaluation:"),Wa.forEach(a),Ye=d(t),y=s(t,"DIV",{class:!0});var L=l(y);T(Y.$$.fragment,L),Ot=d(L),Ne=s(L,"P",{});var Ga=l(Ne);Ut=r(Ga,"Text classification evaluator."),Ga.forEach(a),Rt=d(L),j=s(L,"P",{});var W=l(j);St=r(W,"This text classification evaluator can currently be loaded from "),he=s(W,"A",{href:!0});var Ja=l(he);Bt=r(Ja,"evaluator()"),Ja.forEach(a),Vt=r(W,` using the default task name
`),Le=s(W,"CODE",{});var Ka=l(Le);Ht=r(Ka,"text-classification"),Ka.forEach(a),Wt=r(W," or with a "),Ie=s(W,"CODE",{});var Qa=l(Ie);Gt=r(Qa,'"sentiment-analysis"'),Qa.forEach(a),Jt=r(W," alias."),W.forEach(a),Kt=d(L),Z=s(L,"P",{});var nt=l(Z);Qt=r(nt,"Methods in this class assume a data format compatible with the "),ze=s(nt,"CODE",{});var Xa=l(ze);Xt=r(Xa,"TextClassificationPipeline"),Xa.forEach(a),Yt=r(nt,` - a single textual
feature as input and a categorical label as output.`),nt.forEach(a),Zt=d(L),N=s(L,"DIV",{class:!0});var ye=l(N);T(ee.$$.fragment,ye),ea=d(ye),Ae=s(ye,"P",{});var Ya=l(Ae);ta=r(Ya,"Compute the metric for a given pipeline and dataset combination."),Ya.forEach(a),aa=d(ye),T(S.$$.fragment,ye),ye.forEach(a),L.forEach(a),Ze=d(t),ve=s(t,"P",{});var Za=l(ve);oa=r(Za,"The class for token classification evaluation:"),Za.forEach(a),et=d(t),g=s(t,"DIV",{class:!0});var E=l(g);T(te.$$.fragment,E),sa=d(E),Me=s(E,"P",{});var eo=l(Me);na=r(eo,"Token classification evaluator."),eo.forEach(a),la=d(E),M=s(E,"P",{});var Ee=l(M);ra=r(Ee,"This token classification evaluator can currently be loaded from "),ge=s(Ee,"A",{href:!0});var to=l(ge);ia=r(to,"evaluator()"),to.forEach(a),ca=r(Ee,` using the default task name
`),Fe=s(Ee,"CODE",{});var ao=l(Fe);da=r(ao,"token-classification"),ao.forEach(a),pa=r(Ee,"."),Ee.forEach(a),ua=d(E),ae=s(E,"P",{});var lt=l(ae);ma=r(lt,"Methods in this class assume a data format compatible with the "),Oe=s(lt,"CODE",{});var oo=l(Oe);fa=r(oo,"TokenClassificationPipeline"),oo.forEach(a),ha=r(lt,"."),lt.forEach(a),va=d(E),Ue=s(E,"P",{});var so=l(Ue);ga=r(so,"In particular, the following cases are not handled:"),so.forEach(a),_a=d(E),F=s(E,"UL",{});var we=l(F);oe=s(we,"LI",{});var rt=l(oe);ba=r(rt,"models not splitting tokens by space (e.g. where "),Re=s(rt,"CODE",{});var no=l(Re);ya=r(no,'"he oh"'),no.forEach(a),Ea=r(rt," can be a token)."),rt.forEach(a),wa=d(we),se=s(we,"LI",{});var it=l(se);$a=r(it,"datasets as "),ne=s(it,"A",{href:!0,rel:!0});var lo=l(ne);ka=r(lo,"https://huggingface.co/datasets/msra_ner"),lo.forEach(a),Ta=r(it," , where tokens are provided ideogram by ideogram, and where a tokenizer may map several ideograms to a single token."),it.forEach(a),xa=d(we),Se=s(we,"LI",{});var ro=l(Se);qa=r(ro,"datasets not providing the input and reference columns as a list of \u201Cwords\u201D as the conll2003 dataset."),ro.forEach(a),we.forEach(a),Ca=d(E),B=s(E,"DIV",{class:!0});var ct=l(B);T(le.$$.fragment,ct),Pa=d(ct),Be=s(ct,"P",{});var io=l(Be);Da=r(io,"Compute the metric for a given pipeline and dataset combination."),io.forEach(a),ct.forEach(a),E.forEach(a),this.h()},h(){h(f,"name","hf:doc:metadata"),h(f,"content",JSON.stringify(wo)),h(m,"id","evaluator"),h(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(m,"href","#evaluator"),h(v,"class","relative group"),h(O,"id","evaluate.evaluator"),h(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(O,"href","#evaluate.evaluator"),h(I,"class","relative group"),h(ue,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.Evaluator"),h(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(he,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.evaluator"),h(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(ge,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.evaluator"),h(ne,"href","https://huggingface.co/datasets/msra_ner"),h(ne,"rel","nofollow"),h(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(g,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,p){e(document.head,f),u(t,D,p),u(t,v,p),e(v,m),e(m,b),x(i,b,null),e(v,_),e(v,$e),e($e,dt),u(t,He,p),u(t,de,p),e(de,pt),u(t,We,p),u(t,I,p),e(I,O),e(O,ke),x(G,ke,null),e(I,ut),e(I,Te),e(Te,mt),u(t,Ge,p),u(t,pe,p),e(pe,ft),u(t,Je,p),u(t,w,p),x(J,w,null),e(w,ht),e(w,K),e(K,vt),e(K,ue),e(ue,gt),e(K,_t),e(w,bt),e(w,z),e(z,yt),e(z,xe),e(xe,Et),e(z,wt),e(z,qe),e(qe,$t),e(z,kt),e(w,Tt),x(U,w,null),u(t,Ke,p),u(t,me,p),e(me,xt),u(t,Qe,p),u(t,$,p),x(Q,$,null),e($,qt),e($,Ce),e(Ce,Ct),e($,Pt),e($,Pe),e(Pe,Dt),e($,jt),e($,R),x(X,R,null),e(R,Nt),e(R,A),e(A,Lt),e(A,De),e(De,It),e(A,zt),e(A,je),e(je,At),e(A,Mt),u(t,Xe,p),u(t,fe,p),e(fe,Ft),u(t,Ye,p),u(t,y,p),x(Y,y,null),e(y,Ot),e(y,Ne),e(Ne,Ut),e(y,Rt),e(y,j),e(j,St),e(j,he),e(he,Bt),e(j,Vt),e(j,Le),e(Le,Ht),e(j,Wt),e(j,Ie),e(Ie,Gt),e(j,Jt),e(y,Kt),e(y,Z),e(Z,Qt),e(Z,ze),e(ze,Xt),e(Z,Yt),e(y,Zt),e(y,N),x(ee,N,null),e(N,ea),e(N,Ae),e(Ae,ta),e(N,aa),x(S,N,null),u(t,Ze,p),u(t,ve,p),e(ve,oa),u(t,et,p),u(t,g,p),x(te,g,null),e(g,sa),e(g,Me),e(Me,na),e(g,la),e(g,M),e(M,ra),e(M,ge),e(ge,ia),e(M,ca),e(M,Fe),e(Fe,da),e(M,pa),e(g,ua),e(g,ae),e(ae,ma),e(ae,Oe),e(Oe,fa),e(ae,ha),e(g,va),e(g,Ue),e(Ue,ga),e(g,_a),e(g,F),e(F,oe),e(oe,ba),e(oe,Re),e(Re,ya),e(oe,Ea),e(F,wa),e(F,se),e(se,$a),e(se,ne),e(ne,ka),e(se,Ta),e(F,xa),e(F,Se),e(Se,qa),e(g,Ca),e(g,B),x(le,B,null),e(B,Pa),e(B,Be),e(Be,Da),tt=!0},p(t,[p]){const re={};p&2&&(re.$$scope={dirty:p,ctx:t}),U.$set(re);const Ve={};p&2&&(Ve.$$scope={dirty:p,ctx:t}),S.$set(Ve)},i(t){tt||(q(i.$$.fragment,t),q(G.$$.fragment,t),q(J.$$.fragment,t),q(U.$$.fragment,t),q(Q.$$.fragment,t),q(X.$$.fragment,t),q(Y.$$.fragment,t),q(ee.$$.fragment,t),q(S.$$.fragment,t),q(te.$$.fragment,t),q(le.$$.fragment,t),tt=!0)},o(t){C(i.$$.fragment,t),C(G.$$.fragment,t),C(J.$$.fragment,t),C(U.$$.fragment,t),C(Q.$$.fragment,t),C(X.$$.fragment,t),C(Y.$$.fragment,t),C(ee.$$.fragment,t),C(S.$$.fragment,t),C(te.$$.fragment,t),C(le.$$.fragment,t),tt=!1},d(t){a(f),t&&a(D),t&&a(v),P(i),t&&a(He),t&&a(de),t&&a(We),t&&a(I),P(G),t&&a(Ge),t&&a(pe),t&&a(Je),t&&a(w),P(J),P(U),t&&a(Ke),t&&a(me),t&&a(Qe),t&&a($),P(Q),P(X),t&&a(Xe),t&&a(fe),t&&a(Ye),t&&a(y),P(Y),P(ee),P(S),t&&a(Ze),t&&a(ve),t&&a(et),t&&a(g),P(te),P(le)}}}const wo={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"}],title:"Evaluator"};function $o(ce){return _o(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Po extends fo{constructor(f){super();ho(this,f,$o,Eo,vo,{})}}export{Po as default,wo as metadata};
