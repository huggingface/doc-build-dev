import{S as Do,i as Io,s as No,e as o,k as u,w as $,t as c,M as Fo,c as n,d as a,m,a as r,x as w,h as p,b as h,G as e,g as _,y,q as E,o as q,B as k,v as Ao,L as tt}from"../../chunks/vendor-hf-doc-builder.js";import{T as Po}from"../../chunks/Tip-hf-doc-builder.js";import{D as F}from"../../chunks/Docstring-hf-doc-builder.js";import{C as at}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Ze}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as et}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Lo(T){let l,b,i,d,v;return d=new at({props:{code:`from evaluate import evaluator
# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){l=o("p"),b=c("Examples:"),i=u(),$(d.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);b=p(f,"Examples:"),f.forEach(a),i=m(s),w(d.$$.fragment,s)},m(s,f){_(s,l,f),e(l,b),_(s,i,f),y(d,s,f),v=!0},p:tt,i(s){v||(E(d.$$.fragment,s),v=!0)},o(s){q(d.$$.fragment,s),v=!1},d(s){s&&a(l),s&&a(i),k(d,s)}}}function So(T){let l,b,i,d,v;return d=new at({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("image-classification")
data =  Dataset.from_dict(load_dataset("beans")["test"][:2])
results = e.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    metric="accuracy",
    input_column="image",
    label_column="labels",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;image-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data =  Dataset.from_dict(load_dataset(<span class="hljs-string">&quot;beans&quot;</span>)[<span class="hljs-string">&quot;test&quot;</span>][:<span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;nateraw/vit-base-beans&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;image&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;labels&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&#x27;angular_leaf_spot&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;bean_rust&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>: <span class="hljs-number">2</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){l=o("p"),b=c("Examples:"),i=u(),$(d.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);b=p(f,"Examples:"),f.forEach(a),i=m(s),w(d.$$.fragment,s)},m(s,f){_(s,l,f),e(l,b),_(s,i,f),y(d,s,f),v=!0},p:tt,i(s){v||(E(d.$$.fragment,s),v=!0)},o(s){q(d.$$.fragment,s),v=!1},d(s){s&&a(l),s&&a(i),k(d,s)}}}function Oo(T){let l,b,i,d,v;return d=new at({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("text-classification")
data =  Dataset.from_dict(load_dataset("imdb")["test"][:2])
results = e.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    input_column="text",
    label_column="label",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data =  Dataset.from_dict(load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)[<span class="hljs-string">&quot;test&quot;</span>][:<span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;text&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;label&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){l=o("p"),b=c("Examples:"),i=u(),$(d.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);b=p(f,"Examples:"),f.forEach(a),i=m(s),w(d.$$.fragment,s)},m(s,f){_(s,l,f),e(l,b),_(s,i,f),y(d,s,f),v=!0},p:tt,i(s){v||(E(d.$$.fragment,s),v=!0)},o(s){q(d.$$.fragment,s),v=!1},d(s){s&&a(l),s&&a(i),k(d,s)}}}function zo(T){let l,b,i,d,v;return d=new at({props:{code:`from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("token-classification")
data = load_dataset("conll2003", split="validation[:2]")
results = task_evaluator.compute(
    model_or_pipeline="elastic/distilbert-base-uncased-finetuned-conll03-english",
    data=data,
    metric="seqeval",
    join_by=" ",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;token-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;conll2003&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;elastic/distilbert-base-uncased-finetuned-conll03-english&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;seqeval&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    join_by=<span class="hljs-string">&quot; &quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){l=o("p"),b=c("Examples:"),i=u(),$(d.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);b=p(f,"Examples:"),f.forEach(a),i=m(s),w(d.$$.fragment,s)},m(s,f){_(s,l,f),e(l,b),_(s,i,f),y(d,s,f),v=!0},p:tt,i(s){v||(E(d.$$.fragment,s),v=!0)},o(s){q(d.$$.fragment,s),v=!1},d(s){s&&a(l),s&&a(i),k(d,s)}}}function Mo(T){let l,b,i,d,v;return d=new at({props:{code:`dataset = Dataset.from_dict(
    mapping={
        "tokens": [["New", "York", "is", "a", "city", "and", "Felix", "a", "person", "."]],
        "ner_tags": [[1, 2, 0, 0, 0, 0, 3, 0, 0, 0]],
    },
    features=Features({
        "tokens": Sequence(feature=Value(dtype="string")),
        "ner_tags": Sequence(feature=ClassLabel(names=["O", "B-LOC", "I-LOC", "B-PER", "I-PER"])),
        }),
)`,highlighted:`dataset = Dataset.from_dict(
    mapping={
        <span class="hljs-string">&quot;tokens&quot;</span>: [[<span class="hljs-string">&quot;New&quot;</span>, <span class="hljs-string">&quot;York&quot;</span>, <span class="hljs-string">&quot;is&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;city&quot;</span>, <span class="hljs-string">&quot;and&quot;</span>, <span class="hljs-string">&quot;Felix&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;person&quot;</span>, <span class="hljs-string">&quot;.&quot;</span>]],
        <span class="hljs-string">&quot;ner_tags&quot;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]],
    },
    features=Features({
        <span class="hljs-string">&quot;tokens&quot;</span>: <span class="hljs-type">Sequence</span>(feature=Value(dtype=<span class="hljs-string">&quot;string&quot;</span>)),
        <span class="hljs-string">&quot;ner_tags&quot;</span>: <span class="hljs-type">Sequence</span>(feature=ClassLabel(names=[<span class="hljs-string">&quot;O&quot;</span>, <span class="hljs-string">&quot;B-LOC&quot;</span>, <span class="hljs-string">&quot;I-LOC&quot;</span>, <span class="hljs-string">&quot;B-PER&quot;</span>, <span class="hljs-string">&quot;I-PER&quot;</span>])),
        }),
)`}}),{c(){l=o("p"),b=c("For example, the following dataset format is accepted by the evaluator:"),i=u(),$(d.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);b=p(f,"For example, the following dataset format is accepted by the evaluator:"),f.forEach(a),i=m(s),w(d.$$.fragment,s)},m(s,f){_(s,l,f),e(l,b),_(s,i,f),y(d,s,f),v=!0},p:tt,i(s){v||(E(d.$$.fragment,s),v=!0)},o(s){q(d.$$.fragment,s),v=!1},d(s){s&&a(l),s&&a(i),k(d,s)}}}function Vo(T){let l,b;return l=new et({props:{anchor:"evaluate.TokenClassificationEvaluator.compute.example-2",$$slots:{default:[Mo]},$$scope:{ctx:T}}}),{c(){$(l.$$.fragment)},l(i){w(l.$$.fragment,i)},m(i,d){y(l,i,d),b=!0},p(i,d){const v={};d&2&&(v.$$scope={dirty:d,ctx:i}),l.$set(v)},i(i){b||(E(l.$$.fragment,i),b=!0)},o(i){q(l.$$.fragment,i),b=!1},d(i){k(l,i)}}}function Ro(T){let l,b,i,d,v,s,f,V;return f=new at({props:{code:`dataset = Dataset.from_dict(
    mapping={
        "tokens": [["New York is a city and Felix a person."]],
        "starts": [[0, 23]],
        "ends": [[7, 27]],
        "ner_tags": [["LOC", "PER"]],
    },
    features=Features({
        "tokens": Value(dtype="string"),
        "starts": Sequence(feature=Value(dtype="int32")),
        "ends": Sequence(feature=Value(dtype="int32")),
        "ner_tags": Sequence(feature=Value(dtype="string")),
    }),
)`,highlighted:`dataset = Dataset.from_dict(
    mapping={
        <span class="hljs-string">&quot;tokens&quot;</span>: [[<span class="hljs-string">&quot;New York is a city and Felix a person.&quot;</span>]],
        <span class="hljs-string">&quot;starts&quot;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">23</span>]],
        <span class="hljs-string">&quot;ends&quot;</span>: [[<span class="hljs-number">7</span>, <span class="hljs-number">27</span>]],
        <span class="hljs-string">&quot;ner_tags&quot;</span>: [[<span class="hljs-string">&quot;LOC&quot;</span>, <span class="hljs-string">&quot;PER&quot;</span>]],
    },
    features=Features({
        <span class="hljs-string">&quot;tokens&quot;</span>: Value(dtype=<span class="hljs-string">&quot;string&quot;</span>),
        <span class="hljs-string">&quot;starts&quot;</span>: <span class="hljs-type">Sequence</span>(feature=Value(dtype=<span class="hljs-string">&quot;int32&quot;</span>)),
        <span class="hljs-string">&quot;ends&quot;</span>: <span class="hljs-type">Sequence</span>(feature=Value(dtype=<span class="hljs-string">&quot;int32&quot;</span>)),
        <span class="hljs-string">&quot;ner_tags&quot;</span>: <span class="hljs-type">Sequence</span>(feature=Value(dtype=<span class="hljs-string">&quot;string&quot;</span>)),
    }),
)`}}),{c(){l=o("p"),b=c("For example, the following dataset format is "),i=o("strong"),d=c("not"),v=c(" accepted by the evaluator:"),s=u(),$(f.$$.fragment)},l(x){l=n(x,"P",{});var I=r(l);b=p(I,"For example, the following dataset format is "),i=n(I,"STRONG",{});var Y=r(i);d=p(Y,"not"),Y.forEach(a),v=p(I," accepted by the evaluator:"),I.forEach(a),s=m(x),w(f.$$.fragment,x)},m(x,I){_(x,l,I),e(l,b),e(l,i),e(i,d),e(l,v),_(x,s,I),y(f,x,I),V=!0},p:tt,i(x){V||(E(f.$$.fragment,x),V=!0)},o(x){q(f.$$.fragment,x),V=!1},d(x){x&&a(l),x&&a(s),k(f,x)}}}function Bo(T){let l,b;return l=new et({props:{anchor:"evaluate.TokenClassificationEvaluator.compute.example-3",$$slots:{default:[Ro]},$$scope:{ctx:T}}}),{c(){$(l.$$.fragment)},l(i){w(l.$$.fragment,i)},m(i,d){y(l,i,d),b=!0},p(i,d){const v={};d&2&&(v.$$scope={dirty:d,ctx:i}),l.$set(v)},i(i){b||(E(l.$$.fragment,i),b=!0)},o(i){q(l.$$.fragment,i),b=!1},d(i){k(l,i)}}}function Uo(T){let l,b,i,d,v,s,f,V,x,I,Y,la,Lt,W,Z,st,ve,ia,ot,ca,St,Ve,pa,Ot,L,_e,da,S,ua,Re,ma,fa,nt,ha,ga,rt,va,_a,ba,ee,zt,Be,$a,Mt,j,be,wa,lt,ya,Ea,te,$e,qa,it,ka,xa,ae,we,ja,ye,Ta,ct,Ca,Pa,Da,se,Ee,Ia,pt,Na,Fa,oe,qe,Aa,dt,La,Sa,ne,ke,Oa,ut,za,Vt,G,re,mt,xe,Ma,ft,Va,Rt,J,le,ht,je,Ra,gt,Ba,Bt,O,Te,Ua,z,Ha,Ue,Ya,Wa,vt,Ga,Ja,_t,Ka,Qa,Xa,R,Ce,Za,bt,es,ts,ie,Ut,K,ce,$t,Pe,as,wt,ss,Ht,M,De,os,N,ns,He,rs,ls,yt,is,cs,Et,ps,ds,qt,us,ms,fs,B,Ie,hs,kt,gs,vs,pe,Yt,Q,de,xt,Ne,_s,jt,bs,Wt,P,Fe,$s,Tt,ws,ys,X,Es,Ye,qs,ks,Ct,xs,js,Ts,Ae,Cs,Pt,Ps,Ds,Is,C,Le,Ns,Dt,Fs,As,Se,Ls,Oe,Ss,Os,zs,ue,Ms,me,Vs,fe,Gt;return s=new Ze({}),ve=new Ze({}),_e=new F({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:<ul>
<li><code>&quot;image-classification&quot;</code>: will return a <a href="/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator">ImageClassificationEvaluator</a>.</li>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
<li><code>&quot;token-classification&quot;</code>: will return a <a href="/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.TokenClassificationEvaluator">TokenClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/__init__.py#L79",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),ee=new et({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[Lo]},$$scope:{ctx:T}}}),be=new F({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L48"}}),$e=new F({props:{name:"compute_metric",anchor:"evaluate.Evaluator.compute_metric",parameters:[{name:"metric",val:": EvaluationModule"},{name:"metric_inputs",val:""},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L266"}}),we=new F({props:{name:"predictions_processor",anchor:"evaluate.Evaluator.predictions_processor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L108"}}),Ee=new F({props:{name:"prepare_data",anchor:"evaluate.Evaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"input_column",val:": str"},{name:"label_column",val:": str"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_data.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.Evaluator.prepare_data.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.prepare_data.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L162",returnDescription:`
<p>metric inputs.
<code>list</code>:  pipeline inputs.</p>
`,returnType:`
<p><code>dict</code></p>
`}}),qe=new F({props:{name:"prepare_metric",anchor:"evaluate.Evaluator.prepare_metric",parameters:[{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule]"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_metric.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L238",returnDescription:`
<p>The loaded metric.</p>
`}}),ke=new F({props:{name:"prepare_pipeline",anchor:"evaluate.Evaluator.prepare_pipeline",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"tokenizer",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"},{name:"feature_extractor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_pipeline.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.prepare_pipeline.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.Evaluator.prepare_pipeline.preprocessor",description:`<strong>preprocessor</strong> (<code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"preprocessor"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L194",returnDescription:`
<p>The initialized pipeline.</p>
`}}),xe=new Ze({}),je=new Ze({}),Te=new F({props:{name:"class evaluate.ImageClassificationEvaluator",anchor:"evaluate.ImageClassificationEvaluator",parameters:[{name:"task",val:" = 'image-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/image_classification.py#L20"}}),Ce=new F({props:{name:"compute",anchor:"evaluate.ImageClassificationEvaluator.compute",parameters:[{name:"input_column",val:": str = 'image'"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"evaluate.ImageClassificationEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>image-classification</code>. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.ImageClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.ImageClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>&#x201D; &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"},{anchor:"evaluate.ImageClassificationEvaluator.compute.feature_extractor",description:`<strong>feature_extractor</strong> (<code>str</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default feature extractor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"feature_extractor"},{anchor:"evaluate.ImageClassificationEvaluator.compute.strategy",description:`<strong>strategy</strong> (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;) &#x2014;
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"strategy"},{anchor:"evaluate.ImageClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.ImageClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.ImageClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.ImageClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;image&quot;</code>) &#x2014;
the name of the column containing the images as PIL ImageFile in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/image_classification.py#L37",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ie=new et({props:{anchor:"evaluate.ImageClassificationEvaluator.compute.example",$$slots:{default:[So]},$$scope:{ctx:T}}}),Pe=new Ze({}),De=new F({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/text_classification.py#L20"}}),Ie=new F({props:{name:"compute",anchor:"evaluate.TextClassificationEvaluator.compute",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"evaluate.TextClassificationEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.TextClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.TextClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;&#x201D;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"},{anchor:"evaluate.TextClassificationEvaluator.compute.tokenizer",description:`<strong>tokenizer</strong> (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"tokenizer"},{anchor:"evaluate.TextClassificationEvaluator.compute.strategy",description:`<strong>strategy</strong> (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;) &#x2014;
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"strategy"},{anchor:"evaluate.TextClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TextClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TextClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TextClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/text_classification.py#L41",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),pe=new et({props:{anchor:"evaluate.TextClassificationEvaluator.compute.example",$$slots:{default:[Oo]},$$scope:{ctx:T}}}),Ne=new Ze({}),Fe=new F({props:{name:"class evaluate.TokenClassificationEvaluator",anchor:"evaluate.TokenClassificationEvaluator",parameters:[{name:"task",val:" = 'token-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/token_classification.py#L23"}}),Le=new F({props:{name:"compute",anchor:"evaluate.TokenClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, ForwardRef('EvaluationModule')] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'tokens'"},{name:"label_column",val:": str = 'ner_tags'"},{name:"join_by",val:": typing.Optional[str] = ' '"}],parametersDescription:[{anchor:"evaluate.TokenClassificationEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>token-classification</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.TokenClassificationEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.TokenClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"},{anchor:"evaluate.TokenClassificationEvaluator.compute.tokenizer",description:`<strong>tokenizer</strong> (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"tokenizer"},{anchor:"evaluate.TokenClassificationEvaluator.compute.strategy",description:`<strong>strategy</strong> (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;) &#x2014;
specifies the evaluation strategy. Possible values are:</p>
<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"strategy"},{anchor:"evaluate.TokenClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TokenClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TokenClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TokenClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;tokens&quot;</code>) &#x2014;
the name of the column containing the tokens feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TokenClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TokenClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"},{anchor:"evaluate.TokenClassificationEvaluator.compute.join_by",description:`<strong>join_by</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot; &quot;</code>) &#x2014;
This evaluator supports dataset whose input column is a list of words. This parameter specifies how to join
words to generate a string input. This is especially useful for languages that do not separate words by a space.`,name:"join_by"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/token_classification.py#L127",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ue=new et({props:{anchor:"evaluate.TokenClassificationEvaluator.compute.example",$$slots:{default:[zo]},$$scope:{ctx:T}}}),me=new Po({props:{$$slots:{default:[Vo]},$$scope:{ctx:T}}}),fe=new Po({props:{warning:!0,$$slots:{default:[Bo]},$$scope:{ctx:T}}}),{c(){l=o("meta"),b=u(),i=o("h1"),d=o("a"),v=o("span"),$(s.$$.fragment),f=u(),V=o("span"),x=c("Evaluator"),I=u(),Y=o("p"),la=c("The evaluator classes for automatic evaluation."),Lt=u(),W=o("h2"),Z=o("a"),st=o("span"),$(ve.$$.fragment),ia=u(),ot=o("span"),ca=c("Evaluator classes"),St=u(),Ve=o("p"),pa=c("The main entry point for using the evaluator:"),Ot=u(),L=o("div"),$(_e.$$.fragment),da=u(),S=o("p"),ua=c("Utility factory method to build an "),Re=o("a"),ma=c("Evaluator"),fa=c(`.
Evaluators encapsulate a task and a default metric name. They leverage `),nt=o("code"),ha=c("pipeline"),ga=c(" functionalify from "),rt=o("code"),va=c("transformers"),_a=c(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),ba=u(),$(ee.$$.fragment),zt=u(),Be=o("p"),$a=c("The base class for all evaluator classes:"),Mt=u(),j=o("div"),$(be.$$.fragment),wa=u(),lt=o("p"),ya=c(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),Ea=u(),te=o("div"),$($e.$$.fragment),qa=u(),it=o("p"),ka=c("Compute and return metrics."),xa=u(),ae=o("div"),$(we.$$.fragment),ja=u(),ye=o("p"),Ta=c("A core method of the "),ct=o("code"),Ca=c("Evaluator"),Pa=c(" class, which processes the pipeline outputs for compatibility with the metric."),Da=u(),se=o("div"),$(Ee.$$.fragment),Ia=u(),pt=o("p"),Na=c("Prepare data."),Fa=u(),oe=o("div"),$(qe.$$.fragment),Aa=u(),dt=o("p"),La=c("Prepare metric."),Sa=u(),ne=o("div"),$(ke.$$.fragment),Oa=u(),ut=o("p"),za=c("Prepare pipeline."),Vt=u(),G=o("h2"),re=o("a"),mt=o("span"),$(xe.$$.fragment),Ma=u(),ft=o("span"),Va=c("The task specific evaluators"),Rt=u(),J=o("h3"),le=o("a"),ht=o("span"),$(je.$$.fragment),Ra=u(),gt=o("span"),Ba=c("ImageClassificationEvaluator"),Bt=u(),O=o("div"),$(Te.$$.fragment),Ua=u(),z=o("p"),Ha=c(`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Ue=o("a"),Ya=c("evaluator()"),Wa=c(` using the default task name
`),vt=o("code"),Ga=c("image-classification"),Ja=c(`.
Methods in this class assume a data format compatible with the `),_t=o("code"),Ka=c("ImageClassificationPipeline"),Qa=c("."),Xa=u(),R=o("div"),$(Ce.$$.fragment),Za=u(),bt=o("p"),es=c("Compute the metric for a given pipeline and dataset combination."),ts=u(),$(ie.$$.fragment),Ut=u(),K=o("h3"),ce=o("a"),$t=o("span"),$(Pe.$$.fragment),as=u(),wt=o("span"),ss=c("TextClassificationEvaluator"),Ht=u(),M=o("div"),$(De.$$.fragment),os=u(),N=o("p"),ns=c(`Text classification evaluator.
This text classification evaluator can currently be loaded from `),He=o("a"),rs=c("evaluator()"),ls=c(` using the default task name
`),yt=o("code"),is=c("text-classification"),cs=c(" or with a "),Et=o("code"),ps=c('"sentiment-analysis"'),ds=c(` alias.
Methods in this class assume a data format compatible with the `),qt=o("code"),us=c("TextClassificationPipeline"),ms=c(` - a single textual
feature as input and a categorical label as output.`),fs=u(),B=o("div"),$(Ie.$$.fragment),hs=u(),kt=o("p"),gs=c("Compute the metric for a given pipeline and dataset combination."),vs=u(),$(pe.$$.fragment),Yt=u(),Q=o("h3"),de=o("a"),xt=o("span"),$(Ne.$$.fragment),_s=u(),jt=o("span"),bs=c("TokenClassificationEvaluator"),Wt=u(),P=o("div"),$(Fe.$$.fragment),$s=u(),Tt=o("p"),ws=c("Token classification evaluator."),ys=u(),X=o("p"),Es=c("This token classification evaluator can currently be loaded from "),Ye=o("a"),qs=c("evaluator()"),ks=c(` using the default task name
`),Ct=o("code"),xs=c("token-classification"),js=c("."),Ts=u(),Ae=o("p"),Cs=c("Methods in this class assume a data format compatible with the "),Pt=o("code"),Ps=c("TokenClassificationPipeline"),Ds=c("."),Is=u(),C=o("div"),$(Le.$$.fragment),Ns=u(),Dt=o("p"),Fs=c("Compute the metric for a given pipeline and dataset combination."),As=u(),Se=o("p"),Ls=c("The dataset input and label columns are expected to be formatted as a list of words and a list of labels respectively, following "),Oe=o("a"),Ss=c("conll2003 dataset"),Os=c(". Datasets whose inputs are single strings, and labels are a list of offset are not supported."),zs=u(),$(ue.$$.fragment),Ms=u(),$(me.$$.fragment),Vs=u(),$(fe.$$.fragment),this.h()},l(t){const g=Fo('[data-svelte="svelte-1phssyn"]',document.head);l=n(g,"META",{name:!0,content:!0}),g.forEach(a),b=m(t),i=n(t,"H1",{class:!0});var ze=r(i);d=n(ze,"A",{id:!0,class:!0,href:!0});var It=r(d);v=n(It,"SPAN",{});var Nt=r(v);w(s.$$.fragment,Nt),Nt.forEach(a),It.forEach(a),f=m(ze),V=n(ze,"SPAN",{});var Ft=r(V);x=p(Ft,"Evaluator"),Ft.forEach(a),ze.forEach(a),I=m(t),Y=n(t,"P",{});var At=r(Y);la=p(At,"The evaluator classes for automatic evaluation."),At.forEach(a),Lt=m(t),W=n(t,"H2",{class:!0});var Me=r(W);Z=n(Me,"A",{id:!0,class:!0,href:!0});var Rs=r(Z);st=n(Rs,"SPAN",{});var Bs=r(st);w(ve.$$.fragment,Bs),Bs.forEach(a),Rs.forEach(a),ia=m(Me),ot=n(Me,"SPAN",{});var Us=r(ot);ca=p(Us,"Evaluator classes"),Us.forEach(a),Me.forEach(a),St=m(t),Ve=n(t,"P",{});var Hs=r(Ve);pa=p(Hs,"The main entry point for using the evaluator:"),Hs.forEach(a),Ot=m(t),L=n(t,"DIV",{class:!0});var We=r(L);w(_e.$$.fragment,We),da=m(We),S=n(We,"P",{});var he=r(S);ua=p(he,"Utility factory method to build an "),Re=n(he,"A",{href:!0});var Ys=r(Re);ma=p(Ys,"Evaluator"),Ys.forEach(a),fa=p(he,`.
Evaluators encapsulate a task and a default metric name. They leverage `),nt=n(he,"CODE",{});var Ws=r(nt);ha=p(Ws,"pipeline"),Ws.forEach(a),ga=p(he," functionalify from "),rt=n(he,"CODE",{});var Gs=r(rt);va=p(Gs,"transformers"),Gs.forEach(a),_a=p(he,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),he.forEach(a),ba=m(We),w(ee.$$.fragment,We),We.forEach(a),zt=m(t),Be=n(t,"P",{});var Js=r(Be);$a=p(Js,"The base class for all evaluator classes:"),Js.forEach(a),Mt=m(t),j=n(t,"DIV",{class:!0});var D=r(j);w(be.$$.fragment,D),wa=m(D),lt=n(D,"P",{});var Ks=r(lt);ya=p(Ks,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),Ks.forEach(a),Ea=m(D),te=n(D,"DIV",{class:!0});var Jt=r(te);w($e.$$.fragment,Jt),qa=m(Jt),it=n(Jt,"P",{});var Qs=r(it);ka=p(Qs,"Compute and return metrics."),Qs.forEach(a),Jt.forEach(a),xa=m(D),ae=n(D,"DIV",{class:!0});var Kt=r(ae);w(we.$$.fragment,Kt),ja=m(Kt),ye=n(Kt,"P",{});var Qt=r(ye);Ta=p(Qt,"A core method of the "),ct=n(Qt,"CODE",{});var Xs=r(ct);Ca=p(Xs,"Evaluator"),Xs.forEach(a),Pa=p(Qt," class, which processes the pipeline outputs for compatibility with the metric."),Qt.forEach(a),Kt.forEach(a),Da=m(D),se=n(D,"DIV",{class:!0});var Xt=r(se);w(Ee.$$.fragment,Xt),Ia=m(Xt),pt=n(Xt,"P",{});var Zs=r(pt);Na=p(Zs,"Prepare data."),Zs.forEach(a),Xt.forEach(a),Fa=m(D),oe=n(D,"DIV",{class:!0});var Zt=r(oe);w(qe.$$.fragment,Zt),Aa=m(Zt),dt=n(Zt,"P",{});var eo=r(dt);La=p(eo,"Prepare metric."),eo.forEach(a),Zt.forEach(a),Sa=m(D),ne=n(D,"DIV",{class:!0});var ea=r(ne);w(ke.$$.fragment,ea),Oa=m(ea),ut=n(ea,"P",{});var to=r(ut);za=p(to,"Prepare pipeline."),to.forEach(a),ea.forEach(a),D.forEach(a),Vt=m(t),G=n(t,"H2",{class:!0});var ta=r(G);re=n(ta,"A",{id:!0,class:!0,href:!0});var ao=r(re);mt=n(ao,"SPAN",{});var so=r(mt);w(xe.$$.fragment,so),so.forEach(a),ao.forEach(a),Ma=m(ta),ft=n(ta,"SPAN",{});var oo=r(ft);Va=p(oo,"The task specific evaluators"),oo.forEach(a),ta.forEach(a),Rt=m(t),J=n(t,"H3",{class:!0});var aa=r(J);le=n(aa,"A",{id:!0,class:!0,href:!0});var no=r(le);ht=n(no,"SPAN",{});var ro=r(ht);w(je.$$.fragment,ro),ro.forEach(a),no.forEach(a),Ra=m(aa),gt=n(aa,"SPAN",{});var lo=r(gt);Ba=p(lo,"ImageClassificationEvaluator"),lo.forEach(a),aa.forEach(a),Bt=m(t),O=n(t,"DIV",{class:!0});var Ge=r(O);w(Te.$$.fragment,Ge),Ua=m(Ge),z=n(Ge,"P",{});var ge=r(z);Ha=p(ge,`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Ue=n(ge,"A",{href:!0});var io=r(Ue);Ya=p(io,"evaluator()"),io.forEach(a),Wa=p(ge,` using the default task name
`),vt=n(ge,"CODE",{});var co=r(vt);Ga=p(co,"image-classification"),co.forEach(a),Ja=p(ge,`.
Methods in this class assume a data format compatible with the `),_t=n(ge,"CODE",{});var po=r(_t);Ka=p(po,"ImageClassificationPipeline"),po.forEach(a),Qa=p(ge,"."),ge.forEach(a),Xa=m(Ge),R=n(Ge,"DIV",{class:!0});var Je=r(R);w(Ce.$$.fragment,Je),Za=m(Je),bt=n(Je,"P",{});var uo=r(bt);es=p(uo,"Compute the metric for a given pipeline and dataset combination."),uo.forEach(a),ts=m(Je),w(ie.$$.fragment,Je),Je.forEach(a),Ge.forEach(a),Ut=m(t),K=n(t,"H3",{class:!0});var sa=r(K);ce=n(sa,"A",{id:!0,class:!0,href:!0});var mo=r(ce);$t=n(mo,"SPAN",{});var fo=r($t);w(Pe.$$.fragment,fo),fo.forEach(a),mo.forEach(a),as=m(sa),wt=n(sa,"SPAN",{});var ho=r(wt);ss=p(ho,"TextClassificationEvaluator"),ho.forEach(a),sa.forEach(a),Ht=m(t),M=n(t,"DIV",{class:!0});var Ke=r(M);w(De.$$.fragment,Ke),os=m(Ke),N=n(Ke,"P",{});var U=r(N);ns=p(U,`Text classification evaluator.
This text classification evaluator can currently be loaded from `),He=n(U,"A",{href:!0});var go=r(He);rs=p(go,"evaluator()"),go.forEach(a),ls=p(U,` using the default task name
`),yt=n(U,"CODE",{});var vo=r(yt);is=p(vo,"text-classification"),vo.forEach(a),cs=p(U," or with a "),Et=n(U,"CODE",{});var _o=r(Et);ps=p(_o,'"sentiment-analysis"'),_o.forEach(a),ds=p(U,` alias.
Methods in this class assume a data format compatible with the `),qt=n(U,"CODE",{});var bo=r(qt);us=p(bo,"TextClassificationPipeline"),bo.forEach(a),ms=p(U,` - a single textual
feature as input and a categorical label as output.`),U.forEach(a),fs=m(Ke),B=n(Ke,"DIV",{class:!0});var Qe=r(B);w(Ie.$$.fragment,Qe),hs=m(Qe),kt=n(Qe,"P",{});var $o=r(kt);gs=p($o,"Compute the metric for a given pipeline and dataset combination."),$o.forEach(a),vs=m(Qe),w(pe.$$.fragment,Qe),Qe.forEach(a),Ke.forEach(a),Yt=m(t),Q=n(t,"H3",{class:!0});var oa=r(Q);de=n(oa,"A",{id:!0,class:!0,href:!0});var wo=r(de);xt=n(wo,"SPAN",{});var yo=r(xt);w(Ne.$$.fragment,yo),yo.forEach(a),wo.forEach(a),_s=m(oa),jt=n(oa,"SPAN",{});var Eo=r(jt);bs=p(Eo,"TokenClassificationEvaluator"),Eo.forEach(a),oa.forEach(a),Wt=m(t),P=n(t,"DIV",{class:!0});var H=r(P);w(Fe.$$.fragment,H),$s=m(H),Tt=n(H,"P",{});var qo=r(Tt);ws=p(qo,"Token classification evaluator."),qo.forEach(a),ys=m(H),X=n(H,"P",{});var Xe=r(X);Es=p(Xe,"This token classification evaluator can currently be loaded from "),Ye=n(Xe,"A",{href:!0});var ko=r(Ye);qs=p(ko,"evaluator()"),ko.forEach(a),ks=p(Xe,` using the default task name
`),Ct=n(Xe,"CODE",{});var xo=r(Ct);xs=p(xo,"token-classification"),xo.forEach(a),js=p(Xe,"."),Xe.forEach(a),Ts=m(H),Ae=n(H,"P",{});var na=r(Ae);Cs=p(na,"Methods in this class assume a data format compatible with the "),Pt=n(na,"CODE",{});var jo=r(Pt);Ps=p(jo,"TokenClassificationPipeline"),jo.forEach(a),Ds=p(na,"."),na.forEach(a),Is=m(H),C=n(H,"DIV",{class:!0});var A=r(C);w(Le.$$.fragment,A),Ns=m(A),Dt=n(A,"P",{});var To=r(Dt);Fs=p(To,"Compute the metric for a given pipeline and dataset combination."),To.forEach(a),As=m(A),Se=n(A,"P",{});var ra=r(Se);Ls=p(ra,"The dataset input and label columns are expected to be formatted as a list of words and a list of labels respectively, following "),Oe=n(ra,"A",{href:!0,rel:!0});var Co=r(Oe);Ss=p(Co,"conll2003 dataset"),Co.forEach(a),Os=p(ra,". Datasets whose inputs are single strings, and labels are a list of offset are not supported."),ra.forEach(a),zs=m(A),w(ue.$$.fragment,A),Ms=m(A),w(me.$$.fragment,A),Vs=m(A),w(fe.$$.fragment,A),A.forEach(a),H.forEach(a),this.h()},h(){h(l,"name","hf:doc:metadata"),h(l,"content",JSON.stringify(Ho)),h(d,"id","evaluator"),h(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(d,"href","#evaluator"),h(i,"class","relative group"),h(Z,"id","evaluate.evaluator"),h(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Z,"href","#evaluate.evaluator"),h(W,"class","relative group"),h(Re,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.Evaluator"),h(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(re,"id","the-task-specific-evaluators"),h(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(re,"href","#the-task-specific-evaluators"),h(G,"class","relative group"),h(le,"id","evaluate.ImageClassificationEvaluator"),h(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(le,"href","#evaluate.ImageClassificationEvaluator"),h(J,"class","relative group"),h(Ue,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.evaluator"),h(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(ce,"id","evaluate.TextClassificationEvaluator"),h(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ce,"href","#evaluate.TextClassificationEvaluator"),h(K,"class","relative group"),h(He,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.evaluator"),h(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(de,"id","evaluate.TokenClassificationEvaluator"),h(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(de,"href","#evaluate.TokenClassificationEvaluator"),h(Q,"class","relative group"),h(Ye,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.evaluator"),h(Oe,"href","https://huggingface.co/datasets/conll2003"),h(Oe,"rel","nofollow"),h(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,g){e(document.head,l),_(t,b,g),_(t,i,g),e(i,d),e(d,v),y(s,v,null),e(i,f),e(i,V),e(V,x),_(t,I,g),_(t,Y,g),e(Y,la),_(t,Lt,g),_(t,W,g),e(W,Z),e(Z,st),y(ve,st,null),e(W,ia),e(W,ot),e(ot,ca),_(t,St,g),_(t,Ve,g),e(Ve,pa),_(t,Ot,g),_(t,L,g),y(_e,L,null),e(L,da),e(L,S),e(S,ua),e(S,Re),e(Re,ma),e(S,fa),e(S,nt),e(nt,ha),e(S,ga),e(S,rt),e(rt,va),e(S,_a),e(L,ba),y(ee,L,null),_(t,zt,g),_(t,Be,g),e(Be,$a),_(t,Mt,g),_(t,j,g),y(be,j,null),e(j,wa),e(j,lt),e(lt,ya),e(j,Ea),e(j,te),y($e,te,null),e(te,qa),e(te,it),e(it,ka),e(j,xa),e(j,ae),y(we,ae,null),e(ae,ja),e(ae,ye),e(ye,Ta),e(ye,ct),e(ct,Ca),e(ye,Pa),e(j,Da),e(j,se),y(Ee,se,null),e(se,Ia),e(se,pt),e(pt,Na),e(j,Fa),e(j,oe),y(qe,oe,null),e(oe,Aa),e(oe,dt),e(dt,La),e(j,Sa),e(j,ne),y(ke,ne,null),e(ne,Oa),e(ne,ut),e(ut,za),_(t,Vt,g),_(t,G,g),e(G,re),e(re,mt),y(xe,mt,null),e(G,Ma),e(G,ft),e(ft,Va),_(t,Rt,g),_(t,J,g),e(J,le),e(le,ht),y(je,ht,null),e(J,Ra),e(J,gt),e(gt,Ba),_(t,Bt,g),_(t,O,g),y(Te,O,null),e(O,Ua),e(O,z),e(z,Ha),e(z,Ue),e(Ue,Ya),e(z,Wa),e(z,vt),e(vt,Ga),e(z,Ja),e(z,_t),e(_t,Ka),e(z,Qa),e(O,Xa),e(O,R),y(Ce,R,null),e(R,Za),e(R,bt),e(bt,es),e(R,ts),y(ie,R,null),_(t,Ut,g),_(t,K,g),e(K,ce),e(ce,$t),y(Pe,$t,null),e(K,as),e(K,wt),e(wt,ss),_(t,Ht,g),_(t,M,g),y(De,M,null),e(M,os),e(M,N),e(N,ns),e(N,He),e(He,rs),e(N,ls),e(N,yt),e(yt,is),e(N,cs),e(N,Et),e(Et,ps),e(N,ds),e(N,qt),e(qt,us),e(N,ms),e(M,fs),e(M,B),y(Ie,B,null),e(B,hs),e(B,kt),e(kt,gs),e(B,vs),y(pe,B,null),_(t,Yt,g),_(t,Q,g),e(Q,de),e(de,xt),y(Ne,xt,null),e(Q,_s),e(Q,jt),e(jt,bs),_(t,Wt,g),_(t,P,g),y(Fe,P,null),e(P,$s),e(P,Tt),e(Tt,ws),e(P,ys),e(P,X),e(X,Es),e(X,Ye),e(Ye,qs),e(X,ks),e(X,Ct),e(Ct,xs),e(X,js),e(P,Ts),e(P,Ae),e(Ae,Cs),e(Ae,Pt),e(Pt,Ps),e(Ae,Ds),e(P,Is),e(P,C),y(Le,C,null),e(C,Ns),e(C,Dt),e(Dt,Fs),e(C,As),e(C,Se),e(Se,Ls),e(Se,Oe),e(Oe,Ss),e(Se,Os),e(C,zs),y(ue,C,null),e(C,Ms),y(me,C,null),e(C,Vs),y(fe,C,null),Gt=!0},p(t,[g]){const ze={};g&2&&(ze.$$scope={dirty:g,ctx:t}),ee.$set(ze);const It={};g&2&&(It.$$scope={dirty:g,ctx:t}),ie.$set(It);const Nt={};g&2&&(Nt.$$scope={dirty:g,ctx:t}),pe.$set(Nt);const Ft={};g&2&&(Ft.$$scope={dirty:g,ctx:t}),ue.$set(Ft);const At={};g&2&&(At.$$scope={dirty:g,ctx:t}),me.$set(At);const Me={};g&2&&(Me.$$scope={dirty:g,ctx:t}),fe.$set(Me)},i(t){Gt||(E(s.$$.fragment,t),E(ve.$$.fragment,t),E(_e.$$.fragment,t),E(ee.$$.fragment,t),E(be.$$.fragment,t),E($e.$$.fragment,t),E(we.$$.fragment,t),E(Ee.$$.fragment,t),E(qe.$$.fragment,t),E(ke.$$.fragment,t),E(xe.$$.fragment,t),E(je.$$.fragment,t),E(Te.$$.fragment,t),E(Ce.$$.fragment,t),E(ie.$$.fragment,t),E(Pe.$$.fragment,t),E(De.$$.fragment,t),E(Ie.$$.fragment,t),E(pe.$$.fragment,t),E(Ne.$$.fragment,t),E(Fe.$$.fragment,t),E(Le.$$.fragment,t),E(ue.$$.fragment,t),E(me.$$.fragment,t),E(fe.$$.fragment,t),Gt=!0)},o(t){q(s.$$.fragment,t),q(ve.$$.fragment,t),q(_e.$$.fragment,t),q(ee.$$.fragment,t),q(be.$$.fragment,t),q($e.$$.fragment,t),q(we.$$.fragment,t),q(Ee.$$.fragment,t),q(qe.$$.fragment,t),q(ke.$$.fragment,t),q(xe.$$.fragment,t),q(je.$$.fragment,t),q(Te.$$.fragment,t),q(Ce.$$.fragment,t),q(ie.$$.fragment,t),q(Pe.$$.fragment,t),q(De.$$.fragment,t),q(Ie.$$.fragment,t),q(pe.$$.fragment,t),q(Ne.$$.fragment,t),q(Fe.$$.fragment,t),q(Le.$$.fragment,t),q(ue.$$.fragment,t),q(me.$$.fragment,t),q(fe.$$.fragment,t),Gt=!1},d(t){a(l),t&&a(b),t&&a(i),k(s),t&&a(I),t&&a(Y),t&&a(Lt),t&&a(W),k(ve),t&&a(St),t&&a(Ve),t&&a(Ot),t&&a(L),k(_e),k(ee),t&&a(zt),t&&a(Be),t&&a(Mt),t&&a(j),k(be),k($e),k(we),k(Ee),k(qe),k(ke),t&&a(Vt),t&&a(G),k(xe),t&&a(Rt),t&&a(J),k(je),t&&a(Bt),t&&a(O),k(Te),k(Ce),k(ie),t&&a(Ut),t&&a(K),k(Pe),t&&a(Ht),t&&a(M),k(De),k(Ie),k(pe),t&&a(Yt),t&&a(Q),k(Ne),t&&a(Wt),t&&a(P),k(Fe),k(Le),k(ue),k(me),k(fe)}}}const Ho={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"},{local:"the-task-specific-evaluators",sections:[{local:"evaluate.ImageClassificationEvaluator",title:"ImageClassificationEvaluator"},{local:"evaluate.TextClassificationEvaluator",title:"TextClassificationEvaluator"},{local:"evaluate.TokenClassificationEvaluator",title:"TokenClassificationEvaluator"}],title:"The task specific evaluators"}],title:"Evaluator"};function Yo(T){return Ao(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Zo extends Do{constructor(l){super();Io(this,l,Yo,Uo,No,{})}}export{Zo as default,Ho as metadata};
