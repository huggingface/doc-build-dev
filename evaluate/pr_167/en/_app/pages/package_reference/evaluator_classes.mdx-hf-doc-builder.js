import{S as Rs,i as Vs,s as Bs,e as o,k as c,w as v,t as r,M as Hs,c as s,d as a,m as d,a as n,x as g,h as l,b as p,G as e,g as m,y as _,q as b,o as $,B as w,v as Ws,L as Jo}from"../../chunks/vendor-hf-doc-builder.js";import{D}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Ko}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Je}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Go}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Gs(G){let h,q,E,f,k;return f=new Ko({props:{code:`from evaluate import evaluator
# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){h=o("p"),q=r("Examples:"),E=c(),v(f.$$.fragment)},l(i){h=s(i,"P",{});var y=n(h);q=l(y,"Examples:"),y.forEach(a),E=d(i),g(f.$$.fragment,i)},m(i,y){m(i,h,y),e(h,q),m(i,E,y),_(f,i,y),k=!0},p:Jo,i(i){k||(b(f.$$.fragment,i),k=!0)},o(i){$(f.$$.fragment,i),k=!1},d(i){i&&a(h),i&&a(E),w(f,i)}}}function Js(G){let h,q,E,f,k;return f=new Ko({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("image-classification")
data =  Dataset.from_dict(load_dataset("beans")["test"][:2])
results = e.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    metric="accuracy",
    input_column="image",
    label_column="labels",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;image-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data =  Dataset.from_dict(load_dataset(<span class="hljs-string">&quot;beans&quot;</span>)[<span class="hljs-string">&quot;test&quot;</span>][:<span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;nateraw/vit-base-beans&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;image&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;labels&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&#x27;angular_leaf_spot&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;bean_rust&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>: <span class="hljs-number">2</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){h=o("p"),q=r("Examples:"),E=c(),v(f.$$.fragment)},l(i){h=s(i,"P",{});var y=n(h);q=l(y,"Examples:"),y.forEach(a),E=d(i),g(f.$$.fragment,i)},m(i,y){m(i,h,y),e(h,q),m(i,E,y),_(f,i,y),k=!0},p:Jo,i(i){k||(b(f.$$.fragment,i),k=!0)},o(i){$(f.$$.fragment,i),k=!1},d(i){i&&a(h),i&&a(E),w(f,i)}}}function Ks(G){let h,q,E,f,k;return f=new Ko({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("text-classification")
data =  Dataset.from_dict(load_dataset("imdb")["test"][:2])
results = e.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    input_column="text",
    label_column="label",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data =  Dataset.from_dict(load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)[<span class="hljs-string">&quot;test&quot;</span>][:<span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;text&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;label&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){h=o("p"),q=r("Examples:"),E=c(),v(f.$$.fragment)},l(i){h=s(i,"P",{});var y=n(h);q=l(y,"Examples:"),y.forEach(a),E=d(i),g(f.$$.fragment,i)},m(i,y){m(i,h,y),e(h,q),m(i,E,y),_(f,i,y),k=!0},p:Jo,i(i){k||(b(f.$$.fragment,i),k=!0)},o(i){$(f.$$.fragment,i),k=!1},d(i){i&&a(h),i&&a(E),w(f,i)}}}function Qs(G){let h,q,E,f,k,i,y,Ke,na,Pt,Ae,ra,Dt,S,J,Qe,de,la,Xe,ia,It,Le,ca,Nt,I,pe,da,N,pa,Me,ua,ma,Ye,fa,ha,Ze,va,ga,_a,K,At,ze,ba,Lt,x,ue,$a,et,wa,Ea,Q,me,ya,tt,ka,xa,X,fe,Ta,he,qa,at,Ca,ja,Pa,Y,ve,Da,ot,Ia,Na,Z,ge,Aa,st,La,Ma,ee,_e,za,nt,Fa,Mt,U,te,rt,be,Oa,lt,Sa,zt,R,ae,it,$e,Ua,ct,Ra,Ft,A,we,Va,L,Ba,Fe,Ha,Wa,dt,Ga,Ja,pt,Ka,Qa,Xa,z,Ee,Ya,ut,Za,eo,oe,Ot,V,se,mt,ye,to,ft,ao,St,M,ke,oo,P,so,Oe,no,ro,ht,lo,io,vt,co,po,gt,uo,mo,fo,F,xe,ho,_t,vo,go,ne,Ut,B,re,bt,Te,_o,$t,bo,Rt,T,qe,$o,wt,wo,Eo,H,yo,Se,ko,xo,Et,To,qo,Co,Ce,jo,yt,Po,Do,Io,kt,No,Ao,W,je,Lo,xt,Mo,zo,Fo,Pe,Oo,De,So,Uo,Ro,Tt,Vo,Bo,le,Ie,Ho,qt,Wo,Vt;return i=new Je({}),de=new Je({}),pe=new D({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:<ul>
<li><code>&quot;image-classification&quot;</code>: will return a <a href="/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator">ImageClassificationEvaluator</a>.</li>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
<li><code>&quot;token-classification&quot;</code>: will return a <a href="/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.TokenClassificationEvaluator">TokenClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/__init__.py#L79",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),K=new Go({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[Gs]},$$scope:{ctx:G}}}),ue=new D({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L48"}}),me=new D({props:{name:"compute_metric",anchor:"evaluate.Evaluator.compute_metric",parameters:[{name:"metric",val:": EvaluationModule"},{name:"metric_inputs",val:""},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L266"}}),fe=new D({props:{name:"predictions_processor",anchor:"evaluate.Evaluator.predictions_processor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L108"}}),ve=new D({props:{name:"prepare_data",anchor:"evaluate.Evaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"input_column",val:": str"},{name:"label_column",val:": str"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_data.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.Evaluator.prepare_data.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.prepare_data.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L162",returnDescription:`
<p>metric inputs.
<code>list</code>:  pipeline inputs.</p>
`,returnType:`
<p><code>dict</code></p>
`}}),ge=new D({props:{name:"prepare_metric",anchor:"evaluate.Evaluator.prepare_metric",parameters:[{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule]"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_metric.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L238",returnDescription:`
<p>The loaded metric.</p>
`}}),_e=new D({props:{name:"prepare_pipeline",anchor:"evaluate.Evaluator.prepare_pipeline",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"tokenizer",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"},{name:"feature_extractor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_pipeline.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.prepare_pipeline.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.Evaluator.prepare_pipeline.preprocessor",description:`<strong>preprocessor</strong> (<code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"preprocessor"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/base.py#L194",returnDescription:`
<p>The initialized pipeline.</p>
`}}),be=new Je({}),$e=new Je({}),we=new D({props:{name:"class evaluate.ImageClassificationEvaluator",anchor:"evaluate.ImageClassificationEvaluator",parameters:[{name:"task",val:" = 'image-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/image_classification.py#L20"}}),Ee=new D({props:{name:"compute",anchor:"evaluate.ImageClassificationEvaluator.compute",parameters:[{name:"input_column",val:": str = 'image'"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"evaluate.ImageClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.ImageClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>image-classification</code>. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.ImageClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.ImageClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>&#x201D; &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
feature_extractor &#x2014; (<code>str</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default feature extractor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.ImageClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.ImageClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.ImageClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.ImageClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;image&quot;</code>) &#x2014;
the name of the column containing the images as PIL ImageFile in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/image_classification.py#L37",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),oe=new Go({props:{anchor:"evaluate.ImageClassificationEvaluator.compute.example",$$slots:{default:[Js]},$$scope:{ctx:G}}}),ye=new Je({}),ke=new D({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/text_classification.py#L20"}}),xe=new D({props:{name:"compute",anchor:"evaluate.TextClassificationEvaluator.compute",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"evaluate.TextClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.TextClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.TextClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.TextClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;&#x201D;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.TextClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TextClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TextClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TextClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/text_classification.py#L41",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ne=new Go({props:{anchor:"evaluate.TextClassificationEvaluator.compute.example",$$slots:{default:[Ks]},$$scope:{ctx:G}}}),Te=new Je({}),qe=new D({props:{name:"class evaluate.TokenClassificationEvaluator",anchor:"evaluate.TokenClassificationEvaluator",parameters:[{name:"task",val:" = 'token-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/token_classification.py#L28"}}),Ie=new D({props:{name:"compute",anchor:"evaluate.TokenClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'tokens'"},{name:"label_column",val:": str = 'ner_tags'"},{name:"join_by",val:": typing.Optional[str] = ' '"}],parametersDescription:[{anchor:"evaluate.TokenClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.TokenClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>token-classification</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.TokenClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.TokenClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>&#x201D; &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:</p>
<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.TokenClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TokenClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TokenClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TokenClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;tokens&quot;</code>) &#x2014;
the name of the column containing the tokens feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TokenClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TokenClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"},{anchor:"evaluate.TokenClassificationEvaluator.compute.join_by",description:`<strong>join_by</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot; &quot;</code>) &#x2014;
This evaluator supports dataset whose input column is a list of words. This parameter specifies how to join
words to generate a string input. This is especially useful for languages that do not separate words by a space.`,name:"join_by"}],source:"https://github.com/huggingface/evaluate/blob/vr_167/src/evaluate/evaluator/token_classification.py#L127",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),{c(){h=o("meta"),q=c(),E=o("h1"),f=o("a"),k=o("span"),v(i.$$.fragment),y=c(),Ke=o("span"),na=r("Evaluator"),Pt=c(),Ae=o("p"),ra=r("The evaluator classes for automatic evaluation."),Dt=c(),S=o("h2"),J=o("a"),Qe=o("span"),v(de.$$.fragment),la=c(),Xe=o("span"),ia=r("Evaluator classes"),It=c(),Le=o("p"),ca=r("The main entry point for using the evaluator:"),Nt=c(),I=o("div"),v(pe.$$.fragment),da=c(),N=o("p"),pa=r("Utility factory method to build an "),Me=o("a"),ua=r("Evaluator"),ma=r(`.
Evaluators encapsulate a task and a default metric name. They leverage `),Ye=o("code"),fa=r("pipeline"),ha=r(" functionalify from "),Ze=o("code"),va=r("transformers"),ga=r(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),_a=c(),v(K.$$.fragment),At=c(),ze=o("p"),ba=r("The base class for all evaluator classes:"),Lt=c(),x=o("div"),v(ue.$$.fragment),$a=c(),et=o("p"),wa=r(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),Ea=c(),Q=o("div"),v(me.$$.fragment),ya=c(),tt=o("p"),ka=r("Compute and return metrics."),xa=c(),X=o("div"),v(fe.$$.fragment),Ta=c(),he=o("p"),qa=r("A core method of the "),at=o("code"),Ca=r("Evaluator"),ja=r(" class, which processes the pipeline outputs for compatibility with the metric."),Pa=c(),Y=o("div"),v(ve.$$.fragment),Da=c(),ot=o("p"),Ia=r("Prepare data."),Na=c(),Z=o("div"),v(ge.$$.fragment),Aa=c(),st=o("p"),La=r("Prepare metric."),Ma=c(),ee=o("div"),v(_e.$$.fragment),za=c(),nt=o("p"),Fa=r("Prepare pipeline."),Mt=c(),U=o("h2"),te=o("a"),rt=o("span"),v(be.$$.fragment),Oa=c(),lt=o("span"),Sa=r("The task specific evaluators"),zt=c(),R=o("h3"),ae=o("a"),it=o("span"),v($e.$$.fragment),Ua=c(),ct=o("span"),Ra=r("ImageClassificationEvaluator"),Ft=c(),A=o("div"),v(we.$$.fragment),Va=c(),L=o("p"),Ba=r(`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Fe=o("a"),Ha=r("evaluator()"),Wa=r(` using the default task name
`),dt=o("code"),Ga=r("image-classification"),Ja=r(`.
Methods in this class assume a data format compatible with the `),pt=o("code"),Ka=r("ImageClassificationPipeline"),Qa=r("."),Xa=c(),z=o("div"),v(Ee.$$.fragment),Ya=c(),ut=o("p"),Za=r("Compute the metric for a given pipeline and dataset combination."),eo=c(),v(oe.$$.fragment),Ot=c(),V=o("h3"),se=o("a"),mt=o("span"),v(ye.$$.fragment),to=c(),ft=o("span"),ao=r("TextClassificationEvaluator"),St=c(),M=o("div"),v(ke.$$.fragment),oo=c(),P=o("p"),so=r(`Text classification evaluator.
This text classification evaluator can currently be loaded from `),Oe=o("a"),no=r("evaluator()"),ro=r(` using the default task name
`),ht=o("code"),lo=r("text-classification"),io=r(" or with a "),vt=o("code"),co=r('"sentiment-analysis"'),po=r(` alias.
Methods in this class assume a data format compatible with the `),gt=o("code"),uo=r("TextClassificationPipeline"),mo=r(` - a single textual
feature as input and a categorical label as output.`),fo=c(),F=o("div"),v(xe.$$.fragment),ho=c(),_t=o("p"),vo=r("Compute the metric for a given pipeline and dataset combination."),go=c(),v(ne.$$.fragment),Ut=c(),B=o("h3"),re=o("a"),bt=o("span"),v(Te.$$.fragment),_o=c(),$t=o("span"),bo=r("TokenClassificationEvaluator"),Rt=c(),T=o("div"),v(qe.$$.fragment),$o=c(),wt=o("p"),wo=r("Token classification evaluator."),Eo=c(),H=o("p"),yo=r("This token classification evaluator can currently be loaded from "),Se=o("a"),ko=r("evaluator()"),xo=r(` using the default task name
`),Et=o("code"),To=r("token-classification"),qo=r("."),Co=c(),Ce=o("p"),jo=r("Methods in this class assume a data format compatible with the "),yt=o("code"),Po=r("TokenClassificationPipeline"),Do=r("."),Io=c(),kt=o("p"),No=r("In particular, the following cases are not handled:"),Ao=c(),W=o("ul"),je=o("li"),Lo=r("models not splitting tokens by space (e.g. where "),xt=o("code"),Mo=r('"he oh"'),zo=r(" can be a token)."),Fo=c(),Pe=o("li"),Oo=r("datasets as "),De=o("a"),So=r("https://huggingface.co/datasets/msra_ner"),Uo=r(" , where tokens are provided ideogram by ideogram, and where a tokenizer may map several ideograms to a single token."),Ro=c(),Tt=o("li"),Vo=r("datasets not providing the input and reference columns as a list of \u201Cwords\u201D as the conll2003 dataset."),Bo=c(),le=o("div"),v(Ie.$$.fragment),Ho=c(),qt=o("p"),Wo=r("Compute the metric for a given pipeline and dataset combination."),this.h()},l(t){const u=Hs('[data-svelte="svelte-1phssyn"]',document.head);h=s(u,"META",{name:!0,content:!0}),u.forEach(a),q=d(t),E=s(t,"H1",{class:!0});var Ne=n(E);f=s(Ne,"A",{id:!0,class:!0,href:!0});var Ct=n(f);k=s(Ct,"SPAN",{});var jt=n(k);g(i.$$.fragment,jt),jt.forEach(a),Ct.forEach(a),y=d(Ne),Ke=s(Ne,"SPAN",{});var Qo=n(Ke);na=l(Qo,"Evaluator"),Qo.forEach(a),Ne.forEach(a),Pt=d(t),Ae=s(t,"P",{});var Xo=n(Ae);ra=l(Xo,"The evaluator classes for automatic evaluation."),Xo.forEach(a),Dt=d(t),S=s(t,"H2",{class:!0});var Bt=n(S);J=s(Bt,"A",{id:!0,class:!0,href:!0});var Yo=n(J);Qe=s(Yo,"SPAN",{});var Zo=n(Qe);g(de.$$.fragment,Zo),Zo.forEach(a),Yo.forEach(a),la=d(Bt),Xe=s(Bt,"SPAN",{});var es=n(Xe);ia=l(es,"Evaluator classes"),es.forEach(a),Bt.forEach(a),It=d(t),Le=s(t,"P",{});var ts=n(Le);ca=l(ts,"The main entry point for using the evaluator:"),ts.forEach(a),Nt=d(t),I=s(t,"DIV",{class:!0});var Ue=n(I);g(pe.$$.fragment,Ue),da=d(Ue),N=s(Ue,"P",{});var ie=n(N);pa=l(ie,"Utility factory method to build an "),Me=s(ie,"A",{href:!0});var as=n(Me);ua=l(as,"Evaluator"),as.forEach(a),ma=l(ie,`.
Evaluators encapsulate a task and a default metric name. They leverage `),Ye=s(ie,"CODE",{});var os=n(Ye);fa=l(os,"pipeline"),os.forEach(a),ha=l(ie," functionalify from "),Ze=s(ie,"CODE",{});var ss=n(Ze);va=l(ss,"transformers"),ss.forEach(a),ga=l(ie,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),ie.forEach(a),_a=d(Ue),g(K.$$.fragment,Ue),Ue.forEach(a),At=d(t),ze=s(t,"P",{});var ns=n(ze);ba=l(ns,"The base class for all evaluator classes:"),ns.forEach(a),Lt=d(t),x=s(t,"DIV",{class:!0});var C=n(x);g(ue.$$.fragment,C),$a=d(C),et=s(C,"P",{});var rs=n(et);wa=l(rs,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),rs.forEach(a),Ea=d(C),Q=s(C,"DIV",{class:!0});var Ht=n(Q);g(me.$$.fragment,Ht),ya=d(Ht),tt=s(Ht,"P",{});var ls=n(tt);ka=l(ls,"Compute and return metrics."),ls.forEach(a),Ht.forEach(a),xa=d(C),X=s(C,"DIV",{class:!0});var Wt=n(X);g(fe.$$.fragment,Wt),Ta=d(Wt),he=s(Wt,"P",{});var Gt=n(he);qa=l(Gt,"A core method of the "),at=s(Gt,"CODE",{});var is=n(at);Ca=l(is,"Evaluator"),is.forEach(a),ja=l(Gt," class, which processes the pipeline outputs for compatibility with the metric."),Gt.forEach(a),Wt.forEach(a),Pa=d(C),Y=s(C,"DIV",{class:!0});var Jt=n(Y);g(ve.$$.fragment,Jt),Da=d(Jt),ot=s(Jt,"P",{});var cs=n(ot);Ia=l(cs,"Prepare data."),cs.forEach(a),Jt.forEach(a),Na=d(C),Z=s(C,"DIV",{class:!0});var Kt=n(Z);g(ge.$$.fragment,Kt),Aa=d(Kt),st=s(Kt,"P",{});var ds=n(st);La=l(ds,"Prepare metric."),ds.forEach(a),Kt.forEach(a),Ma=d(C),ee=s(C,"DIV",{class:!0});var Qt=n(ee);g(_e.$$.fragment,Qt),za=d(Qt),nt=s(Qt,"P",{});var ps=n(nt);Fa=l(ps,"Prepare pipeline."),ps.forEach(a),Qt.forEach(a),C.forEach(a),Mt=d(t),U=s(t,"H2",{class:!0});var Xt=n(U);te=s(Xt,"A",{id:!0,class:!0,href:!0});var us=n(te);rt=s(us,"SPAN",{});var ms=n(rt);g(be.$$.fragment,ms),ms.forEach(a),us.forEach(a),Oa=d(Xt),lt=s(Xt,"SPAN",{});var fs=n(lt);Sa=l(fs,"The task specific evaluators"),fs.forEach(a),Xt.forEach(a),zt=d(t),R=s(t,"H3",{class:!0});var Yt=n(R);ae=s(Yt,"A",{id:!0,class:!0,href:!0});var hs=n(ae);it=s(hs,"SPAN",{});var vs=n(it);g($e.$$.fragment,vs),vs.forEach(a),hs.forEach(a),Ua=d(Yt),ct=s(Yt,"SPAN",{});var gs=n(ct);Ra=l(gs,"ImageClassificationEvaluator"),gs.forEach(a),Yt.forEach(a),Ft=d(t),A=s(t,"DIV",{class:!0});var Re=n(A);g(we.$$.fragment,Re),Va=d(Re),L=s(Re,"P",{});var ce=n(L);Ba=l(ce,`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Fe=s(ce,"A",{href:!0});var _s=n(Fe);Ha=l(_s,"evaluator()"),_s.forEach(a),Wa=l(ce,` using the default task name
`),dt=s(ce,"CODE",{});var bs=n(dt);Ga=l(bs,"image-classification"),bs.forEach(a),Ja=l(ce,`.
Methods in this class assume a data format compatible with the `),pt=s(ce,"CODE",{});var $s=n(pt);Ka=l($s,"ImageClassificationPipeline"),$s.forEach(a),Qa=l(ce,"."),ce.forEach(a),Xa=d(Re),z=s(Re,"DIV",{class:!0});var Ve=n(z);g(Ee.$$.fragment,Ve),Ya=d(Ve),ut=s(Ve,"P",{});var ws=n(ut);Za=l(ws,"Compute the metric for a given pipeline and dataset combination."),ws.forEach(a),eo=d(Ve),g(oe.$$.fragment,Ve),Ve.forEach(a),Re.forEach(a),Ot=d(t),V=s(t,"H3",{class:!0});var Zt=n(V);se=s(Zt,"A",{id:!0,class:!0,href:!0});var Es=n(se);mt=s(Es,"SPAN",{});var ys=n(mt);g(ye.$$.fragment,ys),ys.forEach(a),Es.forEach(a),to=d(Zt),ft=s(Zt,"SPAN",{});var ks=n(ft);ao=l(ks,"TextClassificationEvaluator"),ks.forEach(a),Zt.forEach(a),St=d(t),M=s(t,"DIV",{class:!0});var Be=n(M);g(ke.$$.fragment,Be),oo=d(Be),P=s(Be,"P",{});var O=n(P);so=l(O,`Text classification evaluator.
This text classification evaluator can currently be loaded from `),Oe=s(O,"A",{href:!0});var xs=n(Oe);no=l(xs,"evaluator()"),xs.forEach(a),ro=l(O,` using the default task name
`),ht=s(O,"CODE",{});var Ts=n(ht);lo=l(Ts,"text-classification"),Ts.forEach(a),io=l(O," or with a "),vt=s(O,"CODE",{});var qs=n(vt);co=l(qs,'"sentiment-analysis"'),qs.forEach(a),po=l(O,` alias.
Methods in this class assume a data format compatible with the `),gt=s(O,"CODE",{});var Cs=n(gt);uo=l(Cs,"TextClassificationPipeline"),Cs.forEach(a),mo=l(O,` - a single textual
feature as input and a categorical label as output.`),O.forEach(a),fo=d(Be),F=s(Be,"DIV",{class:!0});var He=n(F);g(xe.$$.fragment,He),ho=d(He),_t=s(He,"P",{});var js=n(_t);vo=l(js,"Compute the metric for a given pipeline and dataset combination."),js.forEach(a),go=d(He),g(ne.$$.fragment,He),He.forEach(a),Be.forEach(a),Ut=d(t),B=s(t,"H3",{class:!0});var ea=n(B);re=s(ea,"A",{id:!0,class:!0,href:!0});var Ps=n(re);bt=s(Ps,"SPAN",{});var Ds=n(bt);g(Te.$$.fragment,Ds),Ds.forEach(a),Ps.forEach(a),_o=d(ea),$t=s(ea,"SPAN",{});var Is=n($t);bo=l(Is,"TokenClassificationEvaluator"),Is.forEach(a),ea.forEach(a),Rt=d(t),T=s(t,"DIV",{class:!0});var j=n(T);g(qe.$$.fragment,j),$o=d(j),wt=s(j,"P",{});var Ns=n(wt);wo=l(Ns,"Token classification evaluator."),Ns.forEach(a),Eo=d(j),H=s(j,"P",{});var We=n(H);yo=l(We,"This token classification evaluator can currently be loaded from "),Se=s(We,"A",{href:!0});var As=n(Se);ko=l(As,"evaluator()"),As.forEach(a),xo=l(We,` using the default task name
`),Et=s(We,"CODE",{});var Ls=n(Et);To=l(Ls,"token-classification"),Ls.forEach(a),qo=l(We,"."),We.forEach(a),Co=d(j),Ce=s(j,"P",{});var ta=n(Ce);jo=l(ta,"Methods in this class assume a data format compatible with the "),yt=s(ta,"CODE",{});var Ms=n(yt);Po=l(Ms,"TokenClassificationPipeline"),Ms.forEach(a),Do=l(ta,"."),ta.forEach(a),Io=d(j),kt=s(j,"P",{});var zs=n(kt);No=l(zs,"In particular, the following cases are not handled:"),zs.forEach(a),Ao=d(j),W=s(j,"UL",{});var Ge=n(W);je=s(Ge,"LI",{});var aa=n(je);Lo=l(aa,"models not splitting tokens by space (e.g. where "),xt=s(aa,"CODE",{});var Fs=n(xt);Mo=l(Fs,'"he oh"'),Fs.forEach(a),zo=l(aa," can be a token)."),aa.forEach(a),Fo=d(Ge),Pe=s(Ge,"LI",{});var oa=n(Pe);Oo=l(oa,"datasets as "),De=s(oa,"A",{href:!0,rel:!0});var Os=n(De);So=l(Os,"https://huggingface.co/datasets/msra_ner"),Os.forEach(a),Uo=l(oa," , where tokens are provided ideogram by ideogram, and where a tokenizer may map several ideograms to a single token."),oa.forEach(a),Ro=d(Ge),Tt=s(Ge,"LI",{});var Ss=n(Tt);Vo=l(Ss,"datasets not providing the input and reference columns as a list of \u201Cwords\u201D as the conll2003 dataset."),Ss.forEach(a),Ge.forEach(a),Bo=d(j),le=s(j,"DIV",{class:!0});var sa=n(le);g(Ie.$$.fragment,sa),Ho=d(sa),qt=s(sa,"P",{});var Us=n(qt);Wo=l(Us,"Compute the metric for a given pipeline and dataset combination."),Us.forEach(a),sa.forEach(a),j.forEach(a),this.h()},h(){p(h,"name","hf:doc:metadata"),p(h,"content",JSON.stringify(Xs)),p(f,"id","evaluator"),p(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(f,"href","#evaluator"),p(E,"class","relative group"),p(J,"id","evaluate.evaluator"),p(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(J,"href","#evaluate.evaluator"),p(S,"class","relative group"),p(Me,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.Evaluator"),p(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(te,"id","the-task-specific-evaluators"),p(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(te,"href","#the-task-specific-evaluators"),p(U,"class","relative group"),p(ae,"id","evaluate.ImageClassificationEvaluator"),p(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ae,"href","#evaluate.ImageClassificationEvaluator"),p(R,"class","relative group"),p(Fe,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.evaluator"),p(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(se,"id","evaluate.TextClassificationEvaluator"),p(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(se,"href","#evaluate.TextClassificationEvaluator"),p(V,"class","relative group"),p(Oe,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.evaluator"),p(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(re,"id","evaluate.TokenClassificationEvaluator"),p(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(re,"href","#evaluate.TokenClassificationEvaluator"),p(B,"class","relative group"),p(Se,"href","/docs/evaluate/pr_167/en/package_reference/evaluator_classes#evaluate.evaluator"),p(De,"href","https://huggingface.co/datasets/msra_ner"),p(De,"rel","nofollow"),p(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,u){e(document.head,h),m(t,q,u),m(t,E,u),e(E,f),e(f,k),_(i,k,null),e(E,y),e(E,Ke),e(Ke,na),m(t,Pt,u),m(t,Ae,u),e(Ae,ra),m(t,Dt,u),m(t,S,u),e(S,J),e(J,Qe),_(de,Qe,null),e(S,la),e(S,Xe),e(Xe,ia),m(t,It,u),m(t,Le,u),e(Le,ca),m(t,Nt,u),m(t,I,u),_(pe,I,null),e(I,da),e(I,N),e(N,pa),e(N,Me),e(Me,ua),e(N,ma),e(N,Ye),e(Ye,fa),e(N,ha),e(N,Ze),e(Ze,va),e(N,ga),e(I,_a),_(K,I,null),m(t,At,u),m(t,ze,u),e(ze,ba),m(t,Lt,u),m(t,x,u),_(ue,x,null),e(x,$a),e(x,et),e(et,wa),e(x,Ea),e(x,Q),_(me,Q,null),e(Q,ya),e(Q,tt),e(tt,ka),e(x,xa),e(x,X),_(fe,X,null),e(X,Ta),e(X,he),e(he,qa),e(he,at),e(at,Ca),e(he,ja),e(x,Pa),e(x,Y),_(ve,Y,null),e(Y,Da),e(Y,ot),e(ot,Ia),e(x,Na),e(x,Z),_(ge,Z,null),e(Z,Aa),e(Z,st),e(st,La),e(x,Ma),e(x,ee),_(_e,ee,null),e(ee,za),e(ee,nt),e(nt,Fa),m(t,Mt,u),m(t,U,u),e(U,te),e(te,rt),_(be,rt,null),e(U,Oa),e(U,lt),e(lt,Sa),m(t,zt,u),m(t,R,u),e(R,ae),e(ae,it),_($e,it,null),e(R,Ua),e(R,ct),e(ct,Ra),m(t,Ft,u),m(t,A,u),_(we,A,null),e(A,Va),e(A,L),e(L,Ba),e(L,Fe),e(Fe,Ha),e(L,Wa),e(L,dt),e(dt,Ga),e(L,Ja),e(L,pt),e(pt,Ka),e(L,Qa),e(A,Xa),e(A,z),_(Ee,z,null),e(z,Ya),e(z,ut),e(ut,Za),e(z,eo),_(oe,z,null),m(t,Ot,u),m(t,V,u),e(V,se),e(se,mt),_(ye,mt,null),e(V,to),e(V,ft),e(ft,ao),m(t,St,u),m(t,M,u),_(ke,M,null),e(M,oo),e(M,P),e(P,so),e(P,Oe),e(Oe,no),e(P,ro),e(P,ht),e(ht,lo),e(P,io),e(P,vt),e(vt,co),e(P,po),e(P,gt),e(gt,uo),e(P,mo),e(M,fo),e(M,F),_(xe,F,null),e(F,ho),e(F,_t),e(_t,vo),e(F,go),_(ne,F,null),m(t,Ut,u),m(t,B,u),e(B,re),e(re,bt),_(Te,bt,null),e(B,_o),e(B,$t),e($t,bo),m(t,Rt,u),m(t,T,u),_(qe,T,null),e(T,$o),e(T,wt),e(wt,wo),e(T,Eo),e(T,H),e(H,yo),e(H,Se),e(Se,ko),e(H,xo),e(H,Et),e(Et,To),e(H,qo),e(T,Co),e(T,Ce),e(Ce,jo),e(Ce,yt),e(yt,Po),e(Ce,Do),e(T,Io),e(T,kt),e(kt,No),e(T,Ao),e(T,W),e(W,je),e(je,Lo),e(je,xt),e(xt,Mo),e(je,zo),e(W,Fo),e(W,Pe),e(Pe,Oo),e(Pe,De),e(De,So),e(Pe,Uo),e(W,Ro),e(W,Tt),e(Tt,Vo),e(T,Bo),e(T,le),_(Ie,le,null),e(le,Ho),e(le,qt),e(qt,Wo),Vt=!0},p(t,[u]){const Ne={};u&2&&(Ne.$$scope={dirty:u,ctx:t}),K.$set(Ne);const Ct={};u&2&&(Ct.$$scope={dirty:u,ctx:t}),oe.$set(Ct);const jt={};u&2&&(jt.$$scope={dirty:u,ctx:t}),ne.$set(jt)},i(t){Vt||(b(i.$$.fragment,t),b(de.$$.fragment,t),b(pe.$$.fragment,t),b(K.$$.fragment,t),b(ue.$$.fragment,t),b(me.$$.fragment,t),b(fe.$$.fragment,t),b(ve.$$.fragment,t),b(ge.$$.fragment,t),b(_e.$$.fragment,t),b(be.$$.fragment,t),b($e.$$.fragment,t),b(we.$$.fragment,t),b(Ee.$$.fragment,t),b(oe.$$.fragment,t),b(ye.$$.fragment,t),b(ke.$$.fragment,t),b(xe.$$.fragment,t),b(ne.$$.fragment,t),b(Te.$$.fragment,t),b(qe.$$.fragment,t),b(Ie.$$.fragment,t),Vt=!0)},o(t){$(i.$$.fragment,t),$(de.$$.fragment,t),$(pe.$$.fragment,t),$(K.$$.fragment,t),$(ue.$$.fragment,t),$(me.$$.fragment,t),$(fe.$$.fragment,t),$(ve.$$.fragment,t),$(ge.$$.fragment,t),$(_e.$$.fragment,t),$(be.$$.fragment,t),$($e.$$.fragment,t),$(we.$$.fragment,t),$(Ee.$$.fragment,t),$(oe.$$.fragment,t),$(ye.$$.fragment,t),$(ke.$$.fragment,t),$(xe.$$.fragment,t),$(ne.$$.fragment,t),$(Te.$$.fragment,t),$(qe.$$.fragment,t),$(Ie.$$.fragment,t),Vt=!1},d(t){a(h),t&&a(q),t&&a(E),w(i),t&&a(Pt),t&&a(Ae),t&&a(Dt),t&&a(S),w(de),t&&a(It),t&&a(Le),t&&a(Nt),t&&a(I),w(pe),w(K),t&&a(At),t&&a(ze),t&&a(Lt),t&&a(x),w(ue),w(me),w(fe),w(ve),w(ge),w(_e),t&&a(Mt),t&&a(U),w(be),t&&a(zt),t&&a(R),w($e),t&&a(Ft),t&&a(A),w(we),w(Ee),w(oe),t&&a(Ot),t&&a(V),w(ye),t&&a(St),t&&a(M),w(ke),w(xe),w(ne),t&&a(Ut),t&&a(B),w(Te),t&&a(Rt),t&&a(T),w(qe),w(Ie)}}}const Xs={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"},{local:"the-task-specific-evaluators",sections:[{local:"evaluate.ImageClassificationEvaluator",title:"ImageClassificationEvaluator"},{local:"evaluate.TextClassificationEvaluator",title:"TextClassificationEvaluator"},{local:"evaluate.TokenClassificationEvaluator",title:"TokenClassificationEvaluator"}],title:"The task specific evaluators"}],title:"Evaluator"};function Ys(G){return Ws(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class sn extends Rs{constructor(h){super();Vs(this,h,Ys,Qs,Bs,{})}}export{sn as default,Xs as metadata};
