import{S as _o,i as Eo,s as $o,e as o,k as c,w as R,t as s,M as ko,c as r,d as t,m as u,a as i,x as L,h as l,b as n,$ as wo,G as a,g as f,y as q,L as Po,q as F,o as j,B as z,v as Ao}from"../chunks/vendor-hf-doc-builder.js";import{I as B}from"../chunks/IconCopyLink-hf-doc-builder.js";function xo(Ea){let v,Fe,y,P,ue,J,_t,de,Et,je,A,$t,pe,kt,Pt,ze,x,At,me,xt,St,Be,g,S,ve,U,Ot,ye,Mt,Je,d,It,ge,Nt,Ct,be,Dt,Ht,we,Tt,Gt,_e,Rt,Lt,Ee,qt,Ft,$e,jt,zt,ke,Bt,Jt,Ue,O,Ut,K,Kt,Qt,Ke,b,M,Pe,Q,Vt,Ae,Wt,Qe,I,Xt,xe,Yt,Zt,Ve,m,ea,Se,V,ta,aa,Oe,W,oa,ra,Me,X,ia,sa,We,oe,la,Xe,re,ie,$a,Ye,se,na,Ze,le,ne,ka,et,w,N,Ie,Y,ha,Ne,fa,tt,he,ca,at,fe,ua,ot,_,C,Ce,Z,da,De,pa,rt,E,D,He,ee,ma,Te,va,it,ce,ya,st,$,H,Ge,te,ga,Re,ba,lt,k,T,Le,ae,wa,qe,_a,nt;return J=new B({}),U=new B({}),Q=new B({}),Y=new B({}),Z=new B({}),ee=new B({}),te=new B({}),ae=new B({}),{c(){v=o("meta"),Fe=c(),y=o("h1"),P=o("a"),ue=o("span"),R(J.$$.fragment),_t=c(),de=o("span"),Et=s("Considerations for model evaluation"),je=c(),A=o("p"),$t=s("Developing an ML model is rarely a one-shot deal: it often involves multiple stages of defining the model architecture and tuning hyper-parameters before converging on a final set. Responsible model evaluation is a key part of this process, and the \u{1F917} "),pe=o("code"),kt=s("evaluate"),Pt=s(" package is here to help!"),ze=c(),x=o("p"),At=s("Here are some things to keep in mind when evaluating your model using the \u{1F917} "),me=o("code"),xt=s("evaluate"),St=s(" package:"),Be=c(),g=o("h2"),S=o("a"),ve=o("span"),R(U.$$.fragment),Ot=c(),ye=o("span"),Mt=s("Properly splitting your data"),Je=c(),d=o("p"),It=s("Many of the datasets on the \u{1F917} Hub are separated into 2 splits: "),ge=o("code"),Nt=s("train"),Ct=s(" (used for training models) and "),be=o("code"),Dt=s("validation"),Ht=s(" (used for evaluation); others are split into 3 splits: "),we=o("code"),Tt=s("train"),Gt=s(", "),_e=o("code"),Rt=s("validation"),Lt=s(" and "),Ee=o("code"),qt=s("test"),Ft=s(" \u2014 in this case, the "),$e=o("code"),jt=s("validation"),zt=s(" split can be used for tuning model hyper-parameters, and the "),ke=o("code"),Bt=s("test"),Jt=s(" split for the final model evaluation. Make sure to use the right split for the right purpose!"),Ue=c(),O=o("p"),Ut=s("If the dataset you\u2019re using doesn\u2019t have a predefined train-test split, it is up to you to define which part of the dataset you want to use for training your model and which you want to use for evaluation (since training and evaluating on the same split can misrepresent your results!). Depending on the size of the dataset, you can keep anywhere from 10-30% for evaluation and the rest for testing. Check out "),K=o("a"),Kt=s("this thread"),Qt=s(" for a more in-depth discussion of dataset splitting!"),Ke=c(),b=o("h2"),M=o("a"),Pe=o("span"),R(Q.$$.fragment),Vt=c(),Ae=o("span"),Wt=s("The impact of class imbalance"),Qe=c(),I=o("p"),Xt=s("In the case of supervised learning tasks (i.e. tasks for which labels exist), an "),xe=o("em"),Yt=s("imbalance"),Zt=s(" in the classes can impact evaluation results \u2014 this means that if some classes have much more examples than others, these can impact evaluation results and misrepresent performance."),Ve=c(),m=o("p"),ea=s("Often, using more than one metric can help get a better idea of your model\u2019s performance from different points of view. For instance, metrics like "),Se=o("strong"),V=o("a"),ta=s("accuracy"),aa=s(" and "),Oe=o("strong"),W=o("a"),oa=s("precision"),ra=s(" can be used together, and the "),Me=o("strong"),X=o("a"),ia=s("f1 score"),sa=s(" is actually the harmonic mean of the two."),We=c(),oe=o("p"),la=s("In cases where a dataset is balanced, using accuracy can be reflect the overall model performance:"),Xe=c(),re=o("p"),ie=o("img"),Ye=c(),se=o("p"),na=s("In cases where there is an imbalance, using F1 score can be a better representation of performance:"),Ze=c(),le=o("p"),ne=o("img"),et=c(),w=o("h2"),N=o("a"),Ie=o("span"),R(Y.$$.fragment),ha=c(),Ne=o("span"),fa=s("Offline vs. online model evaluation"),tt=c(),he=o("p"),ca=s("Offline model evaluation is any model evaluation that you do using offline data before you deploy a model or use any insights generated from a model."),at=c(),fe=o("p"),ua=s("Online model evaluation refers to the process of evaluating how your model is performing after it has actually been deployed and is being used in production."),ot=c(),_=o("h2"),C=o("a"),Ce=o("span"),R(Z.$$.fragment),da=c(),De=o("span"),pa=s("Trade-offs in model evaluation"),rt=c(),E=o("h3"),D=o("a"),He=o("span"),R(ee.$$.fragment),ma=c(),Te=o("span"),va=s("Interpretability"),it=c(),ce=o("p"),ya=s("Not all regression metrics are easy to interpret on their own. For example, it is hard to say what a good value for RMSE is because that depends on the scale of your outcome variable. If your outcome variable ranges from 0 to 1 then a model with a MAE of 0.8 would be considered bad, whereas if your outcome variable ranges from -500 to 500 then a MAE of 0.8 would be considered great. If you want your error metrics to be interpretable out of the box, then you should use error metrics that are scaled to account for the range of your outcome variable such as mean absolute percent error (MAPE)."),st=c(),$=o("h3"),H=o("a"),Ge=o("span"),R(te.$$.fragment),ga=c(),Re=o("span"),ba=s("Prediction time"),lt=s(`

Prediction time. One factor that should be considered is the amount of time it takes to make a prediction using your model. This might not be as important if you are using an ad hoc model that only needs to be run once, but if you are building a model that will be used to score data on a regular cadence (whether that be in real time via an API or in batch jobs that run overnight) then you should consider the amount of time it takes to make predictions with your model. Generally, you should favor models that are able to make predictions quicker and with fewer resources.
`),k=o("h3"),T=o("a"),Le=o("span"),R(ae.$$.fragment),wa=c(),qe=o("span"),_a=s("Model complexity"),this.h()},l(e){const h=ko('[data-svelte="svelte-1phssyn"]',document.head);v=r(h,"META",{name:!0,content:!0}),h.forEach(t),Fe=u(e),y=r(e,"H1",{class:!0});var ht=i(y);P=r(ht,"A",{id:!0,class:!0,href:!0});var Pa=i(P);ue=r(Pa,"SPAN",{});var Aa=i(ue);L(J.$$.fragment,Aa),Aa.forEach(t),Pa.forEach(t),_t=u(ht),de=r(ht,"SPAN",{});var xa=i(de);Et=l(xa,"Considerations for model evaluation"),xa.forEach(t),ht.forEach(t),je=u(e),A=r(e,"P",{});var ft=i(A);$t=l(ft,"Developing an ML model is rarely a one-shot deal: it often involves multiple stages of defining the model architecture and tuning hyper-parameters before converging on a final set. Responsible model evaluation is a key part of this process, and the \u{1F917} "),pe=r(ft,"CODE",{});var Sa=i(pe);kt=l(Sa,"evaluate"),Sa.forEach(t),Pt=l(ft," package is here to help!"),ft.forEach(t),ze=u(e),x=r(e,"P",{});var ct=i(x);At=l(ct,"Here are some things to keep in mind when evaluating your model using the \u{1F917} "),me=r(ct,"CODE",{});var Oa=i(me);xt=l(Oa,"evaluate"),Oa.forEach(t),St=l(ct," package:"),ct.forEach(t),Be=u(e),g=r(e,"H2",{class:!0});var ut=i(g);S=r(ut,"A",{id:!0,class:!0,href:!0});var Ma=i(S);ve=r(Ma,"SPAN",{});var Ia=i(ve);L(U.$$.fragment,Ia),Ia.forEach(t),Ma.forEach(t),Ot=u(ut),ye=r(ut,"SPAN",{});var Na=i(ye);Mt=l(Na,"Properly splitting your data"),Na.forEach(t),ut.forEach(t),Je=u(e),d=r(e,"P",{});var p=i(d);It=l(p,"Many of the datasets on the \u{1F917} Hub are separated into 2 splits: "),ge=r(p,"CODE",{});var Ca=i(ge);Nt=l(Ca,"train"),Ca.forEach(t),Ct=l(p," (used for training models) and "),be=r(p,"CODE",{});var Da=i(be);Dt=l(Da,"validation"),Da.forEach(t),Ht=l(p," (used for evaluation); others are split into 3 splits: "),we=r(p,"CODE",{});var Ha=i(we);Tt=l(Ha,"train"),Ha.forEach(t),Gt=l(p,", "),_e=r(p,"CODE",{});var Ta=i(_e);Rt=l(Ta,"validation"),Ta.forEach(t),Lt=l(p," and "),Ee=r(p,"CODE",{});var Ga=i(Ee);qt=l(Ga,"test"),Ga.forEach(t),Ft=l(p," \u2014 in this case, the "),$e=r(p,"CODE",{});var Ra=i($e);jt=l(Ra,"validation"),Ra.forEach(t),zt=l(p," split can be used for tuning model hyper-parameters, and the "),ke=r(p,"CODE",{});var La=i(ke);Bt=l(La,"test"),La.forEach(t),Jt=l(p," split for the final model evaluation. Make sure to use the right split for the right purpose!"),p.forEach(t),Ue=u(e),O=r(e,"P",{});var dt=i(O);Ut=l(dt,"If the dataset you\u2019re using doesn\u2019t have a predefined train-test split, it is up to you to define which part of the dataset you want to use for training your model and which you want to use for evaluation (since training and evaluating on the same split can misrepresent your results!). Depending on the size of the dataset, you can keep anywhere from 10-30% for evaluation and the rest for testing. Check out "),K=r(dt,"A",{href:!0,rel:!0});var qa=i(K);Kt=l(qa,"this thread"),qa.forEach(t),Qt=l(dt," for a more in-depth discussion of dataset splitting!"),dt.forEach(t),Ke=u(e),b=r(e,"H2",{class:!0});var pt=i(b);M=r(pt,"A",{id:!0,class:!0,href:!0});var Fa=i(M);Pe=r(Fa,"SPAN",{});var ja=i(Pe);L(Q.$$.fragment,ja),ja.forEach(t),Fa.forEach(t),Vt=u(pt),Ae=r(pt,"SPAN",{});var za=i(Ae);Wt=l(za,"The impact of class imbalance"),za.forEach(t),pt.forEach(t),Qe=u(e),I=r(e,"P",{});var mt=i(I);Xt=l(mt,"In the case of supervised learning tasks (i.e. tasks for which labels exist), an "),xe=r(mt,"EM",{});var Ba=i(xe);Yt=l(Ba,"imbalance"),Ba.forEach(t),Zt=l(mt," in the classes can impact evaluation results \u2014 this means that if some classes have much more examples than others, these can impact evaluation results and misrepresent performance."),mt.forEach(t),Ve=u(e),m=r(e,"P",{});var G=i(m);ea=l(G,"Often, using more than one metric can help get a better idea of your model\u2019s performance from different points of view. For instance, metrics like "),Se=r(G,"STRONG",{});var Ja=i(Se);V=r(Ja,"A",{href:!0,rel:!0});var Ua=i(V);ta=l(Ua,"accuracy"),Ua.forEach(t),Ja.forEach(t),aa=l(G," and "),Oe=r(G,"STRONG",{});var Ka=i(Oe);W=r(Ka,"A",{href:!0,rel:!0});var Qa=i(W);oa=l(Qa,"precision"),Qa.forEach(t),Ka.forEach(t),ra=l(G," can be used together, and the "),Me=r(G,"STRONG",{});var Va=i(Me);X=r(Va,"A",{href:!0,rel:!0});var Wa=i(X);ia=l(Wa,"f1 score"),Wa.forEach(t),Va.forEach(t),sa=l(G," is actually the harmonic mean of the two."),G.forEach(t),We=u(e),oe=r(e,"P",{});var Xa=i(oe);la=l(Xa,"In cases where a dataset is balanced, using accuracy can be reflect the overall model performance:"),Xa.forEach(t),Xe=u(e),re=r(e,"P",{});var Ya=i(re);ie=r(Ya,"IMG",{src:!0,alt:!0}),Ya.forEach(t),Ye=u(e),se=r(e,"P",{});var Za=i(se);na=l(Za,"In cases where there is an imbalance, using F1 score can be a better representation of performance:"),Za.forEach(t),Ze=u(e),le=r(e,"P",{});var eo=i(le);ne=r(eo,"IMG",{src:!0,alt:!0}),eo.forEach(t),et=u(e),w=r(e,"H2",{class:!0});var vt=i(w);N=r(vt,"A",{id:!0,class:!0,href:!0});var to=i(N);Ie=r(to,"SPAN",{});var ao=i(Ie);L(Y.$$.fragment,ao),ao.forEach(t),to.forEach(t),ha=u(vt),Ne=r(vt,"SPAN",{});var oo=i(Ne);fa=l(oo,"Offline vs. online model evaluation"),oo.forEach(t),vt.forEach(t),tt=u(e),he=r(e,"P",{});var ro=i(he);ca=l(ro,"Offline model evaluation is any model evaluation that you do using offline data before you deploy a model or use any insights generated from a model."),ro.forEach(t),at=u(e),fe=r(e,"P",{});var io=i(fe);ua=l(io,"Online model evaluation refers to the process of evaluating how your model is performing after it has actually been deployed and is being used in production."),io.forEach(t),ot=u(e),_=r(e,"H2",{class:!0});var yt=i(_);C=r(yt,"A",{id:!0,class:!0,href:!0});var so=i(C);Ce=r(so,"SPAN",{});var lo=i(Ce);L(Z.$$.fragment,lo),lo.forEach(t),so.forEach(t),da=u(yt),De=r(yt,"SPAN",{});var no=i(De);pa=l(no,"Trade-offs in model evaluation"),no.forEach(t),yt.forEach(t),rt=u(e),E=r(e,"H3",{class:!0});var gt=i(E);D=r(gt,"A",{id:!0,class:!0,href:!0});var ho=i(D);He=r(ho,"SPAN",{});var fo=i(He);L(ee.$$.fragment,fo),fo.forEach(t),ho.forEach(t),ma=u(gt),Te=r(gt,"SPAN",{});var co=i(Te);va=l(co,"Interpretability"),co.forEach(t),gt.forEach(t),it=u(e),ce=r(e,"P",{});var uo=i(ce);ya=l(uo,"Not all regression metrics are easy to interpret on their own. For example, it is hard to say what a good value for RMSE is because that depends on the scale of your outcome variable. If your outcome variable ranges from 0 to 1 then a model with a MAE of 0.8 would be considered bad, whereas if your outcome variable ranges from -500 to 500 then a MAE of 0.8 would be considered great. If you want your error metrics to be interpretable out of the box, then you should use error metrics that are scaled to account for the range of your outcome variable such as mean absolute percent error (MAPE)."),uo.forEach(t),st=u(e),$=r(e,"H3",{class:!0});var bt=i($);H=r(bt,"A",{id:!0,class:!0,href:!0});var po=i(H);Ge=r(po,"SPAN",{});var mo=i(Ge);L(te.$$.fragment,mo),mo.forEach(t),po.forEach(t),ga=u(bt),Re=r(bt,"SPAN",{});var vo=i(Re);ba=l(vo,"Prediction time"),vo.forEach(t),bt.forEach(t),lt=l(e,`

Prediction time. One factor that should be considered is the amount of time it takes to make a prediction using your model. This might not be as important if you are using an ad hoc model that only needs to be run once, but if you are building a model that will be used to score data on a regular cadence (whether that be in real time via an API or in batch jobs that run overnight) then you should consider the amount of time it takes to make predictions with your model. Generally, you should favor models that are able to make predictions quicker and with fewer resources.
`),k=r(e,"H3",{class:!0});var wt=i(k);T=r(wt,"A",{id:!0,class:!0,href:!0});var yo=i(T);Le=r(yo,"SPAN",{});var go=i(Le);L(ae.$$.fragment,go),go.forEach(t),yo.forEach(t),wa=u(wt),qe=r(wt,"SPAN",{});var bo=i(qe);_a=l(bo,"Model complexity"),bo.forEach(t),wt.forEach(t),this.h()},h(){n(v,"name","hf:doc:metadata"),n(v,"content",JSON.stringify(So)),n(P,"id","considerations-for-model-evaluation"),n(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(P,"href","#considerations-for-model-evaluation"),n(y,"class","relative group"),n(S,"id","properly-splitting-your-data"),n(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(S,"href","#properly-splitting-your-data"),n(g,"class","relative group"),n(K,"href","https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090"),n(K,"rel","nofollow"),n(M,"id","the-impact-of-class-imbalance"),n(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(M,"href","#the-impact-of-class-imbalance"),n(b,"class","relative group"),n(V,"href","https://huggingface.co/metrics/accuracy"),n(V,"rel","nofollow"),n(W,"href","https://huggingface.co/metrics/precision"),n(W,"rel","nofollow"),n(X,"href","https://huggingface.co/metrics/f1"),n(X,"rel","nofollow"),wo(ie.src,$a="media/balanced-classes.png")||n(ie,"src",$a),n(ie,"alt","Balanced Labels"),wo(ne.src,ka="media/imbalanced-classes.png")||n(ne,"src",ka),n(ne,"alt","Imbalanced Labels"),n(N,"id","offline-vs-online-model-evaluation"),n(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(N,"href","#offline-vs-online-model-evaluation"),n(w,"class","relative group"),n(C,"id","tradeoffs-in-model-evaluation"),n(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(C,"href","#tradeoffs-in-model-evaluation"),n(_,"class","relative group"),n(D,"id","interpretability"),n(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(D,"href","#interpretability"),n(E,"class","relative group"),n(H,"id","prediction-time"),n(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(H,"href","#prediction-time"),n($,"class","relative group"),n(T,"id","model-complexity"),n(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(T,"href","#model-complexity"),n(k,"class","relative group")},m(e,h){a(document.head,v),f(e,Fe,h),f(e,y,h),a(y,P),a(P,ue),q(J,ue,null),a(y,_t),a(y,de),a(de,Et),f(e,je,h),f(e,A,h),a(A,$t),a(A,pe),a(pe,kt),a(A,Pt),f(e,ze,h),f(e,x,h),a(x,At),a(x,me),a(me,xt),a(x,St),f(e,Be,h),f(e,g,h),a(g,S),a(S,ve),q(U,ve,null),a(g,Ot),a(g,ye),a(ye,Mt),f(e,Je,h),f(e,d,h),a(d,It),a(d,ge),a(ge,Nt),a(d,Ct),a(d,be),a(be,Dt),a(d,Ht),a(d,we),a(we,Tt),a(d,Gt),a(d,_e),a(_e,Rt),a(d,Lt),a(d,Ee),a(Ee,qt),a(d,Ft),a(d,$e),a($e,jt),a(d,zt),a(d,ke),a(ke,Bt),a(d,Jt),f(e,Ue,h),f(e,O,h),a(O,Ut),a(O,K),a(K,Kt),a(O,Qt),f(e,Ke,h),f(e,b,h),a(b,M),a(M,Pe),q(Q,Pe,null),a(b,Vt),a(b,Ae),a(Ae,Wt),f(e,Qe,h),f(e,I,h),a(I,Xt),a(I,xe),a(xe,Yt),a(I,Zt),f(e,Ve,h),f(e,m,h),a(m,ea),a(m,Se),a(Se,V),a(V,ta),a(m,aa),a(m,Oe),a(Oe,W),a(W,oa),a(m,ra),a(m,Me),a(Me,X),a(X,ia),a(m,sa),f(e,We,h),f(e,oe,h),a(oe,la),f(e,Xe,h),f(e,re,h),a(re,ie),f(e,Ye,h),f(e,se,h),a(se,na),f(e,Ze,h),f(e,le,h),a(le,ne),f(e,et,h),f(e,w,h),a(w,N),a(N,Ie),q(Y,Ie,null),a(w,ha),a(w,Ne),a(Ne,fa),f(e,tt,h),f(e,he,h),a(he,ca),f(e,at,h),f(e,fe,h),a(fe,ua),f(e,ot,h),f(e,_,h),a(_,C),a(C,Ce),q(Z,Ce,null),a(_,da),a(_,De),a(De,pa),f(e,rt,h),f(e,E,h),a(E,D),a(D,He),q(ee,He,null),a(E,ma),a(E,Te),a(Te,va),f(e,it,h),f(e,ce,h),a(ce,ya),f(e,st,h),f(e,$,h),a($,H),a(H,Ge),q(te,Ge,null),a($,ga),a($,Re),a(Re,ba),f(e,lt,h),f(e,k,h),a(k,T),a(T,Le),q(ae,Le,null),a(k,wa),a(k,qe),a(qe,_a),nt=!0},p:Po,i(e){nt||(F(J.$$.fragment,e),F(U.$$.fragment,e),F(Q.$$.fragment,e),F(Y.$$.fragment,e),F(Z.$$.fragment,e),F(ee.$$.fragment,e),F(te.$$.fragment,e),F(ae.$$.fragment,e),nt=!0)},o(e){j(J.$$.fragment,e),j(U.$$.fragment,e),j(Q.$$.fragment,e),j(Y.$$.fragment,e),j(Z.$$.fragment,e),j(ee.$$.fragment,e),j(te.$$.fragment,e),j(ae.$$.fragment,e),nt=!1},d(e){t(v),e&&t(Fe),e&&t(y),z(J),e&&t(je),e&&t(A),e&&t(ze),e&&t(x),e&&t(Be),e&&t(g),z(U),e&&t(Je),e&&t(d),e&&t(Ue),e&&t(O),e&&t(Ke),e&&t(b),z(Q),e&&t(Qe),e&&t(I),e&&t(Ve),e&&t(m),e&&t(We),e&&t(oe),e&&t(Xe),e&&t(re),e&&t(Ye),e&&t(se),e&&t(Ze),e&&t(le),e&&t(et),e&&t(w),z(Y),e&&t(tt),e&&t(he),e&&t(at),e&&t(fe),e&&t(ot),e&&t(_),z(Z),e&&t(rt),e&&t(E),z(ee),e&&t(it),e&&t(ce),e&&t(st),e&&t($),z(te),e&&t(lt),e&&t(k),z(ae)}}}const So={local:"considerations-for-model-evaluation",sections:[{local:"properly-splitting-your-data",title:"Properly splitting your data"},{local:"the-impact-of-class-imbalance",title:"The impact of class imbalance"},{local:"offline-vs-online-model-evaluation",title:"Offline vs. online model evaluation"},{local:"tradeoffs-in-model-evaluation",sections:[{local:"interpretability",title:"Interpretability"},{local:"prediction-time",title:"Prediction time"},{local:"model-complexity",title:"Model complexity"}],title:"Trade-offs in model evaluation"}],title:"Considerations for model evaluation"};function Oo(Ea){return Ao(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class No extends _o{constructor(v){super();Eo(this,v,Oo,xo,$o,{})}}export{No as default,So as metadata};
