import{S as As,i as Is,s as Ds,e as o,k as d,w as _,t as l,M as Ns,c as s,d as a,m as p,a as n,x as b,h as i,b as u,G as e,g as v,y as $,q as w,o as E,B as y,v as Qs,L as Dt}from"../../chunks/vendor-hf-doc-builder.js";import{T as Ms}from"../../chunks/Tip-hf-doc-builder.js";import{D as I}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Nt}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Ye}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as It}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function zs(T){let c,q,f,m,x;return m=new Nt({props:{code:`from evaluate import evaluator
# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){c=o("p"),q=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var g=n(c);q=i(g,"Examples:"),g.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,g){v(r,c,g),e(c,q),v(r,f,g),$(m,r,g),x=!0},p:Dt,i(r){x||(w(m.$$.fragment,r),x=!0)},o(r){E(m.$$.fragment,r),x=!1},d(r){r&&a(c),r&&a(f),y(m,r)}}}function Ls(T){let c,q,f,m,x;return m=new Nt({props:{code:`from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("image-classification")
data = load_dataset("beans", split="test[:40]")
results = task_evaluator.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    label_column="labels",
    metric="accuracy",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap"
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;image-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;beans&quot;</span>, split=<span class="hljs-string">&quot;test[:40]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;nateraw/vit-base-beans&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;labels&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&#x27;angular_leaf_spot&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;bean_rust&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>: <span class="hljs-number">2</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){c=o("p"),q=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var g=n(c);q=i(g,"Examples:"),g.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,g){v(r,c,g),e(c,q),v(r,f,g),$(m,r,g),x=!0},p:Dt,i(r){x||(w(m.$$.fragment,r),x=!0)},o(r){E(m.$$.fragment,r),x=!1},d(r){r&&a(c),r&&a(f),y(m,r)}}}function Ss(T){let c,q,f,m,x;return m=new Nt({props:{code:`from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("question-answering")
data = load_dataset("squad", split="validation[:2]")
results = task_evaluator.compute(
    model_or_pipeline="sshleifer/tiny-distilbert-base-cased-distilled-squad",
    data=data,
    metric="squad",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;sshleifer/tiny-distilbert-base-cased-distilled-squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){c=o("p"),q=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var g=n(c);q=i(g,"Examples:"),g.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,g){v(r,c,g),e(c,q),v(r,f,g),$(m,r,g),x=!0},p:Dt,i(r){x||(w(m.$$.fragment,r),x=!0)},o(r){E(m.$$.fragment,r),x=!1},d(r){r&&a(c),r&&a(f),y(m,r)}}}function Fs(T){let c,q,f,m,x;return{c(){c=o("p"),q=l("Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. In this case, it is safer to pass "),f=o("code"),m=l("squad_v2_format=True"),x=l(` to
the compute() call.`)},l(r){c=s(r,"P",{});var g=n(c);q=i(g,"Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. In this case, it is safer to pass "),f=s(g,"CODE",{});var J=n(f);m=i(J,"squad_v2_format=True"),J.forEach(a),x=i(g,` to
the compute() call.`),g.forEach(a)},m(r,g){v(r,c,g),e(c,q),e(c,f),e(f,m),e(c,x)},d(r){r&&a(c)}}}function Os(T){let c,q;return c=new Nt({props:{code:`from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("question-answering")
data = load_dataset("squad_v2", split="validation[:2]")
results = task_evaluator.compute(
    model_or_pipeline="mrm8488/bert-tiny-finetuned-squadv2",
    data=data,
    metric="squad_v2",
    squad_v2_format=True,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad_v2&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;mrm8488/bert-tiny-finetuned-squadv2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad_v2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    squad_v2_format=<span class="hljs-literal">True</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){_(c.$$.fragment)},l(f){b(c.$$.fragment,f)},m(f,m){$(c,f,m),q=!0},p:Dt,i(f){q||(w(c.$$.fragment,f),q=!0)},o(f){E(c.$$.fragment,f),q=!1},d(f){y(c,f)}}}function Rs(T){let c,q,f,m,x;return m=new Nt({props:{code:`from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("text-classification")
data = load_dataset("imdb", split="test[:2]")
results = task_evaluator.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>, split=<span class="hljs-string">&quot;test[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){c=o("p"),q=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var g=n(c);q=i(g,"Examples:"),g.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,g){v(r,c,g),e(c,q),v(r,f,g),$(m,r,g),x=!0},p:Dt,i(r){x||(w(m.$$.fragment,r),x=!0)},o(r){E(m.$$.fragment,r),x=!1},d(r){r&&a(c),r&&a(f),y(m,r)}}}function Us(T){let c,q,f,m,x,r,g,J,la,Qt,Se,ia,Mt,U,K,Ze,he,ca,et,da,zt,Fe,pa,Lt,D,ge,ua,N,ma,Oe,fa,ha,tt,ga,va,at,_a,ba,$a,X,St,Re,wa,Ft,k,ve,Ea,ot,ya,qa,Y,_e,xa,st,ka,ja,Z,be,Ta,$e,Pa,nt,Ca,Aa,Ia,ee,we,Da,rt,Na,Qa,te,Ee,Ma,lt,za,La,ae,ye,Sa,it,Fa,Ot,V,oe,ct,qe,Oa,dt,Ra,Rt,B,se,pt,xe,Ua,ut,Va,Ut,Q,ke,Ba,M,Ha,Ue,Ga,Wa,mt,Ja,Ka,ft,Xa,Ya,Za,L,je,eo,ht,to,ao,ne,Vt,H,re,gt,Te,oo,vt,so,Bt,j,Pe,no,Ce,ro,le,_t,lo,io,co,po,G,uo,Ve,mo,fo,bt,ho,go,vo,Ae,_o,Ie,$t,bo,$o,wo,P,De,Eo,wt,yo,qo,ie,xo,ce,ko,de,Ht,W,pe,Et,Ne,jo,yt,To,Gt,z,Qe,Po,A,Co,Be,Ao,Io,qt,Do,No,xt,Qo,Mo,kt,zo,Lo,So,S,Me,Fo,jt,Oo,Ro,ue,Wt;return r=new Ye({}),he=new Ye({}),ge=new I({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:<ul>
<li><code>&quot;image-classification&quot;</code>: will return a <a href="/docs/evaluate/pr_192/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator">ImageClassificationEvaluator</a>.</li>
<li><code>&quot;question-answering&quot;</code>: will return a <a href="/docs/evaluate/pr_192/en/package_reference/evaluator_classes#evaluate.QuestionAnsweringEvaluator">QuestionAnsweringEvaluator</a>.</li>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/pr_192/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/vr_192/src/evaluate/evaluator/__init__.py#L79",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/pr_192/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),X=new It({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[zs]},$$scope:{ctx:T}}}),ve=new I({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_192/src/evaluate/evaluator/base.py#L50"}}),_e=new I({props:{name:"compute_metric",anchor:"evaluate.Evaluator.compute_metric",parameters:[{name:"metric",val:": EvaluationModule"},{name:"metric_inputs",val:": typing.Dict"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_192/src/evaluate/evaluator/base.py#L289"}}),be=new I({props:{name:"predictions_processor",anchor:"evaluate.Evaluator.predictions_processor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/evaluate/blob/vr_192/src/evaluate/evaluator/base.py#L129"}}),we=new I({props:{name:"prepare_data",anchor:"evaluate.Evaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"input_column",val:": str"},{name:"label_column",val:": str"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_data.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.Evaluator.prepare_data.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.prepare_data.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_192/src/evaluate/evaluator/base.py#L184",returnDescription:`
<p>metric inputs.
<code>list</code>:  pipeline inputs.</p>
`,returnType:`
<p><code>dict</code></p>
`}}),Ee=new I({props:{name:"prepare_metric",anchor:"evaluate.Evaluator.prepare_metric",parameters:[{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule]"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_metric.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"}],source:"https://github.com/huggingface/evaluate/blob/vr_192/src/evaluate/evaluator/base.py#L259",returnDescription:`
<p>The loaded metric.</p>
`}}),ye=new I({props:{name:"prepare_pipeline",anchor:"evaluate.Evaluator.prepare_pipeline",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"tokenizer",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"},{name:"feature_extractor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_pipeline.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.prepare_pipeline.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.Evaluator.prepare_pipeline.preprocessor",description:`<strong>preprocessor</strong> (<code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"preprocessor"}],source:"https://github.com/huggingface/evaluate/blob/vr_192/src/evaluate/evaluator/base.py#L216",returnDescription:`
<p>The initialized pipeline.</p>
`}}),qe=new Ye({}),xe=new Ye({}),ke=new I({props:{name:"class evaluate.ImageClassificationEvaluator",anchor:"evaluate.ImageClassificationEvaluator",parameters:[{name:"task",val:" = 'image-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_192/src/evaluate/evaluator/image_classification.py#L20"}}),je=new I({props:{name:"compute",anchor:"evaluate.ImageClassificationEvaluator.compute",parameters:[{name:"input_column",val:": str = 'image'"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"evaluate.ImageClassificationEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>image-classification</code>. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.ImageClassificationEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.ImageClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
feature_extractor &#x2014; (<code>str</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default feature extractor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.ImageClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.ImageClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.ImageClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.ImageClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;image&quot;</code>) &#x2014;
the name of the column containing the images as PIL ImageFile in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_192/src/evaluate/evaluator/image_classification.py#L37",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ne=new It({props:{anchor:"evaluate.ImageClassificationEvaluator.compute.example",$$slots:{default:[Ls]},$$scope:{ctx:T}}}),Te=new Ye({}),Pe=new I({props:{name:"class evaluate.QuestionAnsweringEvaluator",anchor:"evaluate.QuestionAnsweringEvaluator",parameters:[{name:"task",val:" = 'question-answering'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_192/src/evaluate/evaluator/question_answering.py#L38"}}),De=new I({props:{name:"compute",anchor:"evaluate.QuestionAnsweringEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"question_column",val:": str = 'question'"},{name:"context_column",val:": str = 'context'"},{name:"id_column",val:": str = 'id'"},{name:"label_column",val:": str = 'answers'"},{name:"squad_v2_format",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"evaluate.QuestionAnsweringEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>question-answering</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.tokenizer",description:`<strong>tokenizer</strong> (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"tokenizer"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.strategy",description:`<strong>strategy</strong> (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;) &#x2014;
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"strategy"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.question_column",description:`<strong>question_column</strong> (<code>str</code>, defaults to <code>&quot;question&quot;</code>) &#x2014;
the name of the column containing the question in the dataset specified by <code>data</code>.`,name:"question_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.context_column",description:`<strong>context_column</strong> (<code>str</code>, defaults to <code>&quot;context&quot;</code>) &#x2014;
the name of the column containing the context in the dataset specified by <code>data</code>.`,name:"context_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.id_column",description:`<strong>id_column</strong> (<code>str</code>, defaults to <code>&quot;id&quot;</code>) &#x2014;
the name of the column cointaing the identification field of the question and answer pair in the
dataset specified by <code>data</code>.`,name:"id_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;answers&quot;</code>) &#x2014;
the name of the column containing the answers in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.squad_v2_format",description:`<strong>squad_v2_format</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
whether the dataset follows the format of squad_v2 dataset where a question may have no answer in the context. If this parameter is not provided,
the format will be automatically inferred.`,name:"squad_v2_format"}],source:"https://github.com/huggingface/evaluate/blob/vr_192/src/evaluate/evaluator/question_answering.py#L117",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ie=new It({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example",$$slots:{default:[Ss]},$$scope:{ctx:T}}}),ce=new Ms({props:{$$slots:{default:[Fs]},$$scope:{ctx:T}}}),de=new It({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example-2",$$slots:{default:[Os]},$$scope:{ctx:T}}}),Ne=new Ye({}),Qe=new I({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_192/src/evaluate/evaluator/text_classification.py#L20"}}),Me=new I({props:{name:"compute",anchor:"evaluate.TextClassificationEvaluator.compute",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"evaluate.TextClassificationEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.TextClassificationEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.TextClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.TextClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TextClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TextClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TextClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_192/src/evaluate/evaluator/text_classification.py#L41",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ue=new It({props:{anchor:"evaluate.TextClassificationEvaluator.compute.example",$$slots:{default:[Rs]},$$scope:{ctx:T}}}),{c(){c=o("meta"),q=d(),f=o("h1"),m=o("a"),x=o("span"),_(r.$$.fragment),g=d(),J=o("span"),la=l("Evaluator"),Qt=d(),Se=o("p"),ia=l("The evaluator classes for automatic evaluation."),Mt=d(),U=o("h2"),K=o("a"),Ze=o("span"),_(he.$$.fragment),ca=d(),et=o("span"),da=l("Evaluator classes"),zt=d(),Fe=o("p"),pa=l("The main entry point for using the evaluator:"),Lt=d(),D=o("div"),_(ge.$$.fragment),ua=d(),N=o("p"),ma=l("Utility factory method to build an "),Oe=o("a"),fa=l("Evaluator"),ha=l(`.
Evaluators encapsulate a task and a default metric name. They leverage `),tt=o("code"),ga=l("pipeline"),va=l(" functionalify from "),at=o("code"),_a=l("transformers"),ba=l(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),$a=d(),_(X.$$.fragment),St=d(),Re=o("p"),wa=l("The base class for all evaluator classes:"),Ft=d(),k=o("div"),_(ve.$$.fragment),Ea=d(),ot=o("p"),ya=l(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),qa=d(),Y=o("div"),_(_e.$$.fragment),xa=d(),st=o("p"),ka=l("Compute and return metrics."),ja=d(),Z=o("div"),_(be.$$.fragment),Ta=d(),$e=o("p"),Pa=l("A core method of the "),nt=o("code"),Ca=l("Evaluator"),Aa=l(" class, which processes the pipeline outputs for compatibility with the metric."),Ia=d(),ee=o("div"),_(we.$$.fragment),Da=d(),rt=o("p"),Na=l("Prepare data."),Qa=d(),te=o("div"),_(Ee.$$.fragment),Ma=d(),lt=o("p"),za=l("Prepare metric."),La=d(),ae=o("div"),_(ye.$$.fragment),Sa=d(),it=o("p"),Fa=l("Prepare pipeline."),Ot=d(),V=o("h2"),oe=o("a"),ct=o("span"),_(qe.$$.fragment),Oa=d(),dt=o("span"),Ra=l("The task specific evaluators"),Rt=d(),B=o("h3"),se=o("a"),pt=o("span"),_(xe.$$.fragment),Ua=d(),ut=o("span"),Va=l("ImageClassificationEvaluator"),Ut=d(),Q=o("div"),_(ke.$$.fragment),Ba=d(),M=o("p"),Ha=l(`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Ue=o("a"),Ga=l("evaluator()"),Wa=l(` using the default task name
`),mt=o("code"),Ja=l("image-classification"),Ka=l(`.
Methods in this class assume a data format compatible with the `),ft=o("code"),Xa=l("ImageClassificationPipeline"),Ya=l("."),Za=d(),L=o("div"),_(je.$$.fragment),eo=d(),ht=o("p"),to=l("Compute the metric for a given pipeline and dataset combination."),ao=d(),_(ne.$$.fragment),Vt=d(),H=o("h3"),re=o("a"),gt=o("span"),_(Te.$$.fragment),oo=d(),vt=o("span"),so=l("QuestionAnsweringEvaluator"),Bt=d(),j=o("div"),_(Pe.$$.fragment),no=d(),Ce=o("p"),ro=l(`Question answering evaluator. This evaluator handles
`),le=o("a"),_t=o("strong"),lo=l("extractive"),io=l(" question answering"),co=l(`,
where the answer to the question is extracted from a context.`),po=d(),G=o("p"),uo=l("This question answering evaluator can currently be loaded from "),Ve=o("a"),mo=l("evaluator()"),fo=l(` using the default task name
`),bt=o("code"),ho=l("question-answering"),go=l("."),vo=d(),Ae=o("p"),_o=l(`Methods in this class assume a data format compatible with the
`),Ie=o("a"),$t=o("code"),bo=l("QuestionAnsweringPipeline"),$o=l("."),wo=d(),P=o("div"),_(De.$$.fragment),Eo=d(),wt=o("p"),yo=l("Compute the metric for a given pipeline and dataset combination."),qo=d(),_(ie.$$.fragment),xo=d(),_(ce.$$.fragment),ko=d(),_(de.$$.fragment),Ht=d(),W=o("h3"),pe=o("a"),Et=o("span"),_(Ne.$$.fragment),jo=d(),yt=o("span"),To=l("TextClassificationEvaluator"),Gt=d(),z=o("div"),_(Qe.$$.fragment),Po=d(),A=o("p"),Co=l(`Text classification evaluator.
This text classification evaluator can currently be loaded from `),Be=o("a"),Ao=l("evaluator()"),Io=l(` using the default task name
`),qt=o("code"),Do=l("text-classification"),No=l(" or with a "),xt=o("code"),Qo=l('"sentiment-analysis"'),Mo=l(` alias.
Methods in this class assume a data format compatible with the `),kt=o("code"),zo=l("TextClassificationPipeline"),Lo=l(` - a single textual
feature as input and a categorical label as output.`),So=d(),S=o("div"),_(Me.$$.fragment),Fo=d(),jt=o("p"),Oo=l("Compute the metric for a given pipeline and dataset combination."),Ro=d(),_(ue.$$.fragment),this.h()},l(t){const h=Ns('[data-svelte="svelte-1phssyn"]',document.head);c=s(h,"META",{name:!0,content:!0}),h.forEach(a),q=p(t),f=s(t,"H1",{class:!0});var ze=n(f);m=s(ze,"A",{id:!0,class:!0,href:!0});var Tt=n(m);x=s(Tt,"SPAN",{});var Pt=n(x);b(r.$$.fragment,Pt),Pt.forEach(a),Tt.forEach(a),g=p(ze),J=s(ze,"SPAN",{});var Ct=n(J);la=i(Ct,"Evaluator"),Ct.forEach(a),ze.forEach(a),Qt=p(t),Se=s(t,"P",{});var At=n(Se);ia=i(At,"The evaluator classes for automatic evaluation."),At.forEach(a),Mt=p(t),U=s(t,"H2",{class:!0});var Le=n(U);K=s(Le,"A",{id:!0,class:!0,href:!0});var Vo=n(K);Ze=s(Vo,"SPAN",{});var Bo=n(Ze);b(he.$$.fragment,Bo),Bo.forEach(a),Vo.forEach(a),ca=p(Le),et=s(Le,"SPAN",{});var Ho=n(et);da=i(Ho,"Evaluator classes"),Ho.forEach(a),Le.forEach(a),zt=p(t),Fe=s(t,"P",{});var Go=n(Fe);pa=i(Go,"The main entry point for using the evaluator:"),Go.forEach(a),Lt=p(t),D=s(t,"DIV",{class:!0});var He=n(D);b(ge.$$.fragment,He),ua=p(He),N=s(He,"P",{});var me=n(N);ma=i(me,"Utility factory method to build an "),Oe=s(me,"A",{href:!0});var Wo=n(Oe);fa=i(Wo,"Evaluator"),Wo.forEach(a),ha=i(me,`.
Evaluators encapsulate a task and a default metric name. They leverage `),tt=s(me,"CODE",{});var Jo=n(tt);ga=i(Jo,"pipeline"),Jo.forEach(a),va=i(me," functionalify from "),at=s(me,"CODE",{});var Ko=n(at);_a=i(Ko,"transformers"),Ko.forEach(a),ba=i(me,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),me.forEach(a),$a=p(He),b(X.$$.fragment,He),He.forEach(a),St=p(t),Re=s(t,"P",{});var Xo=n(Re);wa=i(Xo,"The base class for all evaluator classes:"),Xo.forEach(a),Ft=p(t),k=s(t,"DIV",{class:!0});var C=n(k);b(ve.$$.fragment,C),Ea=p(C),ot=s(C,"P",{});var Yo=n(ot);ya=i(Yo,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),Yo.forEach(a),qa=p(C),Y=s(C,"DIV",{class:!0});var Jt=n(Y);b(_e.$$.fragment,Jt),xa=p(Jt),st=s(Jt,"P",{});var Zo=n(st);ka=i(Zo,"Compute and return metrics."),Zo.forEach(a),Jt.forEach(a),ja=p(C),Z=s(C,"DIV",{class:!0});var Kt=n(Z);b(be.$$.fragment,Kt),Ta=p(Kt),$e=s(Kt,"P",{});var Xt=n($e);Pa=i(Xt,"A core method of the "),nt=s(Xt,"CODE",{});var es=n(nt);Ca=i(es,"Evaluator"),es.forEach(a),Aa=i(Xt," class, which processes the pipeline outputs for compatibility with the metric."),Xt.forEach(a),Kt.forEach(a),Ia=p(C),ee=s(C,"DIV",{class:!0});var Yt=n(ee);b(we.$$.fragment,Yt),Da=p(Yt),rt=s(Yt,"P",{});var ts=n(rt);Na=i(ts,"Prepare data."),ts.forEach(a),Yt.forEach(a),Qa=p(C),te=s(C,"DIV",{class:!0});var Zt=n(te);b(Ee.$$.fragment,Zt),Ma=p(Zt),lt=s(Zt,"P",{});var as=n(lt);za=i(as,"Prepare metric."),as.forEach(a),Zt.forEach(a),La=p(C),ae=s(C,"DIV",{class:!0});var ea=n(ae);b(ye.$$.fragment,ea),Sa=p(ea),it=s(ea,"P",{});var os=n(it);Fa=i(os,"Prepare pipeline."),os.forEach(a),ea.forEach(a),C.forEach(a),Ot=p(t),V=s(t,"H2",{class:!0});var ta=n(V);oe=s(ta,"A",{id:!0,class:!0,href:!0});var ss=n(oe);ct=s(ss,"SPAN",{});var ns=n(ct);b(qe.$$.fragment,ns),ns.forEach(a),ss.forEach(a),Oa=p(ta),dt=s(ta,"SPAN",{});var rs=n(dt);Ra=i(rs,"The task specific evaluators"),rs.forEach(a),ta.forEach(a),Rt=p(t),B=s(t,"H3",{class:!0});var aa=n(B);se=s(aa,"A",{id:!0,class:!0,href:!0});var ls=n(se);pt=s(ls,"SPAN",{});var is=n(pt);b(xe.$$.fragment,is),is.forEach(a),ls.forEach(a),Ua=p(aa),ut=s(aa,"SPAN",{});var cs=n(ut);Va=i(cs,"ImageClassificationEvaluator"),cs.forEach(a),aa.forEach(a),Ut=p(t),Q=s(t,"DIV",{class:!0});var Ge=n(Q);b(ke.$$.fragment,Ge),Ba=p(Ge),M=s(Ge,"P",{});var fe=n(M);Ha=i(fe,`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Ue=s(fe,"A",{href:!0});var ds=n(Ue);Ga=i(ds,"evaluator()"),ds.forEach(a),Wa=i(fe,` using the default task name
`),mt=s(fe,"CODE",{});var ps=n(mt);Ja=i(ps,"image-classification"),ps.forEach(a),Ka=i(fe,`.
Methods in this class assume a data format compatible with the `),ft=s(fe,"CODE",{});var us=n(ft);Xa=i(us,"ImageClassificationPipeline"),us.forEach(a),Ya=i(fe,"."),fe.forEach(a),Za=p(Ge),L=s(Ge,"DIV",{class:!0});var We=n(L);b(je.$$.fragment,We),eo=p(We),ht=s(We,"P",{});var ms=n(ht);to=i(ms,"Compute the metric for a given pipeline and dataset combination."),ms.forEach(a),ao=p(We),b(ne.$$.fragment,We),We.forEach(a),Ge.forEach(a),Vt=p(t),H=s(t,"H3",{class:!0});var oa=n(H);re=s(oa,"A",{id:!0,class:!0,href:!0});var fs=n(re);gt=s(fs,"SPAN",{});var hs=n(gt);b(Te.$$.fragment,hs),hs.forEach(a),fs.forEach(a),oo=p(oa),vt=s(oa,"SPAN",{});var gs=n(vt);so=i(gs,"QuestionAnsweringEvaluator"),gs.forEach(a),oa.forEach(a),Bt=p(t),j=s(t,"DIV",{class:!0});var F=n(j);b(Pe.$$.fragment,F),no=p(F),Ce=s(F,"P",{});var sa=n(Ce);ro=i(sa,`Question answering evaluator. This evaluator handles
`),le=s(sa,"A",{href:!0,rel:!0});var Uo=n(le);_t=s(Uo,"STRONG",{});var vs=n(_t);lo=i(vs,"extractive"),vs.forEach(a),io=i(Uo," question answering"),Uo.forEach(a),co=i(sa,`,
where the answer to the question is extracted from a context.`),sa.forEach(a),po=p(F),G=s(F,"P",{});var Je=n(G);uo=i(Je,"This question answering evaluator can currently be loaded from "),Ve=s(Je,"A",{href:!0});var _s=n(Ve);mo=i(_s,"evaluator()"),_s.forEach(a),fo=i(Je,` using the default task name
`),bt=s(Je,"CODE",{});var bs=n(bt);ho=i(bs,"question-answering"),bs.forEach(a),go=i(Je,"."),Je.forEach(a),vo=p(F),Ae=s(F,"P",{});var na=n(Ae);_o=i(na,`Methods in this class assume a data format compatible with the
`),Ie=s(na,"A",{href:!0,rel:!0});var $s=n(Ie);$t=s($s,"CODE",{});var ws=n($t);bo=i(ws,"QuestionAnsweringPipeline"),ws.forEach(a),$s.forEach(a),$o=i(na,"."),na.forEach(a),wo=p(F),P=s(F,"DIV",{class:!0});var O=n(P);b(De.$$.fragment,O),Eo=p(O),wt=s(O,"P",{});var Es=n(wt);yo=i(Es,"Compute the metric for a given pipeline and dataset combination."),Es.forEach(a),qo=p(O),b(ie.$$.fragment,O),xo=p(O),b(ce.$$.fragment,O),ko=p(O),b(de.$$.fragment,O),O.forEach(a),F.forEach(a),Ht=p(t),W=s(t,"H3",{class:!0});var ra=n(W);pe=s(ra,"A",{id:!0,class:!0,href:!0});var ys=n(pe);Et=s(ys,"SPAN",{});var qs=n(Et);b(Ne.$$.fragment,qs),qs.forEach(a),ys.forEach(a),jo=p(ra),yt=s(ra,"SPAN",{});var xs=n(yt);To=i(xs,"TextClassificationEvaluator"),xs.forEach(a),ra.forEach(a),Gt=p(t),z=s(t,"DIV",{class:!0});var Ke=n(z);b(Qe.$$.fragment,Ke),Po=p(Ke),A=s(Ke,"P",{});var R=n(A);Co=i(R,`Text classification evaluator.
This text classification evaluator can currently be loaded from `),Be=s(R,"A",{href:!0});var ks=n(Be);Ao=i(ks,"evaluator()"),ks.forEach(a),Io=i(R,` using the default task name
`),qt=s(R,"CODE",{});var js=n(qt);Do=i(js,"text-classification"),js.forEach(a),No=i(R," or with a "),xt=s(R,"CODE",{});var Ts=n(xt);Qo=i(Ts,'"sentiment-analysis"'),Ts.forEach(a),Mo=i(R,` alias.
Methods in this class assume a data format compatible with the `),kt=s(R,"CODE",{});var Ps=n(kt);zo=i(Ps,"TextClassificationPipeline"),Ps.forEach(a),Lo=i(R,` - a single textual
feature as input and a categorical label as output.`),R.forEach(a),So=p(Ke),S=s(Ke,"DIV",{class:!0});var Xe=n(S);b(Me.$$.fragment,Xe),Fo=p(Xe),jt=s(Xe,"P",{});var Cs=n(jt);Oo=i(Cs,"Compute the metric for a given pipeline and dataset combination."),Cs.forEach(a),Ro=p(Xe),b(ue.$$.fragment,Xe),Xe.forEach(a),Ke.forEach(a),this.h()},h(){u(c,"name","hf:doc:metadata"),u(c,"content",JSON.stringify(Vs)),u(m,"id","evaluator"),u(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(m,"href","#evaluator"),u(f,"class","relative group"),u(K,"id","evaluate.evaluator"),u(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(K,"href","#evaluate.evaluator"),u(U,"class","relative group"),u(Oe,"href","/docs/evaluate/pr_192/en/package_reference/evaluator_classes#evaluate.Evaluator"),u(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(oe,"id","the-task-specific-evaluators"),u(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(oe,"href","#the-task-specific-evaluators"),u(V,"class","relative group"),u(se,"id","evaluate.ImageClassificationEvaluator"),u(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(se,"href","#evaluate.ImageClassificationEvaluator"),u(B,"class","relative group"),u(Ue,"href","/docs/evaluate/pr_192/en/package_reference/evaluator_classes#evaluate.evaluator"),u(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(re,"id","evaluate.QuestionAnsweringEvaluator"),u(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(re,"href","#evaluate.QuestionAnsweringEvaluator"),u(H,"class","relative group"),u(le,"href","https://huggingface.co/docs/transformers/task_summary#extractive-question-answering"),u(le,"rel","nofollow"),u(Ve,"href","/docs/evaluate/pr_192/en/package_reference/evaluator_classes#evaluate.evaluator"),u(Ie,"href","https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline"),u(Ie,"rel","nofollow"),u(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(pe,"id","evaluate.TextClassificationEvaluator"),u(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(pe,"href","#evaluate.TextClassificationEvaluator"),u(W,"class","relative group"),u(Be,"href","/docs/evaluate/pr_192/en/package_reference/evaluator_classes#evaluate.evaluator"),u(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,h){e(document.head,c),v(t,q,h),v(t,f,h),e(f,m),e(m,x),$(r,x,null),e(f,g),e(f,J),e(J,la),v(t,Qt,h),v(t,Se,h),e(Se,ia),v(t,Mt,h),v(t,U,h),e(U,K),e(K,Ze),$(he,Ze,null),e(U,ca),e(U,et),e(et,da),v(t,zt,h),v(t,Fe,h),e(Fe,pa),v(t,Lt,h),v(t,D,h),$(ge,D,null),e(D,ua),e(D,N),e(N,ma),e(N,Oe),e(Oe,fa),e(N,ha),e(N,tt),e(tt,ga),e(N,va),e(N,at),e(at,_a),e(N,ba),e(D,$a),$(X,D,null),v(t,St,h),v(t,Re,h),e(Re,wa),v(t,Ft,h),v(t,k,h),$(ve,k,null),e(k,Ea),e(k,ot),e(ot,ya),e(k,qa),e(k,Y),$(_e,Y,null),e(Y,xa),e(Y,st),e(st,ka),e(k,ja),e(k,Z),$(be,Z,null),e(Z,Ta),e(Z,$e),e($e,Pa),e($e,nt),e(nt,Ca),e($e,Aa),e(k,Ia),e(k,ee),$(we,ee,null),e(ee,Da),e(ee,rt),e(rt,Na),e(k,Qa),e(k,te),$(Ee,te,null),e(te,Ma),e(te,lt),e(lt,za),e(k,La),e(k,ae),$(ye,ae,null),e(ae,Sa),e(ae,it),e(it,Fa),v(t,Ot,h),v(t,V,h),e(V,oe),e(oe,ct),$(qe,ct,null),e(V,Oa),e(V,dt),e(dt,Ra),v(t,Rt,h),v(t,B,h),e(B,se),e(se,pt),$(xe,pt,null),e(B,Ua),e(B,ut),e(ut,Va),v(t,Ut,h),v(t,Q,h),$(ke,Q,null),e(Q,Ba),e(Q,M),e(M,Ha),e(M,Ue),e(Ue,Ga),e(M,Wa),e(M,mt),e(mt,Ja),e(M,Ka),e(M,ft),e(ft,Xa),e(M,Ya),e(Q,Za),e(Q,L),$(je,L,null),e(L,eo),e(L,ht),e(ht,to),e(L,ao),$(ne,L,null),v(t,Vt,h),v(t,H,h),e(H,re),e(re,gt),$(Te,gt,null),e(H,oo),e(H,vt),e(vt,so),v(t,Bt,h),v(t,j,h),$(Pe,j,null),e(j,no),e(j,Ce),e(Ce,ro),e(Ce,le),e(le,_t),e(_t,lo),e(le,io),e(Ce,co),e(j,po),e(j,G),e(G,uo),e(G,Ve),e(Ve,mo),e(G,fo),e(G,bt),e(bt,ho),e(G,go),e(j,vo),e(j,Ae),e(Ae,_o),e(Ae,Ie),e(Ie,$t),e($t,bo),e(Ae,$o),e(j,wo),e(j,P),$(De,P,null),e(P,Eo),e(P,wt),e(wt,yo),e(P,qo),$(ie,P,null),e(P,xo),$(ce,P,null),e(P,ko),$(de,P,null),v(t,Ht,h),v(t,W,h),e(W,pe),e(pe,Et),$(Ne,Et,null),e(W,jo),e(W,yt),e(yt,To),v(t,Gt,h),v(t,z,h),$(Qe,z,null),e(z,Po),e(z,A),e(A,Co),e(A,Be),e(Be,Ao),e(A,Io),e(A,qt),e(qt,Do),e(A,No),e(A,xt),e(xt,Qo),e(A,Mo),e(A,kt),e(kt,zo),e(A,Lo),e(z,So),e(z,S),$(Me,S,null),e(S,Fo),e(S,jt),e(jt,Oo),e(S,Ro),$(ue,S,null),Wt=!0},p(t,[h]){const ze={};h&2&&(ze.$$scope={dirty:h,ctx:t}),X.$set(ze);const Tt={};h&2&&(Tt.$$scope={dirty:h,ctx:t}),ne.$set(Tt);const Pt={};h&2&&(Pt.$$scope={dirty:h,ctx:t}),ie.$set(Pt);const Ct={};h&2&&(Ct.$$scope={dirty:h,ctx:t}),ce.$set(Ct);const At={};h&2&&(At.$$scope={dirty:h,ctx:t}),de.$set(At);const Le={};h&2&&(Le.$$scope={dirty:h,ctx:t}),ue.$set(Le)},i(t){Wt||(w(r.$$.fragment,t),w(he.$$.fragment,t),w(ge.$$.fragment,t),w(X.$$.fragment,t),w(ve.$$.fragment,t),w(_e.$$.fragment,t),w(be.$$.fragment,t),w(we.$$.fragment,t),w(Ee.$$.fragment,t),w(ye.$$.fragment,t),w(qe.$$.fragment,t),w(xe.$$.fragment,t),w(ke.$$.fragment,t),w(je.$$.fragment,t),w(ne.$$.fragment,t),w(Te.$$.fragment,t),w(Pe.$$.fragment,t),w(De.$$.fragment,t),w(ie.$$.fragment,t),w(ce.$$.fragment,t),w(de.$$.fragment,t),w(Ne.$$.fragment,t),w(Qe.$$.fragment,t),w(Me.$$.fragment,t),w(ue.$$.fragment,t),Wt=!0)},o(t){E(r.$$.fragment,t),E(he.$$.fragment,t),E(ge.$$.fragment,t),E(X.$$.fragment,t),E(ve.$$.fragment,t),E(_e.$$.fragment,t),E(be.$$.fragment,t),E(we.$$.fragment,t),E(Ee.$$.fragment,t),E(ye.$$.fragment,t),E(qe.$$.fragment,t),E(xe.$$.fragment,t),E(ke.$$.fragment,t),E(je.$$.fragment,t),E(ne.$$.fragment,t),E(Te.$$.fragment,t),E(Pe.$$.fragment,t),E(De.$$.fragment,t),E(ie.$$.fragment,t),E(ce.$$.fragment,t),E(de.$$.fragment,t),E(Ne.$$.fragment,t),E(Qe.$$.fragment,t),E(Me.$$.fragment,t),E(ue.$$.fragment,t),Wt=!1},d(t){a(c),t&&a(q),t&&a(f),y(r),t&&a(Qt),t&&a(Se),t&&a(Mt),t&&a(U),y(he),t&&a(zt),t&&a(Fe),t&&a(Lt),t&&a(D),y(ge),y(X),t&&a(St),t&&a(Re),t&&a(Ft),t&&a(k),y(ve),y(_e),y(be),y(we),y(Ee),y(ye),t&&a(Ot),t&&a(V),y(qe),t&&a(Rt),t&&a(B),y(xe),t&&a(Ut),t&&a(Q),y(ke),y(je),y(ne),t&&a(Vt),t&&a(H),y(Te),t&&a(Bt),t&&a(j),y(Pe),y(De),y(ie),y(ce),y(de),t&&a(Ht),t&&a(W),y(Ne),t&&a(Gt),t&&a(z),y(Qe),y(Me),y(ue)}}}const Vs={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"},{local:"the-task-specific-evaluators",sections:[{local:"evaluate.ImageClassificationEvaluator",title:"ImageClassificationEvaluator"},{local:"evaluate.QuestionAnsweringEvaluator",title:"QuestionAnsweringEvaluator"},{local:"evaluate.TextClassificationEvaluator",title:"TextClassificationEvaluator"}],title:"The task specific evaluators"}],title:"Evaluator"};function Bs(T){return Qs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ys extends As{constructor(c){super();Is(this,c,Bs,Us,Ds,{})}}export{Ys as default,Vs as metadata};
