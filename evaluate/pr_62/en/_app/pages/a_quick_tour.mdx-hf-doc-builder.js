import{S as qi,i as Mi,s as Bi,e as s,k as d,w as h,t as l,M as Li,c as o,d as a,m as p,a as r,x as f,h as n,b as u,G as t,g as c,y as m,q as v,o as y,B as _,v as Ri}from"../chunks/vendor-hf-doc-builder.js";import{T as Fi}from"../chunks/Tip-hf-doc-builder.js";import{I as q}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as M}from"../chunks/CodeBlock-hf-doc-builder.js";function Gi(Ma){let E,z,$,A,B;return{c(){E=s("p"),z=l("Note that features always describe the type of a single input element. In general we will add lists of elements so you can always think of a list around the types in "),$=s("code"),A=l("features"),B=l(". Evaluate accepts various input formats (Python lists, NumPy arrays, PyTorch tensors, etc.) and converts them to an appropriate format for storage and computation.")},l(T){E=o(T,"P",{});var D=r(E);z=n(D,"Note that features always describe the type of a single input element. In general we will add lists of elements so you can always think of a list around the types in "),$=o(D,"CODE",{});var J=r($);A=n(J,"features"),J.forEach(a),B=n(D,". Evaluate accepts various input formats (Python lists, NumPy arrays, PyTorch tensors, etc.) and converts them to an appropriate format for storage and computation."),D.forEach(a)},m(T,D){c(T,E,D),t(E,z),t(E,$),t($,A),t(E,B)},d(T){T&&a(E)}}}function Hi(Ma){let E,z,$,A,B,T,D,J,ao,Ba,Qe,so,La,L,K,ut,ye,oo,ht,ro,Ra,Xe,lo,Fa,C,Q,ft,no,io,Ze,co,po,uo,X,mt,ho,fo,et,mo,vo,yo,Z,vt,_o,wo,tt,Eo,bo,Ga,ee,$o,yt,go,ko,Ha,R,te,_t,_e,jo,wt,To,Ua,ae,Ao,Et,Po,Do,Wa,we,Ya,at,Co,Va,Ee,za,F,se,bt,be,Oo,$t,xo,Ja,st,No,Ka,$e,Qa,G,oe,gt,ge,So,kt,Io,Xa,re,qo,ke,jt,Mo,Bo,Za,le,Tt,je,At,Lo,Ro,Pt,Fo,Go,w,Te,Dt,Ct,Ho,Uo,Ot,Wo,Yo,Ae,xt,Nt,Vo,zo,St,Jo,Ko,Pe,It,qt,Qo,Xo,De,Zo,Mt,er,tr,ar,Ce,Bt,Lt,sr,or,Rt,rr,lr,Oe,Ft,Gt,nr,ir,Ht,cr,dr,xe,Ut,Wt,pr,ur,Yt,hr,fr,Ne,Vt,zt,mr,vr,Jt,yr,_r,Se,Kt,Qt,wr,Er,Xt,br,es,ne,$r,Zt,gr,kr,ts,Ie,as,ie,jr,ea,Tr,Ar,ss,qe,os,ot,Pr,rs,Me,ls,ce,ns,H,de,ta,Be,Dr,aa,Cr,is,rt,Or,cs,pe,sa,xr,Nr,oa,Sr,ds,g,Ir,ra,qr,Mr,la,Br,Lr,na,Rr,Fr,ia,Gr,Hr,ps,U,ue,ca,Le,Ur,da,pa,Wr,us,Re,hs,O,Yr,ua,Vr,zr,ha,Jr,Kr,fs,W,he,fa,Fe,Qr,Ge,ma,Xr,Zr,va,el,ms,k,tl,ya,al,sl,_a,ol,rl,wa,ll,nl,Ea,il,cl,vs,He,ys,P,dl,ba,pl,ul,$a,hl,fl,ga,ml,vl,_s,Ue,ws,lt,yl,Es,We,bs,Y,fe,ka,Ye,_l,ja,wl,$s,j,El,Ta,bl,$l,Aa,gl,kl,Ve,jl,Tl,Pa,Al,Pl,gs,nt,Dl,ks,x,Cl,Da,Ol,xl,Ca,Nl,Sl,js,it,Il,Ts,V,me,Oa,ze,ql,xa,Ml,As,Je,Na,Bl,Ll,Ps;return T=new q({}),ye=new q({}),_e=new q({}),we=new M({props:{code:`import evaluate
accuracy = evaluate.load("accuracy")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate
<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)`}}),Ee=new M({props:{code:'word_length = evaluate.load("word_length", module_type="measurement")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>word_length = evaluate.load(<span class="hljs-string">&quot;word_length&quot;</span>, module_type=<span class="hljs-string">&quot;measurement&quot;</span>)'}}),be=new q({}),$e=new M({props:{code:'element_count = evaluate.load("lvwerra/element_count", module_type="measurement")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>element_count = evaluate.load(<span class="hljs-string">&quot;lvwerra/element_count&quot;</span>, module_type=<span class="hljs-string">&quot;measurement&quot;</span>)'}}),ge=new q({}),Ie=new M({props:{code:`accuracy = evaluate.load("accuracy")
accuracy.description`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.description
Accuracy <span class="hljs-keyword">is</span> the proportion of correct predictions among the total number of cases processed. It can be computed <span class="hljs-keyword">with</span>:
Accuracy = (TP + TN) / (TP + TN + FP + FN)
 Where:
TP: <span class="hljs-literal">True</span> positive
TN: <span class="hljs-literal">True</span> negative
FP: <span class="hljs-literal">False</span> positive
FN: <span class="hljs-literal">False</span> negative`}}),qe=new M({props:{code:"accuracy.citation",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.citation
<span class="hljs-meta">@article{scikit-learn,</span>
  title={Scikit-learn: Machine Learning <span class="hljs-keyword">in</span> {P}ython},
  author={Pedregosa, F. <span class="hljs-keyword">and</span> Varoquaux, G. <span class="hljs-keyword">and</span> Gramfort, A. <span class="hljs-keyword">and</span> Michel, V.
         <span class="hljs-keyword">and</span> Thirion, B. <span class="hljs-keyword">and</span> Grisel, O. <span class="hljs-keyword">and</span> Blondel, M. <span class="hljs-keyword">and</span> Prettenhofer, P.
         <span class="hljs-keyword">and</span> Weiss, R. <span class="hljs-keyword">and</span> Dubourg, V. <span class="hljs-keyword">and</span> Vanderplas, J. <span class="hljs-keyword">and</span> Passos, A. <span class="hljs-keyword">and</span>
         Cournapeau, D. <span class="hljs-keyword">and</span> Brucher, M. <span class="hljs-keyword">and</span> Perrot, M. <span class="hljs-keyword">and</span> Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={<span class="hljs-number">12</span>},
  pages={<span class="hljs-number">2825</span>--<span class="hljs-number">2830</span>},
  year={<span class="hljs-number">2011</span>}
}`}}),Me=new M({props:{code:"accuracy.features",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.features
{
    <span class="hljs-string">&#x27;predictions&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
    <span class="hljs-string">&#x27;references&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)
}`}}),ce=new Fi({props:{$$slots:{default:[Gi]},$$scope:{ctx:Ma}}}),Be=new q({}),Le=new q({}),Re=new M({props:{code:"accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1])",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.compute(references=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], predictions=[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>])
{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.5</span>}`}}),Fe=new q({}),He=new M({props:{code:`for ref, pred in zip([0,1,0,1], [1,0,0,1]):
    accuracy.add(references=ref, predictions=pred)
accuracy.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> ref, pred <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]):
<span class="hljs-meta">&gt;&gt;&gt; </span>    accuracy.add(references=ref, predictions=pred)
<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.compute()
{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.5</span>}`}}),Ue=new M({props:{code:`for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):
    accuracy.add_batch(references=refs, predictions=preds)
accuracy.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> refs, preds <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>([[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]], [[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]]):
<span class="hljs-meta">&gt;&gt;&gt; </span>    accuracy.add_batch(references=refs, predictions=preds)
<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.compute()
{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.5</span>}`}}),We=new M({props:{code:`for model_inputs, gold_standards in evaluation_dataset:
    predictions = model(model_inputs)
    metric.add_batch(references=gold_standards, predictions=predictions)
metric.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> model_inputs, gold_standards <span class="hljs-keyword">in</span> evaluation_dataset:
<span class="hljs-meta">&gt;&gt;&gt; </span>    predictions = model(model_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric.add_batch(references=gold_standards, predictions=predictions)
<span class="hljs-meta">&gt;&gt;&gt; </span>metric.compute()`}}),Ye=new q({}),ze=new q({}),{c(){E=s("meta"),z=d(),$=s("h1"),A=s("a"),B=s("span"),h(T.$$.fragment),D=d(),J=s("span"),ao=l("A quick tour"),Ba=d(),Qe=s("p"),so=l("\u{1F917} Evaluate provides access to a wide range of evaluation tools. It covers a range of modalities such as text, computer vision, audio, etc. as well as tools to evaluate models or datasets. These tools are split into three categories."),La=d(),L=s("h2"),K=s("a"),ut=s("span"),h(ye.$$.fragment),oo=d(),ht=s("span"),ro=l("Three types of evaluations"),Ra=d(),Xe=s("p"),lo=l("There are different aspects of a typical machine learning pipeline that can be evaluated and for aspect we provide a tool:"),Fa=d(),C=s("ul"),Q=s("li"),ft=s("strong"),no=l("Metric"),io=l(": A metric is used to evaluate a model\u2019s performance and usually involves the model\u2019s predictions as well as some ground truth labels. You can find all integrated metrics at "),Ze=s("a"),co=l("evaluate-metric"),po=l("."),uo=d(),X=s("li"),mt=s("strong"),ho=l("Comparison"),fo=l(": A comparison is used to compare two models. This can for example be done by comparing their predictions to ground truth labels and computing their agreement. You can find all integrated comparisons at "),et=s("a"),mo=l("evaluate-comparison"),vo=l("."),yo=d(),Z=s("li"),vt=s("strong"),_o=l("Measurement"),wo=l(": The dataset is as important as the model trained on it. With measurements one can investigate a dataset\u2019s properties. You can find all integrated measurements at "),tt=s("a"),Eo=l("evaluate-measurement"),bo=l("."),Ga=d(),ee=s("p"),$o=l("Each metric, comparison, and measurement is a separate Python module, but for using any of them, there is a single entry point: "),yt=s("code"),go=l("evaluate.load"),ko=l("!"),Ha=d(),R=s("h2"),te=s("a"),_t=s("span"),h(_e.$$.fragment),jo=d(),wt=s("span"),To=l("Load"),Ua=d(),ae=s("p"),Ao=l("Any metric, comparison, or measurement is loaded with the "),Et=s("code"),Po=l("evaluate.load"),Do=l(" function:"),Wa=d(),h(we.$$.fragment),Ya=d(),at=s("p"),Co=l("If you want to make sure you are loading the right type of module (especially if there are name clashes) you can explicitely pass the type:"),Va=d(),h(Ee.$$.fragment),za=d(),F=s("h3"),se=s("a"),bt=s("span"),h(be.$$.fragment),Oo=d(),$t=s("span"),xo=l("Community modules"),Ja=d(),st=s("p"),No=l("Besides the modules implemented in \u{1F917} Evaluate you can also load any community module by prepending the user\u2019s username:"),Ka=d(),h($e.$$.fragment),Qa=d(),G=s("h2"),oe=s("a"),gt=s("span"),h(ge.$$.fragment),So=d(),kt=s("span"),Io=l("Module attributes"),Xa=d(),re=s("p"),qo=l("All evalution modules come with a range of useful attributes that help to use a module stored in a "),ke=s("a"),jt=s("code"),Mo=l("EvaluationModuleInfo"),Bo=l(" object."),Za=d(),le=s("table"),Tt=s("thead"),je=s("tr"),At=s("th"),Lo=l("Attribute"),Ro=d(),Pt=s("th"),Fo=l("Description"),Go=d(),w=s("tbody"),Te=s("tr"),Dt=s("td"),Ct=s("code"),Ho=l("description"),Uo=d(),Ot=s("td"),Wo=l("A short description of the evaluation module."),Yo=d(),Ae=s("tr"),xt=s("td"),Nt=s("code"),Vo=l("citation"),zo=d(),St=s("td"),Jo=l("A BibTex string for citation when available."),Ko=d(),Pe=s("tr"),It=s("td"),qt=s("code"),Qo=l("features"),Xo=d(),De=s("td"),Zo=l("A "),Mt=s("code"),er=l("Features"),tr=l(" object defining the input format."),ar=d(),Ce=s("tr"),Bt=s("td"),Lt=s("code"),sr=l("inputs_description"),or=d(),Rt=s("td"),rr=l("This is equivalent to the modules docstring."),lr=d(),Oe=s("tr"),Ft=s("td"),Gt=s("code"),nr=l("homepage"),ir=d(),Ht=s("td"),cr=l("The homepage of the module."),dr=d(),xe=s("tr"),Ut=s("td"),Wt=s("code"),pr=l("license"),ur=d(),Yt=s("td"),hr=l("The license of the module."),fr=d(),Ne=s("tr"),Vt=s("td"),zt=s("code"),mr=l("codebase_urls"),vr=d(),Jt=s("td"),yr=l("Link to the code behind the module."),_r=d(),Se=s("tr"),Kt=s("td"),Qt=s("code"),wr=l("reference_urls"),Er=d(),Xt=s("td"),br=l("Additional reference URLs."),es=d(),ne=s("p"),$r=l("Let\u2019s have a look at a few examples. First, let\u2019s look at the "),Zt=s("code"),gr=l("description"),kr=l(" attribute of the accuracy metric:"),ts=d(),h(Ie.$$.fragment),as=d(),ie=s("p"),jr=l("You can see that it describes how the metric works in theory. If you use this metric for your work, especially if it is an academic publication you want to reference it properly. For that you can look at the "),ea=s("code"),Tr=l("citation"),Ar=l(" attribute:"),ss=d(),h(qe.$$.fragment),os=d(),ot=s("p"),Pr=l("Before we can apply a metric or other evaluation module to a use-case, we need to know what the input format of the metric is:"),rs=d(),h(Me.$$.fragment),ls=d(),h(ce.$$.fragment),ns=d(),H=s("h2"),de=s("a"),ta=s("span"),h(Be.$$.fragment),Dr=d(),aa=s("span"),Cr=l("Compute"),is=d(),rt=s("p"),Or=l("Now that we know how the evaluation module works and what should go in there we want to actually use it! When it comes to computing the actual score there are two main ways to do it:"),cs=d(),pe=s("ol"),sa=s("li"),xr=l("All-in-one"),Nr=d(),oa=s("li"),Sr=l("Incremental"),ds=d(),g=s("p"),Ir=l("In the incremental approach the necessary inputs are added to the module with "),ra=s("code"),qr=l("add"),Mr=l(" or "),la=s("code"),Br=l("add_batch"),Lr=l(" and the score is calculated at the end with "),na=s("code"),Rr=l("compute()"),Fr=l(". Alternatively, one can pass all the inputs at once to "),ia=s("code"),Gr=l("compute()"),Hr=l(". Let\u2019s have a look at the two approaches."),ps=d(),U=s("h3"),ue=s("a"),ca=s("span"),h(Le.$$.fragment),Ur=d(),da=s("span"),pa=s("code"),Wr=l("compute()"),us=l("\n\nThe simplest way to calculate the score of an evaluation module is by calling `compute()` directly with the necessary inputs. Simply pass the inputs as seen in `features` to the `compute()` method.\n\n	"),h(Re.$$.fragment),hs=d(),O=s("p"),Yr=l("Evaluation modules return the results in a dictionary. However, in some instances you build up the predictions iteratively or in a distributed fashion in which case "),ua=s("code"),Vr=l("add"),zr=l(" or "),ha=s("code"),Jr=l("add_batch"),Kr=l(" are useful."),fs=d(),W=s("h3"),he=s("a"),fa=s("span"),h(Fe.$$.fragment),Qr=d(),Ge=s("span"),ma=s("code"),Xr=l("add"),Zr=l(" and "),va=s("code"),el=l("add_batch"),ms=d(),k=s("p"),tl=l("In many evaluation pipelines you build the predictions iteratively such as in a for-loop. In that case you could store the predictions in a list and at the end pass them to "),ya=s("code"),al=l("compute()"),sl=l(". With "),_a=s("code"),ol=l("add"),rl=l(" and "),wa=s("code"),ll=l("add_batch"),nl=l(" you can circumvent the step of storing the predictions separately. If you are only creating single predictions at a time you can use "),Ea=s("code"),il=l("add"),cl=l(":"),vs=d(),h(He.$$.fragment),ys=d(),P=s("p"),dl=l("Once you have gathered all predictions you can call "),ba=s("code"),pl=l("compute()"),ul=l(" to compute the score based on all stored values. When getting predictions and references in batches you can use "),$a=s("code"),hl=l("add_batch"),fl=l(" which adds a list elements for later processing. The rest works as with "),ga=s("code"),ml=l("add"),vl=l(":"),_s=d(),h(Ue.$$.fragment),ws=d(),lt=s("p"),yl=l("This is especially useful when you need to get the predictions from your model in batches:"),Es=d(),h(We.$$.fragment),bs=d(),Y=s("h3"),fe=s("a"),ka=s("span"),h(Ye.$$.fragment),_l=d(),ja=s("span"),wl=l("Distributed evaluation"),$s=d(),j=s("p"),El=l("Computing metrics in a distributed environment can be tricky. Metric evaluation is executed in separate Python processes, or nodes, on different subsets of a dataset. Typically, when a metric score is additive ("),Ta=s("code"),bl=l("f(AuB) = f(A) + f(B)"),$l=l("), you can use distributed reduce operations to gather the scores for each subset of the dataset. But when a metric is non-additive ("),Aa=s("code"),gl=l("f(AuB) \u2260 f(A) + f(B)"),kl=l("), it\u2019s not that simple. For example, you can\u2019t take the sum of the "),Ve=s("a"),jl=l("F1"),Tl=l(" scores of each data subset as your "),Pa=s("strong"),Al=l("final metric"),Pl=l("."),gs=d(),nt=s("p"),Dl=l("A common way to overcome this issue is to fallback on single process evaluation. The metrics are evaluated on a single GPU, which becomes inefficient."),ks=d(),x=s("p"),Cl=l("\u{1F917} Evaluate solves this issue by only computing the final metric on the first node. The predictions and references are computed and provided to the metric separately for each node. These are temporarily stored in an Apache Arrow table, avoiding cluttering the GPU or CPU memory. When you are ready to "),Da=s("code"),Ol=l("EvaluationModule.compute"),xl=l(" the final metric, the first node is able to access the predictions and references stored on all the other nodes. Once it has gathered all the predictions and references, "),Ca=s("code"),Nl=l("EvaluationModule.compute"),Sl=l(" will perform the final metric evaluation."),js=d(),it=s("p"),Il=l("This solution allows \u{1F917} Evaluate to perform distributed predictions, which is important for evaluation speed in distributed settings. At the same time, you can also use complex non-additive metrics without wasting valuable GPU or CPU memory."),Ts=d(),V=s("h2"),me=s("a"),Oa=s("span"),h(ze.$$.fragment),ql=d(),xa=s("span"),Ml=l("Save and share"),As=d(),Je=s("p"),Na=s("strong"),Bl=l("TODO"),Ll=l(": add references to save and push_to_hub."),this.h()},l(e){const i=Li('[data-svelte="svelte-1phssyn"]',document.head);E=o(i,"META",{name:!0,content:!0}),i.forEach(a),z=p(e),$=o(e,"H1",{class:!0});var Ke=r($);A=o(Ke,"A",{id:!0,class:!0,href:!0});var Fl=r(A);B=o(Fl,"SPAN",{});var Gl=r(B);f(T.$$.fragment,Gl),Gl.forEach(a),Fl.forEach(a),D=p(Ke),J=o(Ke,"SPAN",{});var Hl=r(J);ao=n(Hl,"A quick tour"),Hl.forEach(a),Ke.forEach(a),Ba=p(e),Qe=o(e,"P",{});var Ul=r(Qe);so=n(Ul,"\u{1F917} Evaluate provides access to a wide range of evaluation tools. It covers a range of modalities such as text, computer vision, audio, etc. as well as tools to evaluate models or datasets. These tools are split into three categories."),Ul.forEach(a),La=p(e),L=o(e,"H2",{class:!0});var Ds=r(L);K=o(Ds,"A",{id:!0,class:!0,href:!0});var Wl=r(K);ut=o(Wl,"SPAN",{});var Yl=r(ut);f(ye.$$.fragment,Yl),Yl.forEach(a),Wl.forEach(a),oo=p(Ds),ht=o(Ds,"SPAN",{});var Vl=r(ht);ro=n(Vl,"Three types of evaluations"),Vl.forEach(a),Ds.forEach(a),Ra=p(e),Xe=o(e,"P",{});var zl=r(Xe);lo=n(zl,"There are different aspects of a typical machine learning pipeline that can be evaluated and for aspect we provide a tool:"),zl.forEach(a),Fa=p(e),C=o(e,"UL",{});var ct=r(C);Q=o(ct,"LI",{});var Sa=r(Q);ft=o(Sa,"STRONG",{});var Jl=r(ft);no=n(Jl,"Metric"),Jl.forEach(a),io=n(Sa,": A metric is used to evaluate a model\u2019s performance and usually involves the model\u2019s predictions as well as some ground truth labels. You can find all integrated metrics at "),Ze=o(Sa,"A",{href:!0});var Kl=r(Ze);co=n(Kl,"evaluate-metric"),Kl.forEach(a),po=n(Sa,"."),Sa.forEach(a),uo=p(ct),X=o(ct,"LI",{});var Ia=r(X);mt=o(Ia,"STRONG",{});var Ql=r(mt);ho=n(Ql,"Comparison"),Ql.forEach(a),fo=n(Ia,": A comparison is used to compare two models. This can for example be done by comparing their predictions to ground truth labels and computing their agreement. You can find all integrated comparisons at "),et=o(Ia,"A",{href:!0});var Xl=r(et);mo=n(Xl,"evaluate-comparison"),Xl.forEach(a),vo=n(Ia,"."),Ia.forEach(a),yo=p(ct),Z=o(ct,"LI",{});var qa=r(Z);vt=o(qa,"STRONG",{});var Zl=r(vt);_o=n(Zl,"Measurement"),Zl.forEach(a),wo=n(qa,": The dataset is as important as the model trained on it. With measurements one can investigate a dataset\u2019s properties. You can find all integrated measurements at "),tt=o(qa,"A",{href:!0});var en=r(tt);Eo=n(en,"evaluate-measurement"),en.forEach(a),bo=n(qa,"."),qa.forEach(a),ct.forEach(a),Ga=p(e),ee=o(e,"P",{});var Cs=r(ee);$o=n(Cs,"Each metric, comparison, and measurement is a separate Python module, but for using any of them, there is a single entry point: "),yt=o(Cs,"CODE",{});var tn=r(yt);go=n(tn,"evaluate.load"),tn.forEach(a),ko=n(Cs,"!"),Cs.forEach(a),Ha=p(e),R=o(e,"H2",{class:!0});var Os=r(R);te=o(Os,"A",{id:!0,class:!0,href:!0});var an=r(te);_t=o(an,"SPAN",{});var sn=r(_t);f(_e.$$.fragment,sn),sn.forEach(a),an.forEach(a),jo=p(Os),wt=o(Os,"SPAN",{});var on=r(wt);To=n(on,"Load"),on.forEach(a),Os.forEach(a),Ua=p(e),ae=o(e,"P",{});var xs=r(ae);Ao=n(xs,"Any metric, comparison, or measurement is loaded with the "),Et=o(xs,"CODE",{});var rn=r(Et);Po=n(rn,"evaluate.load"),rn.forEach(a),Do=n(xs," function:"),xs.forEach(a),Wa=p(e),f(we.$$.fragment,e),Ya=p(e),at=o(e,"P",{});var ln=r(at);Co=n(ln,"If you want to make sure you are loading the right type of module (especially if there are name clashes) you can explicitely pass the type:"),ln.forEach(a),Va=p(e),f(Ee.$$.fragment,e),za=p(e),F=o(e,"H3",{class:!0});var Ns=r(F);se=o(Ns,"A",{id:!0,class:!0,href:!0});var nn=r(se);bt=o(nn,"SPAN",{});var cn=r(bt);f(be.$$.fragment,cn),cn.forEach(a),nn.forEach(a),Oo=p(Ns),$t=o(Ns,"SPAN",{});var dn=r($t);xo=n(dn,"Community modules"),dn.forEach(a),Ns.forEach(a),Ja=p(e),st=o(e,"P",{});var pn=r(st);No=n(pn,"Besides the modules implemented in \u{1F917} Evaluate you can also load any community module by prepending the user\u2019s username:"),pn.forEach(a),Ka=p(e),f($e.$$.fragment,e),Qa=p(e),G=o(e,"H2",{class:!0});var Ss=r(G);oe=o(Ss,"A",{id:!0,class:!0,href:!0});var un=r(oe);gt=o(un,"SPAN",{});var hn=r(gt);f(ge.$$.fragment,hn),hn.forEach(a),un.forEach(a),So=p(Ss),kt=o(Ss,"SPAN",{});var fn=r(kt);Io=n(fn,"Module attributes"),fn.forEach(a),Ss.forEach(a),Xa=p(e),re=o(e,"P",{});var Is=r(re);qo=n(Is,"All evalution modules come with a range of useful attributes that help to use a module stored in a "),ke=o(Is,"A",{href:!0,rel:!0});var mn=r(ke);jt=o(mn,"CODE",{});var vn=r(jt);Mo=n(vn,"EvaluationModuleInfo"),vn.forEach(a),mn.forEach(a),Bo=n(Is," object."),Is.forEach(a),Za=p(e),le=o(e,"TABLE",{});var qs=r(le);Tt=o(qs,"THEAD",{});var yn=r(Tt);je=o(yn,"TR",{});var Ms=r(je);At=o(Ms,"TH",{});var _n=r(At);Lo=n(_n,"Attribute"),_n.forEach(a),Ro=p(Ms),Pt=o(Ms,"TH",{});var wn=r(Pt);Fo=n(wn,"Description"),wn.forEach(a),Ms.forEach(a),yn.forEach(a),Go=p(qs),w=o(qs,"TBODY",{});var b=r(w);Te=o(b,"TR",{});var Bs=r(Te);Dt=o(Bs,"TD",{});var En=r(Dt);Ct=o(En,"CODE",{});var bn=r(Ct);Ho=n(bn,"description"),bn.forEach(a),En.forEach(a),Uo=p(Bs),Ot=o(Bs,"TD",{});var $n=r(Ot);Wo=n($n,"A short description of the evaluation module."),$n.forEach(a),Bs.forEach(a),Yo=p(b),Ae=o(b,"TR",{});var Ls=r(Ae);xt=o(Ls,"TD",{});var gn=r(xt);Nt=o(gn,"CODE",{});var kn=r(Nt);Vo=n(kn,"citation"),kn.forEach(a),gn.forEach(a),zo=p(Ls),St=o(Ls,"TD",{});var jn=r(St);Jo=n(jn,"A BibTex string for citation when available."),jn.forEach(a),Ls.forEach(a),Ko=p(b),Pe=o(b,"TR",{});var Rs=r(Pe);It=o(Rs,"TD",{});var Tn=r(It);qt=o(Tn,"CODE",{});var An=r(qt);Qo=n(An,"features"),An.forEach(a),Tn.forEach(a),Xo=p(Rs),De=o(Rs,"TD",{});var Fs=r(De);Zo=n(Fs,"A "),Mt=o(Fs,"CODE",{});var Pn=r(Mt);er=n(Pn,"Features"),Pn.forEach(a),tr=n(Fs," object defining the input format."),Fs.forEach(a),Rs.forEach(a),ar=p(b),Ce=o(b,"TR",{});var Gs=r(Ce);Bt=o(Gs,"TD",{});var Dn=r(Bt);Lt=o(Dn,"CODE",{});var Cn=r(Lt);sr=n(Cn,"inputs_description"),Cn.forEach(a),Dn.forEach(a),or=p(Gs),Rt=o(Gs,"TD",{});var On=r(Rt);rr=n(On,"This is equivalent to the modules docstring."),On.forEach(a),Gs.forEach(a),lr=p(b),Oe=o(b,"TR",{});var Hs=r(Oe);Ft=o(Hs,"TD",{});var xn=r(Ft);Gt=o(xn,"CODE",{});var Nn=r(Gt);nr=n(Nn,"homepage"),Nn.forEach(a),xn.forEach(a),ir=p(Hs),Ht=o(Hs,"TD",{});var Sn=r(Ht);cr=n(Sn,"The homepage of the module."),Sn.forEach(a),Hs.forEach(a),dr=p(b),xe=o(b,"TR",{});var Us=r(xe);Ut=o(Us,"TD",{});var In=r(Ut);Wt=o(In,"CODE",{});var qn=r(Wt);pr=n(qn,"license"),qn.forEach(a),In.forEach(a),ur=p(Us),Yt=o(Us,"TD",{});var Mn=r(Yt);hr=n(Mn,"The license of the module."),Mn.forEach(a),Us.forEach(a),fr=p(b),Ne=o(b,"TR",{});var Ws=r(Ne);Vt=o(Ws,"TD",{});var Bn=r(Vt);zt=o(Bn,"CODE",{});var Ln=r(zt);mr=n(Ln,"codebase_urls"),Ln.forEach(a),Bn.forEach(a),vr=p(Ws),Jt=o(Ws,"TD",{});var Rn=r(Jt);yr=n(Rn,"Link to the code behind the module."),Rn.forEach(a),Ws.forEach(a),_r=p(b),Se=o(b,"TR",{});var Ys=r(Se);Kt=o(Ys,"TD",{});var Fn=r(Kt);Qt=o(Fn,"CODE",{});var Gn=r(Qt);wr=n(Gn,"reference_urls"),Gn.forEach(a),Fn.forEach(a),Er=p(Ys),Xt=o(Ys,"TD",{});var Hn=r(Xt);br=n(Hn,"Additional reference URLs."),Hn.forEach(a),Ys.forEach(a),b.forEach(a),qs.forEach(a),es=p(e),ne=o(e,"P",{});var Vs=r(ne);$r=n(Vs,"Let\u2019s have a look at a few examples. First, let\u2019s look at the "),Zt=o(Vs,"CODE",{});var Un=r(Zt);gr=n(Un,"description"),Un.forEach(a),kr=n(Vs," attribute of the accuracy metric:"),Vs.forEach(a),ts=p(e),f(Ie.$$.fragment,e),as=p(e),ie=o(e,"P",{});var zs=r(ie);jr=n(zs,"You can see that it describes how the metric works in theory. If you use this metric for your work, especially if it is an academic publication you want to reference it properly. For that you can look at the "),ea=o(zs,"CODE",{});var Wn=r(ea);Tr=n(Wn,"citation"),Wn.forEach(a),Ar=n(zs," attribute:"),zs.forEach(a),ss=p(e),f(qe.$$.fragment,e),os=p(e),ot=o(e,"P",{});var Yn=r(ot);Pr=n(Yn,"Before we can apply a metric or other evaluation module to a use-case, we need to know what the input format of the metric is:"),Yn.forEach(a),rs=p(e),f(Me.$$.fragment,e),ls=p(e),f(ce.$$.fragment,e),ns=p(e),H=o(e,"H2",{class:!0});var Js=r(H);de=o(Js,"A",{id:!0,class:!0,href:!0});var Vn=r(de);ta=o(Vn,"SPAN",{});var zn=r(ta);f(Be.$$.fragment,zn),zn.forEach(a),Vn.forEach(a),Dr=p(Js),aa=o(Js,"SPAN",{});var Jn=r(aa);Cr=n(Jn,"Compute"),Jn.forEach(a),Js.forEach(a),is=p(e),rt=o(e,"P",{});var Kn=r(rt);Or=n(Kn,"Now that we know how the evaluation module works and what should go in there we want to actually use it! When it comes to computing the actual score there are two main ways to do it:"),Kn.forEach(a),cs=p(e),pe=o(e,"OL",{});var Ks=r(pe);sa=o(Ks,"LI",{});var Qn=r(sa);xr=n(Qn,"All-in-one"),Qn.forEach(a),Nr=p(Ks),oa=o(Ks,"LI",{});var Xn=r(oa);Sr=n(Xn,"Incremental"),Xn.forEach(a),Ks.forEach(a),ds=p(e),g=o(e,"P",{});var N=r(g);Ir=n(N,"In the incremental approach the necessary inputs are added to the module with "),ra=o(N,"CODE",{});var Zn=r(ra);qr=n(Zn,"add"),Zn.forEach(a),Mr=n(N," or "),la=o(N,"CODE",{});var ei=r(la);Br=n(ei,"add_batch"),ei.forEach(a),Lr=n(N," and the score is calculated at the end with "),na=o(N,"CODE",{});var ti=r(na);Rr=n(ti,"compute()"),ti.forEach(a),Fr=n(N,". Alternatively, one can pass all the inputs at once to "),ia=o(N,"CODE",{});var ai=r(ia);Gr=n(ai,"compute()"),ai.forEach(a),Hr=n(N,". Let\u2019s have a look at the two approaches."),N.forEach(a),ps=p(e),U=o(e,"H3",{class:!0});var Qs=r(U);ue=o(Qs,"A",{id:!0,class:!0,href:!0});var si=r(ue);ca=o(si,"SPAN",{});var oi=r(ca);f(Le.$$.fragment,oi),oi.forEach(a),si.forEach(a),Ur=p(Qs),da=o(Qs,"SPAN",{});var ri=r(da);pa=o(ri,"CODE",{});var li=r(pa);Wr=n(li,"compute()"),li.forEach(a),ri.forEach(a),Qs.forEach(a),us=n(e,"\n\nThe simplest way to calculate the score of an evaluation module is by calling `compute()` directly with the necessary inputs. Simply pass the inputs as seen in `features` to the `compute()` method.\n\n	"),f(Re.$$.fragment,e),hs=p(e),O=o(e,"P",{});var dt=r(O);Yr=n(dt,"Evaluation modules return the results in a dictionary. However, in some instances you build up the predictions iteratively or in a distributed fashion in which case "),ua=o(dt,"CODE",{});var ni=r(ua);Vr=n(ni,"add"),ni.forEach(a),zr=n(dt," or "),ha=o(dt,"CODE",{});var ii=r(ha);Jr=n(ii,"add_batch"),ii.forEach(a),Kr=n(dt," are useful."),dt.forEach(a),fs=p(e),W=o(e,"H3",{class:!0});var Xs=r(W);he=o(Xs,"A",{id:!0,class:!0,href:!0});var ci=r(he);fa=o(ci,"SPAN",{});var di=r(fa);f(Fe.$$.fragment,di),di.forEach(a),ci.forEach(a),Qr=p(Xs),Ge=o(Xs,"SPAN",{});var Zs=r(Ge);ma=o(Zs,"CODE",{});var pi=r(ma);Xr=n(pi,"add"),pi.forEach(a),Zr=n(Zs," and "),va=o(Zs,"CODE",{});var ui=r(va);el=n(ui,"add_batch"),ui.forEach(a),Zs.forEach(a),Xs.forEach(a),ms=p(e),k=o(e,"P",{});var S=r(k);tl=n(S,"In many evaluation pipelines you build the predictions iteratively such as in a for-loop. In that case you could store the predictions in a list and at the end pass them to "),ya=o(S,"CODE",{});var hi=r(ya);al=n(hi,"compute()"),hi.forEach(a),sl=n(S,". With "),_a=o(S,"CODE",{});var fi=r(_a);ol=n(fi,"add"),fi.forEach(a),rl=n(S," and "),wa=o(S,"CODE",{});var mi=r(wa);ll=n(mi,"add_batch"),mi.forEach(a),nl=n(S," you can circumvent the step of storing the predictions separately. If you are only creating single predictions at a time you can use "),Ea=o(S,"CODE",{});var vi=r(Ea);il=n(vi,"add"),vi.forEach(a),cl=n(S,":"),S.forEach(a),vs=p(e),f(He.$$.fragment,e),ys=p(e),P=o(e,"P",{});var ve=r(P);dl=n(ve,"Once you have gathered all predictions you can call "),ba=o(ve,"CODE",{});var yi=r(ba);pl=n(yi,"compute()"),yi.forEach(a),ul=n(ve," to compute the score based on all stored values. When getting predictions and references in batches you can use "),$a=o(ve,"CODE",{});var _i=r($a);hl=n(_i,"add_batch"),_i.forEach(a),fl=n(ve," which adds a list elements for later processing. The rest works as with "),ga=o(ve,"CODE",{});var wi=r(ga);ml=n(wi,"add"),wi.forEach(a),vl=n(ve,":"),ve.forEach(a),_s=p(e),f(Ue.$$.fragment,e),ws=p(e),lt=o(e,"P",{});var Ei=r(lt);yl=n(Ei,"This is especially useful when you need to get the predictions from your model in batches:"),Ei.forEach(a),Es=p(e),f(We.$$.fragment,e),bs=p(e),Y=o(e,"H3",{class:!0});var eo=r(Y);fe=o(eo,"A",{id:!0,class:!0,href:!0});var bi=r(fe);ka=o(bi,"SPAN",{});var $i=r(ka);f(Ye.$$.fragment,$i),$i.forEach(a),bi.forEach(a),_l=p(eo),ja=o(eo,"SPAN",{});var gi=r(ja);wl=n(gi,"Distributed evaluation"),gi.forEach(a),eo.forEach(a),$s=p(e),j=o(e,"P",{});var I=r(j);El=n(I,"Computing metrics in a distributed environment can be tricky. Metric evaluation is executed in separate Python processes, or nodes, on different subsets of a dataset. Typically, when a metric score is additive ("),Ta=o(I,"CODE",{});var ki=r(Ta);bl=n(ki,"f(AuB) = f(A) + f(B)"),ki.forEach(a),$l=n(I,"), you can use distributed reduce operations to gather the scores for each subset of the dataset. But when a metric is non-additive ("),Aa=o(I,"CODE",{});var ji=r(Aa);gl=n(ji,"f(AuB) \u2260 f(A) + f(B)"),ji.forEach(a),kl=n(I,"), it\u2019s not that simple. For example, you can\u2019t take the sum of the "),Ve=o(I,"A",{href:!0,rel:!0});var Ti=r(Ve);jl=n(Ti,"F1"),Ti.forEach(a),Tl=n(I," scores of each data subset as your "),Pa=o(I,"STRONG",{});var Ai=r(Pa);Al=n(Ai,"final metric"),Ai.forEach(a),Pl=n(I,"."),I.forEach(a),gs=p(e),nt=o(e,"P",{});var Pi=r(nt);Dl=n(Pi,"A common way to overcome this issue is to fallback on single process evaluation. The metrics are evaluated on a single GPU, which becomes inefficient."),Pi.forEach(a),ks=p(e),x=o(e,"P",{});var pt=r(x);Cl=n(pt,"\u{1F917} Evaluate solves this issue by only computing the final metric on the first node. The predictions and references are computed and provided to the metric separately for each node. These are temporarily stored in an Apache Arrow table, avoiding cluttering the GPU or CPU memory. When you are ready to "),Da=o(pt,"CODE",{});var Di=r(Da);Ol=n(Di,"EvaluationModule.compute"),Di.forEach(a),xl=n(pt," the final metric, the first node is able to access the predictions and references stored on all the other nodes. Once it has gathered all the predictions and references, "),Ca=o(pt,"CODE",{});var Ci=r(Ca);Nl=n(Ci,"EvaluationModule.compute"),Ci.forEach(a),Sl=n(pt," will perform the final metric evaluation."),pt.forEach(a),js=p(e),it=o(e,"P",{});var Oi=r(it);Il=n(Oi,"This solution allows \u{1F917} Evaluate to perform distributed predictions, which is important for evaluation speed in distributed settings. At the same time, you can also use complex non-additive metrics without wasting valuable GPU or CPU memory."),Oi.forEach(a),Ts=p(e),V=o(e,"H2",{class:!0});var to=r(V);me=o(to,"A",{id:!0,class:!0,href:!0});var xi=r(me);Oa=o(xi,"SPAN",{});var Ni=r(Oa);f(ze.$$.fragment,Ni),Ni.forEach(a),xi.forEach(a),ql=p(to),xa=o(to,"SPAN",{});var Si=r(xa);Ml=n(Si,"Save and share"),Si.forEach(a),to.forEach(a),As=p(e),Je=o(e,"P",{});var Rl=r(Je);Na=o(Rl,"STRONG",{});var Ii=r(Na);Bl=n(Ii,"TODO"),Ii.forEach(a),Ll=n(Rl,": add references to save and push_to_hub."),Rl.forEach(a),this.h()},h(){u(E,"name","hf:doc:metadata"),u(E,"content",JSON.stringify(Ui)),u(A,"id","a-quick-tour"),u(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(A,"href","#a-quick-tour"),u($,"class","relative group"),u(K,"id","three-types-of-evaluations"),u(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(K,"href","#three-types-of-evaluations"),u(L,"class","relative group"),u(Ze,"href","hf.co/evaluate-metric"),u(et,"href","hf.co/evaluate-comparison"),u(tt,"href","hf.co/evaluate-measurement"),u(te,"id","load"),u(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(te,"href","#load"),u(R,"class","relative group"),u(se,"id","community-modules"),u(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(se,"href","#community-modules"),u(F,"class","relative group"),u(oe,"id","module-attributes"),u(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(oe,"href","#module-attributes"),u(G,"class","relative group"),u(ke,"href","https://moon-ci-docs.huggingface.co/docs/evaluate/pr_72/en/package_reference/main_classes#evaluate.EvaluationModuleInfo"),u(ke,"rel","nofollow"),u(de,"id","compute"),u(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(de,"href","#compute"),u(H,"class","relative group"),u(ue,"id","compute"),u(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ue,"href","#compute"),u(U,"class","relative group"),u(he,"id","add-and-addbatch"),u(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(he,"href","#add-and-addbatch"),u(W,"class","relative group"),u(fe,"id","distributed-evaluation"),u(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(fe,"href","#distributed-evaluation"),u(Y,"class","relative group"),u(Ve,"href","https://huggingface.co/metrics/f1"),u(Ve,"rel","nofollow"),u(me,"id","save-and-share"),u(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(me,"href","#save-and-share"),u(V,"class","relative group")},m(e,i){t(document.head,E),c(e,z,i),c(e,$,i),t($,A),t(A,B),m(T,B,null),t($,D),t($,J),t(J,ao),c(e,Ba,i),c(e,Qe,i),t(Qe,so),c(e,La,i),c(e,L,i),t(L,K),t(K,ut),m(ye,ut,null),t(L,oo),t(L,ht),t(ht,ro),c(e,Ra,i),c(e,Xe,i),t(Xe,lo),c(e,Fa,i),c(e,C,i),t(C,Q),t(Q,ft),t(ft,no),t(Q,io),t(Q,Ze),t(Ze,co),t(Q,po),t(C,uo),t(C,X),t(X,mt),t(mt,ho),t(X,fo),t(X,et),t(et,mo),t(X,vo),t(C,yo),t(C,Z),t(Z,vt),t(vt,_o),t(Z,wo),t(Z,tt),t(tt,Eo),t(Z,bo),c(e,Ga,i),c(e,ee,i),t(ee,$o),t(ee,yt),t(yt,go),t(ee,ko),c(e,Ha,i),c(e,R,i),t(R,te),t(te,_t),m(_e,_t,null),t(R,jo),t(R,wt),t(wt,To),c(e,Ua,i),c(e,ae,i),t(ae,Ao),t(ae,Et),t(Et,Po),t(ae,Do),c(e,Wa,i),m(we,e,i),c(e,Ya,i),c(e,at,i),t(at,Co),c(e,Va,i),m(Ee,e,i),c(e,za,i),c(e,F,i),t(F,se),t(se,bt),m(be,bt,null),t(F,Oo),t(F,$t),t($t,xo),c(e,Ja,i),c(e,st,i),t(st,No),c(e,Ka,i),m($e,e,i),c(e,Qa,i),c(e,G,i),t(G,oe),t(oe,gt),m(ge,gt,null),t(G,So),t(G,kt),t(kt,Io),c(e,Xa,i),c(e,re,i),t(re,qo),t(re,ke),t(ke,jt),t(jt,Mo),t(re,Bo),c(e,Za,i),c(e,le,i),t(le,Tt),t(Tt,je),t(je,At),t(At,Lo),t(je,Ro),t(je,Pt),t(Pt,Fo),t(le,Go),t(le,w),t(w,Te),t(Te,Dt),t(Dt,Ct),t(Ct,Ho),t(Te,Uo),t(Te,Ot),t(Ot,Wo),t(w,Yo),t(w,Ae),t(Ae,xt),t(xt,Nt),t(Nt,Vo),t(Ae,zo),t(Ae,St),t(St,Jo),t(w,Ko),t(w,Pe),t(Pe,It),t(It,qt),t(qt,Qo),t(Pe,Xo),t(Pe,De),t(De,Zo),t(De,Mt),t(Mt,er),t(De,tr),t(w,ar),t(w,Ce),t(Ce,Bt),t(Bt,Lt),t(Lt,sr),t(Ce,or),t(Ce,Rt),t(Rt,rr),t(w,lr),t(w,Oe),t(Oe,Ft),t(Ft,Gt),t(Gt,nr),t(Oe,ir),t(Oe,Ht),t(Ht,cr),t(w,dr),t(w,xe),t(xe,Ut),t(Ut,Wt),t(Wt,pr),t(xe,ur),t(xe,Yt),t(Yt,hr),t(w,fr),t(w,Ne),t(Ne,Vt),t(Vt,zt),t(zt,mr),t(Ne,vr),t(Ne,Jt),t(Jt,yr),t(w,_r),t(w,Se),t(Se,Kt),t(Kt,Qt),t(Qt,wr),t(Se,Er),t(Se,Xt),t(Xt,br),c(e,es,i),c(e,ne,i),t(ne,$r),t(ne,Zt),t(Zt,gr),t(ne,kr),c(e,ts,i),m(Ie,e,i),c(e,as,i),c(e,ie,i),t(ie,jr),t(ie,ea),t(ea,Tr),t(ie,Ar),c(e,ss,i),m(qe,e,i),c(e,os,i),c(e,ot,i),t(ot,Pr),c(e,rs,i),m(Me,e,i),c(e,ls,i),m(ce,e,i),c(e,ns,i),c(e,H,i),t(H,de),t(de,ta),m(Be,ta,null),t(H,Dr),t(H,aa),t(aa,Cr),c(e,is,i),c(e,rt,i),t(rt,Or),c(e,cs,i),c(e,pe,i),t(pe,sa),t(sa,xr),t(pe,Nr),t(pe,oa),t(oa,Sr),c(e,ds,i),c(e,g,i),t(g,Ir),t(g,ra),t(ra,qr),t(g,Mr),t(g,la),t(la,Br),t(g,Lr),t(g,na),t(na,Rr),t(g,Fr),t(g,ia),t(ia,Gr),t(g,Hr),c(e,ps,i),c(e,U,i),t(U,ue),t(ue,ca),m(Le,ca,null),t(U,Ur),t(U,da),t(da,pa),t(pa,Wr),c(e,us,i),m(Re,e,i),c(e,hs,i),c(e,O,i),t(O,Yr),t(O,ua),t(ua,Vr),t(O,zr),t(O,ha),t(ha,Jr),t(O,Kr),c(e,fs,i),c(e,W,i),t(W,he),t(he,fa),m(Fe,fa,null),t(W,Qr),t(W,Ge),t(Ge,ma),t(ma,Xr),t(Ge,Zr),t(Ge,va),t(va,el),c(e,ms,i),c(e,k,i),t(k,tl),t(k,ya),t(ya,al),t(k,sl),t(k,_a),t(_a,ol),t(k,rl),t(k,wa),t(wa,ll),t(k,nl),t(k,Ea),t(Ea,il),t(k,cl),c(e,vs,i),m(He,e,i),c(e,ys,i),c(e,P,i),t(P,dl),t(P,ba),t(ba,pl),t(P,ul),t(P,$a),t($a,hl),t(P,fl),t(P,ga),t(ga,ml),t(P,vl),c(e,_s,i),m(Ue,e,i),c(e,ws,i),c(e,lt,i),t(lt,yl),c(e,Es,i),m(We,e,i),c(e,bs,i),c(e,Y,i),t(Y,fe),t(fe,ka),m(Ye,ka,null),t(Y,_l),t(Y,ja),t(ja,wl),c(e,$s,i),c(e,j,i),t(j,El),t(j,Ta),t(Ta,bl),t(j,$l),t(j,Aa),t(Aa,gl),t(j,kl),t(j,Ve),t(Ve,jl),t(j,Tl),t(j,Pa),t(Pa,Al),t(j,Pl),c(e,gs,i),c(e,nt,i),t(nt,Dl),c(e,ks,i),c(e,x,i),t(x,Cl),t(x,Da),t(Da,Ol),t(x,xl),t(x,Ca),t(Ca,Nl),t(x,Sl),c(e,js,i),c(e,it,i),t(it,Il),c(e,Ts,i),c(e,V,i),t(V,me),t(me,Oa),m(ze,Oa,null),t(V,ql),t(V,xa),t(xa,Ml),c(e,As,i),c(e,Je,i),t(Je,Na),t(Na,Bl),t(Je,Ll),Ps=!0},p(e,[i]){const Ke={};i&2&&(Ke.$$scope={dirty:i,ctx:e}),ce.$set(Ke)},i(e){Ps||(v(T.$$.fragment,e),v(ye.$$.fragment,e),v(_e.$$.fragment,e),v(we.$$.fragment,e),v(Ee.$$.fragment,e),v(be.$$.fragment,e),v($e.$$.fragment,e),v(ge.$$.fragment,e),v(Ie.$$.fragment,e),v(qe.$$.fragment,e),v(Me.$$.fragment,e),v(ce.$$.fragment,e),v(Be.$$.fragment,e),v(Le.$$.fragment,e),v(Re.$$.fragment,e),v(Fe.$$.fragment,e),v(He.$$.fragment,e),v(Ue.$$.fragment,e),v(We.$$.fragment,e),v(Ye.$$.fragment,e),v(ze.$$.fragment,e),Ps=!0)},o(e){y(T.$$.fragment,e),y(ye.$$.fragment,e),y(_e.$$.fragment,e),y(we.$$.fragment,e),y(Ee.$$.fragment,e),y(be.$$.fragment,e),y($e.$$.fragment,e),y(ge.$$.fragment,e),y(Ie.$$.fragment,e),y(qe.$$.fragment,e),y(Me.$$.fragment,e),y(ce.$$.fragment,e),y(Be.$$.fragment,e),y(Le.$$.fragment,e),y(Re.$$.fragment,e),y(Fe.$$.fragment,e),y(He.$$.fragment,e),y(Ue.$$.fragment,e),y(We.$$.fragment,e),y(Ye.$$.fragment,e),y(ze.$$.fragment,e),Ps=!1},d(e){a(E),e&&a(z),e&&a($),_(T),e&&a(Ba),e&&a(Qe),e&&a(La),e&&a(L),_(ye),e&&a(Ra),e&&a(Xe),e&&a(Fa),e&&a(C),e&&a(Ga),e&&a(ee),e&&a(Ha),e&&a(R),_(_e),e&&a(Ua),e&&a(ae),e&&a(Wa),_(we,e),e&&a(Ya),e&&a(at),e&&a(Va),_(Ee,e),e&&a(za),e&&a(F),_(be),e&&a(Ja),e&&a(st),e&&a(Ka),_($e,e),e&&a(Qa),e&&a(G),_(ge),e&&a(Xa),e&&a(re),e&&a(Za),e&&a(le),e&&a(es),e&&a(ne),e&&a(ts),_(Ie,e),e&&a(as),e&&a(ie),e&&a(ss),_(qe,e),e&&a(os),e&&a(ot),e&&a(rs),_(Me,e),e&&a(ls),_(ce,e),e&&a(ns),e&&a(H),_(Be),e&&a(is),e&&a(rt),e&&a(cs),e&&a(pe),e&&a(ds),e&&a(g),e&&a(ps),e&&a(U),_(Le),e&&a(us),_(Re,e),e&&a(hs),e&&a(O),e&&a(fs),e&&a(W),_(Fe),e&&a(ms),e&&a(k),e&&a(vs),_(He,e),e&&a(ys),e&&a(P),e&&a(_s),_(Ue,e),e&&a(ws),e&&a(lt),e&&a(Es),_(We,e),e&&a(bs),e&&a(Y),_(Ye),e&&a($s),e&&a(j),e&&a(gs),e&&a(nt),e&&a(ks),e&&a(x),e&&a(js),e&&a(it),e&&a(Ts),e&&a(V),_(ze),e&&a(As),e&&a(Je)}}}const Ui={local:"a-quick-tour",sections:[{local:"three-types-of-evaluations",title:"Three types of evaluations"},{local:"load",sections:[{local:"community-modules",title:"Community modules"}],title:"Load"},{local:"module-attributes",title:"Module attributes"},{local:"compute",sections:[{local:"compute",title:"`compute()`"},{local:"add-and-addbatch",title:"`add` and `add_batch`"},{local:"distributed-evaluation",title:"Distributed evaluation"}],title:"Compute"},{local:"save-and-share",title:"Save and share"}],title:"A quick tour"};function Wi(Ma){return Ri(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ki extends qi{constructor(E){super();Mi(this,E,Wi,Hi,Bi,{})}}export{Ki as default,Ui as metadata};
