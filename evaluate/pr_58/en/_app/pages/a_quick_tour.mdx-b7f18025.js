import{S as _n,i as bn,s as En,e as s,k as p,w as f,t as r,M as gn,c as o,d as a,m as d,a as l,x as m,h as n,b as h,G as t,g as c,y as v,L as $n,q as y,o as w,B as _,v as jn}from"../chunks/vendor-1392b85b.js";import{I as oe}from"../chunks/IconCopyLink-667b25d9.js";import{C as A}from"../chunks/CodeBlock-b4520661.js";function kn(Bl){let P,ca,D,L,Ve,le,Es,ze,gs,pa,Ie,$s,da,C,M,Je,re,js,Ke,ks,ha,Se,Ts,ua,j,B,Qe,As,Ps,qe,Ds,Cs,xs,F,Xe,Os,Ns,Le,Is,Ss,qs,R,Ze,Ls,Ms,Me,Bs,Fs,fa,H,Rs,Be,Hs,Gs,ma,x,G,et,ne,Us,tt,Ws,va,U,Ys,at,Vs,zs,ya,ie,wa,Fe,Js,_a,ce,ba,O,W,st,pe,Ks,ot,Qs,Ea,Re,Xs,ga,de,$a,N,Y,lt,he,Zs,rt,eo,ja,V,to,nt,ao,so,ka,z,it,ue,ct,oo,lo,pt,ro,no,u,fe,dt,ht,io,co,ut,po,ho,me,ft,mt,uo,fo,vt,mo,vo,ve,yt,wt,yo,wo,ye,_o,_t,bo,Eo,go,we,bt,Et,$o,jo,gt,ko,To,_e,$t,jt,Ao,Po,kt,Do,Co,be,Tt,At,xo,Oo,Pt,No,Io,Ee,Dt,Ct,So,qo,xt,Lo,Mo,ge,Ot,Nt,Bo,Fo,It,Ro,Ta,J,Ho,St,Go,Uo,Aa,$e,Pa,K,Wo,qt,Yo,Vo,Da,je,Ca,He,zo,xa,ke,Oa,Q,Jo,Lt,Ko,Qo,Na,I,X,Mt,Te,Xo,Bt,Zo,Ia,Ge,el,Sa,Z,Ft,tl,al,Rt,sl,qa,E,ol,Ht,ll,rl,Gt,nl,il,Ut,cl,pl,Wt,dl,hl,La,S,ee,Yt,Ae,ul,Vt,zt,fl,Ma,Pe,Ba,k,ml,Jt,vl,yl,Kt,wl,_l,Fa,q,te,Qt,De,bl,Ce,Xt,El,gl,Zt,$l,Ra,g,jl,ea,kl,Tl,ta,Al,Pl,aa,Dl,Cl,Ha,xe,Ga,$,xl,sa,Ol,Nl,oa,Il,Sl,la,ql,Ll,Ua,Oe,Wa,Ue,Ml,Ya,Ne,Va;return le=new oe({}),re=new oe({}),ne=new oe({}),ie=new A({props:{code:`import evaluate
accuracy = evaluate.load("accuracy")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate
<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)`}}),ce=new A({props:{code:'word_length = evaluate.load("word_length", type="measurement")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>word_length = evaluate.load(<span class="hljs-string">&quot;word_length&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;measurement&quot;</span>)'}}),pe=new oe({}),de=new A({props:{code:'element_count = evaluate.load("lvwerra/element_count", type="measurement")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>element_count = evaluate.load(<span class="hljs-string">&quot;lvwerra/element_count&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;measurement&quot;</span>)'}}),he=new oe({}),$e=new A({props:{code:`accuracy = evaluate.load("accuracy")
accuracy.description`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.description
Accuracy <span class="hljs-keyword">is</span> the proportion of correct predictions among the total number of cases processed. It can be computed <span class="hljs-keyword">with</span>:
Accuracy = (TP + TN) / (TP + TN + FP + FN)
 Where:
TP: <span class="hljs-literal">True</span> positive
TN: <span class="hljs-literal">True</span> negative
FP: <span class="hljs-literal">False</span> positive
FN: <span class="hljs-literal">False</span> negative`}}),je=new A({props:{code:"accuracy.citation",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.citation
<span class="hljs-meta">@article{scikit-learn,</span>
  title={Scikit-learn: Machine Learning <span class="hljs-keyword">in</span> {P}ython},
  author={Pedregosa, F. <span class="hljs-keyword">and</span> Varoquaux, G. <span class="hljs-keyword">and</span> Gramfort, A. <span class="hljs-keyword">and</span> Michel, V.
         <span class="hljs-keyword">and</span> Thirion, B. <span class="hljs-keyword">and</span> Grisel, O. <span class="hljs-keyword">and</span> Blondel, M. <span class="hljs-keyword">and</span> Prettenhofer, P.
         <span class="hljs-keyword">and</span> Weiss, R. <span class="hljs-keyword">and</span> Dubourg, V. <span class="hljs-keyword">and</span> Vanderplas, J. <span class="hljs-keyword">and</span> Passos, A. <span class="hljs-keyword">and</span>
         Cournapeau, D. <span class="hljs-keyword">and</span> Brucher, M. <span class="hljs-keyword">and</span> Perrot, M. <span class="hljs-keyword">and</span> Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={<span class="hljs-number">12</span>},
  pages={<span class="hljs-number">2825</span>--<span class="hljs-number">2830</span>},
  year={<span class="hljs-number">2011</span>}
}`}}),ke=new A({props:{code:"accuracy.features",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.features
{
    <span class="hljs-string">&#x27;predictions&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
    <span class="hljs-string">&#x27;references&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)
}`}}),Te=new oe({}),Ae=new oe({}),Pe=new A({props:{code:"accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1])",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.compute(references=[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], predictions=[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>])
{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.5</span>}`}}),De=new oe({}),xe=new A({props:{code:`for ref, pred in zip([0,1,0,1], [1,0,0,1]):
    accuracy.add(references=ref, predictions=pred)
accuracy.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> ref, pred <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]):
<span class="hljs-meta">&gt;&gt;&gt; </span>    accuracy.add(references=ref, predictions=pred)
<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.compute()
{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.5</span>}`}}),Oe=new A({props:{code:`for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):
    accuracy.add_batch(references=refs, predictions=preds)
accuracy.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> refs, preds <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>([[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]], [[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]]):
<span class="hljs-meta">&gt;&gt;&gt; </span>    accuracy.add_batch(references=refs, predictions=preds)
<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.compute()
{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.5</span>}`}}),Ne=new A({props:{code:`for model_inputs, gold_standards in evaluation_dataset:
    predictions = model(model_inputs)
    metric.add_batch(references=gold_standards, predictions=predictions)
metric.compute()






`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> model_inputs, gold_standards <span class="hljs-keyword">in</span> evaluation_dataset:
<span class="hljs-meta">&gt;&gt;&gt; </span>    predictions = model(model_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric.add_batch(references=gold_standards, predictions=predictions)
<span class="hljs-meta">&gt;&gt;&gt; </span>metric.compute()

<span class="hljs-comment">### Distributed evaluation</span>

Computing metrics <span class="hljs-keyword">in</span> a distributed environment can be tricky. Metric evaluation <span class="hljs-keyword">is</span> executed <span class="hljs-keyword">in</span> separate Python processes, <span class="hljs-keyword">or</span> nodes, on different subsets of a dataset. Typically, when a metric score <span class="hljs-keyword">is</span> additive (\`f(AuB) = f(A) + f(B)\`), you can use distributed reduce operations to gather the scores <span class="hljs-keyword">for</span> each subset of the dataset. But when a metric <span class="hljs-keyword">is</span> non-additive (\`f(AuB) \u2260 f(A) + f(B)\`), it<span class="hljs-string">&#x27;s not that simple. For example, you can&#x27;</span>t take the <span class="hljs-built_in">sum</span> of the [F1](https://huggingface.co/metrics/f1) scores of each data subset <span class="hljs-keyword">as</span> your **final metric**.

A common way to overcome this issue <span class="hljs-keyword">is</span> to fallback on single process evaluation. The metrics are evaluated on a single GPU, which becomes inefficient.

\u{1F917} Evaluate solves this issue by only computing the final metric on the first node. The predictions <span class="hljs-keyword">and</span> references are computed <span class="hljs-keyword">and</span> provided to the metric separately <span class="hljs-keyword">for</span> each node. These are temporarily stored <span class="hljs-keyword">in</span> an Apache Arrow table, avoiding cluttering the GPU <span class="hljs-keyword">or</span> CPU memory. When you are ready to \`EvaluationModule.compute\` the final metric, the first node <span class="hljs-keyword">is</span> able to access the predictions <span class="hljs-keyword">and</span> references stored on <span class="hljs-built_in">all</span> the other nodes. Once it has gathered <span class="hljs-built_in">all</span> the predictions <span class="hljs-keyword">and</span> references, \`EvaluationModule.compute\` will perform the final metric evaluation.

This solution allows \u{1F917} Evaluate to perform distributed predictions, which <span class="hljs-keyword">is</span> important <span class="hljs-keyword">for</span> evaluation speed <span class="hljs-keyword">in</span> distributed settings. At the same time, you can also use <span class="hljs-built_in">complex</span> non-additive metrics without wasting valuable GPU <span class="hljs-keyword">or</span> CPU memory. distributed predictions, which <span class="hljs-keyword">is</span> important <span class="hljs-keyword">for</span> evaluation speed <span class="hljs-keyword">in</span> distributed settings. At the same time, you can also use <span class="hljs-built_in">complex</span> non-additive metrics without wasting valuable GPU <span class="hljs-keyword">or</span> CPU memory.

<span class="hljs-comment">## Save and share</span>

**TODO**: add references to \`evalute.save\` <span class="hljs-keyword">and</span> \`evalute.push_to_hub\`.`}}),{c(){P=s("meta"),ca=p(),D=s("h1"),L=s("a"),Ve=s("span"),f(le.$$.fragment),Es=p(),ze=s("span"),gs=r("A quick tour"),pa=p(),Ie=s("p"),$s=r("\u{1F917} Evaluate provides access to a wide range of evaluation tools. It covers a range of modalities such as text, computer vision, audio, etc. as well as tools to evaluate models or datasets. These tools are split into three categories."),da=p(),C=s("h2"),M=s("a"),Je=s("span"),f(re.$$.fragment),js=p(),Ke=s("span"),ks=r("Three types of evaluations"),ha=p(),Se=s("p"),Ts=r("There are different aspects of a typical machine learning pipeline that can be evaluated and for aspect we provide a tool:"),ua=p(),j=s("ul"),B=s("li"),Qe=s("strong"),As=r("Metric"),Ps=r(": A metric is used to evaluate a model\u2019s performance and usually involves the model\u2019s predictions as well as some ground truth labels. You can find all integrated metrics at "),qe=s("a"),Ds=r("evaluate-metric"),Cs=r("."),xs=p(),F=s("li"),Xe=s("strong"),Os=r("Comparison"),Ns=r(": A comparison is used to compare two models. This can for example be done by comparing their predictions to ground truth labels and compute their agreement. You can find all integrated comparisons at "),Le=s("a"),Is=r("evaluate-comparison"),Ss=r("."),qs=p(),R=s("li"),Ze=s("strong"),Ls=r("Measurement"),Ms=r(": As important as the model is the dataset used to train it. With measurements one can investigate a dataset\u2019s properties. You can find all integrated measurements at "),Me=s("a"),Bs=r("evaluate-measurement"),Fs=r("."),fa=p(),H=s("p"),Rs=r("For all of these methods there is a single entry point: "),Be=s("a"),Hs=r("evaluate.load()"),Gs=r("!"),ma=p(),x=s("h2"),G=s("a"),et=s("span"),f(ne.$$.fragment),Us=p(),tt=s("span"),Ws=r("Load"),va=p(),U=s("p"),Ys=r("Any metric, comparison, or measurement is loaded with the "),at=s("code"),Vs=r("evaluate.load"),zs=r(" function:"),ya=p(),f(ie.$$.fragment),wa=p(),Fe=s("p"),Js=r("If you want to make sure you are loading the right type of module especially if there are name clashes you can explicitely pass the type:"),_a=p(),f(ce.$$.fragment),ba=p(),O=s("h3"),W=s("a"),st=s("span"),f(pe.$$.fragment),Ks=p(),ot=s("span"),Qs=r("Community modules"),Ea=p(),Re=s("p"),Xs=r("Besides the modules implemented in \u{1F917} Evaluate you can also load any community module by prepending the users name:"),ga=p(),f(de.$$.fragment),$a=p(),N=s("h2"),Y=s("a"),lt=s("span"),f(he.$$.fragment),Zs=p(),rt=s("span"),eo=r("Module attributes"),ja=p(),V=s("p"),to=r("All evalution modules come with a range of useful attributes that help to use a module stored in a "),nt=s("code"),ao=r("EvaluationModuleInfo"),so=r(" object."),ka=p(),z=s("table"),it=s("thead"),ue=s("tr"),ct=s("th"),oo=r("Attribute"),lo=p(),pt=s("th"),ro=r("Description"),no=p(),u=s("tbody"),fe=s("tr"),dt=s("td"),ht=s("code"),io=r("description"),co=p(),ut=s("td"),po=r("A short description of the evaluation module."),ho=p(),me=s("tr"),ft=s("td"),mt=s("code"),uo=r("citation"),fo=p(),vt=s("td"),mo=r("A BibTex string for citation."),vo=p(),ve=s("tr"),yt=s("td"),wt=s("code"),yo=r("features"),wo=p(),ye=s("td"),_o=r("A "),_t=s("code"),bo=r("Features"),Eo=r(" object defining the input format."),go=p(),we=s("tr"),bt=s("td"),Et=s("code"),$o=r("inputs_description"),jo=p(),gt=s("td"),ko=r("This is equivalent to the modules docstring."),To=p(),_e=s("tr"),$t=s("td"),jt=s("code"),Ao=r("homepage"),Po=p(),kt=s("td"),Do=r("The homepage of the module."),Co=p(),be=s("tr"),Tt=s("td"),At=s("code"),xo=r("license"),Oo=p(),Pt=s("td"),No=r("The license of the module."),Io=p(),Ee=s("tr"),Dt=s("td"),Ct=s("code"),So=r("codebase_urls"),qo=p(),xt=s("td"),Lo=r("Link to the code behind the module."),Mo=p(),ge=s("tr"),Ot=s("td"),Nt=s("code"),Bo=r("reference_urls"),Fo=p(),It=s("td"),Ro=r("Additional reference URLs."),Ta=p(),J=s("p"),Ho=r("Let\u2019s have a look at a few examples. First, let\u2019s look at the "),St=s("code"),Go=r("description"),Uo=r(" attribute of the accuracy metric:"),Aa=p(),f($e.$$.fragment),Pa=p(),K=s("p"),Wo=r("You can see that it describes how the metric works in theory. If you use this metric for you work, especially if it is an academic publication you want to reference it properly. For that you can look at the "),qt=s("code"),Yo=r("citation"),Vo=r(" attribute:"),Da=p(),f(je.$$.fragment),Ca=p(),He=s("p"),zo=r("Before we can apply a metric or other evaluation module to a use-case we need to know what the input format of the metric is:"),xa=p(),f(ke.$$.fragment),Oa=p(),Q=s("p"),Jo=r(`<Tip>
Note that features always describe the type of a single input element. In general we will add list of elements so you can always think of a list around the types in `),Lt=s("code"),Ko=r("features"),Qo=r(`. Evaluate accepts various input formats (Python lists, NumPy arrays, PyTorch tensors, etc.) and converts them to an appropriate format for storage and computation.
<\\Tip>`),Na=p(),I=s("h2"),X=s("a"),Mt=s("span"),f(Te.$$.fragment),Xo=p(),Bt=s("span"),Zo=r("Compute"),Ia=p(),Ge=s("p"),el=r("Now that we know how the evaluation module works and what should go in there we want to actually use it! When it comes to computing the actual score there are two main ways to do it:"),Sa=p(),Z=s("ol"),Ft=s("li"),tl=r("All-in-one"),al=p(),Rt=s("li"),sl=r("Incremental"),qa=p(),E=s("p"),ol=r("In the incremental approach the necessary inputs are added to the module with "),Ht=s("code"),ll=r("add"),rl=r(" or "),Gt=s("code"),nl=r("add_batch"),il=r(" and the score is calculated at the end with "),Ut=s("code"),cl=r("compute"),pl=r(". Alternatively, one can pass all the inputs at once to "),Wt=s("code"),dl=r("compute"),hl=r(". Let\u2019s have a look at the two approaches."),La=p(),S=s("h3"),ee=s("a"),Yt=s("span"),f(Ae.$$.fragment),ul=p(),Vt=s("span"),zt=s("code"),fl=r("compute"),Ma=r("\n\nThe simplest way to calculate the score of an evaluation module is by calling `compute` directly with the necessary inputs. Simply pass the inputs as seen in `features` to the `compute` method.\n\n	"),f(Pe.$$.fragment),Ba=p(),k=s("p"),ml=r("Evaluation modules return the results in a dictionary. However, in some instances you build up the predictions iteratively or in a distributed fashion in which case "),Jt=s("code"),vl=r("add"),yl=r(" or "),Kt=s("code"),wl=r("add_batch"),_l=r(" are useful."),Fa=p(),q=s("h3"),te=s("a"),Qt=s("span"),f(De.$$.fragment),bl=p(),Ce=s("span"),Xt=s("code"),El=r("add"),gl=r(" and "),Zt=s("code"),$l=r("add_batch"),Ra=p(),g=s("p"),jl=r("In many evaluation pipelines you build the predictions iteratively such as in a for-loop. In that case you could store the predictions in a list and at the end pass them to "),ea=s("code"),kl=r("compute"),Tl=r(". With "),ta=s("code"),Al=r("add"),Pl=r(" and add batch you can circumvent the step of storing the predictions separately. If you are only creating single predictions at a time you can use "),aa=s("code"),Dl=r("add"),Cl=r(":"),Ha=p(),f(xe.$$.fragment),Ga=p(),$=s("p"),xl=r("Once you have gathered all predictions you can call"),sa=s("code"),Ol=r("compute"),Nl=r(" to compute the score based on all stored values. When getting predictions and references in batches you can use "),oa=s("code"),Il=r("add_batch"),Sl=r(" which adds a list elements for later processing. The rest works as with "),la=s("code"),ql=r("add"),Ll=r(":"),Ua=p(),f(Oe.$$.fragment),Wa=p(),Ue=s("p"),Ml=r("This is especially useful when you need to get the predictions from your model in batches:"),Ya=p(),f(Ne.$$.fragment),this.h()},l(e){const i=gn('[data-svelte="svelte-1phssyn"]',document.head);P=o(i,"META",{name:!0,content:!0}),i.forEach(a),ca=d(e),D=o(e,"H1",{class:!0});var za=l(D);L=o(za,"A",{id:!0,class:!0,href:!0});var Fl=l(L);Ve=o(Fl,"SPAN",{});var Rl=l(Ve);m(le.$$.fragment,Rl),Rl.forEach(a),Fl.forEach(a),Es=d(za),ze=o(za,"SPAN",{});var Hl=l(ze);gs=n(Hl,"A quick tour"),Hl.forEach(a),za.forEach(a),pa=d(e),Ie=o(e,"P",{});var Gl=l(Ie);$s=n(Gl,"\u{1F917} Evaluate provides access to a wide range of evaluation tools. It covers a range of modalities such as text, computer vision, audio, etc. as well as tools to evaluate models or datasets. These tools are split into three categories."),Gl.forEach(a),da=d(e),C=o(e,"H2",{class:!0});var Ja=l(C);M=o(Ja,"A",{id:!0,class:!0,href:!0});var Ul=l(M);Je=o(Ul,"SPAN",{});var Wl=l(Je);m(re.$$.fragment,Wl),Wl.forEach(a),Ul.forEach(a),js=d(Ja),Ke=o(Ja,"SPAN",{});var Yl=l(Ke);ks=n(Yl,"Three types of evaluations"),Yl.forEach(a),Ja.forEach(a),ha=d(e),Se=o(e,"P",{});var Vl=l(Se);Ts=n(Vl,"There are different aspects of a typical machine learning pipeline that can be evaluated and for aspect we provide a tool:"),Vl.forEach(a),ua=d(e),j=o(e,"UL",{});var We=l(j);B=o(We,"LI",{});var ra=l(B);Qe=o(ra,"STRONG",{});var zl=l(Qe);As=n(zl,"Metric"),zl.forEach(a),Ps=n(ra,": A metric is used to evaluate a model\u2019s performance and usually involves the model\u2019s predictions as well as some ground truth labels. You can find all integrated metrics at "),qe=o(ra,"A",{href:!0});var Jl=l(qe);Ds=n(Jl,"evaluate-metric"),Jl.forEach(a),Cs=n(ra,"."),ra.forEach(a),xs=d(We),F=o(We,"LI",{});var na=l(F);Xe=o(na,"STRONG",{});var Kl=l(Xe);Os=n(Kl,"Comparison"),Kl.forEach(a),Ns=n(na,": A comparison is used to compare two models. This can for example be done by comparing their predictions to ground truth labels and compute their agreement. You can find all integrated comparisons at "),Le=o(na,"A",{href:!0});var Ql=l(Le);Is=n(Ql,"evaluate-comparison"),Ql.forEach(a),Ss=n(na,"."),na.forEach(a),qs=d(We),R=o(We,"LI",{});var ia=l(R);Ze=o(ia,"STRONG",{});var Xl=l(Ze);Ls=n(Xl,"Measurement"),Xl.forEach(a),Ms=n(ia,": As important as the model is the dataset used to train it. With measurements one can investigate a dataset\u2019s properties. You can find all integrated measurements at "),Me=o(ia,"A",{href:!0});var Zl=l(Me);Bs=n(Zl,"evaluate-measurement"),Zl.forEach(a),Fs=n(ia,"."),ia.forEach(a),We.forEach(a),fa=d(e),H=o(e,"P",{});var Ka=l(H);Rs=n(Ka,"For all of these methods there is a single entry point: "),Be=o(Ka,"A",{href:!0});var er=l(Be);Hs=n(er,"evaluate.load()"),er.forEach(a),Gs=n(Ka,"!"),Ka.forEach(a),ma=d(e),x=o(e,"H2",{class:!0});var Qa=l(x);G=o(Qa,"A",{id:!0,class:!0,href:!0});var tr=l(G);et=o(tr,"SPAN",{});var ar=l(et);m(ne.$$.fragment,ar),ar.forEach(a),tr.forEach(a),Us=d(Qa),tt=o(Qa,"SPAN",{});var sr=l(tt);Ws=n(sr,"Load"),sr.forEach(a),Qa.forEach(a),va=d(e),U=o(e,"P",{});var Xa=l(U);Ys=n(Xa,"Any metric, comparison, or measurement is loaded with the "),at=o(Xa,"CODE",{});var or=l(at);Vs=n(or,"evaluate.load"),or.forEach(a),zs=n(Xa," function:"),Xa.forEach(a),ya=d(e),m(ie.$$.fragment,e),wa=d(e),Fe=o(e,"P",{});var lr=l(Fe);Js=n(lr,"If you want to make sure you are loading the right type of module especially if there are name clashes you can explicitely pass the type:"),lr.forEach(a),_a=d(e),m(ce.$$.fragment,e),ba=d(e),O=o(e,"H3",{class:!0});var Za=l(O);W=o(Za,"A",{id:!0,class:!0,href:!0});var rr=l(W);st=o(rr,"SPAN",{});var nr=l(st);m(pe.$$.fragment,nr),nr.forEach(a),rr.forEach(a),Ks=d(Za),ot=o(Za,"SPAN",{});var ir=l(ot);Qs=n(ir,"Community modules"),ir.forEach(a),Za.forEach(a),Ea=d(e),Re=o(e,"P",{});var cr=l(Re);Xs=n(cr,"Besides the modules implemented in \u{1F917} Evaluate you can also load any community module by prepending the users name:"),cr.forEach(a),ga=d(e),m(de.$$.fragment,e),$a=d(e),N=o(e,"H2",{class:!0});var es=l(N);Y=o(es,"A",{id:!0,class:!0,href:!0});var pr=l(Y);lt=o(pr,"SPAN",{});var dr=l(lt);m(he.$$.fragment,dr),dr.forEach(a),pr.forEach(a),Zs=d(es),rt=o(es,"SPAN",{});var hr=l(rt);eo=n(hr,"Module attributes"),hr.forEach(a),es.forEach(a),ja=d(e),V=o(e,"P",{});var ts=l(V);to=n(ts,"All evalution modules come with a range of useful attributes that help to use a module stored in a "),nt=o(ts,"CODE",{});var ur=l(nt);ao=n(ur,"EvaluationModuleInfo"),ur.forEach(a),so=n(ts," object."),ts.forEach(a),ka=d(e),z=o(e,"TABLE",{});var as=l(z);it=o(as,"THEAD",{});var fr=l(it);ue=o(fr,"TR",{});var ss=l(ue);ct=o(ss,"TH",{});var mr=l(ct);oo=n(mr,"Attribute"),mr.forEach(a),lo=d(ss),pt=o(ss,"TH",{});var vr=l(pt);ro=n(vr,"Description"),vr.forEach(a),ss.forEach(a),fr.forEach(a),no=d(as),u=o(as,"TBODY",{});var b=l(u);fe=o(b,"TR",{});var os=l(fe);dt=o(os,"TD",{});var yr=l(dt);ht=o(yr,"CODE",{});var wr=l(ht);io=n(wr,"description"),wr.forEach(a),yr.forEach(a),co=d(os),ut=o(os,"TD",{});var _r=l(ut);po=n(_r,"A short description of the evaluation module."),_r.forEach(a),os.forEach(a),ho=d(b),me=o(b,"TR",{});var ls=l(me);ft=o(ls,"TD",{});var br=l(ft);mt=o(br,"CODE",{});var Er=l(mt);uo=n(Er,"citation"),Er.forEach(a),br.forEach(a),fo=d(ls),vt=o(ls,"TD",{});var gr=l(vt);mo=n(gr,"A BibTex string for citation."),gr.forEach(a),ls.forEach(a),vo=d(b),ve=o(b,"TR",{});var rs=l(ve);yt=o(rs,"TD",{});var $r=l(yt);wt=o($r,"CODE",{});var jr=l(wt);yo=n(jr,"features"),jr.forEach(a),$r.forEach(a),wo=d(rs),ye=o(rs,"TD",{});var ns=l(ye);_o=n(ns,"A "),_t=o(ns,"CODE",{});var kr=l(_t);bo=n(kr,"Features"),kr.forEach(a),Eo=n(ns," object defining the input format."),ns.forEach(a),rs.forEach(a),go=d(b),we=o(b,"TR",{});var is=l(we);bt=o(is,"TD",{});var Tr=l(bt);Et=o(Tr,"CODE",{});var Ar=l(Et);$o=n(Ar,"inputs_description"),Ar.forEach(a),Tr.forEach(a),jo=d(is),gt=o(is,"TD",{});var Pr=l(gt);ko=n(Pr,"This is equivalent to the modules docstring."),Pr.forEach(a),is.forEach(a),To=d(b),_e=o(b,"TR",{});var cs=l(_e);$t=o(cs,"TD",{});var Dr=l($t);jt=o(Dr,"CODE",{});var Cr=l(jt);Ao=n(Cr,"homepage"),Cr.forEach(a),Dr.forEach(a),Po=d(cs),kt=o(cs,"TD",{});var xr=l(kt);Do=n(xr,"The homepage of the module."),xr.forEach(a),cs.forEach(a),Co=d(b),be=o(b,"TR",{});var ps=l(be);Tt=o(ps,"TD",{});var Or=l(Tt);At=o(Or,"CODE",{});var Nr=l(At);xo=n(Nr,"license"),Nr.forEach(a),Or.forEach(a),Oo=d(ps),Pt=o(ps,"TD",{});var Ir=l(Pt);No=n(Ir,"The license of the module."),Ir.forEach(a),ps.forEach(a),Io=d(b),Ee=o(b,"TR",{});var ds=l(Ee);Dt=o(ds,"TD",{});var Sr=l(Dt);Ct=o(Sr,"CODE",{});var qr=l(Ct);So=n(qr,"codebase_urls"),qr.forEach(a),Sr.forEach(a),qo=d(ds),xt=o(ds,"TD",{});var Lr=l(xt);Lo=n(Lr,"Link to the code behind the module."),Lr.forEach(a),ds.forEach(a),Mo=d(b),ge=o(b,"TR",{});var hs=l(ge);Ot=o(hs,"TD",{});var Mr=l(Ot);Nt=o(Mr,"CODE",{});var Br=l(Nt);Bo=n(Br,"reference_urls"),Br.forEach(a),Mr.forEach(a),Fo=d(hs),It=o(hs,"TD",{});var Fr=l(It);Ro=n(Fr,"Additional reference URLs."),Fr.forEach(a),hs.forEach(a),b.forEach(a),as.forEach(a),Ta=d(e),J=o(e,"P",{});var us=l(J);Ho=n(us,"Let\u2019s have a look at a few examples. First, let\u2019s look at the "),St=o(us,"CODE",{});var Rr=l(St);Go=n(Rr,"description"),Rr.forEach(a),Uo=n(us," attribute of the accuracy metric:"),us.forEach(a),Aa=d(e),m($e.$$.fragment,e),Pa=d(e),K=o(e,"P",{});var fs=l(K);Wo=n(fs,"You can see that it describes how the metric works in theory. If you use this metric for you work, especially if it is an academic publication you want to reference it properly. For that you can look at the "),qt=o(fs,"CODE",{});var Hr=l(qt);Yo=n(Hr,"citation"),Hr.forEach(a),Vo=n(fs," attribute:"),fs.forEach(a),Da=d(e),m(je.$$.fragment,e),Ca=d(e),He=o(e,"P",{});var Gr=l(He);zo=n(Gr,"Before we can apply a metric or other evaluation module to a use-case we need to know what the input format of the metric is:"),Gr.forEach(a),xa=d(e),m(ke.$$.fragment,e),Oa=d(e),Q=o(e,"P",{});var ms=l(Q);Jo=n(ms,`<Tip>
Note that features always describe the type of a single input element. In general we will add list of elements so you can always think of a list around the types in `),Lt=o(ms,"CODE",{});var Ur=l(Lt);Ko=n(Ur,"features"),Ur.forEach(a),Qo=n(ms,`. Evaluate accepts various input formats (Python lists, NumPy arrays, PyTorch tensors, etc.) and converts them to an appropriate format for storage and computation.
<\\Tip>`),ms.forEach(a),Na=d(e),I=o(e,"H2",{class:!0});var vs=l(I);X=o(vs,"A",{id:!0,class:!0,href:!0});var Wr=l(X);Mt=o(Wr,"SPAN",{});var Yr=l(Mt);m(Te.$$.fragment,Yr),Yr.forEach(a),Wr.forEach(a),Xo=d(vs),Bt=o(vs,"SPAN",{});var Vr=l(Bt);Zo=n(Vr,"Compute"),Vr.forEach(a),vs.forEach(a),Ia=d(e),Ge=o(e,"P",{});var zr=l(Ge);el=n(zr,"Now that we know how the evaluation module works and what should go in there we want to actually use it! When it comes to computing the actual score there are two main ways to do it:"),zr.forEach(a),Sa=d(e),Z=o(e,"OL",{});var ys=l(Z);Ft=o(ys,"LI",{});var Jr=l(Ft);tl=n(Jr,"All-in-one"),Jr.forEach(a),al=d(ys),Rt=o(ys,"LI",{});var Kr=l(Rt);sl=n(Kr,"Incremental"),Kr.forEach(a),ys.forEach(a),qa=d(e),E=o(e,"P",{});var T=l(E);ol=n(T,"In the incremental approach the necessary inputs are added to the module with "),Ht=o(T,"CODE",{});var Qr=l(Ht);ll=n(Qr,"add"),Qr.forEach(a),rl=n(T," or "),Gt=o(T,"CODE",{});var Xr=l(Gt);nl=n(Xr,"add_batch"),Xr.forEach(a),il=n(T," and the score is calculated at the end with "),Ut=o(T,"CODE",{});var Zr=l(Ut);cl=n(Zr,"compute"),Zr.forEach(a),pl=n(T,". Alternatively, one can pass all the inputs at once to "),Wt=o(T,"CODE",{});var en=l(Wt);dl=n(en,"compute"),en.forEach(a),hl=n(T,". Let\u2019s have a look at the two approaches."),T.forEach(a),La=d(e),S=o(e,"H3",{class:!0});var ws=l(S);ee=o(ws,"A",{id:!0,class:!0,href:!0});var tn=l(ee);Yt=o(tn,"SPAN",{});var an=l(Yt);m(Ae.$$.fragment,an),an.forEach(a),tn.forEach(a),ul=d(ws),Vt=o(ws,"SPAN",{});var sn=l(Vt);zt=o(sn,"CODE",{});var on=l(zt);fl=n(on,"compute"),on.forEach(a),sn.forEach(a),ws.forEach(a),Ma=n(e,"\n\nThe simplest way to calculate the score of an evaluation module is by calling `compute` directly with the necessary inputs. Simply pass the inputs as seen in `features` to the `compute` method.\n\n	"),m(Pe.$$.fragment,e),Ba=d(e),k=o(e,"P",{});var Ye=l(k);ml=n(Ye,"Evaluation modules return the results in a dictionary. However, in some instances you build up the predictions iteratively or in a distributed fashion in which case "),Jt=o(Ye,"CODE",{});var ln=l(Jt);vl=n(ln,"add"),ln.forEach(a),yl=n(Ye," or "),Kt=o(Ye,"CODE",{});var rn=l(Kt);wl=n(rn,"add_batch"),rn.forEach(a),_l=n(Ye," are useful."),Ye.forEach(a),Fa=d(e),q=o(e,"H3",{class:!0});var _s=l(q);te=o(_s,"A",{id:!0,class:!0,href:!0});var nn=l(te);Qt=o(nn,"SPAN",{});var cn=l(Qt);m(De.$$.fragment,cn),cn.forEach(a),nn.forEach(a),bl=d(_s),Ce=o(_s,"SPAN",{});var bs=l(Ce);Xt=o(bs,"CODE",{});var pn=l(Xt);El=n(pn,"add"),pn.forEach(a),gl=n(bs," and "),Zt=o(bs,"CODE",{});var dn=l(Zt);$l=n(dn,"add_batch"),dn.forEach(a),bs.forEach(a),_s.forEach(a),Ra=d(e),g=o(e,"P",{});var ae=l(g);jl=n(ae,"In many evaluation pipelines you build the predictions iteratively such as in a for-loop. In that case you could store the predictions in a list and at the end pass them to "),ea=o(ae,"CODE",{});var hn=l(ea);kl=n(hn,"compute"),hn.forEach(a),Tl=n(ae,". With "),ta=o(ae,"CODE",{});var un=l(ta);Al=n(un,"add"),un.forEach(a),Pl=n(ae," and add batch you can circumvent the step of storing the predictions separately. If you are only creating single predictions at a time you can use "),aa=o(ae,"CODE",{});var fn=l(aa);Dl=n(fn,"add"),fn.forEach(a),Cl=n(ae,":"),ae.forEach(a),Ha=d(e),m(xe.$$.fragment,e),Ga=d(e),$=o(e,"P",{});var se=l($);xl=n(se,"Once you have gathered all predictions you can call"),sa=o(se,"CODE",{});var mn=l(sa);Ol=n(mn,"compute"),mn.forEach(a),Nl=n(se," to compute the score based on all stored values. When getting predictions and references in batches you can use "),oa=o(se,"CODE",{});var vn=l(oa);Il=n(vn,"add_batch"),vn.forEach(a),Sl=n(se," which adds a list elements for later processing. The rest works as with "),la=o(se,"CODE",{});var yn=l(la);ql=n(yn,"add"),yn.forEach(a),Ll=n(se,":"),se.forEach(a),Ua=d(e),m(Oe.$$.fragment,e),Wa=d(e),Ue=o(e,"P",{});var wn=l(Ue);Ml=n(wn,"This is especially useful when you need to get the predictions from your model in batches:"),wn.forEach(a),Ya=d(e),m(Ne.$$.fragment,e),this.h()},h(){h(P,"name","hf:doc:metadata"),h(P,"content",JSON.stringify(Tn)),h(L,"id","a-quick-tour"),h(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(L,"href","#a-quick-tour"),h(D,"class","relative group"),h(M,"id","three-types-of-evaluations"),h(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(M,"href","#three-types-of-evaluations"),h(C,"class","relative group"),h(qe,"href","hf.co/evaluate-metric"),h(Le,"href","hf.co/evaluate-comparison"),h(Me,"href","hf.co/evaluate-measurement"),h(Be,"href","/docs/evaluate/pr_58/en/package_reference/loading_methods#evaluate.load"),h(G,"id","load"),h(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(G,"href","#load"),h(x,"class","relative group"),h(W,"id","community-modules"),h(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(W,"href","#community-modules"),h(O,"class","relative group"),h(Y,"id","module-attributes"),h(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Y,"href","#module-attributes"),h(N,"class","relative group"),h(X,"id","compute"),h(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(X,"href","#compute"),h(I,"class","relative group"),h(ee,"id","compute"),h(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ee,"href","#compute"),h(S,"class","relative group"),h(te,"id","add-and-addbatch"),h(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(te,"href","#add-and-addbatch"),h(q,"class","relative group")},m(e,i){t(document.head,P),c(e,ca,i),c(e,D,i),t(D,L),t(L,Ve),v(le,Ve,null),t(D,Es),t(D,ze),t(ze,gs),c(e,pa,i),c(e,Ie,i),t(Ie,$s),c(e,da,i),c(e,C,i),t(C,M),t(M,Je),v(re,Je,null),t(C,js),t(C,Ke),t(Ke,ks),c(e,ha,i),c(e,Se,i),t(Se,Ts),c(e,ua,i),c(e,j,i),t(j,B),t(B,Qe),t(Qe,As),t(B,Ps),t(B,qe),t(qe,Ds),t(B,Cs),t(j,xs),t(j,F),t(F,Xe),t(Xe,Os),t(F,Ns),t(F,Le),t(Le,Is),t(F,Ss),t(j,qs),t(j,R),t(R,Ze),t(Ze,Ls),t(R,Ms),t(R,Me),t(Me,Bs),t(R,Fs),c(e,fa,i),c(e,H,i),t(H,Rs),t(H,Be),t(Be,Hs),t(H,Gs),c(e,ma,i),c(e,x,i),t(x,G),t(G,et),v(ne,et,null),t(x,Us),t(x,tt),t(tt,Ws),c(e,va,i),c(e,U,i),t(U,Ys),t(U,at),t(at,Vs),t(U,zs),c(e,ya,i),v(ie,e,i),c(e,wa,i),c(e,Fe,i),t(Fe,Js),c(e,_a,i),v(ce,e,i),c(e,ba,i),c(e,O,i),t(O,W),t(W,st),v(pe,st,null),t(O,Ks),t(O,ot),t(ot,Qs),c(e,Ea,i),c(e,Re,i),t(Re,Xs),c(e,ga,i),v(de,e,i),c(e,$a,i),c(e,N,i),t(N,Y),t(Y,lt),v(he,lt,null),t(N,Zs),t(N,rt),t(rt,eo),c(e,ja,i),c(e,V,i),t(V,to),t(V,nt),t(nt,ao),t(V,so),c(e,ka,i),c(e,z,i),t(z,it),t(it,ue),t(ue,ct),t(ct,oo),t(ue,lo),t(ue,pt),t(pt,ro),t(z,no),t(z,u),t(u,fe),t(fe,dt),t(dt,ht),t(ht,io),t(fe,co),t(fe,ut),t(ut,po),t(u,ho),t(u,me),t(me,ft),t(ft,mt),t(mt,uo),t(me,fo),t(me,vt),t(vt,mo),t(u,vo),t(u,ve),t(ve,yt),t(yt,wt),t(wt,yo),t(ve,wo),t(ve,ye),t(ye,_o),t(ye,_t),t(_t,bo),t(ye,Eo),t(u,go),t(u,we),t(we,bt),t(bt,Et),t(Et,$o),t(we,jo),t(we,gt),t(gt,ko),t(u,To),t(u,_e),t(_e,$t),t($t,jt),t(jt,Ao),t(_e,Po),t(_e,kt),t(kt,Do),t(u,Co),t(u,be),t(be,Tt),t(Tt,At),t(At,xo),t(be,Oo),t(be,Pt),t(Pt,No),t(u,Io),t(u,Ee),t(Ee,Dt),t(Dt,Ct),t(Ct,So),t(Ee,qo),t(Ee,xt),t(xt,Lo),t(u,Mo),t(u,ge),t(ge,Ot),t(Ot,Nt),t(Nt,Bo),t(ge,Fo),t(ge,It),t(It,Ro),c(e,Ta,i),c(e,J,i),t(J,Ho),t(J,St),t(St,Go),t(J,Uo),c(e,Aa,i),v($e,e,i),c(e,Pa,i),c(e,K,i),t(K,Wo),t(K,qt),t(qt,Yo),t(K,Vo),c(e,Da,i),v(je,e,i),c(e,Ca,i),c(e,He,i),t(He,zo),c(e,xa,i),v(ke,e,i),c(e,Oa,i),c(e,Q,i),t(Q,Jo),t(Q,Lt),t(Lt,Ko),t(Q,Qo),c(e,Na,i),c(e,I,i),t(I,X),t(X,Mt),v(Te,Mt,null),t(I,Xo),t(I,Bt),t(Bt,Zo),c(e,Ia,i),c(e,Ge,i),t(Ge,el),c(e,Sa,i),c(e,Z,i),t(Z,Ft),t(Ft,tl),t(Z,al),t(Z,Rt),t(Rt,sl),c(e,qa,i),c(e,E,i),t(E,ol),t(E,Ht),t(Ht,ll),t(E,rl),t(E,Gt),t(Gt,nl),t(E,il),t(E,Ut),t(Ut,cl),t(E,pl),t(E,Wt),t(Wt,dl),t(E,hl),c(e,La,i),c(e,S,i),t(S,ee),t(ee,Yt),v(Ae,Yt,null),t(S,ul),t(S,Vt),t(Vt,zt),t(zt,fl),c(e,Ma,i),v(Pe,e,i),c(e,Ba,i),c(e,k,i),t(k,ml),t(k,Jt),t(Jt,vl),t(k,yl),t(k,Kt),t(Kt,wl),t(k,_l),c(e,Fa,i),c(e,q,i),t(q,te),t(te,Qt),v(De,Qt,null),t(q,bl),t(q,Ce),t(Ce,Xt),t(Xt,El),t(Ce,gl),t(Ce,Zt),t(Zt,$l),c(e,Ra,i),c(e,g,i),t(g,jl),t(g,ea),t(ea,kl),t(g,Tl),t(g,ta),t(ta,Al),t(g,Pl),t(g,aa),t(aa,Dl),t(g,Cl),c(e,Ha,i),v(xe,e,i),c(e,Ga,i),c(e,$,i),t($,xl),t($,sa),t(sa,Ol),t($,Nl),t($,oa),t(oa,Il),t($,Sl),t($,la),t(la,ql),t($,Ll),c(e,Ua,i),v(Oe,e,i),c(e,Wa,i),c(e,Ue,i),t(Ue,Ml),c(e,Ya,i),v(Ne,e,i),Va=!0},p:$n,i(e){Va||(y(le.$$.fragment,e),y(re.$$.fragment,e),y(ne.$$.fragment,e),y(ie.$$.fragment,e),y(ce.$$.fragment,e),y(pe.$$.fragment,e),y(de.$$.fragment,e),y(he.$$.fragment,e),y($e.$$.fragment,e),y(je.$$.fragment,e),y(ke.$$.fragment,e),y(Te.$$.fragment,e),y(Ae.$$.fragment,e),y(Pe.$$.fragment,e),y(De.$$.fragment,e),y(xe.$$.fragment,e),y(Oe.$$.fragment,e),y(Ne.$$.fragment,e),Va=!0)},o(e){w(le.$$.fragment,e),w(re.$$.fragment,e),w(ne.$$.fragment,e),w(ie.$$.fragment,e),w(ce.$$.fragment,e),w(pe.$$.fragment,e),w(de.$$.fragment,e),w(he.$$.fragment,e),w($e.$$.fragment,e),w(je.$$.fragment,e),w(ke.$$.fragment,e),w(Te.$$.fragment,e),w(Ae.$$.fragment,e),w(Pe.$$.fragment,e),w(De.$$.fragment,e),w(xe.$$.fragment,e),w(Oe.$$.fragment,e),w(Ne.$$.fragment,e),Va=!1},d(e){a(P),e&&a(ca),e&&a(D),_(le),e&&a(pa),e&&a(Ie),e&&a(da),e&&a(C),_(re),e&&a(ha),e&&a(Se),e&&a(ua),e&&a(j),e&&a(fa),e&&a(H),e&&a(ma),e&&a(x),_(ne),e&&a(va),e&&a(U),e&&a(ya),_(ie,e),e&&a(wa),e&&a(Fe),e&&a(_a),_(ce,e),e&&a(ba),e&&a(O),_(pe),e&&a(Ea),e&&a(Re),e&&a(ga),_(de,e),e&&a($a),e&&a(N),_(he),e&&a(ja),e&&a(V),e&&a(ka),e&&a(z),e&&a(Ta),e&&a(J),e&&a(Aa),_($e,e),e&&a(Pa),e&&a(K),e&&a(Da),_(je,e),e&&a(Ca),e&&a(He),e&&a(xa),_(ke,e),e&&a(Oa),e&&a(Q),e&&a(Na),e&&a(I),_(Te),e&&a(Ia),e&&a(Ge),e&&a(Sa),e&&a(Z),e&&a(qa),e&&a(E),e&&a(La),e&&a(S),_(Ae),e&&a(Ma),_(Pe,e),e&&a(Ba),e&&a(k),e&&a(Fa),e&&a(q),_(De),e&&a(Ra),e&&a(g),e&&a(Ha),_(xe,e),e&&a(Ga),e&&a($),e&&a(Ua),_(Oe,e),e&&a(Wa),e&&a(Ue),e&&a(Ya),_(Ne,e)}}}const Tn={local:"a-quick-tour",sections:[{local:"three-types-of-evaluations",title:"Three types of evaluations"},{local:"load",sections:[{local:"community-modules",title:"Community modules"}],title:"Load"},{local:"module-attributes",title:"Module attributes"},{local:"compute",sections:[{local:"compute",title:"`compute`"},{local:"add-and-addbatch",title:"`add` and `add_batch`"}],title:"Compute"}],title:"A quick tour"};function An(Bl){return jn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class xn extends _n{constructor(P){super();bn(this,P,An,kn,En,{})}}export{xn as default,Tn as metadata};
