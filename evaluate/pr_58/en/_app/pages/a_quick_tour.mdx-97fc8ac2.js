import{S as md,i as vd,s as _d,e as s,k as c,w as h,t as i,M as gd,c as o,d as t,m as u,a as r,x as f,h as n,b as d,F as a,g as p,y as m,q as v,o as _,B as g,v as wd}from"../chunks/vendor-e830fc9c.js";import{C as A,T as Np}from"../chunks/CodeBlock-7b28e4e9.js";import{I as E}from"../chunks/IconCopyLink-af4c5c5b.js";function yd(ie){let w,b;return{c(){w=s("p"),b=i("Metrics accepts various input formats (Python lists, NumPy arrays, PyTorch tensors, etc.) and converts them to an appropriate format for storage and computation.")},l(y){w=o(y,"P",{});var $=r(w);b=n($,"Metrics accepts various input formats (Python lists, NumPy arrays, PyTorch tensors, etc.) and converts them to an appropriate format for storage and computation."),$.forEach(t)},m(y,$){p(y,w,$),a(w,b)},d(y){y&&t(w)}}}function $d(ie){let w,b,y,$,U;return{c(){w=s("p"),b=i("Get jump started with our metric loading script "),y=s("a"),$=i("template"),U=i("!"),this.h()},l(q){w=o(q,"P",{});var C=r(w);b=n(C,"Get jump started with our metric loading script "),y=o(C,"A",{href:!0,rel:!0});var ne=r(y);$=n(ne,"template"),ne.forEach(t),U=n(C,"!"),C.forEach(t),this.h()},h(){d(y,"href","https://github.com/huggingface/datasets/blob/master/templates/new_metric_script.py"),d(y,"rel","nofollow")},m(q,C){p(q,w,C),a(w,b),a(w,y),a(y,$),a(w,U)},d(q){q&&t(w)}}}function Ed(ie){let w,b;return{c(){w=s("p"),b=i("If the files are stored locally, provide a dictionary of path(s) instead of URLs.")},l(y){w=o(y,"P",{});var $=r(w);b=n($,"If the files are stored locally, provide a dictionary of path(s) instead of URLs."),$.forEach(t)},m(y,$){p(y,w,$),a(w,b)},d(y){y&&t(w)}}}function bd(ie){let w,b,y,$,U,q,C,ne,cl,io,Bt,ul,no,B,pe,wa,Fe,dl,ya,hl,po,Ht,fl,co,x,ce,$a,ml,vl,Rt,_l,gl,wl,ue,Ea,yl,$l,Gt,El,bl,kl,de,ba,jl,ql,zt,Al,Pl,uo,he,Cl,Wt,xl,Sl,ho,H,fe,ka,Ke,Tl,ja,Il,fo,me,Ll,qa,Ol,Dl,mo,Ve,vo,Yt,Nl,_o,Qe,go,R,ve,Aa,Je,Ml,Pa,Ul,wo,Ft,Bl,yo,Xe,$o,G,_e,Ca,Ze,Hl,xa,Rl,Eo,Kt,Gl,bo,ge,Sa,zl,Wl,Ta,Yl,ko,k,Fl,Ia,Kl,Vl,La,Ql,Jl,Oa,Xl,Zl,Da,ei,ti,jo,z,we,Na,et,ai,tt,Ma,si,oi,Ua,ri,qo,W,ye,Ba,at,li,Ha,Ra,ii,Ao,Y,$e,Ga,st,ni,za,pi,Po,j,ci,Wa,ui,di,Ya,hi,fi,ot,mi,vi,Fa,_i,gi,Co,Vt,wi,xo,S,yi,Ka,$i,Ei,Va,bi,ki,So,Qt,ji,To,F,Ee,Qa,rt,qi,Ja,Ai,Io,K,be,Xa,lt,Pi,Za,Ci,Lo,V,ke,es,it,xi,Jt,Si,ts,Ti,Oo,je,Ii,Xt,Li,Oi,Do,T,Di,as,Ni,Mi,ss,Ui,Bi,No,Q,qe,os,nt,Hi,rs,Ri,Mo,J,Ae,ls,pt,Gi,is,zi,Uo,Zt,Wi,Bo,ea,Yi,Ho,I,ns,Fi,Ki,ps,Vi,Qi,cs,Ji,Ro,X,Pe,us,ct,Xi,ds,Zi,Go,Ce,en,hs,tn,an,zo,xe,fs,L,ms,sn,on,vs,rn,ln,_s,nn,pn,cn,gs,O,ws,un,dn,ys,hn,fn,$s,mn,vn,Wo,Se,_n,Es,gn,wn,Yo,ut,Fo,Te,Ko,Z,Ie,bs,dt,yn,ks,$n,Vo,Le,En,js,bn,kn,Qo,Oe,jn,ht,qn,An,Jo,ta,qs,Pn,Xo,ft,Zo,mt,As,Cn,er,vt,tr,_t,ee,xn,Ps,Sn,Tn,Cs,In,Ln,ar,gt,sr,aa,or,te,De,xs,wt,On,Ss,Dn,rr,Ne,Nn,sa,Mn,Un,lr,Me,Bn,yt,Hn,Rn,ir,Ue,nr,ae,Be,Ts,$t,Gn,Is,zn,pr,He,Wn,Ls,Yn,Fn,cr,P,Os,oa,Ds,Kn,Vn,Qn,Ns,ra,Ms,Jn,Xn,Zn,Us,la,Bs,ep,tp,ap,Hs,ia,Rs,sp,op,ur,na,rp,dr,Et,hr,se,Re,Gs,bt,lp,zs,ip,fr,D,np,Ws,pp,cp,kt,up,dp,mr,pa,Ys,hp,vr,jt,_r,Ge,gr,qt,ca,Fs,fp,mp,wr,At,yr,oe,ze,Ks,Pt,vp,Vs,_p,$r,re,Qs,gp,wp,Ct,yp,$p,Er,ua,xt,Ep,Js,bp,kp,br,St,kr,Tt,It,jp,Xs,qp,Ap,jr,Lt,qr,le,We,Zs,Ot,Pp,eo,Cp,Ar,da,xp,Pr,Dt,Cr;return q=new E({}),Fe=new E({}),Ke=new E({}),Ve=new A({props:{code:`import evaluate
accuracy = evaluate.load("accuracy")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate
<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)`}}),Qe=new A({props:{code:'word_length = evaluate.load("word_length", type="measurement")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>word_length = evaluate.load(<span class="hljs-string">&quot;word_length&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;measurement&quot;</span>)'}}),Je=new E({}),Xe=new A({props:{code:'element_count = evaluate.load("lvwerra/element_count", type="measurement")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>element_count = evaluate.load(<span class="hljs-string">&quot;lvwerra/element_count&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;measurement&quot;</span>)'}}),Ze=new E({}),et=new E({}),at=new E({}),st=new E({}),rt=new E({}),lt=new E({}),it=new E({}),nt=new E({}),pt=new E({}),ct=new E({}),ut=new A({props:{code:`import datasets
metric = evaluate.load('my_metric')
for model_input, gold_references in evaluation_dataset:
    model_predictions = model(model_inputs)
    metric.add_batch(predictions=model_predictions, references=gold_references)
final_score = metric.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = evaluate.load(<span class="hljs-string">&#x27;my_metric&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> model_input, gold_references <span class="hljs-keyword">in</span> evaluation_dataset:
<span class="hljs-meta">... </span>    model_predictions = model(model_inputs)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=model_predictions, references=gold_references)
<span class="hljs-meta">&gt;&gt;&gt; </span>final_score = metric.compute()`}}),Te=new Np({props:{$$slots:{default:[yd]},$$scope:{ctx:ie}}}),dt=new E({}),ft=new A({props:{code:`import datasets
metric = evaluate.load('sacrebleu')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = evaluate.load(<span class="hljs-string">&#x27;sacrebleu&#x27;</span>)`}}),vt=new A({props:{code:`print(metric.inputs_description)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(metric.inputs_description)
Produces BLEU scores along <span class="hljs-keyword">with</span> its sufficient statistics
<span class="hljs-keyword">from</span> a source against one <span class="hljs-keyword">or</span> more references.

Args:
    predictions: The system stream (a sequence of segments).
    references: A <span class="hljs-built_in">list</span> of one <span class="hljs-keyword">or</span> more reference streams (each a sequence of segments).
    smooth_method: The smoothing method to use. (Default: <span class="hljs-string">&#x27;exp&#x27;</span>).
    smooth_value: The smoothing value. Only valid <span class="hljs-keyword">for</span> <span class="hljs-string">&#x27;floor&#x27;</span> <span class="hljs-keyword">and</span> <span class="hljs-string">&#x27;add-k&#x27;</span>. (Defaults: floor: <span class="hljs-number">0.1</span>, add-k: <span class="hljs-number">1</span>).
    tokenize: Tokenization method to use <span class="hljs-keyword">for</span> BLEU. If <span class="hljs-keyword">not</span> provided, defaults to <span class="hljs-string">&#x27;zh&#x27;</span> <span class="hljs-keyword">for</span> Chinese, <span class="hljs-string">&#x27;ja-mecab&#x27;</span> <span class="hljs-keyword">for</span> Japanese <span class="hljs-keyword">and</span> <span class="hljs-string">&#x27;13a&#x27;</span> (mteval) otherwise.
    lowercase: Lowercase the data. If <span class="hljs-literal">True</span>, enables case-insensitivity. (Default: <span class="hljs-literal">False</span>).
    force: Insist that your tokenized <span class="hljs-built_in">input</span> <span class="hljs-keyword">is</span> actually detokenized.
...`}}),gt=new A({props:{code:'score = metric.compute(smooth_method="floor", smooth_value=0.2)',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>score = metric.compute(smooth_method=<span class="hljs-string">&quot;floor&quot;</span>, smooth_value=<span class="hljs-number">0.2</span>)'}}),wt=new E({}),Ue=new Np({props:{$$slots:{default:[$d]},$$scope:{ctx:ie}}}),$t=new E({}),Et=new A({props:{code:`class Squad(evaluate.Metric):
    def _info(self):
        return evaluate.EvaluationModuleInfo(
            description=_DESCRIPTION,
            citation=_CITATION,
            inputs_description=_KWARGS_DESCRIPTION,
            features=datasets.Features(
                {
                    "predictions": {"id": datasets.Value("string"), "prediction_text": datasets.Value("string")},
                    "references": {
                        "id": datasets.Value("string"),
                        "answers": datasets.features.Sequence(
                            {
                                "text": datasets.Value("string"),
                                "answer_start": datasets.Value("int32"),
                            }
                        ),
                    },
                }
            ),
            codebase_urls=["https://rajpurkar.github.io/SQuAD-explorer/"],
            reference_urls=["https://rajpurkar.github.io/SQuAD-explorer/"],
        )`,highlighted:`<span class="hljs-keyword">class</span> <span class="hljs-title class_">Squad</span>(evaluate.Metric):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_info</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> evaluate.EvaluationModuleInfo(
            description=_DESCRIPTION,
            citation=_CITATION,
            inputs_description=_KWARGS_DESCRIPTION,
            features=datasets.Features(
                {
                    <span class="hljs-string">&quot;predictions&quot;</span>: {<span class="hljs-string">&quot;id&quot;</span>: datasets.Value(<span class="hljs-string">&quot;string&quot;</span>), <span class="hljs-string">&quot;prediction_text&quot;</span>: datasets.Value(<span class="hljs-string">&quot;string&quot;</span>)},
                    <span class="hljs-string">&quot;references&quot;</span>: {
                        <span class="hljs-string">&quot;id&quot;</span>: datasets.Value(<span class="hljs-string">&quot;string&quot;</span>),
                        <span class="hljs-string">&quot;answers&quot;</span>: datasets.features.<span class="hljs-type">Sequence</span>(
                            {
                                <span class="hljs-string">&quot;text&quot;</span>: datasets.Value(<span class="hljs-string">&quot;string&quot;</span>),
                                <span class="hljs-string">&quot;answer_start&quot;</span>: datasets.Value(<span class="hljs-string">&quot;int32&quot;</span>),
                            }
                        ),
                    },
                }
            ),
            codebase_urls=[<span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;</span>],
            reference_urls=[<span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;</span>],
        )`}}),bt=new E({}),jt=new A({props:{code:`CHECKPOINT_URLS = {
    "bleurt-tiny-128": "https://storage.googleapis.com/bleurt-oss/bleurt-tiny-128.zip",
    "bleurt-tiny-512": "https://storage.googleapis.com/bleurt-oss/bleurt-tiny-512.zip",
    "bleurt-base-128": "https://storage.googleapis.com/bleurt-oss/bleurt-base-128.zip",
    "bleurt-base-512": "https://storage.googleapis.com/bleurt-oss/bleurt-base-512.zip",
    "bleurt-large-128": "https://storage.googleapis.com/bleurt-oss/bleurt-large-128.zip",
    "bleurt-large-512": "https://storage.googleapis.com/bleurt-oss/bleurt-large-512.zip",
}`,highlighted:`CHECKPOINT_URLS = {
    <span class="hljs-string">&quot;bleurt-tiny-128&quot;</span>: <span class="hljs-string">&quot;https://storage.googleapis.com/bleurt-oss/bleurt-tiny-128.zip&quot;</span>,
    <span class="hljs-string">&quot;bleurt-tiny-512&quot;</span>: <span class="hljs-string">&quot;https://storage.googleapis.com/bleurt-oss/bleurt-tiny-512.zip&quot;</span>,
    <span class="hljs-string">&quot;bleurt-base-128&quot;</span>: <span class="hljs-string">&quot;https://storage.googleapis.com/bleurt-oss/bleurt-base-128.zip&quot;</span>,
    <span class="hljs-string">&quot;bleurt-base-512&quot;</span>: <span class="hljs-string">&quot;https://storage.googleapis.com/bleurt-oss/bleurt-base-512.zip&quot;</span>,
    <span class="hljs-string">&quot;bleurt-large-128&quot;</span>: <span class="hljs-string">&quot;https://storage.googleapis.com/bleurt-oss/bleurt-large-128.zip&quot;</span>,
    <span class="hljs-string">&quot;bleurt-large-512&quot;</span>: <span class="hljs-string">&quot;https://storage.googleapis.com/bleurt-oss/bleurt-large-512.zip&quot;</span>,
}`}}),Ge=new Np({props:{$$slots:{default:[Ed]},$$scope:{ctx:ie}}}),At=new A({props:{code:`def _download_and_prepare(self, dl_manager):

    # check that config name specifies a valid BLEURT model
    if self.config_name == "default":
        logger.warning(
            "Using default BLEURT-Base checkpoint for sequence maximum length 128. "
            "You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512')."
        )
        self.config_name = "bleurt-base-128"
    if self.config_name not in CHECKPOINT_URLS.keys():
        raise KeyError(
            f"{self.config_name} model not found. You should supply the name of a model checkpoint for bleurt in {CHECKPOINT_URLS.keys()}"
        )

    # download the model checkpoint specified by self.config_name and set up the scorer
    model_path = dl_manager.download_and_extract(CHECKPOINT_URLS[self.config_name])
    self.scorer = score.BleurtScorer(os.path.join(model_path, self.config_name))`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">_download_and_prepare</span>(<span class="hljs-params">self, dl_manager</span>):

    <span class="hljs-comment"># check that config name specifies a valid BLEURT model</span>
    <span class="hljs-keyword">if</span> self.config_name == <span class="hljs-string">&quot;default&quot;</span>:
        logger.warning(
            <span class="hljs-string">&quot;Using default BLEURT-Base checkpoint for sequence maximum length 128. &quot;</span>
            <span class="hljs-string">&quot;You can use a bigger model for better results with e.g.: evaluate.load(&#x27;bleurt&#x27;, &#x27;bleurt-large-512&#x27;).&quot;</span>
        )
        self.config_name = <span class="hljs-string">&quot;bleurt-base-128&quot;</span>
    <span class="hljs-keyword">if</span> self.config_name <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> CHECKPOINT_URLS.keys():
        <span class="hljs-keyword">raise</span> KeyError(
            <span class="hljs-string">f&quot;<span class="hljs-subst">{self.config_name}</span> model not found. You should supply the name of a model checkpoint for bleurt in <span class="hljs-subst">{CHECKPOINT_URLS.keys()}</span>&quot;</span>
        )

    <span class="hljs-comment"># download the model checkpoint specified by self.config_name and set up the scorer</span>
    model_path = dl_manager.download_and_extract(CHECKPOINT_URLS[self.config_name])
    self.scorer = score.BleurtScorer(os.path.join(model_path, self.config_name))`}}),Pt=new E({}),St=new A({props:{code:`def simple_accuracy(preds, labels):
    return (preds == labels).mean().item()

def acc_and_f1(preds, labels):
    acc = simple_accuracy(preds, labels)
    f1 = f1_score(y_true=labels, y_pred=preds).item()
    return {
        "accuracy": acc,
        "f1": f1,
    }

def pearson_and_spearman(preds, labels):
    pearson_corr = pearsonr(preds, labels)[0].item()
    spearman_corr = spearmanr(preds, labels)[0].item()
    return {
        "pearson": pearson_corr,
        "spearmanr": spearman_corr,
    }`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">simple_accuracy</span>(<span class="hljs-params">preds, labels</span>):
    <span class="hljs-keyword">return</span> (preds == labels).mean().item()

<span class="hljs-keyword">def</span> <span class="hljs-title function_">acc_and_f1</span>(<span class="hljs-params">preds, labels</span>):
    acc = simple_accuracy(preds, labels)
    f1 = f1_score(y_true=labels, y_pred=preds).item()
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">&quot;accuracy&quot;</span>: acc,
        <span class="hljs-string">&quot;f1&quot;</span>: f1,
    }

<span class="hljs-keyword">def</span> <span class="hljs-title function_">pearson_and_spearman</span>(<span class="hljs-params">preds, labels</span>):
    pearson_corr = pearsonr(preds, labels)[<span class="hljs-number">0</span>].item()
    spearman_corr = spearmanr(preds, labels)[<span class="hljs-number">0</span>].item()
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">&quot;pearson&quot;</span>: pearson_corr,
        <span class="hljs-string">&quot;spearmanr&quot;</span>: spearman_corr,
    }`}}),Lt=new A({props:{code:`def _compute(self, predictions, references):
    if self.config_name == "cola":
        return {"matthews_correlation": matthews_corrcoef(references, predictions)}
    elif self.config_name == "stsb":
        return pearson_and_spearman(predictions, references)
    elif self.config_name in ["mrpc", "qqp"]:
        return acc_and_f1(predictions, references)
    elif self.config_name in ["sst2", "mnli", "mnli_mismatched", "mnli_matched", "qnli", "rte", "wnli", "hans"]:
        return {"accuracy": simple_accuracy(predictions, references)}
    else:
        raise KeyError(
            "You should supply a configuration name selected in "
            '["sst2", "mnli", "mnli_mismatched", "mnli_matched", '
            '"cola", "stsb", "mrpc", "qqp", "qnli", "rte", "wnli", "hans"]'
        )`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">_compute</span>(<span class="hljs-params">self, predictions, references</span>):
    <span class="hljs-keyword">if</span> self.config_name == <span class="hljs-string">&quot;cola&quot;</span>:
        <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;matthews_correlation&quot;</span>: matthews_corrcoef(references, predictions)}
    <span class="hljs-keyword">elif</span> self.config_name == <span class="hljs-string">&quot;stsb&quot;</span>:
        <span class="hljs-keyword">return</span> pearson_and_spearman(predictions, references)
    <span class="hljs-keyword">elif</span> self.config_name <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;mrpc&quot;</span>, <span class="hljs-string">&quot;qqp&quot;</span>]:
        <span class="hljs-keyword">return</span> acc_and_f1(predictions, references)
    <span class="hljs-keyword">elif</span> self.config_name <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;sst2&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>, <span class="hljs-string">&quot;mnli_mismatched&quot;</span>, <span class="hljs-string">&quot;mnli_matched&quot;</span>, <span class="hljs-string">&quot;qnli&quot;</span>, <span class="hljs-string">&quot;rte&quot;</span>, <span class="hljs-string">&quot;wnli&quot;</span>, <span class="hljs-string">&quot;hans&quot;</span>]:
        <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;accuracy&quot;</span>: simple_accuracy(predictions, references)}
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">raise</span> KeyError(
            <span class="hljs-string">&quot;You should supply a configuration name selected in &quot;</span>
            <span class="hljs-string">&#x27;[&quot;sst2&quot;, &quot;mnli&quot;, &quot;mnli_mismatched&quot;, &quot;mnli_matched&quot;, &#x27;</span>
            <span class="hljs-string">&#x27;&quot;cola&quot;, &quot;stsb&quot;, &quot;mrpc&quot;, &quot;qqp&quot;, &quot;qnli&quot;, &quot;rte&quot;, &quot;wnli&quot;, &quot;hans&quot;]&#x27;</span>
        )`}}),Ot=new E({}),Dt=new A({props:{code:`from evaluate import load
metric = load('PATH/TO/MY/SCRIPT.py')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> load
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load(<span class="hljs-string">&#x27;PATH/TO/MY/SCRIPT.py&#x27;</span>)`}}),{c(){w=s("meta"),b=c(),y=s("h1"),$=s("a"),U=s("span"),h(q.$$.fragment),C=c(),ne=s("span"),cl=i("A quick tour"),io=c(),Bt=s("p"),ul=i("\u{1F917} Evaluate provides access to a wide range of evaluation tools. It covers a range of modalities such as text, computer vision, audio, etc. as well as tools to evaluate models or datasets. These tools are split into three categories."),no=c(),B=s("h2"),pe=s("a"),wa=s("span"),h(Fe.$$.fragment),dl=c(),ya=s("span"),hl=i("Three types of evaluations"),po=c(),Ht=s("p"),fl=i("There are different aspects of a typical machine learning pipeline that can be evaluated and for aspect we provide a tool:"),co=c(),x=s("ul"),ce=s("li"),$a=s("strong"),ml=i("Metric"),vl=i(": A metric is used to evaluate a model\u2019s performance and usually involves the model\u2019s predictions as well as some ground truth labels. You can find all integrated metrics at "),Rt=s("a"),_l=i("evaluate-metric"),gl=i("."),wl=c(),ue=s("li"),Ea=s("strong"),yl=i("Comparison"),$l=i(": A comparison is used to compare two models. This can for example be done by comparing their predictions to ground truth labels and compute their agreement. You can find all integrated comparisons at "),Gt=s("a"),El=i("evaluate-comparison"),bl=i("."),kl=c(),de=s("li"),ba=s("strong"),jl=i("Measurement"),ql=i(": As important as the model is the dataset used to train it. With measurements one can investigate a dataset\u2019s properties. You can find all integrated measurements at "),zt=s("a"),Al=i("evaluate-measurement"),Pl=i("."),uo=c(),he=s("p"),Cl=i("For all of these methods there is a single entry point: "),Wt=s("a"),xl=i("evaluate.load()"),Sl=i("!"),ho=c(),H=s("h2"),fe=s("a"),ka=s("span"),h(Ke.$$.fragment),Tl=c(),ja=s("span"),Il=i("Load"),fo=c(),me=s("p"),Ll=i("Any metric, comparison, or measurement is loaded with the "),qa=s("code"),Ol=i("evaluate.load"),Dl=i(" function:"),mo=c(),h(Ve.$$.fragment),vo=c(),Yt=s("p"),Nl=i("If you want to make sure you are loading the right type of module especially if there are name clashes you can explicitely pass the type:"),_o=c(),h(Qe.$$.fragment),go=c(),R=s("h3"),ve=s("a"),Aa=s("span"),h(Je.$$.fragment),Ml=c(),Pa=s("span"),Ul=i("Community modules"),wo=c(),Ft=s("p"),Bl=i("Besides the modules implemented in \u{1F917} Evaluate you can also load any community module by prepending the users name:"),yo=c(),h(Xe.$$.fragment),$o=c(),G=s("h2"),_e=s("a"),Ca=s("span"),h(Ze.$$.fragment),Hl=c(),xa=s("span"),Rl=i("Compute"),Eo=c(),Kt=s("p"),Gl=i("When it comes to computing the actual score there are two main ways to do it:"),bo=c(),ge=s("ol"),Sa=s("li"),zl=i("Incremental"),Wl=c(),Ta=s("li"),Yl=i("All-in-one"),ko=c(),k=s("p"),Fl=i("In the incremental approach the necessary inputs are added to the module with "),Ia=s("code"),Kl=i("add"),Vl=i(" or "),La=s("code"),Ql=i("add_batch"),Jl=i(" and the score is calculated at the end with "),Oa=s("code"),Xl=i("compute"),Zl=i(". Alternatively, one can pass all the inputs at once to "),Da=s("code"),ei=i("compute"),ti=i(". Let\u2019s have a look at the two approaches."),jo=c(),z=s("h3"),we=s("a"),Na=s("span"),h(et.$$.fragment),ai=c(),tt=s("span"),Ma=s("code"),si=i("add"),oi=i(" and "),Ua=s("code"),ri=i("add_batch"),qo=c(),W=s("h3"),ye=s("a"),Ba=s("span"),h(at.$$.fragment),li=c(),Ha=s("span"),Ra=s("code"),ii=i("compute"),Ao=c(),Y=s("h3"),$e=s("a"),Ga=s("span"),h(st.$$.fragment),ni=c(),za=s("span"),pi=i("Distributed evaluation"),Po=c(),j=s("p"),ci=i("Computing metrics in a distributed environment can be tricky. Metric evaluation is executed in separate Python processes, or nodes, on different subsets of a dataset. Typically, when a metric score is additive ("),Wa=s("code"),ui=i("f(AuB) = f(A) + f(B)"),di=i("), you can use distributed reduce operations to gather the scores for each subset of the dataset. But when a metric is non-additive ("),Ya=s("code"),hi=i("f(AuB) \u2260 f(A) + f(B)"),fi=i("), it\u2019s not that simple. For example, you can\u2019t take the sum of the "),ot=s("a"),mi=i("F1"),vi=i(" scores of each data subset as your "),Fa=s("strong"),_i=i("final metric"),gi=i("."),Co=c(),Vt=s("p"),wi=i("A common way to overcome this issue is to fallback on single process evaluation. The metrics are evaluated on a single GPU, which becomes inefficient."),xo=c(),S=s("p"),yi=i("\u{1F917} Evaluate solves this issue by only computing the final metric on the first node. The predictions and references are computed and provided to the metric separately for each node. These are temporarily stored in an Apache Arrow table, avoiding cluttering the GPU or CPU memory. When you are ready to "),Ka=s("code"),$i=i("EvaluationModule.compute"),Ei=i(" the final metric, the first node is able to access the predictions and references stored on all the other nodes. Once it has gathered all the predictions and references, "),Va=s("code"),bi=i("EvaluationModule.compute"),ki=i(" will perform the final metric evaluation."),So=c(),Qt=s("p"),ji=i("This solution allows \u{1F917} Evaluate to perform distributed predictions, which is important for evaluation speed in distributed settings. At the same time, you can also use complex non-additive metrics without wasting valuable GPU or CPU memory. distributed predictions, which is important for evaluation speed in distributed settings. At the same time, you can also use complex non-additive metrics without wasting valuable GPU or CPU memory."),To=c(),F=s("h2"),Ee=s("a"),Qa=s("span"),h(rt.$$.fragment),qi=c(),Ja=s("span"),Ai=i("Features"),Io=c(),K=s("h2"),be=s("a"),Xa=s("span"),h(lt.$$.fragment),Pi=c(),Za=s("span"),Ci=i("Save and share"),Lo=c(),V=s("h2"),ke=s("a"),es=s("span"),h(it.$$.fragment),xi=c(),Jt=s("span"),Si=i("ELI5: "),ts=s("code"),Ti=i("evaluate.load"),Oo=c(),je=s("p"),Ii=i("When you call "),Xt=s("a"),Li=i("evaluate.load()"),Oi=i(", the metric loading script is downloaded and imported from GitHub (if it hasn\u2019t already been downloaded before). It contains information about the metric such as it\u2019s citation, homepage, and description."),Do=c(),T=s("p"),Di=i("The metric loading script will instantiate and return a "),as=s("code"),Ni=i("EvaluationModule"),Mi=i(" object. This stores the predictions and references, which you need to compute the metric values. The "),ss=s("code"),Ui=i("EvaluationModule"),Bi=i(" object is stored as an Apache Arrow table. As a result, the predictions and references are stored directly on disk with memory-mapping. This enables \u{1F917} Evaluate to do a lazy computation of the metric, and makes it easier to gather all the predictions in a distributed setting."),No=c(),Q=s("h2"),qe=s("a"),os=s("span"),h(nt.$$.fragment),Hi=c(),rs=s("span"),Ri=i("Distributed evaluation"),Mo=c(),J=s("h2"),Ae=s("a"),ls=s("span"),h(pt.$$.fragment),Gi=c(),is=s("span"),zi=i("Metrics"),Uo=c(),Zt=s("p"),Wi=i("Metrics are important for evaluating a model\u2019s predictions. In the tutorial, you learned how to compute a metric over an entire evaluation set. You have also seen how to load a metric."),Bo=c(),ea=s("p"),Yi=i("This guide will show you how to:"),Ho=c(),I=s("ul"),ns=s("li"),Fi=i("Add predictions and references."),Ki=c(),ps=s("li"),Vi=i("Compute metrics using different methods."),Qi=c(),cs=s("li"),Ji=i("Write your own metric loading script."),Ro=c(),X=s("h2"),Pe=s("a"),us=s("span"),h(ct.$$.fragment),Xi=c(),ds=s("span"),Zi=i("Add predictions and references"),Go=c(),Ce=s("p"),en=i("When you want to add model predictions and references to a "),hs=s("code"),tn=i("Metric"),an=i(" instance, you have two options:"),zo=c(),xe=s("ul"),fs=s("li"),L=s("p"),ms=s("code"),sn=i("Metric.add"),on=i(" adds a single "),vs=s("code"),rn=i("prediction"),ln=i(" and "),_s=s("code"),nn=i("reference"),pn=i("."),cn=c(),gs=s("li"),O=s("p"),ws=s("code"),un=i("Metric.add_batch"),dn=i(" adds a batch of "),ys=s("code"),hn=i("predictions"),fn=i(" and "),$s=s("code"),mn=i("references"),vn=i("."),Wo=c(),Se=s("p"),_n=i("Use "),Es=s("code"),gn=i("Metric.add_batch"),wn=i(" by passing it your model predictions, and the references the model predictions should be evaluated against:"),Yo=c(),h(ut.$$.fragment),Fo=c(),h(Te.$$.fragment),Ko=c(),Z=s("h2"),Ie=s("a"),bs=s("span"),h(dt.$$.fragment),yn=c(),ks=s("span"),$n=i("Compute scores"),Vo=c(),Le=s("p"),En=i("The most straightforward way to calculate a metric is to call "),js=s("code"),bn=i("Metric.compute"),kn=i(". But some metrics have additional arguments that allow you to modify the metrics behavior."),Qo=c(),Oe=s("p"),jn=i("Let\u2019s load the "),ht=s("a"),qn=i("SacreBLEU"),An=i(" metric, and compute it with a different smoothing method."),Jo=c(),ta=s("ol"),qs=s("li"),Pn=i("Load the SacreBLEU metric:"),Xo=c(),h(ft.$$.fragment),Zo=c(),mt=s("ol"),As=s("li"),Cn=i("Inspect the different argument methods for computing the metric:"),er=c(),h(vt.$$.fragment),tr=c(),_t=s("ol"),ee=s("li"),xn=i("Compute the metric with the "),Ps=s("code"),Sn=i("floor"),Tn=i(" method, and a different "),Cs=s("code"),In=i("smooth_value"),Ln=i(":"),ar=c(),h(gt.$$.fragment),sr=c(),aa=s("a"),or=c(),te=s("h2"),De=s("a"),xs=s("span"),h(wt.$$.fragment),On=c(),Ss=s("span"),Dn=i("Custom metric loading script"),rr=c(),Ne=s("p"),Nn=i("Write a metric loading script to use your own custom metric (or one that is not on the Hub). Then you can load it as usual with "),sa=s("a"),Mn=i("load()"),Un=i("."),lr=c(),Me=s("p"),Bn=i("To help you get started, open the "),yt=s("a"),Hn=i("SQuAD metric loading script"),Rn=i(" and follow along."),ir=c(),h(Ue.$$.fragment),nr=c(),ae=s("h3"),Be=s("a"),Ts=s("span"),h($t.$$.fragment),Gn=c(),Is=s("span"),zn=i("Add metric attributes"),pr=c(),He=s("p"),Wn=i("Start by adding some information about your metric in "),Ls=s("code"),Yn=i("Metric._info"),Fn=i(". The most important attributes you should specify are:"),cr=c(),P=s("ol"),Os=s("li"),oa=s("p"),Ds=s("code"),Kn=i("EvaluationModuleInfo.description"),Vn=i(" provides a brief description about your metric."),Qn=c(),Ns=s("li"),ra=s("p"),Ms=s("code"),Jn=i("EvaluationModuleInfo.citation"),Xn=i(" contains a BibTex citation for the metric."),Zn=c(),Us=s("li"),la=s("p"),Bs=s("code"),ep=i("EvaluationModuleInfo.inputs_description"),tp=i(" describes the expected inputs and outputs. It may also provide an example usage of the metric."),ap=c(),Hs=s("li"),ia=s("p"),Rs=s("code"),sp=i("EvaluationModuleInfo.features"),op=i(" defines the name and type of the predictions and references."),ur=c(),na=s("p"),rp=i("After you\u2019ve filled out all these fields in the template, it should look like the following example from the SQuAD metric script:"),dr=c(),h(Et.$$.fragment),hr=c(),se=s("h3"),Re=s("a"),Gs=s("span"),h(bt.$$.fragment),lp=c(),zs=s("span"),ip=i("Download metric files"),fr=c(),D=s("p"),np=i("If your metric needs to download, or retrieve local files, you will need to use the "),Ws=s("code"),pp=i("Metric._download_and_prepare"),cp=i(" method. For this example, let\u2019s examine the "),kt=s("a"),up=i("BLEURT metric loading script"),dp=i("."),mr=c(),pa=s("ol"),Ys=s("li"),hp=i("Provide a dictionary of URLs that point to the metric files:"),vr=c(),h(jt.$$.fragment),_r=c(),h(Ge.$$.fragment),gr=c(),qt=s("ol"),ca=s("li"),Fs=s("code"),fp=i("Metric._download_and_prepare"),mp=i(" will take the URLs and download the metric files specified:"),wr=c(),h(At.$$.fragment),yr=c(),oe=s("h3"),ze=s("a"),Ks=s("span"),h(Pt.$$.fragment),vp=c(),Vs=s("span"),_p=i("Compute score"),$r=c(),re=s("p"),Qs=s("code"),gp=i("DatasetBuilder._compute"),wp=i(" provides the actual instructions for how to compute a metric given the predictions and references. Now let\u2019s take a look at the "),Ct=s("a"),yp=i("GLUE metric loading script"),$p=i("."),Er=c(),ua=s("ol"),xt=s("li"),Ep=i("Provide the functions for "),Js=s("code"),bp=i("DatasetBuilder._compute"),kp=i(" to calculate your metric:"),br=c(),h(St.$$.fragment),kr=c(),Tt=s("ol"),It=s("li"),jp=i("Create "),Xs=s("code"),qp=i("DatasetBuilder._compute"),Ap=i(" with instructions for what metric to calculate for each configuration:"),jr=c(),h(Lt.$$.fragment),qr=c(),le=s("h3"),We=s("a"),Zs=s("span"),h(Ot.$$.fragment),Pp=c(),eo=s("span"),Cp=i("Test"),Ar=c(),da=s("p"),xp=i("Once you\u2019re finished writing your metric loading script, try to load it locally:"),Pr=c(),h(Dt.$$.fragment),this.h()},l(e){const l=gd('[data-svelte="svelte-1phssyn"]',document.head);w=o(l,"META",{name:!0,content:!0}),l.forEach(t),b=u(e),y=o(e,"H1",{class:!0});var Nt=r(y);$=o(Nt,"A",{id:!0,class:!0,href:!0});var to=r($);U=o(to,"SPAN",{});var ao=r(U);f(q.$$.fragment,ao),ao.forEach(t),to.forEach(t),C=u(Nt),ne=o(Nt,"SPAN",{});var Mp=r(ne);cl=n(Mp,"A quick tour"),Mp.forEach(t),Nt.forEach(t),io=u(e),Bt=o(e,"P",{});var Up=r(Bt);ul=n(Up,"\u{1F917} Evaluate provides access to a wide range of evaluation tools. It covers a range of modalities such as text, computer vision, audio, etc. as well as tools to evaluate models or datasets. These tools are split into three categories."),Up.forEach(t),no=u(e),B=o(e,"H2",{class:!0});var xr=r(B);pe=o(xr,"A",{id:!0,class:!0,href:!0});var Bp=r(pe);wa=o(Bp,"SPAN",{});var Hp=r(wa);f(Fe.$$.fragment,Hp),Hp.forEach(t),Bp.forEach(t),dl=u(xr),ya=o(xr,"SPAN",{});var Rp=r(ya);hl=n(Rp,"Three types of evaluations"),Rp.forEach(t),xr.forEach(t),po=u(e),Ht=o(e,"P",{});var Gp=r(Ht);fl=n(Gp,"There are different aspects of a typical machine learning pipeline that can be evaluated and for aspect we provide a tool:"),Gp.forEach(t),co=u(e),x=o(e,"UL",{});var ha=r(x);ce=o(ha,"LI",{});var so=r(ce);$a=o(so,"STRONG",{});var zp=r($a);ml=n(zp,"Metric"),zp.forEach(t),vl=n(so,": A metric is used to evaluate a model\u2019s performance and usually involves the model\u2019s predictions as well as some ground truth labels. You can find all integrated metrics at "),Rt=o(so,"A",{href:!0});var Wp=r(Rt);_l=n(Wp,"evaluate-metric"),Wp.forEach(t),gl=n(so,"."),so.forEach(t),wl=u(ha),ue=o(ha,"LI",{});var oo=r(ue);Ea=o(oo,"STRONG",{});var Yp=r(Ea);yl=n(Yp,"Comparison"),Yp.forEach(t),$l=n(oo,": A comparison is used to compare two models. This can for example be done by comparing their predictions to ground truth labels and compute their agreement. You can find all integrated comparisons at "),Gt=o(oo,"A",{href:!0});var Fp=r(Gt);El=n(Fp,"evaluate-comparison"),Fp.forEach(t),bl=n(oo,"."),oo.forEach(t),kl=u(ha),de=o(ha,"LI",{});var ro=r(de);ba=o(ro,"STRONG",{});var Kp=r(ba);jl=n(Kp,"Measurement"),Kp.forEach(t),ql=n(ro,": As important as the model is the dataset used to train it. With measurements one can investigate a dataset\u2019s properties. You can find all integrated measurements at "),zt=o(ro,"A",{href:!0});var Vp=r(zt);Al=n(Vp,"evaluate-measurement"),Vp.forEach(t),Pl=n(ro,"."),ro.forEach(t),ha.forEach(t),uo=u(e),he=o(e,"P",{});var Sr=r(he);Cl=n(Sr,"For all of these methods there is a single entry point: "),Wt=o(Sr,"A",{href:!0});var Qp=r(Wt);xl=n(Qp,"evaluate.load()"),Qp.forEach(t),Sl=n(Sr,"!"),Sr.forEach(t),ho=u(e),H=o(e,"H2",{class:!0});var Tr=r(H);fe=o(Tr,"A",{id:!0,class:!0,href:!0});var Jp=r(fe);ka=o(Jp,"SPAN",{});var Xp=r(ka);f(Ke.$$.fragment,Xp),Xp.forEach(t),Jp.forEach(t),Tl=u(Tr),ja=o(Tr,"SPAN",{});var Zp=r(ja);Il=n(Zp,"Load"),Zp.forEach(t),Tr.forEach(t),fo=u(e),me=o(e,"P",{});var Ir=r(me);Ll=n(Ir,"Any metric, comparison, or measurement is loaded with the "),qa=o(Ir,"CODE",{});var ec=r(qa);Ol=n(ec,"evaluate.load"),ec.forEach(t),Dl=n(Ir," function:"),Ir.forEach(t),mo=u(e),f(Ve.$$.fragment,e),vo=u(e),Yt=o(e,"P",{});var tc=r(Yt);Nl=n(tc,"If you want to make sure you are loading the right type of module especially if there are name clashes you can explicitely pass the type:"),tc.forEach(t),_o=u(e),f(Qe.$$.fragment,e),go=u(e),R=o(e,"H3",{class:!0});var Lr=r(R);ve=o(Lr,"A",{id:!0,class:!0,href:!0});var ac=r(ve);Aa=o(ac,"SPAN",{});var sc=r(Aa);f(Je.$$.fragment,sc),sc.forEach(t),ac.forEach(t),Ml=u(Lr),Pa=o(Lr,"SPAN",{});var oc=r(Pa);Ul=n(oc,"Community modules"),oc.forEach(t),Lr.forEach(t),wo=u(e),Ft=o(e,"P",{});var rc=r(Ft);Bl=n(rc,"Besides the modules implemented in \u{1F917} Evaluate you can also load any community module by prepending the users name:"),rc.forEach(t),yo=u(e),f(Xe.$$.fragment,e),$o=u(e),G=o(e,"H2",{class:!0});var Or=r(G);_e=o(Or,"A",{id:!0,class:!0,href:!0});var lc=r(_e);Ca=o(lc,"SPAN",{});var ic=r(Ca);f(Ze.$$.fragment,ic),ic.forEach(t),lc.forEach(t),Hl=u(Or),xa=o(Or,"SPAN",{});var nc=r(xa);Rl=n(nc,"Compute"),nc.forEach(t),Or.forEach(t),Eo=u(e),Kt=o(e,"P",{});var pc=r(Kt);Gl=n(pc,"When it comes to computing the actual score there are two main ways to do it:"),pc.forEach(t),bo=u(e),ge=o(e,"OL",{});var Dr=r(ge);Sa=o(Dr,"LI",{});var cc=r(Sa);zl=n(cc,"Incremental"),cc.forEach(t),Wl=u(Dr),Ta=o(Dr,"LI",{});var uc=r(Ta);Yl=n(uc,"All-in-one"),uc.forEach(t),Dr.forEach(t),ko=u(e),k=o(e,"P",{});var N=r(k);Fl=n(N,"In the incremental approach the necessary inputs are added to the module with "),Ia=o(N,"CODE",{});var dc=r(Ia);Kl=n(dc,"add"),dc.forEach(t),Vl=n(N," or "),La=o(N,"CODE",{});var hc=r(La);Ql=n(hc,"add_batch"),hc.forEach(t),Jl=n(N," and the score is calculated at the end with "),Oa=o(N,"CODE",{});var fc=r(Oa);Xl=n(fc,"compute"),fc.forEach(t),Zl=n(N,". Alternatively, one can pass all the inputs at once to "),Da=o(N,"CODE",{});var mc=r(Da);ei=n(mc,"compute"),mc.forEach(t),ti=n(N,". Let\u2019s have a look at the two approaches."),N.forEach(t),jo=u(e),z=o(e,"H3",{class:!0});var Nr=r(z);we=o(Nr,"A",{id:!0,class:!0,href:!0});var vc=r(we);Na=o(vc,"SPAN",{});var _c=r(Na);f(et.$$.fragment,_c),_c.forEach(t),vc.forEach(t),ai=u(Nr),tt=o(Nr,"SPAN",{});var Mr=r(tt);Ma=o(Mr,"CODE",{});var gc=r(Ma);si=n(gc,"add"),gc.forEach(t),oi=n(Mr," and "),Ua=o(Mr,"CODE",{});var wc=r(Ua);ri=n(wc,"add_batch"),wc.forEach(t),Mr.forEach(t),Nr.forEach(t),qo=u(e),W=o(e,"H3",{class:!0});var Ur=r(W);ye=o(Ur,"A",{id:!0,class:!0,href:!0});var yc=r(ye);Ba=o(yc,"SPAN",{});var $c=r(Ba);f(at.$$.fragment,$c),$c.forEach(t),yc.forEach(t),li=u(Ur),Ha=o(Ur,"SPAN",{});var Ec=r(Ha);Ra=o(Ec,"CODE",{});var bc=r(Ra);ii=n(bc,"compute"),bc.forEach(t),Ec.forEach(t),Ur.forEach(t),Ao=u(e),Y=o(e,"H3",{class:!0});var Br=r(Y);$e=o(Br,"A",{id:!0,class:!0,href:!0});var kc=r($e);Ga=o(kc,"SPAN",{});var jc=r(Ga);f(st.$$.fragment,jc),jc.forEach(t),kc.forEach(t),ni=u(Br),za=o(Br,"SPAN",{});var qc=r(za);pi=n(qc,"Distributed evaluation"),qc.forEach(t),Br.forEach(t),Po=u(e),j=o(e,"P",{});var M=r(j);ci=n(M,"Computing metrics in a distributed environment can be tricky. Metric evaluation is executed in separate Python processes, or nodes, on different subsets of a dataset. Typically, when a metric score is additive ("),Wa=o(M,"CODE",{});var Ac=r(Wa);ui=n(Ac,"f(AuB) = f(A) + f(B)"),Ac.forEach(t),di=n(M,"), you can use distributed reduce operations to gather the scores for each subset of the dataset. But when a metric is non-additive ("),Ya=o(M,"CODE",{});var Pc=r(Ya);hi=n(Pc,"f(AuB) \u2260 f(A) + f(B)"),Pc.forEach(t),fi=n(M,"), it\u2019s not that simple. For example, you can\u2019t take the sum of the "),ot=o(M,"A",{href:!0,rel:!0});var Cc=r(ot);mi=n(Cc,"F1"),Cc.forEach(t),vi=n(M," scores of each data subset as your "),Fa=o(M,"STRONG",{});var xc=r(Fa);_i=n(xc,"final metric"),xc.forEach(t),gi=n(M,"."),M.forEach(t),Co=u(e),Vt=o(e,"P",{});var Sc=r(Vt);wi=n(Sc,"A common way to overcome this issue is to fallback on single process evaluation. The metrics are evaluated on a single GPU, which becomes inefficient."),Sc.forEach(t),xo=u(e),S=o(e,"P",{});var fa=r(S);yi=n(fa,"\u{1F917} Evaluate solves this issue by only computing the final metric on the first node. The predictions and references are computed and provided to the metric separately for each node. These are temporarily stored in an Apache Arrow table, avoiding cluttering the GPU or CPU memory. When you are ready to "),Ka=o(fa,"CODE",{});var Tc=r(Ka);$i=n(Tc,"EvaluationModule.compute"),Tc.forEach(t),Ei=n(fa," the final metric, the first node is able to access the predictions and references stored on all the other nodes. Once it has gathered all the predictions and references, "),Va=o(fa,"CODE",{});var Ic=r(Va);bi=n(Ic,"EvaluationModule.compute"),Ic.forEach(t),ki=n(fa," will perform the final metric evaluation."),fa.forEach(t),So=u(e),Qt=o(e,"P",{});var Lc=r(Qt);ji=n(Lc,"This solution allows \u{1F917} Evaluate to perform distributed predictions, which is important for evaluation speed in distributed settings. At the same time, you can also use complex non-additive metrics without wasting valuable GPU or CPU memory. distributed predictions, which is important for evaluation speed in distributed settings. At the same time, you can also use complex non-additive metrics without wasting valuable GPU or CPU memory."),Lc.forEach(t),To=u(e),F=o(e,"H2",{class:!0});var Hr=r(F);Ee=o(Hr,"A",{id:!0,class:!0,href:!0});var Oc=r(Ee);Qa=o(Oc,"SPAN",{});var Dc=r(Qa);f(rt.$$.fragment,Dc),Dc.forEach(t),Oc.forEach(t),qi=u(Hr),Ja=o(Hr,"SPAN",{});var Nc=r(Ja);Ai=n(Nc,"Features"),Nc.forEach(t),Hr.forEach(t),Io=u(e),K=o(e,"H2",{class:!0});var Rr=r(K);be=o(Rr,"A",{id:!0,class:!0,href:!0});var Mc=r(be);Xa=o(Mc,"SPAN",{});var Uc=r(Xa);f(lt.$$.fragment,Uc),Uc.forEach(t),Mc.forEach(t),Pi=u(Rr),Za=o(Rr,"SPAN",{});var Bc=r(Za);Ci=n(Bc,"Save and share"),Bc.forEach(t),Rr.forEach(t),Lo=u(e),V=o(e,"H2",{class:!0});var Gr=r(V);ke=o(Gr,"A",{id:!0,class:!0,href:!0});var Hc=r(ke);es=o(Hc,"SPAN",{});var Rc=r(es);f(it.$$.fragment,Rc),Rc.forEach(t),Hc.forEach(t),xi=u(Gr),Jt=o(Gr,"SPAN",{});var Sp=r(Jt);Si=n(Sp,"ELI5: "),ts=o(Sp,"CODE",{});var Gc=r(ts);Ti=n(Gc,"evaluate.load"),Gc.forEach(t),Sp.forEach(t),Gr.forEach(t),Oo=u(e),je=o(e,"P",{});var zr=r(je);Ii=n(zr,"When you call "),Xt=o(zr,"A",{href:!0});var zc=r(Xt);Li=n(zc,"evaluate.load()"),zc.forEach(t),Oi=n(zr,", the metric loading script is downloaded and imported from GitHub (if it hasn\u2019t already been downloaded before). It contains information about the metric such as it\u2019s citation, homepage, and description."),zr.forEach(t),Do=u(e),T=o(e,"P",{});var ma=r(T);Di=n(ma,"The metric loading script will instantiate and return a "),as=o(ma,"CODE",{});var Wc=r(as);Ni=n(Wc,"EvaluationModule"),Wc.forEach(t),Mi=n(ma," object. This stores the predictions and references, which you need to compute the metric values. The "),ss=o(ma,"CODE",{});var Yc=r(ss);Ui=n(Yc,"EvaluationModule"),Yc.forEach(t),Bi=n(ma," object is stored as an Apache Arrow table. As a result, the predictions and references are stored directly on disk with memory-mapping. This enables \u{1F917} Evaluate to do a lazy computation of the metric, and makes it easier to gather all the predictions in a distributed setting."),ma.forEach(t),No=u(e),Q=o(e,"H2",{class:!0});var Wr=r(Q);qe=o(Wr,"A",{id:!0,class:!0,href:!0});var Fc=r(qe);os=o(Fc,"SPAN",{});var Kc=r(os);f(nt.$$.fragment,Kc),Kc.forEach(t),Fc.forEach(t),Hi=u(Wr),rs=o(Wr,"SPAN",{});var Vc=r(rs);Ri=n(Vc,"Distributed evaluation"),Vc.forEach(t),Wr.forEach(t),Mo=u(e),J=o(e,"H2",{class:!0});var Yr=r(J);Ae=o(Yr,"A",{id:!0,class:!0,href:!0});var Qc=r(Ae);ls=o(Qc,"SPAN",{});var Jc=r(ls);f(pt.$$.fragment,Jc),Jc.forEach(t),Qc.forEach(t),Gi=u(Yr),is=o(Yr,"SPAN",{});var Xc=r(is);zi=n(Xc,"Metrics"),Xc.forEach(t),Yr.forEach(t),Uo=u(e),Zt=o(e,"P",{});var Zc=r(Zt);Wi=n(Zc,"Metrics are important for evaluating a model\u2019s predictions. In the tutorial, you learned how to compute a metric over an entire evaluation set. You have also seen how to load a metric."),Zc.forEach(t),Bo=u(e),ea=o(e,"P",{});var eu=r(ea);Yi=n(eu,"This guide will show you how to:"),eu.forEach(t),Ho=u(e),I=o(e,"UL",{});var va=r(I);ns=o(va,"LI",{});var tu=r(ns);Fi=n(tu,"Add predictions and references."),tu.forEach(t),Ki=u(va),ps=o(va,"LI",{});var au=r(ps);Vi=n(au,"Compute metrics using different methods."),au.forEach(t),Qi=u(va),cs=o(va,"LI",{});var su=r(cs);Ji=n(su,"Write your own metric loading script."),su.forEach(t),va.forEach(t),Ro=u(e),X=o(e,"H2",{class:!0});var Fr=r(X);Pe=o(Fr,"A",{id:!0,class:!0,href:!0});var ou=r(Pe);us=o(ou,"SPAN",{});var ru=r(us);f(ct.$$.fragment,ru),ru.forEach(t),ou.forEach(t),Xi=u(Fr),ds=o(Fr,"SPAN",{});var lu=r(ds);Zi=n(lu,"Add predictions and references"),lu.forEach(t),Fr.forEach(t),Go=u(e),Ce=o(e,"P",{});var Kr=r(Ce);en=n(Kr,"When you want to add model predictions and references to a "),hs=o(Kr,"CODE",{});var iu=r(hs);tn=n(iu,"Metric"),iu.forEach(t),an=n(Kr," instance, you have two options:"),Kr.forEach(t),zo=u(e),xe=o(e,"UL",{});var Vr=r(xe);fs=o(Vr,"LI",{});var nu=r(fs);L=o(nu,"P",{});var Mt=r(L);ms=o(Mt,"CODE",{});var pu=r(ms);sn=n(pu,"Metric.add"),pu.forEach(t),on=n(Mt," adds a single "),vs=o(Mt,"CODE",{});var cu=r(vs);rn=n(cu,"prediction"),cu.forEach(t),ln=n(Mt," and "),_s=o(Mt,"CODE",{});var uu=r(_s);nn=n(uu,"reference"),uu.forEach(t),pn=n(Mt,"."),Mt.forEach(t),nu.forEach(t),cn=u(Vr),gs=o(Vr,"LI",{});var du=r(gs);O=o(du,"P",{});var Ut=r(O);ws=o(Ut,"CODE",{});var hu=r(ws);un=n(hu,"Metric.add_batch"),hu.forEach(t),dn=n(Ut," adds a batch of "),ys=o(Ut,"CODE",{});var fu=r(ys);hn=n(fu,"predictions"),fu.forEach(t),fn=n(Ut," and "),$s=o(Ut,"CODE",{});var mu=r($s);mn=n(mu,"references"),mu.forEach(t),vn=n(Ut,"."),Ut.forEach(t),du.forEach(t),Vr.forEach(t),Wo=u(e),Se=o(e,"P",{});var Qr=r(Se);_n=n(Qr,"Use "),Es=o(Qr,"CODE",{});var vu=r(Es);gn=n(vu,"Metric.add_batch"),vu.forEach(t),wn=n(Qr," by passing it your model predictions, and the references the model predictions should be evaluated against:"),Qr.forEach(t),Yo=u(e),f(ut.$$.fragment,e),Fo=u(e),f(Te.$$.fragment,e),Ko=u(e),Z=o(e,"H2",{class:!0});var Jr=r(Z);Ie=o(Jr,"A",{id:!0,class:!0,href:!0});var _u=r(Ie);bs=o(_u,"SPAN",{});var gu=r(bs);f(dt.$$.fragment,gu),gu.forEach(t),_u.forEach(t),yn=u(Jr),ks=o(Jr,"SPAN",{});var wu=r(ks);$n=n(wu,"Compute scores"),wu.forEach(t),Jr.forEach(t),Vo=u(e),Le=o(e,"P",{});var Xr=r(Le);En=n(Xr,"The most straightforward way to calculate a metric is to call "),js=o(Xr,"CODE",{});var yu=r(js);bn=n(yu,"Metric.compute"),yu.forEach(t),kn=n(Xr,". But some metrics have additional arguments that allow you to modify the metrics behavior."),Xr.forEach(t),Qo=u(e),Oe=o(e,"P",{});var Zr=r(Oe);jn=n(Zr,"Let\u2019s load the "),ht=o(Zr,"A",{href:!0,rel:!0});var $u=r(ht);qn=n($u,"SacreBLEU"),$u.forEach(t),An=n(Zr," metric, and compute it with a different smoothing method."),Zr.forEach(t),Jo=u(e),ta=o(e,"OL",{});var Eu=r(ta);qs=o(Eu,"LI",{});var bu=r(qs);Pn=n(bu,"Load the SacreBLEU metric:"),bu.forEach(t),Eu.forEach(t),Xo=u(e),f(ft.$$.fragment,e),Zo=u(e),mt=o(e,"OL",{start:!0});var ku=r(mt);As=o(ku,"LI",{});var ju=r(As);Cn=n(ju,"Inspect the different argument methods for computing the metric:"),ju.forEach(t),ku.forEach(t),er=u(e),f(vt.$$.fragment,e),tr=u(e),_t=o(e,"OL",{start:!0});var qu=r(_t);ee=o(qu,"LI",{});var _a=r(ee);xn=n(_a,"Compute the metric with the "),Ps=o(_a,"CODE",{});var Au=r(Ps);Sn=n(Au,"floor"),Au.forEach(t),Tn=n(_a," method, and a different "),Cs=o(_a,"CODE",{});var Pu=r(Cs);In=n(Pu,"smooth_value"),Pu.forEach(t),Ln=n(_a,":"),_a.forEach(t),qu.forEach(t),ar=u(e),f(gt.$$.fragment,e),sr=u(e),aa=o(e,"A",{id:!0}),r(aa).forEach(t),or=u(e),te=o(e,"H2",{class:!0});var el=r(te);De=o(el,"A",{id:!0,class:!0,href:!0});var Cu=r(De);xs=o(Cu,"SPAN",{});var xu=r(xs);f(wt.$$.fragment,xu),xu.forEach(t),Cu.forEach(t),On=u(el),Ss=o(el,"SPAN",{});var Su=r(Ss);Dn=n(Su,"Custom metric loading script"),Su.forEach(t),el.forEach(t),rr=u(e),Ne=o(e,"P",{});var tl=r(Ne);Nn=n(tl,"Write a metric loading script to use your own custom metric (or one that is not on the Hub). Then you can load it as usual with "),sa=o(tl,"A",{href:!0});var Tu=r(sa);Mn=n(Tu,"load()"),Tu.forEach(t),Un=n(tl,"."),tl.forEach(t),lr=u(e),Me=o(e,"P",{});var al=r(Me);Bn=n(al,"To help you get started, open the "),yt=o(al,"A",{href:!0,rel:!0});var Iu=r(yt);Hn=n(Iu,"SQuAD metric loading script"),Iu.forEach(t),Rn=n(al," and follow along."),al.forEach(t),ir=u(e),f(Ue.$$.fragment,e),nr=u(e),ae=o(e,"H3",{class:!0});var sl=r(ae);Be=o(sl,"A",{id:!0,class:!0,href:!0});var Lu=r(Be);Ts=o(Lu,"SPAN",{});var Ou=r(Ts);f($t.$$.fragment,Ou),Ou.forEach(t),Lu.forEach(t),Gn=u(sl),Is=o(sl,"SPAN",{});var Du=r(Is);zn=n(Du,"Add metric attributes"),Du.forEach(t),sl.forEach(t),pr=u(e),He=o(e,"P",{});var ol=r(He);Wn=n(ol,"Start by adding some information about your metric in "),Ls=o(ol,"CODE",{});var Nu=r(Ls);Yn=n(Nu,"Metric._info"),Nu.forEach(t),Fn=n(ol,". The most important attributes you should specify are:"),ol.forEach(t),cr=u(e),P=o(e,"OL",{});var Ye=r(P);Os=o(Ye,"LI",{});var Mu=r(Os);oa=o(Mu,"P",{});var Tp=r(oa);Ds=o(Tp,"CODE",{});var Uu=r(Ds);Kn=n(Uu,"EvaluationModuleInfo.description"),Uu.forEach(t),Vn=n(Tp," provides a brief description about your metric."),Tp.forEach(t),Mu.forEach(t),Qn=u(Ye),Ns=o(Ye,"LI",{});var Bu=r(Ns);ra=o(Bu,"P",{});var Ip=r(ra);Ms=o(Ip,"CODE",{});var Hu=r(Ms);Jn=n(Hu,"EvaluationModuleInfo.citation"),Hu.forEach(t),Xn=n(Ip," contains a BibTex citation for the metric."),Ip.forEach(t),Bu.forEach(t),Zn=u(Ye),Us=o(Ye,"LI",{});var Ru=r(Us);la=o(Ru,"P",{});var Lp=r(la);Bs=o(Lp,"CODE",{});var Gu=r(Bs);ep=n(Gu,"EvaluationModuleInfo.inputs_description"),Gu.forEach(t),tp=n(Lp," describes the expected inputs and outputs. It may also provide an example usage of the metric."),Lp.forEach(t),Ru.forEach(t),ap=u(Ye),Hs=o(Ye,"LI",{});var zu=r(Hs);ia=o(zu,"P",{});var Op=r(ia);Rs=o(Op,"CODE",{});var Wu=r(Rs);sp=n(Wu,"EvaluationModuleInfo.features"),Wu.forEach(t),op=n(Op," defines the name and type of the predictions and references."),Op.forEach(t),zu.forEach(t),Ye.forEach(t),ur=u(e),na=o(e,"P",{});var Yu=r(na);rp=n(Yu,"After you\u2019ve filled out all these fields in the template, it should look like the following example from the SQuAD metric script:"),Yu.forEach(t),dr=u(e),f(Et.$$.fragment,e),hr=u(e),se=o(e,"H3",{class:!0});var rl=r(se);Re=o(rl,"A",{id:!0,class:!0,href:!0});var Fu=r(Re);Gs=o(Fu,"SPAN",{});var Ku=r(Gs);f(bt.$$.fragment,Ku),Ku.forEach(t),Fu.forEach(t),lp=u(rl),zs=o(rl,"SPAN",{});var Vu=r(zs);ip=n(Vu,"Download metric files"),Vu.forEach(t),rl.forEach(t),fr=u(e),D=o(e,"P",{});var ga=r(D);np=n(ga,"If your metric needs to download, or retrieve local files, you will need to use the "),Ws=o(ga,"CODE",{});var Qu=r(Ws);pp=n(Qu,"Metric._download_and_prepare"),Qu.forEach(t),cp=n(ga," method. For this example, let\u2019s examine the "),kt=o(ga,"A",{href:!0,rel:!0});var Ju=r(kt);up=n(Ju,"BLEURT metric loading script"),Ju.forEach(t),dp=n(ga,"."),ga.forEach(t),mr=u(e),pa=o(e,"OL",{});var Xu=r(pa);Ys=o(Xu,"LI",{});var Zu=r(Ys);hp=n(Zu,"Provide a dictionary of URLs that point to the metric files:"),Zu.forEach(t),Xu.forEach(t),vr=u(e),f(jt.$$.fragment,e),_r=u(e),f(Ge.$$.fragment,e),gr=u(e),qt=o(e,"OL",{start:!0});var ed=r(qt);ca=o(ed,"LI",{});var Dp=r(ca);Fs=o(Dp,"CODE",{});var td=r(Fs);fp=n(td,"Metric._download_and_prepare"),td.forEach(t),mp=n(Dp," will take the URLs and download the metric files specified:"),Dp.forEach(t),ed.forEach(t),wr=u(e),f(At.$$.fragment,e),yr=u(e),oe=o(e,"H3",{class:!0});var ll=r(oe);ze=o(ll,"A",{id:!0,class:!0,href:!0});var ad=r(ze);Ks=o(ad,"SPAN",{});var sd=r(Ks);f(Pt.$$.fragment,sd),sd.forEach(t),ad.forEach(t),vp=u(ll),Vs=o(ll,"SPAN",{});var od=r(Vs);_p=n(od,"Compute score"),od.forEach(t),ll.forEach(t),$r=u(e),re=o(e,"P",{});var lo=r(re);Qs=o(lo,"CODE",{});var rd=r(Qs);gp=n(rd,"DatasetBuilder._compute"),rd.forEach(t),wp=n(lo," provides the actual instructions for how to compute a metric given the predictions and references. Now let\u2019s take a look at the "),Ct=o(lo,"A",{href:!0,rel:!0});var ld=r(Ct);yp=n(ld,"GLUE metric loading script"),ld.forEach(t),$p=n(lo,"."),lo.forEach(t),Er=u(e),ua=o(e,"OL",{});var id=r(ua);xt=o(id,"LI",{});var il=r(xt);Ep=n(il,"Provide the functions for "),Js=o(il,"CODE",{});var nd=r(Js);bp=n(nd,"DatasetBuilder._compute"),nd.forEach(t),kp=n(il," to calculate your metric:"),il.forEach(t),id.forEach(t),br=u(e),f(St.$$.fragment,e),kr=u(e),Tt=o(e,"OL",{start:!0});var pd=r(Tt);It=o(pd,"LI",{});var nl=r(It);jp=n(nl,"Create "),Xs=o(nl,"CODE",{});var cd=r(Xs);qp=n(cd,"DatasetBuilder._compute"),cd.forEach(t),Ap=n(nl," with instructions for what metric to calculate for each configuration:"),nl.forEach(t),pd.forEach(t),jr=u(e),f(Lt.$$.fragment,e),qr=u(e),le=o(e,"H3",{class:!0});var pl=r(le);We=o(pl,"A",{id:!0,class:!0,href:!0});var ud=r(We);Zs=o(ud,"SPAN",{});var dd=r(Zs);f(Ot.$$.fragment,dd),dd.forEach(t),ud.forEach(t),Pp=u(pl),eo=o(pl,"SPAN",{});var hd=r(eo);Cp=n(hd,"Test"),hd.forEach(t),pl.forEach(t),Ar=u(e),da=o(e,"P",{});var fd=r(da);xp=n(fd,"Once you\u2019re finished writing your metric loading script, try to load it locally:"),fd.forEach(t),Pr=u(e),f(Dt.$$.fragment,e),this.h()},h(){d(w,"name","hf:doc:metadata"),d(w,"content",JSON.stringify(kd)),d($,"id","a-quick-tour"),d($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d($,"href","#a-quick-tour"),d(y,"class","relative group"),d(pe,"id","three-types-of-evaluations"),d(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(pe,"href","#three-types-of-evaluations"),d(B,"class","relative group"),d(Rt,"href","hf.co/evaluate-metric"),d(Gt,"href","hf.co/evaluate-comparison"),d(zt,"href","hf.co/evaluate-measurement"),d(Wt,"href","/docs/evaluate/pr_58/en/package_reference/loading_methods#evaluate.load"),d(fe,"id","load"),d(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fe,"href","#load"),d(H,"class","relative group"),d(ve,"id","community-modules"),d(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ve,"href","#community-modules"),d(R,"class","relative group"),d(_e,"id","compute"),d(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_e,"href","#compute"),d(G,"class","relative group"),d(we,"id","add-and-addbatch"),d(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(we,"href","#add-and-addbatch"),d(z,"class","relative group"),d(ye,"id","compute"),d(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ye,"href","#compute"),d(W,"class","relative group"),d($e,"id","distributed-evaluation"),d($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d($e,"href","#distributed-evaluation"),d(Y,"class","relative group"),d(ot,"href","https://huggingface.co/metrics/f1"),d(ot,"rel","nofollow"),d(Ee,"id","features"),d(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ee,"href","#features"),d(F,"class","relative group"),d(be,"id","save-and-share"),d(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(be,"href","#save-and-share"),d(K,"class","relative group"),d(ke,"id","eli5-evaluateload"),d(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ke,"href","#eli5-evaluateload"),d(V,"class","relative group"),d(Xt,"href","/docs/evaluate/pr_58/en/package_reference/loading_methods#evaluate.load"),d(qe,"id","distributed-evaluation"),d(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qe,"href","#distributed-evaluation"),d(Q,"class","relative group"),d(Ae,"id","metrics"),d(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ae,"href","#metrics"),d(J,"class","relative group"),d(Pe,"id","add-predictions-and-references"),d(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Pe,"href","#add-predictions-and-references"),d(X,"class","relative group"),d(Ie,"id","compute-scores"),d(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ie,"href","#compute-scores"),d(Z,"class","relative group"),d(ht,"href","https://huggingface.co/metrics/sacrebleu"),d(ht,"rel","nofollow"),d(mt,"start","2"),d(_t,"start","3"),d(aa,"id","metric_script"),d(De,"id","custom-metric-loading-script"),d(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(De,"href","#custom-metric-loading-script"),d(te,"class","relative group"),d(sa,"href","/docs/evaluate/pr_58/en/package_reference/loading_methods#evaluate.load"),d(yt,"href","https://github.com/huggingface/datasets/blob/master/metrics/squad/squad.py"),d(yt,"rel","nofollow"),d(Be,"id","add-metric-attributes"),d(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Be,"href","#add-metric-attributes"),d(ae,"class","relative group"),d(Re,"id","download-metric-files"),d(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Re,"href","#download-metric-files"),d(se,"class","relative group"),d(kt,"href","https://github.com/huggingface/datasets/blob/master/metrics/bleurt/bleurt.py"),d(kt,"rel","nofollow"),d(qt,"start","2"),d(ze,"id","compute-score"),d(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ze,"href","#compute-score"),d(oe,"class","relative group"),d(Ct,"href","https://github.com/huggingface/datasets/blob/master/metrics/glue/glue.py"),d(Ct,"rel","nofollow"),d(Tt,"start","2"),d(We,"id","test"),d(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(We,"href","#test"),d(le,"class","relative group")},m(e,l){a(document.head,w),p(e,b,l),p(e,y,l),a(y,$),a($,U),m(q,U,null),a(y,C),a(y,ne),a(ne,cl),p(e,io,l),p(e,Bt,l),a(Bt,ul),p(e,no,l),p(e,B,l),a(B,pe),a(pe,wa),m(Fe,wa,null),a(B,dl),a(B,ya),a(ya,hl),p(e,po,l),p(e,Ht,l),a(Ht,fl),p(e,co,l),p(e,x,l),a(x,ce),a(ce,$a),a($a,ml),a(ce,vl),a(ce,Rt),a(Rt,_l),a(ce,gl),a(x,wl),a(x,ue),a(ue,Ea),a(Ea,yl),a(ue,$l),a(ue,Gt),a(Gt,El),a(ue,bl),a(x,kl),a(x,de),a(de,ba),a(ba,jl),a(de,ql),a(de,zt),a(zt,Al),a(de,Pl),p(e,uo,l),p(e,he,l),a(he,Cl),a(he,Wt),a(Wt,xl),a(he,Sl),p(e,ho,l),p(e,H,l),a(H,fe),a(fe,ka),m(Ke,ka,null),a(H,Tl),a(H,ja),a(ja,Il),p(e,fo,l),p(e,me,l),a(me,Ll),a(me,qa),a(qa,Ol),a(me,Dl),p(e,mo,l),m(Ve,e,l),p(e,vo,l),p(e,Yt,l),a(Yt,Nl),p(e,_o,l),m(Qe,e,l),p(e,go,l),p(e,R,l),a(R,ve),a(ve,Aa),m(Je,Aa,null),a(R,Ml),a(R,Pa),a(Pa,Ul),p(e,wo,l),p(e,Ft,l),a(Ft,Bl),p(e,yo,l),m(Xe,e,l),p(e,$o,l),p(e,G,l),a(G,_e),a(_e,Ca),m(Ze,Ca,null),a(G,Hl),a(G,xa),a(xa,Rl),p(e,Eo,l),p(e,Kt,l),a(Kt,Gl),p(e,bo,l),p(e,ge,l),a(ge,Sa),a(Sa,zl),a(ge,Wl),a(ge,Ta),a(Ta,Yl),p(e,ko,l),p(e,k,l),a(k,Fl),a(k,Ia),a(Ia,Kl),a(k,Vl),a(k,La),a(La,Ql),a(k,Jl),a(k,Oa),a(Oa,Xl),a(k,Zl),a(k,Da),a(Da,ei),a(k,ti),p(e,jo,l),p(e,z,l),a(z,we),a(we,Na),m(et,Na,null),a(z,ai),a(z,tt),a(tt,Ma),a(Ma,si),a(tt,oi),a(tt,Ua),a(Ua,ri),p(e,qo,l),p(e,W,l),a(W,ye),a(ye,Ba),m(at,Ba,null),a(W,li),a(W,Ha),a(Ha,Ra),a(Ra,ii),p(e,Ao,l),p(e,Y,l),a(Y,$e),a($e,Ga),m(st,Ga,null),a(Y,ni),a(Y,za),a(za,pi),p(e,Po,l),p(e,j,l),a(j,ci),a(j,Wa),a(Wa,ui),a(j,di),a(j,Ya),a(Ya,hi),a(j,fi),a(j,ot),a(ot,mi),a(j,vi),a(j,Fa),a(Fa,_i),a(j,gi),p(e,Co,l),p(e,Vt,l),a(Vt,wi),p(e,xo,l),p(e,S,l),a(S,yi),a(S,Ka),a(Ka,$i),a(S,Ei),a(S,Va),a(Va,bi),a(S,ki),p(e,So,l),p(e,Qt,l),a(Qt,ji),p(e,To,l),p(e,F,l),a(F,Ee),a(Ee,Qa),m(rt,Qa,null),a(F,qi),a(F,Ja),a(Ja,Ai),p(e,Io,l),p(e,K,l),a(K,be),a(be,Xa),m(lt,Xa,null),a(K,Pi),a(K,Za),a(Za,Ci),p(e,Lo,l),p(e,V,l),a(V,ke),a(ke,es),m(it,es,null),a(V,xi),a(V,Jt),a(Jt,Si),a(Jt,ts),a(ts,Ti),p(e,Oo,l),p(e,je,l),a(je,Ii),a(je,Xt),a(Xt,Li),a(je,Oi),p(e,Do,l),p(e,T,l),a(T,Di),a(T,as),a(as,Ni),a(T,Mi),a(T,ss),a(ss,Ui),a(T,Bi),p(e,No,l),p(e,Q,l),a(Q,qe),a(qe,os),m(nt,os,null),a(Q,Hi),a(Q,rs),a(rs,Ri),p(e,Mo,l),p(e,J,l),a(J,Ae),a(Ae,ls),m(pt,ls,null),a(J,Gi),a(J,is),a(is,zi),p(e,Uo,l),p(e,Zt,l),a(Zt,Wi),p(e,Bo,l),p(e,ea,l),a(ea,Yi),p(e,Ho,l),p(e,I,l),a(I,ns),a(ns,Fi),a(I,Ki),a(I,ps),a(ps,Vi),a(I,Qi),a(I,cs),a(cs,Ji),p(e,Ro,l),p(e,X,l),a(X,Pe),a(Pe,us),m(ct,us,null),a(X,Xi),a(X,ds),a(ds,Zi),p(e,Go,l),p(e,Ce,l),a(Ce,en),a(Ce,hs),a(hs,tn),a(Ce,an),p(e,zo,l),p(e,xe,l),a(xe,fs),a(fs,L),a(L,ms),a(ms,sn),a(L,on),a(L,vs),a(vs,rn),a(L,ln),a(L,_s),a(_s,nn),a(L,pn),a(xe,cn),a(xe,gs),a(gs,O),a(O,ws),a(ws,un),a(O,dn),a(O,ys),a(ys,hn),a(O,fn),a(O,$s),a($s,mn),a(O,vn),p(e,Wo,l),p(e,Se,l),a(Se,_n),a(Se,Es),a(Es,gn),a(Se,wn),p(e,Yo,l),m(ut,e,l),p(e,Fo,l),m(Te,e,l),p(e,Ko,l),p(e,Z,l),a(Z,Ie),a(Ie,bs),m(dt,bs,null),a(Z,yn),a(Z,ks),a(ks,$n),p(e,Vo,l),p(e,Le,l),a(Le,En),a(Le,js),a(js,bn),a(Le,kn),p(e,Qo,l),p(e,Oe,l),a(Oe,jn),a(Oe,ht),a(ht,qn),a(Oe,An),p(e,Jo,l),p(e,ta,l),a(ta,qs),a(qs,Pn),p(e,Xo,l),m(ft,e,l),p(e,Zo,l),p(e,mt,l),a(mt,As),a(As,Cn),p(e,er,l),m(vt,e,l),p(e,tr,l),p(e,_t,l),a(_t,ee),a(ee,xn),a(ee,Ps),a(Ps,Sn),a(ee,Tn),a(ee,Cs),a(Cs,In),a(ee,Ln),p(e,ar,l),m(gt,e,l),p(e,sr,l),p(e,aa,l),p(e,or,l),p(e,te,l),a(te,De),a(De,xs),m(wt,xs,null),a(te,On),a(te,Ss),a(Ss,Dn),p(e,rr,l),p(e,Ne,l),a(Ne,Nn),a(Ne,sa),a(sa,Mn),a(Ne,Un),p(e,lr,l),p(e,Me,l),a(Me,Bn),a(Me,yt),a(yt,Hn),a(Me,Rn),p(e,ir,l),m(Ue,e,l),p(e,nr,l),p(e,ae,l),a(ae,Be),a(Be,Ts),m($t,Ts,null),a(ae,Gn),a(ae,Is),a(Is,zn),p(e,pr,l),p(e,He,l),a(He,Wn),a(He,Ls),a(Ls,Yn),a(He,Fn),p(e,cr,l),p(e,P,l),a(P,Os),a(Os,oa),a(oa,Ds),a(Ds,Kn),a(oa,Vn),a(P,Qn),a(P,Ns),a(Ns,ra),a(ra,Ms),a(Ms,Jn),a(ra,Xn),a(P,Zn),a(P,Us),a(Us,la),a(la,Bs),a(Bs,ep),a(la,tp),a(P,ap),a(P,Hs),a(Hs,ia),a(ia,Rs),a(Rs,sp),a(ia,op),p(e,ur,l),p(e,na,l),a(na,rp),p(e,dr,l),m(Et,e,l),p(e,hr,l),p(e,se,l),a(se,Re),a(Re,Gs),m(bt,Gs,null),a(se,lp),a(se,zs),a(zs,ip),p(e,fr,l),p(e,D,l),a(D,np),a(D,Ws),a(Ws,pp),a(D,cp),a(D,kt),a(kt,up),a(D,dp),p(e,mr,l),p(e,pa,l),a(pa,Ys),a(Ys,hp),p(e,vr,l),m(jt,e,l),p(e,_r,l),m(Ge,e,l),p(e,gr,l),p(e,qt,l),a(qt,ca),a(ca,Fs),a(Fs,fp),a(ca,mp),p(e,wr,l),m(At,e,l),p(e,yr,l),p(e,oe,l),a(oe,ze),a(ze,Ks),m(Pt,Ks,null),a(oe,vp),a(oe,Vs),a(Vs,_p),p(e,$r,l),p(e,re,l),a(re,Qs),a(Qs,gp),a(re,wp),a(re,Ct),a(Ct,yp),a(re,$p),p(e,Er,l),p(e,ua,l),a(ua,xt),a(xt,Ep),a(xt,Js),a(Js,bp),a(xt,kp),p(e,br,l),m(St,e,l),p(e,kr,l),p(e,Tt,l),a(Tt,It),a(It,jp),a(It,Xs),a(Xs,qp),a(It,Ap),p(e,jr,l),m(Lt,e,l),p(e,qr,l),p(e,le,l),a(le,We),a(We,Zs),m(Ot,Zs,null),a(le,Pp),a(le,eo),a(eo,Cp),p(e,Ar,l),p(e,da,l),a(da,xp),p(e,Pr,l),m(Dt,e,l),Cr=!0},p(e,[l]){const Nt={};l&2&&(Nt.$$scope={dirty:l,ctx:e}),Te.$set(Nt);const to={};l&2&&(to.$$scope={dirty:l,ctx:e}),Ue.$set(to);const ao={};l&2&&(ao.$$scope={dirty:l,ctx:e}),Ge.$set(ao)},i(e){Cr||(v(q.$$.fragment,e),v(Fe.$$.fragment,e),v(Ke.$$.fragment,e),v(Ve.$$.fragment,e),v(Qe.$$.fragment,e),v(Je.$$.fragment,e),v(Xe.$$.fragment,e),v(Ze.$$.fragment,e),v(et.$$.fragment,e),v(at.$$.fragment,e),v(st.$$.fragment,e),v(rt.$$.fragment,e),v(lt.$$.fragment,e),v(it.$$.fragment,e),v(nt.$$.fragment,e),v(pt.$$.fragment,e),v(ct.$$.fragment,e),v(ut.$$.fragment,e),v(Te.$$.fragment,e),v(dt.$$.fragment,e),v(ft.$$.fragment,e),v(vt.$$.fragment,e),v(gt.$$.fragment,e),v(wt.$$.fragment,e),v(Ue.$$.fragment,e),v($t.$$.fragment,e),v(Et.$$.fragment,e),v(bt.$$.fragment,e),v(jt.$$.fragment,e),v(Ge.$$.fragment,e),v(At.$$.fragment,e),v(Pt.$$.fragment,e),v(St.$$.fragment,e),v(Lt.$$.fragment,e),v(Ot.$$.fragment,e),v(Dt.$$.fragment,e),Cr=!0)},o(e){_(q.$$.fragment,e),_(Fe.$$.fragment,e),_(Ke.$$.fragment,e),_(Ve.$$.fragment,e),_(Qe.$$.fragment,e),_(Je.$$.fragment,e),_(Xe.$$.fragment,e),_(Ze.$$.fragment,e),_(et.$$.fragment,e),_(at.$$.fragment,e),_(st.$$.fragment,e),_(rt.$$.fragment,e),_(lt.$$.fragment,e),_(it.$$.fragment,e),_(nt.$$.fragment,e),_(pt.$$.fragment,e),_(ct.$$.fragment,e),_(ut.$$.fragment,e),_(Te.$$.fragment,e),_(dt.$$.fragment,e),_(ft.$$.fragment,e),_(vt.$$.fragment,e),_(gt.$$.fragment,e),_(wt.$$.fragment,e),_(Ue.$$.fragment,e),_($t.$$.fragment,e),_(Et.$$.fragment,e),_(bt.$$.fragment,e),_(jt.$$.fragment,e),_(Ge.$$.fragment,e),_(At.$$.fragment,e),_(Pt.$$.fragment,e),_(St.$$.fragment,e),_(Lt.$$.fragment,e),_(Ot.$$.fragment,e),_(Dt.$$.fragment,e),Cr=!1},d(e){t(w),e&&t(b),e&&t(y),g(q),e&&t(io),e&&t(Bt),e&&t(no),e&&t(B),g(Fe),e&&t(po),e&&t(Ht),e&&t(co),e&&t(x),e&&t(uo),e&&t(he),e&&t(ho),e&&t(H),g(Ke),e&&t(fo),e&&t(me),e&&t(mo),g(Ve,e),e&&t(vo),e&&t(Yt),e&&t(_o),g(Qe,e),e&&t(go),e&&t(R),g(Je),e&&t(wo),e&&t(Ft),e&&t(yo),g(Xe,e),e&&t($o),e&&t(G),g(Ze),e&&t(Eo),e&&t(Kt),e&&t(bo),e&&t(ge),e&&t(ko),e&&t(k),e&&t(jo),e&&t(z),g(et),e&&t(qo),e&&t(W),g(at),e&&t(Ao),e&&t(Y),g(st),e&&t(Po),e&&t(j),e&&t(Co),e&&t(Vt),e&&t(xo),e&&t(S),e&&t(So),e&&t(Qt),e&&t(To),e&&t(F),g(rt),e&&t(Io),e&&t(K),g(lt),e&&t(Lo),e&&t(V),g(it),e&&t(Oo),e&&t(je),e&&t(Do),e&&t(T),e&&t(No),e&&t(Q),g(nt),e&&t(Mo),e&&t(J),g(pt),e&&t(Uo),e&&t(Zt),e&&t(Bo),e&&t(ea),e&&t(Ho),e&&t(I),e&&t(Ro),e&&t(X),g(ct),e&&t(Go),e&&t(Ce),e&&t(zo),e&&t(xe),e&&t(Wo),e&&t(Se),e&&t(Yo),g(ut,e),e&&t(Fo),g(Te,e),e&&t(Ko),e&&t(Z),g(dt),e&&t(Vo),e&&t(Le),e&&t(Qo),e&&t(Oe),e&&t(Jo),e&&t(ta),e&&t(Xo),g(ft,e),e&&t(Zo),e&&t(mt),e&&t(er),g(vt,e),e&&t(tr),e&&t(_t),e&&t(ar),g(gt,e),e&&t(sr),e&&t(aa),e&&t(or),e&&t(te),g(wt),e&&t(rr),e&&t(Ne),e&&t(lr),e&&t(Me),e&&t(ir),g(Ue,e),e&&t(nr),e&&t(ae),g($t),e&&t(pr),e&&t(He),e&&t(cr),e&&t(P),e&&t(ur),e&&t(na),e&&t(dr),g(Et,e),e&&t(hr),e&&t(se),g(bt),e&&t(fr),e&&t(D),e&&t(mr),e&&t(pa),e&&t(vr),g(jt,e),e&&t(_r),g(Ge,e),e&&t(gr),e&&t(qt),e&&t(wr),g(At,e),e&&t(yr),e&&t(oe),g(Pt),e&&t($r),e&&t(re),e&&t(Er),e&&t(ua),e&&t(br),g(St,e),e&&t(kr),e&&t(Tt),e&&t(jr),g(Lt,e),e&&t(qr),e&&t(le),g(Ot),e&&t(Ar),e&&t(da),e&&t(Pr),g(Dt,e)}}}const kd={local:"a-quick-tour",sections:[{local:"three-types-of-evaluations",title:"Three types of evaluations"},{local:"load",sections:[{local:"community-modules",title:"Community modules"}],title:"Load"},{local:"compute",sections:[{local:"add-and-addbatch",title:"`add` and `add_batch`"},{local:"compute",title:"`compute`"},{local:"distributed-evaluation",title:"Distributed evaluation"}],title:"Compute"},{local:"features",title:"Features"},{local:"save-and-share",title:"Save and share"},{local:"eli5-evaluateload",title:"ELI5: `evaluate.load`"},{local:"distributed-evaluation",title:"Distributed evaluation"},{local:"metrics",title:"Metrics"},{local:"add-predictions-and-references",title:"Add predictions and references"},{local:"compute-scores",title:"Compute scores"},{local:"custom-metric-loading-script",sections:[{local:"add-metric-attributes",title:"Add metric attributes"},{local:"download-metric-files",title:"Download metric files"},{local:"compute-score",title:"Compute score"},{local:"test",title:"Test"}],title:"Custom metric loading script"}],title:"A quick tour"};function jd(ie){return wd(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Cd extends md{constructor(w){super();vd(this,w,jd,bd,_d,{})}}export{Cd as default,kd as metadata};
