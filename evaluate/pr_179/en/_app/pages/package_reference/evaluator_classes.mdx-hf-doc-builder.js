import{S as As,i as Fs,s as Ms,e as o,k as c,w as _,t as l,M as Ls,c as s,d as a,m as d,a as n,x as b,h as i,b as p,G as e,g,y as w,q as $,o as y,B as E,v as zs,L as ra}from"../../chunks/vendor-hf-doc-builder.js";import{D as j}from"../../chunks/Docstring-hf-doc-builder.js";import{C as la}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Ke}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as na}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Os(L){let f,q,v,u,x;return u=new la({props:{code:`from evaluate import evaluator
# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){f=o("p"),q=l("Examples:"),v=c(),_(u.$$.fragment)},l(r){f=s(r,"P",{});var h=n(f);q=i(h,"Examples:"),h.forEach(a),v=d(r),b(u.$$.fragment,r)},m(r,h){g(r,f,h),e(f,q),g(r,v,h),w(u,r,h),x=!0},p:ra,i(r){x||($(u.$$.fragment,r),x=!0)},o(r){y(u.$$.fragment,r),x=!1},d(r){r&&a(f),r&&a(v),E(u,r)}}}function Qs(L){let f,q,v,u,x;return u=new la({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("image-classification")
data = load_dataset("beans", split="test[:2]")
results = e.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    metric="accuracy",
    input_column="image",
    label_column="labels",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;image-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;beans&quot;</span>, split=<span class="hljs-string">&quot;test[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;nateraw/vit-base-beans&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;image&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;labels&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&#x27;angular_leaf_spot&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;bean_rust&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>: <span class="hljs-number">2</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){f=o("p"),q=l("Examples:"),v=c(),_(u.$$.fragment)},l(r){f=s(r,"P",{});var h=n(f);q=i(h,"Examples:"),h.forEach(a),v=d(r),b(u.$$.fragment,r)},m(r,h){g(r,f,h),e(f,q),g(r,v,h),w(u,r,h),x=!0},p:ra,i(r){x||($(u.$$.fragment,r),x=!0)},o(r){y(u.$$.fragment,r),x=!1},d(r){r&&a(f),r&&a(v),E(u,r)}}}function Us(L){let f,q,v,u,x;return u=new la({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("question-answering")
data = load_dataset("squad", split="validation[:2]")
results = e.compute(
    model_or_pipeline="mrm8488/bert-tiny-finetuned-squadv2",
    data=data,
    metric="squad",
    question_column="question",
    context_column="context",
    label_column="answers",
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;mrm8488/bert-tiny-finetuned-squadv2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    question_column=<span class="hljs-string">&quot;question&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    context_column=<span class="hljs-string">&quot;context&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;answers&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){f=o("p"),q=l("Examples:"),v=c(),_(u.$$.fragment)},l(r){f=s(r,"P",{});var h=n(f);q=i(h,"Examples:"),h.forEach(a),v=d(r),b(u.$$.fragment,r)},m(r,h){g(r,f,h),e(f,q),g(r,v,h),w(u,r,h),x=!0},p:ra,i(r){x||($(u.$$.fragment,r),x=!0)},o(r){y(u.$$.fragment,r),x=!1},d(r){r&&a(f),r&&a(v),E(u,r)}}}function Ss(L){let f,q,v,u,x;return u=new la({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("text-classification")
data = load_dataset("imdb", split="test[:2]")
results = e.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    input_column="text",
    label_column="label",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>, split=<span class="hljs-string">&quot;test[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;text&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;label&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){f=o("p"),q=l("Examples:"),v=c(),_(u.$$.fragment)},l(r){f=s(r,"P",{});var h=n(f);q=i(h,"Examples:"),h.forEach(a),v=d(r),b(u.$$.fragment,r)},m(r,h){g(r,f,h),e(f,q),g(r,v,h),w(u,r,h),x=!0},p:ra,i(r){x||($(u.$$.fragment,r),x=!0)},o(r){y(u.$$.fragment,r),x=!1},d(r){r&&a(f),r&&a(v),E(u,r)}}}function Rs(L){let f,q,v,u,x,r,h,Xe,ia,Nt,Me,ca,It,R,J,Ye,fe,da,Ze,pa,At,Le,ua,Ft,D,ge,ma,N,fa,ze,ga,ha,et,va,_a,tt,ba,wa,$a,K,Mt,Oe,ya,Lt,k,he,Ea,at,xa,qa,X,ve,ka,V,Ta,ot,ja,Pa,st,Ca,Da,Na,Y,_e,Ia,nt,Aa,Fa,Z,be,Ma,rt,La,za,ee,we,Oa,lt,Qa,Ua,te,$e,Sa,it,Ra,zt,B,ae,ct,ye,Va,dt,Ba,Ot,H,oe,pt,Ee,Ha,ut,Wa,Qt,I,xe,Ga,A,Ja,Qe,Ka,Xa,mt,Ya,Za,ft,eo,to,ao,z,qe,oo,gt,so,no,se,Ut,W,ne,ht,ke,ro,vt,lo,St,T,Te,io,F,co,Ue,po,uo,_t,mo,fo,bt,go,ho,vo,O,je,_o,wt,bo,wo,re,$o,le,Pe,yo,Ce,Eo,$t,xo,qo,ko,ie,De,To,yt,jo,Rt,G,ce,Et,Ne,Po,xt,Co,Vt,M,Ie,Do,C,No,Se,Io,Ao,qt,Fo,Mo,kt,Lo,zo,Tt,Oo,Qo,Uo,Q,Ae,So,jt,Ro,Vo,de,Bt;return r=new Ke({}),fe=new Ke({}),ge=new j({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:<ul>
<li><code>&quot;image-classification&quot;</code>: will return a <a href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator">ImageClassificationEvaluator</a>.</li>
<li><code>&quot;question-answering&quot;</code>: will return a <a href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.QuestionAnsweringEvaluator">QuestionAnsweringEvaluator</a>.</li>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/__init__.py#L79",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),K=new na({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[Os]},$$scope:{ctx:L}}}),he=new j({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L54"}}),ve=new j({props:{name:"compute",anchor:"evaluate.Evaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"**compute_parameters",val:": typing.Dict"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L107"}}),_e=new j({props:{name:"core_compute",anchor:"evaluate.Evaluator.core_compute",parameters:[{name:"references",val:": typing.List"},{name:"predictions",val:": typing.List"},{name:"metric",val:": EvaluationModule"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L220"}}),be=new j({props:{name:"prepare_data",anchor:"evaluate.Evaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"input_column",val:": str"},{name:"label_column",val:": str"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_data.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.Evaluator.prepare_data.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.prepare_data.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L125",returnDescription:`
<p>Loaded <code>datasets.Dataset</code>.</p>
`}}),we=new j({props:{name:"prepare_metric",anchor:"evaluate.Evaluator.prepare_metric",parameters:[{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule]"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_metric.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L196",returnDescription:`
<p>The loaded metric.</p>
`}}),$e=new j({props:{name:"prepare_pipeline",anchor:"evaluate.Evaluator.prepare_pipeline",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"preprocessor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_pipeline.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.prepare_pipeline.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.Evaluator.prepare_pipeline.preprocessor",description:`<strong>preprocessor</strong> (<code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"preprocessor"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L156",returnDescription:`
<p>The initialized pipeline.</p>
`}}),ye=new Ke({}),Ee=new Ke({}),xe=new j({props:{name:"class evaluate.ImageClassificationEvaluator",anchor:"evaluate.ImageClassificationEvaluator",parameters:[{name:"task",val:" = 'image-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/image_classification.py#L39"}}),qe=new j({props:{name:"compute",anchor:"evaluate.ImageClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"feature_extractor",val:": typing.Union[str, ForwardRef('FeatureExtractionMixin'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'image'"},{name:"label_column",val:": str = 'labels'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"}],parametersDescription:[{anchor:"evaluate.ImageClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.ImageClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>image-classification</code>. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.ImageClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.ImageClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>&#x201D; &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
feature_extractor &#x2014; (<code>str</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default feature extractor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.ImageClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.ImageClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.ImageClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.ImageClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;image&quot;</code>) &#x2014;
the name of the column containing the images as PIL ImageFile in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;labels&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/image_classification.py#L50",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),se=new na({props:{anchor:"evaluate.ImageClassificationEvaluator.compute.example",$$slots:{default:[Qs]},$$scope:{ctx:L}}}),ke=new Ke({}),Te=new j({props:{name:"class evaluate.QuestionAnsweringEvaluator",anchor:"evaluate.QuestionAnsweringEvaluator",parameters:[{name:"task",val:" = 'question-answering'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L38"}}),je=new j({props:{name:"compute",anchor:"evaluate.QuestionAnsweringEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"question_column",val:": str = 'question'"},{name:"context_column",val:": str = 'context'"},{name:"id_column",val:": str = 'id'"},{name:"label_column",val:": str = 'answers'"}],parametersDescription:[{anchor:"evaluate.QuestionAnsweringEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>question-answering</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;&#x201D;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.question_column",description:`<strong>question_column</strong> (<code>str</code>, defaults to <code>&quot;question&quot;</code>) &#x2014;
the name of the column containing the question in the dataset specified by <code>data</code>.`,name:"question_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.context_column",description:`<strong>context_column</strong> (<code>str</code>, defaults to <code>&quot;context&quot;</code>) &#x2014;
the name of the column containing the context in the dataset specified by <code>data</code>.`,name:"context_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.id_column",description:`<strong>id_column</strong> (<code>str</code>, defaults to <code>&quot;id&quot;</code>) &#x2014;
the name of the column cointaing the identification field of the question and answer pair in the
dataset specified by <code>data</code>.`,name:"id_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;answers&quot;</code>) &#x2014;
the name of the column containing the answers in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L96",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),re=new na({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example",$$slots:{default:[Us]},$$scope:{ctx:L}}}),Pe=new j({props:{name:"is_squad_v2_schema",anchor:"evaluate.QuestionAnsweringEvaluator.is_squad_v2_schema",parameters:[{name:"data",val:": Dataset"},{name:"label_column",val:": str = 'answers'"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L82"}}),De=new j({props:{name:"prepare_data",anchor:"evaluate.QuestionAnsweringEvaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"question_column",val:": str"},{name:"context_column",val:": str"},{name:"id_column",val:": str"},{name:"label_column",val:": str"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L49"}}),Ne=new Ke({}),Ie=new j({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/text_classification.py#L39"}}),Ae=new j({props:{name:"compute",anchor:"evaluate.TextClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'text'"},{name:"label_column",val:": str = 'label'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"}],parametersDescription:[{anchor:"evaluate.TextClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.TextClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.TextClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.TextClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;&#x201D;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.TextClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TextClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TextClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TextClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/text_classification.py#L51",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),de=new na({props:{anchor:"evaluate.TextClassificationEvaluator.compute.example",$$slots:{default:[Ss]},$$scope:{ctx:L}}}),{c(){f=o("meta"),q=c(),v=o("h1"),u=o("a"),x=o("span"),_(r.$$.fragment),h=c(),Xe=o("span"),ia=l("Evaluator"),Nt=c(),Me=o("p"),ca=l("The evaluator classes for automatic evaluation."),It=c(),R=o("h2"),J=o("a"),Ye=o("span"),_(fe.$$.fragment),da=c(),Ze=o("span"),pa=l("Evaluator classes"),At=c(),Le=o("p"),ua=l("The main entry point for using the evaluator:"),Ft=c(),D=o("div"),_(ge.$$.fragment),ma=c(),N=o("p"),fa=l("Utility factory method to build an "),ze=o("a"),ga=l("Evaluator"),ha=l(`.
Evaluators encapsulate a task and a default metric name. They leverage `),et=o("code"),va=l("pipeline"),_a=l(" functionalify from "),tt=o("code"),ba=l("transformers"),wa=l(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),$a=c(),_(K.$$.fragment),Mt=c(),Oe=o("p"),ya=l("The base class for all evaluator classes:"),Lt=c(),k=o("div"),_(he.$$.fragment),Ea=c(),at=o("p"),xa=l(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),qa=c(),X=o("div"),_(ve.$$.fragment),ka=c(),V=o("p"),Ta=l("A core method of the "),ot=o("code"),ja=l("Evaluator"),Pa=l(` class, computes the metric value for a pipeline and dataset compatible
with the task specified by the `),st=o("code"),Ca=l("Evaluator"),Da=l("."),Na=c(),Y=o("div"),_(_e.$$.fragment),Ia=c(),nt=o("p"),Aa=l("Compute and return metrics."),Fa=c(),Z=o("div"),_(be.$$.fragment),Ma=c(),rt=o("p"),La=l("Prepare data."),za=c(),ee=o("div"),_(we.$$.fragment),Oa=c(),lt=o("p"),Qa=l("Prepare metric."),Ua=c(),te=o("div"),_($e.$$.fragment),Sa=c(),it=o("p"),Ra=l("Prepare pipeline."),zt=c(),B=o("h2"),ae=o("a"),ct=o("span"),_(ye.$$.fragment),Va=c(),dt=o("span"),Ba=l("The task specific evaluators"),Ot=c(),H=o("h3"),oe=o("a"),pt=o("span"),_(Ee.$$.fragment),Ha=c(),ut=o("span"),Wa=l("ImageClassificationEvaluator"),Qt=c(),I=o("div"),_(xe.$$.fragment),Ga=c(),A=o("p"),Ja=l(`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Qe=o("a"),Ka=l("evaluator()"),Xa=l(` using the default task name
`),mt=o("code"),Ya=l("image-classification"),Za=l(`.
Methods in this class assume a data format compatible with the `),ft=o("code"),eo=l("ImageClassificationPipeline"),to=l("."),ao=c(),z=o("div"),_(qe.$$.fragment),oo=c(),gt=o("p"),so=l("Compute the metric for a given pipeline and dataset combination."),no=c(),_(se.$$.fragment),Ut=c(),W=o("h3"),ne=o("a"),ht=o("span"),_(ke.$$.fragment),ro=c(),vt=o("span"),lo=l("QuestionAnsweringEvaluator"),St=c(),T=o("div"),_(Te.$$.fragment),io=c(),F=o("p"),co=l(`Question answering evaluator.
This question answering evaluator can currently be loaded from `),Ue=o("a"),po=l("evaluator()"),uo=l(` using the default task name
`),_t=o("code"),mo=l("question-answering"),fo=l(`.
Methods in this class assume a data format compatible with the `),bt=o("code"),go=l("QuestionAnsweringPipeline"),ho=l("."),vo=c(),O=o("div"),_(je.$$.fragment),_o=c(),wt=o("p"),bo=l("Compute the metric for a given pipeline and dataset combination."),wo=c(),_(re.$$.fragment),$o=c(),le=o("div"),_(Pe.$$.fragment),yo=c(),Ce=o("p"),Eo=l(`Check if the provided dataset follows the squad v2 data schema, namely possible samples where the answer is not in the context.
In this case, the answer text list should be `),$t=o("code"),xo=l("[]"),qo=l("."),ko=c(),ie=o("div"),_(De.$$.fragment),To=c(),yt=o("p"),jo=l("Prepare data."),Rt=c(),G=o("h3"),ce=o("a"),Et=o("span"),_(Ne.$$.fragment),Po=c(),xt=o("span"),Co=l("TextClassificationEvaluator"),Vt=c(),M=o("div"),_(Ie.$$.fragment),Do=c(),C=o("p"),No=l(`Text classification evaluator.
This text classification evaluator can currently be loaded from `),Se=o("a"),Io=l("evaluator()"),Ao=l(` using the default task name
`),qt=o("code"),Fo=l("text-classification"),Mo=l(" or with a "),kt=o("code"),Lo=l('"sentiment-analysis"'),zo=l(` alias.
Methods in this class assume a data format compatible with the `),Tt=o("code"),Oo=l("TextClassificationPipeline"),Qo=l(` - a single textual
feature as input and a categorical label as output.`),Uo=c(),Q=o("div"),_(Ae.$$.fragment),So=c(),jt=o("p"),Ro=l("Compute the metric for a given pipeline and dataset combination."),Vo=c(),_(de.$$.fragment),this.h()},l(t){const m=Ls('[data-svelte="svelte-1phssyn"]',document.head);f=s(m,"META",{name:!0,content:!0}),m.forEach(a),q=d(t),v=s(t,"H1",{class:!0});var Fe=n(v);u=s(Fe,"A",{id:!0,class:!0,href:!0});var Pt=n(u);x=s(Pt,"SPAN",{});var Ct=n(x);b(r.$$.fragment,Ct),Ct.forEach(a),Pt.forEach(a),h=d(Fe),Xe=s(Fe,"SPAN",{});var Dt=n(Xe);ia=i(Dt,"Evaluator"),Dt.forEach(a),Fe.forEach(a),Nt=d(t),Me=s(t,"P",{});var Bo=n(Me);ca=i(Bo,"The evaluator classes for automatic evaluation."),Bo.forEach(a),It=d(t),R=s(t,"H2",{class:!0});var Ht=n(R);J=s(Ht,"A",{id:!0,class:!0,href:!0});var Ho=n(J);Ye=s(Ho,"SPAN",{});var Wo=n(Ye);b(fe.$$.fragment,Wo),Wo.forEach(a),Ho.forEach(a),da=d(Ht),Ze=s(Ht,"SPAN",{});var Go=n(Ze);pa=i(Go,"Evaluator classes"),Go.forEach(a),Ht.forEach(a),At=d(t),Le=s(t,"P",{});var Jo=n(Le);ua=i(Jo,"The main entry point for using the evaluator:"),Jo.forEach(a),Ft=d(t),D=s(t,"DIV",{class:!0});var Re=n(D);b(ge.$$.fragment,Re),ma=d(Re),N=s(Re,"P",{});var pe=n(N);fa=i(pe,"Utility factory method to build an "),ze=s(pe,"A",{href:!0});var Ko=n(ze);ga=i(Ko,"Evaluator"),Ko.forEach(a),ha=i(pe,`.
Evaluators encapsulate a task and a default metric name. They leverage `),et=s(pe,"CODE",{});var Xo=n(et);va=i(Xo,"pipeline"),Xo.forEach(a),_a=i(pe," functionalify from "),tt=s(pe,"CODE",{});var Yo=n(tt);ba=i(Yo,"transformers"),Yo.forEach(a),wa=i(pe,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),pe.forEach(a),$a=d(Re),b(K.$$.fragment,Re),Re.forEach(a),Mt=d(t),Oe=s(t,"P",{});var Zo=n(Oe);ya=i(Zo,"The base class for all evaluator classes:"),Zo.forEach(a),Lt=d(t),k=s(t,"DIV",{class:!0});var P=n(k);b(he.$$.fragment,P),Ea=d(P),at=s(P,"P",{});var es=n(at);xa=i(es,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),es.forEach(a),qa=d(P),X=s(P,"DIV",{class:!0});var Wt=n(X);b(ve.$$.fragment,Wt),ka=d(Wt),V=s(Wt,"P",{});var Ve=n(V);Ta=i(Ve,"A core method of the "),ot=s(Ve,"CODE",{});var ts=n(ot);ja=i(ts,"Evaluator"),ts.forEach(a),Pa=i(Ve,` class, computes the metric value for a pipeline and dataset compatible
with the task specified by the `),st=s(Ve,"CODE",{});var as=n(st);Ca=i(as,"Evaluator"),as.forEach(a),Da=i(Ve,"."),Ve.forEach(a),Wt.forEach(a),Na=d(P),Y=s(P,"DIV",{class:!0});var Gt=n(Y);b(_e.$$.fragment,Gt),Ia=d(Gt),nt=s(Gt,"P",{});var os=n(nt);Aa=i(os,"Compute and return metrics."),os.forEach(a),Gt.forEach(a),Fa=d(P),Z=s(P,"DIV",{class:!0});var Jt=n(Z);b(be.$$.fragment,Jt),Ma=d(Jt),rt=s(Jt,"P",{});var ss=n(rt);La=i(ss,"Prepare data."),ss.forEach(a),Jt.forEach(a),za=d(P),ee=s(P,"DIV",{class:!0});var Kt=n(ee);b(we.$$.fragment,Kt),Oa=d(Kt),lt=s(Kt,"P",{});var ns=n(lt);Qa=i(ns,"Prepare metric."),ns.forEach(a),Kt.forEach(a),Ua=d(P),te=s(P,"DIV",{class:!0});var Xt=n(te);b($e.$$.fragment,Xt),Sa=d(Xt),it=s(Xt,"P",{});var rs=n(it);Ra=i(rs,"Prepare pipeline."),rs.forEach(a),Xt.forEach(a),P.forEach(a),zt=d(t),B=s(t,"H2",{class:!0});var Yt=n(B);ae=s(Yt,"A",{id:!0,class:!0,href:!0});var ls=n(ae);ct=s(ls,"SPAN",{});var is=n(ct);b(ye.$$.fragment,is),is.forEach(a),ls.forEach(a),Va=d(Yt),dt=s(Yt,"SPAN",{});var cs=n(dt);Ba=i(cs,"The task specific evaluators"),cs.forEach(a),Yt.forEach(a),Ot=d(t),H=s(t,"H3",{class:!0});var Zt=n(H);oe=s(Zt,"A",{id:!0,class:!0,href:!0});var ds=n(oe);pt=s(ds,"SPAN",{});var ps=n(pt);b(Ee.$$.fragment,ps),ps.forEach(a),ds.forEach(a),Ha=d(Zt),ut=s(Zt,"SPAN",{});var us=n(ut);Wa=i(us,"ImageClassificationEvaluator"),us.forEach(a),Zt.forEach(a),Qt=d(t),I=s(t,"DIV",{class:!0});var Be=n(I);b(xe.$$.fragment,Be),Ga=d(Be),A=s(Be,"P",{});var ue=n(A);Ja=i(ue,`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Qe=s(ue,"A",{href:!0});var ms=n(Qe);Ka=i(ms,"evaluator()"),ms.forEach(a),Xa=i(ue,` using the default task name
`),mt=s(ue,"CODE",{});var fs=n(mt);Ya=i(fs,"image-classification"),fs.forEach(a),Za=i(ue,`.
Methods in this class assume a data format compatible with the `),ft=s(ue,"CODE",{});var gs=n(ft);eo=i(gs,"ImageClassificationPipeline"),gs.forEach(a),to=i(ue,"."),ue.forEach(a),ao=d(Be),z=s(Be,"DIV",{class:!0});var He=n(z);b(qe.$$.fragment,He),oo=d(He),gt=s(He,"P",{});var hs=n(gt);so=i(hs,"Compute the metric for a given pipeline and dataset combination."),hs.forEach(a),no=d(He),b(se.$$.fragment,He),He.forEach(a),Be.forEach(a),Ut=d(t),W=s(t,"H3",{class:!0});var ea=n(W);ne=s(ea,"A",{id:!0,class:!0,href:!0});var vs=n(ne);ht=s(vs,"SPAN",{});var _s=n(ht);b(ke.$$.fragment,_s),_s.forEach(a),vs.forEach(a),ro=d(ea),vt=s(ea,"SPAN",{});var bs=n(vt);lo=i(bs,"QuestionAnsweringEvaluator"),bs.forEach(a),ea.forEach(a),St=d(t),T=s(t,"DIV",{class:!0});var U=n(T);b(Te.$$.fragment,U),io=d(U),F=s(U,"P",{});var me=n(F);co=i(me,`Question answering evaluator.
This question answering evaluator can currently be loaded from `),Ue=s(me,"A",{href:!0});var ws=n(Ue);po=i(ws,"evaluator()"),ws.forEach(a),uo=i(me,` using the default task name
`),_t=s(me,"CODE",{});var $s=n(_t);mo=i($s,"question-answering"),$s.forEach(a),fo=i(me,`.
Methods in this class assume a data format compatible with the `),bt=s(me,"CODE",{});var ys=n(bt);go=i(ys,"QuestionAnsweringPipeline"),ys.forEach(a),ho=i(me,"."),me.forEach(a),vo=d(U),O=s(U,"DIV",{class:!0});var We=n(O);b(je.$$.fragment,We),_o=d(We),wt=s(We,"P",{});var Es=n(wt);bo=i(Es,"Compute the metric for a given pipeline and dataset combination."),Es.forEach(a),wo=d(We),b(re.$$.fragment,We),We.forEach(a),$o=d(U),le=s(U,"DIV",{class:!0});var ta=n(le);b(Pe.$$.fragment,ta),yo=d(ta),Ce=s(ta,"P",{});var aa=n(Ce);Eo=i(aa,`Check if the provided dataset follows the squad v2 data schema, namely possible samples where the answer is not in the context.
In this case, the answer text list should be `),$t=s(aa,"CODE",{});var xs=n($t);xo=i(xs,"[]"),xs.forEach(a),qo=i(aa,"."),aa.forEach(a),ta.forEach(a),ko=d(U),ie=s(U,"DIV",{class:!0});var oa=n(ie);b(De.$$.fragment,oa),To=d(oa),yt=s(oa,"P",{});var qs=n(yt);jo=i(qs,"Prepare data."),qs.forEach(a),oa.forEach(a),U.forEach(a),Rt=d(t),G=s(t,"H3",{class:!0});var sa=n(G);ce=s(sa,"A",{id:!0,class:!0,href:!0});var ks=n(ce);Et=s(ks,"SPAN",{});var Ts=n(Et);b(Ne.$$.fragment,Ts),Ts.forEach(a),ks.forEach(a),Po=d(sa),xt=s(sa,"SPAN",{});var js=n(xt);Co=i(js,"TextClassificationEvaluator"),js.forEach(a),sa.forEach(a),Vt=d(t),M=s(t,"DIV",{class:!0});var Ge=n(M);b(Ie.$$.fragment,Ge),Do=d(Ge),C=s(Ge,"P",{});var S=n(C);No=i(S,`Text classification evaluator.
This text classification evaluator can currently be loaded from `),Se=s(S,"A",{href:!0});var Ps=n(Se);Io=i(Ps,"evaluator()"),Ps.forEach(a),Ao=i(S,` using the default task name
`),qt=s(S,"CODE",{});var Cs=n(qt);Fo=i(Cs,"text-classification"),Cs.forEach(a),Mo=i(S," or with a "),kt=s(S,"CODE",{});var Ds=n(kt);Lo=i(Ds,'"sentiment-analysis"'),Ds.forEach(a),zo=i(S,` alias.
Methods in this class assume a data format compatible with the `),Tt=s(S,"CODE",{});var Ns=n(Tt);Oo=i(Ns,"TextClassificationPipeline"),Ns.forEach(a),Qo=i(S,` - a single textual
feature as input and a categorical label as output.`),S.forEach(a),Uo=d(Ge),Q=s(Ge,"DIV",{class:!0});var Je=n(Q);b(Ae.$$.fragment,Je),So=d(Je),jt=s(Je,"P",{});var Is=n(jt);Ro=i(Is,"Compute the metric for a given pipeline and dataset combination."),Is.forEach(a),Vo=d(Je),b(de.$$.fragment,Je),Je.forEach(a),Ge.forEach(a),this.h()},h(){p(f,"name","hf:doc:metadata"),p(f,"content",JSON.stringify(Vs)),p(u,"id","evaluator"),p(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(u,"href","#evaluator"),p(v,"class","relative group"),p(J,"id","evaluate.evaluator"),p(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(J,"href","#evaluate.evaluator"),p(R,"class","relative group"),p(ze,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.Evaluator"),p(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ae,"id","the-task-specific-evaluators"),p(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ae,"href","#the-task-specific-evaluators"),p(B,"class","relative group"),p(oe,"id","evaluate.ImageClassificationEvaluator"),p(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(oe,"href","#evaluate.ImageClassificationEvaluator"),p(H,"class","relative group"),p(Qe,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.evaluator"),p(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ne,"id","evaluate.QuestionAnsweringEvaluator"),p(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ne,"href","#evaluate.QuestionAnsweringEvaluator"),p(W,"class","relative group"),p(Ue,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.evaluator"),p(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ce,"id","evaluate.TextClassificationEvaluator"),p(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ce,"href","#evaluate.TextClassificationEvaluator"),p(G,"class","relative group"),p(Se,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.evaluator"),p(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,m){e(document.head,f),g(t,q,m),g(t,v,m),e(v,u),e(u,x),w(r,x,null),e(v,h),e(v,Xe),e(Xe,ia),g(t,Nt,m),g(t,Me,m),e(Me,ca),g(t,It,m),g(t,R,m),e(R,J),e(J,Ye),w(fe,Ye,null),e(R,da),e(R,Ze),e(Ze,pa),g(t,At,m),g(t,Le,m),e(Le,ua),g(t,Ft,m),g(t,D,m),w(ge,D,null),e(D,ma),e(D,N),e(N,fa),e(N,ze),e(ze,ga),e(N,ha),e(N,et),e(et,va),e(N,_a),e(N,tt),e(tt,ba),e(N,wa),e(D,$a),w(K,D,null),g(t,Mt,m),g(t,Oe,m),e(Oe,ya),g(t,Lt,m),g(t,k,m),w(he,k,null),e(k,Ea),e(k,at),e(at,xa),e(k,qa),e(k,X),w(ve,X,null),e(X,ka),e(X,V),e(V,Ta),e(V,ot),e(ot,ja),e(V,Pa),e(V,st),e(st,Ca),e(V,Da),e(k,Na),e(k,Y),w(_e,Y,null),e(Y,Ia),e(Y,nt),e(nt,Aa),e(k,Fa),e(k,Z),w(be,Z,null),e(Z,Ma),e(Z,rt),e(rt,La),e(k,za),e(k,ee),w(we,ee,null),e(ee,Oa),e(ee,lt),e(lt,Qa),e(k,Ua),e(k,te),w($e,te,null),e(te,Sa),e(te,it),e(it,Ra),g(t,zt,m),g(t,B,m),e(B,ae),e(ae,ct),w(ye,ct,null),e(B,Va),e(B,dt),e(dt,Ba),g(t,Ot,m),g(t,H,m),e(H,oe),e(oe,pt),w(Ee,pt,null),e(H,Ha),e(H,ut),e(ut,Wa),g(t,Qt,m),g(t,I,m),w(xe,I,null),e(I,Ga),e(I,A),e(A,Ja),e(A,Qe),e(Qe,Ka),e(A,Xa),e(A,mt),e(mt,Ya),e(A,Za),e(A,ft),e(ft,eo),e(A,to),e(I,ao),e(I,z),w(qe,z,null),e(z,oo),e(z,gt),e(gt,so),e(z,no),w(se,z,null),g(t,Ut,m),g(t,W,m),e(W,ne),e(ne,ht),w(ke,ht,null),e(W,ro),e(W,vt),e(vt,lo),g(t,St,m),g(t,T,m),w(Te,T,null),e(T,io),e(T,F),e(F,co),e(F,Ue),e(Ue,po),e(F,uo),e(F,_t),e(_t,mo),e(F,fo),e(F,bt),e(bt,go),e(F,ho),e(T,vo),e(T,O),w(je,O,null),e(O,_o),e(O,wt),e(wt,bo),e(O,wo),w(re,O,null),e(T,$o),e(T,le),w(Pe,le,null),e(le,yo),e(le,Ce),e(Ce,Eo),e(Ce,$t),e($t,xo),e(Ce,qo),e(T,ko),e(T,ie),w(De,ie,null),e(ie,To),e(ie,yt),e(yt,jo),g(t,Rt,m),g(t,G,m),e(G,ce),e(ce,Et),w(Ne,Et,null),e(G,Po),e(G,xt),e(xt,Co),g(t,Vt,m),g(t,M,m),w(Ie,M,null),e(M,Do),e(M,C),e(C,No),e(C,Se),e(Se,Io),e(C,Ao),e(C,qt),e(qt,Fo),e(C,Mo),e(C,kt),e(kt,Lo),e(C,zo),e(C,Tt),e(Tt,Oo),e(C,Qo),e(M,Uo),e(M,Q),w(Ae,Q,null),e(Q,So),e(Q,jt),e(jt,Ro),e(Q,Vo),w(de,Q,null),Bt=!0},p(t,[m]){const Fe={};m&2&&(Fe.$$scope={dirty:m,ctx:t}),K.$set(Fe);const Pt={};m&2&&(Pt.$$scope={dirty:m,ctx:t}),se.$set(Pt);const Ct={};m&2&&(Ct.$$scope={dirty:m,ctx:t}),re.$set(Ct);const Dt={};m&2&&(Dt.$$scope={dirty:m,ctx:t}),de.$set(Dt)},i(t){Bt||($(r.$$.fragment,t),$(fe.$$.fragment,t),$(ge.$$.fragment,t),$(K.$$.fragment,t),$(he.$$.fragment,t),$(ve.$$.fragment,t),$(_e.$$.fragment,t),$(be.$$.fragment,t),$(we.$$.fragment,t),$($e.$$.fragment,t),$(ye.$$.fragment,t),$(Ee.$$.fragment,t),$(xe.$$.fragment,t),$(qe.$$.fragment,t),$(se.$$.fragment,t),$(ke.$$.fragment,t),$(Te.$$.fragment,t),$(je.$$.fragment,t),$(re.$$.fragment,t),$(Pe.$$.fragment,t),$(De.$$.fragment,t),$(Ne.$$.fragment,t),$(Ie.$$.fragment,t),$(Ae.$$.fragment,t),$(de.$$.fragment,t),Bt=!0)},o(t){y(r.$$.fragment,t),y(fe.$$.fragment,t),y(ge.$$.fragment,t),y(K.$$.fragment,t),y(he.$$.fragment,t),y(ve.$$.fragment,t),y(_e.$$.fragment,t),y(be.$$.fragment,t),y(we.$$.fragment,t),y($e.$$.fragment,t),y(ye.$$.fragment,t),y(Ee.$$.fragment,t),y(xe.$$.fragment,t),y(qe.$$.fragment,t),y(se.$$.fragment,t),y(ke.$$.fragment,t),y(Te.$$.fragment,t),y(je.$$.fragment,t),y(re.$$.fragment,t),y(Pe.$$.fragment,t),y(De.$$.fragment,t),y(Ne.$$.fragment,t),y(Ie.$$.fragment,t),y(Ae.$$.fragment,t),y(de.$$.fragment,t),Bt=!1},d(t){a(f),t&&a(q),t&&a(v),E(r),t&&a(Nt),t&&a(Me),t&&a(It),t&&a(R),E(fe),t&&a(At),t&&a(Le),t&&a(Ft),t&&a(D),E(ge),E(K),t&&a(Mt),t&&a(Oe),t&&a(Lt),t&&a(k),E(he),E(ve),E(_e),E(be),E(we),E($e),t&&a(zt),t&&a(B),E(ye),t&&a(Ot),t&&a(H),E(Ee),t&&a(Qt),t&&a(I),E(xe),E(qe),E(se),t&&a(Ut),t&&a(W),E(ke),t&&a(St),t&&a(T),E(Te),E(je),E(re),E(Pe),E(De),t&&a(Rt),t&&a(G),E(Ne),t&&a(Vt),t&&a(M),E(Ie),E(Ae),E(de)}}}const Vs={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"},{local:"the-task-specific-evaluators",sections:[{local:"evaluate.ImageClassificationEvaluator",title:"ImageClassificationEvaluator"},{local:"evaluate.QuestionAnsweringEvaluator",title:"QuestionAnsweringEvaluator"},{local:"evaluate.TextClassificationEvaluator",title:"TextClassificationEvaluator"}],title:"The task specific evaluators"}],title:"Evaluator"};function Bs(L){return zs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Xs extends As{constructor(f){super();Fs(this,f,Bs,Rs,Ms,{})}}export{Xs as default,Vs as metadata};
