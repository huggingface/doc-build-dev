import{S as ys,i as Es,s as xs,e as o,k as c,w as _,t as l,M as qs,c as s,d as a,m as d,a as n,x as b,h as i,b as p,G as e,g,y as w,q as $,o as y,B as E,v as ks,L as ea}from"../../chunks/vendor-hf-doc-builder.js";import{D as j}from"../../chunks/Docstring-hf-doc-builder.js";import{C as ta}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as We}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Zt}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Ts(L){let f,q,v,u,x;return u=new ta({props:{code:`from evaluate import evaluator
# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){f=o("p"),q=l("Examples:"),v=c(),_(u.$$.fragment)},l(r){f=s(r,"P",{});var h=n(f);q=i(h,"Examples:"),h.forEach(a),v=d(r),b(u.$$.fragment,r)},m(r,h){g(r,f,h),e(f,q),g(r,v,h),w(u,r,h),x=!0},p:ea,i(r){x||($(u.$$.fragment,r),x=!0)},o(r){y(u.$$.fragment,r),x=!1},d(r){r&&a(f),r&&a(v),E(u,r)}}}function js(L){let f,q,v,u,x;return u=new ta({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("image-classification")
data = load_dataset("beans", split="test[:2]")
results = e.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    metric="accuracy",
    input_column="image",
    label_column="labels",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;image-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;beans&quot;</span>, split=<span class="hljs-string">&quot;test[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;nateraw/vit-base-beans&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;image&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;labels&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&#x27;angular_leaf_spot&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;bean_rust&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>: <span class="hljs-number">2</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){f=o("p"),q=l("Examples:"),v=c(),_(u.$$.fragment)},l(r){f=s(r,"P",{});var h=n(f);q=i(h,"Examples:"),h.forEach(a),v=d(r),b(u.$$.fragment,r)},m(r,h){g(r,f,h),e(f,q),g(r,v,h),w(u,r,h),x=!0},p:ea,i(r){x||($(u.$$.fragment,r),x=!0)},o(r){y(u.$$.fragment,r),x=!1},d(r){r&&a(f),r&&a(v),E(u,r)}}}function Ps(L){let f,q,v,u,x;return u=new ta({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("question-answering")
data = load_dataset("squad", split="validation[:2]")
results = e.compute(
    model_or_pipeline="mrm8488/bert-tiny-finetuned-squadv2",
    data=data,
    metric="squad",
    question_column="question",
    context_column="context",
    label_column="answers",
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;mrm8488/bert-tiny-finetuned-squadv2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    question_column=<span class="hljs-string">&quot;question&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    context_column=<span class="hljs-string">&quot;context&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;answers&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){f=o("p"),q=l("Examples:"),v=c(),_(u.$$.fragment)},l(r){f=s(r,"P",{});var h=n(f);q=i(h,"Examples:"),h.forEach(a),v=d(r),b(u.$$.fragment,r)},m(r,h){g(r,f,h),e(f,q),g(r,v,h),w(u,r,h),x=!0},p:ea,i(r){x||($(u.$$.fragment,r),x=!0)},o(r){y(u.$$.fragment,r),x=!1},d(r){r&&a(f),r&&a(v),E(u,r)}}}function Cs(L){let f,q,v,u,x;return u=new ta({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("text-classification")
data = load_dataset("imdb", split="test[:2]")
results = e.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    input_column="text",
    label_column="label",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>, split=<span class="hljs-string">&quot;test[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;text&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;label&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){f=o("p"),q=l("Examples:"),v=c(),_(u.$$.fragment)},l(r){f=s(r,"P",{});var h=n(f);q=i(h,"Examples:"),h.forEach(a),v=d(r),b(u.$$.fragment,r)},m(r,h){g(r,f,h),e(f,q),g(r,v,h),w(u,r,h),x=!0},p:ea,i(r){x||($(u.$$.fragment,r),x=!0)},o(r){y(u.$$.fragment,r),x=!1},d(r){r&&a(f),r&&a(v),E(u,r)}}}function Ns(L){let f,q,v,u,x,r,h,Ge,aa,jt,Ie,oa,Pt,S,G,Je,me,sa,Ke,na,Ct,Ae,ra,Nt,N,fe,la,D,ia,Fe,ca,da,Xe,pa,ua,Ye,ma,fa,ga,J,Dt,Me,ha,It,k,ge,va,Ze,_a,ba,K,he,wa,R,$a,et,ya,Ea,tt,xa,qa,ka,X,ve,Ta,at,ja,Pa,Y,_e,Ca,ot,Na,Da,Z,be,Ia,st,Aa,Fa,ee,we,Ma,nt,La,At,V,te,rt,$e,za,lt,Ua,Ft,B,ae,it,ye,Oa,ct,Qa,Mt,I,Ee,Sa,A,Ra,Le,Va,Ba,dt,Ha,Wa,pt,Ga,Ja,Ka,z,xe,Xa,ut,Ya,Za,oe,Lt,H,se,mt,qe,eo,ft,to,zt,P,ke,ao,F,oo,ze,so,no,gt,ro,lo,ht,io,co,po,U,Te,uo,vt,mo,fo,ne,go,re,je,ho,_t,vo,Ut,W,le,bt,Pe,_o,wt,bo,Ot,M,Ce,wo,C,$o,Ue,yo,Eo,$t,xo,qo,yt,ko,To,Et,jo,Po,Co,O,Ne,No,xt,Do,Io,ie,Qt;return r=new We({}),me=new We({}),fe=new j({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:<ul>
<li><code>&quot;image-classification&quot;</code>: will return a <a href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator">ImageClassificationEvaluator</a>.</li>
<li><code>&quot;question-answering&quot;</code>: will return a <a href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.QuestionAnsweringEvaluator">QuestionAnsweringEvaluator</a>.</li>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/__init__.py#L79",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),J=new Zt({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[Ts]},$$scope:{ctx:L}}}),ge=new j({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L54"}}),he=new j({props:{name:"compute",anchor:"evaluate.Evaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"**compute_parameters",val:": typing.Dict"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L107"}}),ve=new j({props:{name:"core_compute",anchor:"evaluate.Evaluator.core_compute",parameters:[{name:"references",val:": typing.List"},{name:"predictions",val:": typing.List"},{name:"metric",val:": EvaluationModule"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L220"}}),_e=new j({props:{name:"prepare_data",anchor:"evaluate.Evaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"input_column",val:": str"},{name:"label_column",val:": str"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_data.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.Evaluator.prepare_data.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.prepare_data.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L125",returnDescription:`
<p>Loaded <code>datasets.Dataset</code>.</p>
`}}),be=new j({props:{name:"prepare_metric",anchor:"evaluate.Evaluator.prepare_metric",parameters:[{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule]"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_metric.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L196",returnDescription:`
<p>The loaded metric.</p>
`}}),we=new j({props:{name:"prepare_pipeline",anchor:"evaluate.Evaluator.prepare_pipeline",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"preprocessor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_pipeline.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.prepare_pipeline.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.Evaluator.prepare_pipeline.preprocessor",description:`<strong>preprocessor</strong> (<code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"preprocessor"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L156",returnDescription:`
<p>The initialized pipeline.</p>
`}}),$e=new We({}),ye=new We({}),Ee=new j({props:{name:"class evaluate.ImageClassificationEvaluator",anchor:"evaluate.ImageClassificationEvaluator",parameters:[{name:"task",val:" = 'image-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/image_classification.py#L39"}}),xe=new j({props:{name:"compute",anchor:"evaluate.ImageClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"feature_extractor",val:": typing.Union[str, ForwardRef('FeatureExtractionMixin'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'image'"},{name:"label_column",val:": str = 'labels'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"}],parametersDescription:[{anchor:"evaluate.ImageClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.ImageClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>image-classification</code>. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.ImageClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.ImageClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>&#x201D; &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
feature_extractor &#x2014; (<code>str</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default feature extractor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.ImageClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.ImageClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.ImageClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.ImageClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;image&quot;</code>) &#x2014;
the name of the column containing the images as PIL ImageFile in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;labels&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/image_classification.py#L50",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),oe=new Zt({props:{anchor:"evaluate.ImageClassificationEvaluator.compute.example",$$slots:{default:[js]},$$scope:{ctx:L}}}),qe=new We({}),ke=new j({props:{name:"class evaluate.QuestionAnsweringEvaluator",anchor:"evaluate.QuestionAnsweringEvaluator",parameters:[{name:"task",val:" = 'question-answering'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L38"}}),Te=new j({props:{name:"compute",anchor:"evaluate.QuestionAnsweringEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"question_column",val:": str = 'question'"},{name:"context_column",val:": str = 'context'"},{name:"id_column",val:": str = 'id'"},{name:"label_column",val:": str = 'answers'"}],parametersDescription:[{anchor:"evaluate.QuestionAnsweringEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>question-answering</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;&#x201D;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.question_column",description:`<strong>question_column</strong> (<code>str</code>, defaults to <code>&quot;question&quot;</code>) &#x2014;
the name of the column containing the question in the dataset specified by <code>data</code>.`,name:"question_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.context_column",description:`<strong>context_column</strong> (<code>str</code>, defaults to <code>&quot;context&quot;</code>) &#x2014;
the name of the column containing the context in the dataset specified by <code>data</code>.`,name:"context_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.id_column",description:`<strong>id_column</strong> (<code>str</code>, defaults to <code>&quot;id&quot;</code>) &#x2014;
the name of the column cointaing the identification field of the question and answer pair in the
dataset specified by <code>data</code>.`,name:"id_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;answers&quot;</code>) &#x2014;
the name of the column containing the answers in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L95",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ne=new Zt({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example",$$slots:{default:[Ps]},$$scope:{ctx:L}}}),je=new j({props:{name:"prepare_data",anchor:"evaluate.QuestionAnsweringEvaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"question_column",val:": str"},{name:"context_column",val:": str"},{name:"id_column",val:": str"},{name:"label_column",val:": str"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L49"}}),Pe=new We({}),Ce=new j({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/text_classification.py#L39"}}),Ne=new j({props:{name:"compute",anchor:"evaluate.TextClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'text'"},{name:"label_column",val:": str = 'label'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"}],parametersDescription:[{anchor:"evaluate.TextClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.TextClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.TextClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.TextClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;&#x201D;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.TextClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TextClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TextClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TextClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/text_classification.py#L51",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ie=new Zt({props:{anchor:"evaluate.TextClassificationEvaluator.compute.example",$$slots:{default:[Cs]},$$scope:{ctx:L}}}),{c(){f=o("meta"),q=c(),v=o("h1"),u=o("a"),x=o("span"),_(r.$$.fragment),h=c(),Ge=o("span"),aa=l("Evaluator"),jt=c(),Ie=o("p"),oa=l("The evaluator classes for automatic evaluation."),Pt=c(),S=o("h2"),G=o("a"),Je=o("span"),_(me.$$.fragment),sa=c(),Ke=o("span"),na=l("Evaluator classes"),Ct=c(),Ae=o("p"),ra=l("The main entry point for using the evaluator:"),Nt=c(),N=o("div"),_(fe.$$.fragment),la=c(),D=o("p"),ia=l("Utility factory method to build an "),Fe=o("a"),ca=l("Evaluator"),da=l(`.
Evaluators encapsulate a task and a default metric name. They leverage `),Xe=o("code"),pa=l("pipeline"),ua=l(" functionalify from "),Ye=o("code"),ma=l("transformers"),fa=l(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),ga=c(),_(J.$$.fragment),Dt=c(),Me=o("p"),ha=l("The base class for all evaluator classes:"),It=c(),k=o("div"),_(ge.$$.fragment),va=c(),Ze=o("p"),_a=l(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),ba=c(),K=o("div"),_(he.$$.fragment),wa=c(),R=o("p"),$a=l("A core method of the "),et=o("code"),ya=l("Evaluator"),Ea=l(` class, computes the metric value for a pipeline and dataset compatible
with the task specified by the `),tt=o("code"),xa=l("Evaluator"),qa=l("."),ka=c(),X=o("div"),_(ve.$$.fragment),Ta=c(),at=o("p"),ja=l("Compute and return metrics."),Pa=c(),Y=o("div"),_(_e.$$.fragment),Ca=c(),ot=o("p"),Na=l("Prepare data."),Da=c(),Z=o("div"),_(be.$$.fragment),Ia=c(),st=o("p"),Aa=l("Prepare metric."),Fa=c(),ee=o("div"),_(we.$$.fragment),Ma=c(),nt=o("p"),La=l("Prepare pipeline."),At=c(),V=o("h2"),te=o("a"),rt=o("span"),_($e.$$.fragment),za=c(),lt=o("span"),Ua=l("The task specific evaluators"),Ft=c(),B=o("h3"),ae=o("a"),it=o("span"),_(ye.$$.fragment),Oa=c(),ct=o("span"),Qa=l("ImageClassificationEvaluator"),Mt=c(),I=o("div"),_(Ee.$$.fragment),Sa=c(),A=o("p"),Ra=l(`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Le=o("a"),Va=l("evaluator()"),Ba=l(` using the default task name
`),dt=o("code"),Ha=l("image-classification"),Wa=l(`.
Methods in this class assume a data format compatible with the `),pt=o("code"),Ga=l("ImageClassificationPipeline"),Ja=l("."),Ka=c(),z=o("div"),_(xe.$$.fragment),Xa=c(),ut=o("p"),Ya=l("Compute the metric for a given pipeline and dataset combination."),Za=c(),_(oe.$$.fragment),Lt=c(),H=o("h3"),se=o("a"),mt=o("span"),_(qe.$$.fragment),eo=c(),ft=o("span"),to=l("QuestionAnsweringEvaluator"),zt=c(),P=o("div"),_(ke.$$.fragment),ao=c(),F=o("p"),oo=l(`Question answering evaluator.
This question answering evaluator can currently be loaded from `),ze=o("a"),so=l("evaluator()"),no=l(` using the default task name
`),gt=o("code"),ro=l("question-answering"),lo=l(`.
Methods in this class assume a data format compatible with the `),ht=o("code"),io=l("QuestionAnsweringPipeline"),co=l("."),po=c(),U=o("div"),_(Te.$$.fragment),uo=c(),vt=o("p"),mo=l("Compute the metric for a given pipeline and dataset combination."),fo=c(),_(ne.$$.fragment),go=c(),re=o("div"),_(je.$$.fragment),ho=c(),_t=o("p"),vo=l("Prepare data."),Ut=c(),W=o("h3"),le=o("a"),bt=o("span"),_(Pe.$$.fragment),_o=c(),wt=o("span"),bo=l("TextClassificationEvaluator"),Ot=c(),M=o("div"),_(Ce.$$.fragment),wo=c(),C=o("p"),$o=l(`Text classification evaluator.
This text classification evaluator can currently be loaded from `),Ue=o("a"),yo=l("evaluator()"),Eo=l(` using the default task name
`),$t=o("code"),xo=l("text-classification"),qo=l(" or with a "),yt=o("code"),ko=l('"sentiment-analysis"'),To=l(` alias.
Methods in this class assume a data format compatible with the `),Et=o("code"),jo=l("TextClassificationPipeline"),Po=l(` - a single textual
feature as input and a categorical label as output.`),Co=c(),O=o("div"),_(Ne.$$.fragment),No=c(),xt=o("p"),Do=l("Compute the metric for a given pipeline and dataset combination."),Io=c(),_(ie.$$.fragment),this.h()},l(t){const m=qs('[data-svelte="svelte-1phssyn"]',document.head);f=s(m,"META",{name:!0,content:!0}),m.forEach(a),q=d(t),v=s(t,"H1",{class:!0});var De=n(v);u=s(De,"A",{id:!0,class:!0,href:!0});var qt=n(u);x=s(qt,"SPAN",{});var kt=n(x);b(r.$$.fragment,kt),kt.forEach(a),qt.forEach(a),h=d(De),Ge=s(De,"SPAN",{});var Tt=n(Ge);aa=i(Tt,"Evaluator"),Tt.forEach(a),De.forEach(a),jt=d(t),Ie=s(t,"P",{});var Ao=n(Ie);oa=i(Ao,"The evaluator classes for automatic evaluation."),Ao.forEach(a),Pt=d(t),S=s(t,"H2",{class:!0});var St=n(S);G=s(St,"A",{id:!0,class:!0,href:!0});var Fo=n(G);Je=s(Fo,"SPAN",{});var Mo=n(Je);b(me.$$.fragment,Mo),Mo.forEach(a),Fo.forEach(a),sa=d(St),Ke=s(St,"SPAN",{});var Lo=n(Ke);na=i(Lo,"Evaluator classes"),Lo.forEach(a),St.forEach(a),Ct=d(t),Ae=s(t,"P",{});var zo=n(Ae);ra=i(zo,"The main entry point for using the evaluator:"),zo.forEach(a),Nt=d(t),N=s(t,"DIV",{class:!0});var Oe=n(N);b(fe.$$.fragment,Oe),la=d(Oe),D=s(Oe,"P",{});var ce=n(D);ia=i(ce,"Utility factory method to build an "),Fe=s(ce,"A",{href:!0});var Uo=n(Fe);ca=i(Uo,"Evaluator"),Uo.forEach(a),da=i(ce,`.
Evaluators encapsulate a task and a default metric name. They leverage `),Xe=s(ce,"CODE",{});var Oo=n(Xe);pa=i(Oo,"pipeline"),Oo.forEach(a),ua=i(ce," functionalify from "),Ye=s(ce,"CODE",{});var Qo=n(Ye);ma=i(Qo,"transformers"),Qo.forEach(a),fa=i(ce,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),ce.forEach(a),ga=d(Oe),b(J.$$.fragment,Oe),Oe.forEach(a),Dt=d(t),Me=s(t,"P",{});var So=n(Me);ha=i(So,"The base class for all evaluator classes:"),So.forEach(a),It=d(t),k=s(t,"DIV",{class:!0});var T=n(k);b(ge.$$.fragment,T),va=d(T),Ze=s(T,"P",{});var Ro=n(Ze);_a=i(Ro,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),Ro.forEach(a),ba=d(T),K=s(T,"DIV",{class:!0});var Rt=n(K);b(he.$$.fragment,Rt),wa=d(Rt),R=s(Rt,"P",{});var Qe=n(R);$a=i(Qe,"A core method of the "),et=s(Qe,"CODE",{});var Vo=n(et);ya=i(Vo,"Evaluator"),Vo.forEach(a),Ea=i(Qe,` class, computes the metric value for a pipeline and dataset compatible
with the task specified by the `),tt=s(Qe,"CODE",{});var Bo=n(tt);xa=i(Bo,"Evaluator"),Bo.forEach(a),qa=i(Qe,"."),Qe.forEach(a),Rt.forEach(a),ka=d(T),X=s(T,"DIV",{class:!0});var Vt=n(X);b(ve.$$.fragment,Vt),Ta=d(Vt),at=s(Vt,"P",{});var Ho=n(at);ja=i(Ho,"Compute and return metrics."),Ho.forEach(a),Vt.forEach(a),Pa=d(T),Y=s(T,"DIV",{class:!0});var Bt=n(Y);b(_e.$$.fragment,Bt),Ca=d(Bt),ot=s(Bt,"P",{});var Wo=n(ot);Na=i(Wo,"Prepare data."),Wo.forEach(a),Bt.forEach(a),Da=d(T),Z=s(T,"DIV",{class:!0});var Ht=n(Z);b(be.$$.fragment,Ht),Ia=d(Ht),st=s(Ht,"P",{});var Go=n(st);Aa=i(Go,"Prepare metric."),Go.forEach(a),Ht.forEach(a),Fa=d(T),ee=s(T,"DIV",{class:!0});var Wt=n(ee);b(we.$$.fragment,Wt),Ma=d(Wt),nt=s(Wt,"P",{});var Jo=n(nt);La=i(Jo,"Prepare pipeline."),Jo.forEach(a),Wt.forEach(a),T.forEach(a),At=d(t),V=s(t,"H2",{class:!0});var Gt=n(V);te=s(Gt,"A",{id:!0,class:!0,href:!0});var Ko=n(te);rt=s(Ko,"SPAN",{});var Xo=n(rt);b($e.$$.fragment,Xo),Xo.forEach(a),Ko.forEach(a),za=d(Gt),lt=s(Gt,"SPAN",{});var Yo=n(lt);Ua=i(Yo,"The task specific evaluators"),Yo.forEach(a),Gt.forEach(a),Ft=d(t),B=s(t,"H3",{class:!0});var Jt=n(B);ae=s(Jt,"A",{id:!0,class:!0,href:!0});var Zo=n(ae);it=s(Zo,"SPAN",{});var es=n(it);b(ye.$$.fragment,es),es.forEach(a),Zo.forEach(a),Oa=d(Jt),ct=s(Jt,"SPAN",{});var ts=n(ct);Qa=i(ts,"ImageClassificationEvaluator"),ts.forEach(a),Jt.forEach(a),Mt=d(t),I=s(t,"DIV",{class:!0});var Se=n(I);b(Ee.$$.fragment,Se),Sa=d(Se),A=s(Se,"P",{});var de=n(A);Ra=i(de,`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Le=s(de,"A",{href:!0});var as=n(Le);Va=i(as,"evaluator()"),as.forEach(a),Ba=i(de,` using the default task name
`),dt=s(de,"CODE",{});var os=n(dt);Ha=i(os,"image-classification"),os.forEach(a),Wa=i(de,`.
Methods in this class assume a data format compatible with the `),pt=s(de,"CODE",{});var ss=n(pt);Ga=i(ss,"ImageClassificationPipeline"),ss.forEach(a),Ja=i(de,"."),de.forEach(a),Ka=d(Se),z=s(Se,"DIV",{class:!0});var Re=n(z);b(xe.$$.fragment,Re),Xa=d(Re),ut=s(Re,"P",{});var ns=n(ut);Ya=i(ns,"Compute the metric for a given pipeline and dataset combination."),ns.forEach(a),Za=d(Re),b(oe.$$.fragment,Re),Re.forEach(a),Se.forEach(a),Lt=d(t),H=s(t,"H3",{class:!0});var Kt=n(H);se=s(Kt,"A",{id:!0,class:!0,href:!0});var rs=n(se);mt=s(rs,"SPAN",{});var ls=n(mt);b(qe.$$.fragment,ls),ls.forEach(a),rs.forEach(a),eo=d(Kt),ft=s(Kt,"SPAN",{});var is=n(ft);to=i(is,"QuestionAnsweringEvaluator"),is.forEach(a),Kt.forEach(a),zt=d(t),P=s(t,"DIV",{class:!0});var pe=n(P);b(ke.$$.fragment,pe),ao=d(pe),F=s(pe,"P",{});var ue=n(F);oo=i(ue,`Question answering evaluator.
This question answering evaluator can currently be loaded from `),ze=s(ue,"A",{href:!0});var cs=n(ze);so=i(cs,"evaluator()"),cs.forEach(a),no=i(ue,` using the default task name
`),gt=s(ue,"CODE",{});var ds=n(gt);ro=i(ds,"question-answering"),ds.forEach(a),lo=i(ue,`.
Methods in this class assume a data format compatible with the `),ht=s(ue,"CODE",{});var ps=n(ht);io=i(ps,"QuestionAnsweringPipeline"),ps.forEach(a),co=i(ue,"."),ue.forEach(a),po=d(pe),U=s(pe,"DIV",{class:!0});var Ve=n(U);b(Te.$$.fragment,Ve),uo=d(Ve),vt=s(Ve,"P",{});var us=n(vt);mo=i(us,"Compute the metric for a given pipeline and dataset combination."),us.forEach(a),fo=d(Ve),b(ne.$$.fragment,Ve),Ve.forEach(a),go=d(pe),re=s(pe,"DIV",{class:!0});var Xt=n(re);b(je.$$.fragment,Xt),ho=d(Xt),_t=s(Xt,"P",{});var ms=n(_t);vo=i(ms,"Prepare data."),ms.forEach(a),Xt.forEach(a),pe.forEach(a),Ut=d(t),W=s(t,"H3",{class:!0});var Yt=n(W);le=s(Yt,"A",{id:!0,class:!0,href:!0});var fs=n(le);bt=s(fs,"SPAN",{});var gs=n(bt);b(Pe.$$.fragment,gs),gs.forEach(a),fs.forEach(a),_o=d(Yt),wt=s(Yt,"SPAN",{});var hs=n(wt);bo=i(hs,"TextClassificationEvaluator"),hs.forEach(a),Yt.forEach(a),Ot=d(t),M=s(t,"DIV",{class:!0});var Be=n(M);b(Ce.$$.fragment,Be),wo=d(Be),C=s(Be,"P",{});var Q=n(C);$o=i(Q,`Text classification evaluator.
This text classification evaluator can currently be loaded from `),Ue=s(Q,"A",{href:!0});var vs=n(Ue);yo=i(vs,"evaluator()"),vs.forEach(a),Eo=i(Q,` using the default task name
`),$t=s(Q,"CODE",{});var _s=n($t);xo=i(_s,"text-classification"),_s.forEach(a),qo=i(Q," or with a "),yt=s(Q,"CODE",{});var bs=n(yt);ko=i(bs,'"sentiment-analysis"'),bs.forEach(a),To=i(Q,` alias.
Methods in this class assume a data format compatible with the `),Et=s(Q,"CODE",{});var ws=n(Et);jo=i(ws,"TextClassificationPipeline"),ws.forEach(a),Po=i(Q,` - a single textual
feature as input and a categorical label as output.`),Q.forEach(a),Co=d(Be),O=s(Be,"DIV",{class:!0});var He=n(O);b(Ne.$$.fragment,He),No=d(He),xt=s(He,"P",{});var $s=n(xt);Do=i($s,"Compute the metric for a given pipeline and dataset combination."),$s.forEach(a),Io=d(He),b(ie.$$.fragment,He),He.forEach(a),Be.forEach(a),this.h()},h(){p(f,"name","hf:doc:metadata"),p(f,"content",JSON.stringify(Ds)),p(u,"id","evaluator"),p(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(u,"href","#evaluator"),p(v,"class","relative group"),p(G,"id","evaluate.evaluator"),p(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(G,"href","#evaluate.evaluator"),p(S,"class","relative group"),p(Fe,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.Evaluator"),p(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(te,"id","the-task-specific-evaluators"),p(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(te,"href","#the-task-specific-evaluators"),p(V,"class","relative group"),p(ae,"id","evaluate.ImageClassificationEvaluator"),p(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ae,"href","#evaluate.ImageClassificationEvaluator"),p(B,"class","relative group"),p(Le,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.evaluator"),p(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(se,"id","evaluate.QuestionAnsweringEvaluator"),p(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(se,"href","#evaluate.QuestionAnsweringEvaluator"),p(H,"class","relative group"),p(ze,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.evaluator"),p(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(le,"id","evaluate.TextClassificationEvaluator"),p(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(le,"href","#evaluate.TextClassificationEvaluator"),p(W,"class","relative group"),p(Ue,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.evaluator"),p(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,m){e(document.head,f),g(t,q,m),g(t,v,m),e(v,u),e(u,x),w(r,x,null),e(v,h),e(v,Ge),e(Ge,aa),g(t,jt,m),g(t,Ie,m),e(Ie,oa),g(t,Pt,m),g(t,S,m),e(S,G),e(G,Je),w(me,Je,null),e(S,sa),e(S,Ke),e(Ke,na),g(t,Ct,m),g(t,Ae,m),e(Ae,ra),g(t,Nt,m),g(t,N,m),w(fe,N,null),e(N,la),e(N,D),e(D,ia),e(D,Fe),e(Fe,ca),e(D,da),e(D,Xe),e(Xe,pa),e(D,ua),e(D,Ye),e(Ye,ma),e(D,fa),e(N,ga),w(J,N,null),g(t,Dt,m),g(t,Me,m),e(Me,ha),g(t,It,m),g(t,k,m),w(ge,k,null),e(k,va),e(k,Ze),e(Ze,_a),e(k,ba),e(k,K),w(he,K,null),e(K,wa),e(K,R),e(R,$a),e(R,et),e(et,ya),e(R,Ea),e(R,tt),e(tt,xa),e(R,qa),e(k,ka),e(k,X),w(ve,X,null),e(X,Ta),e(X,at),e(at,ja),e(k,Pa),e(k,Y),w(_e,Y,null),e(Y,Ca),e(Y,ot),e(ot,Na),e(k,Da),e(k,Z),w(be,Z,null),e(Z,Ia),e(Z,st),e(st,Aa),e(k,Fa),e(k,ee),w(we,ee,null),e(ee,Ma),e(ee,nt),e(nt,La),g(t,At,m),g(t,V,m),e(V,te),e(te,rt),w($e,rt,null),e(V,za),e(V,lt),e(lt,Ua),g(t,Ft,m),g(t,B,m),e(B,ae),e(ae,it),w(ye,it,null),e(B,Oa),e(B,ct),e(ct,Qa),g(t,Mt,m),g(t,I,m),w(Ee,I,null),e(I,Sa),e(I,A),e(A,Ra),e(A,Le),e(Le,Va),e(A,Ba),e(A,dt),e(dt,Ha),e(A,Wa),e(A,pt),e(pt,Ga),e(A,Ja),e(I,Ka),e(I,z),w(xe,z,null),e(z,Xa),e(z,ut),e(ut,Ya),e(z,Za),w(oe,z,null),g(t,Lt,m),g(t,H,m),e(H,se),e(se,mt),w(qe,mt,null),e(H,eo),e(H,ft),e(ft,to),g(t,zt,m),g(t,P,m),w(ke,P,null),e(P,ao),e(P,F),e(F,oo),e(F,ze),e(ze,so),e(F,no),e(F,gt),e(gt,ro),e(F,lo),e(F,ht),e(ht,io),e(F,co),e(P,po),e(P,U),w(Te,U,null),e(U,uo),e(U,vt),e(vt,mo),e(U,fo),w(ne,U,null),e(P,go),e(P,re),w(je,re,null),e(re,ho),e(re,_t),e(_t,vo),g(t,Ut,m),g(t,W,m),e(W,le),e(le,bt),w(Pe,bt,null),e(W,_o),e(W,wt),e(wt,bo),g(t,Ot,m),g(t,M,m),w(Ce,M,null),e(M,wo),e(M,C),e(C,$o),e(C,Ue),e(Ue,yo),e(C,Eo),e(C,$t),e($t,xo),e(C,qo),e(C,yt),e(yt,ko),e(C,To),e(C,Et),e(Et,jo),e(C,Po),e(M,Co),e(M,O),w(Ne,O,null),e(O,No),e(O,xt),e(xt,Do),e(O,Io),w(ie,O,null),Qt=!0},p(t,[m]){const De={};m&2&&(De.$$scope={dirty:m,ctx:t}),J.$set(De);const qt={};m&2&&(qt.$$scope={dirty:m,ctx:t}),oe.$set(qt);const kt={};m&2&&(kt.$$scope={dirty:m,ctx:t}),ne.$set(kt);const Tt={};m&2&&(Tt.$$scope={dirty:m,ctx:t}),ie.$set(Tt)},i(t){Qt||($(r.$$.fragment,t),$(me.$$.fragment,t),$(fe.$$.fragment,t),$(J.$$.fragment,t),$(ge.$$.fragment,t),$(he.$$.fragment,t),$(ve.$$.fragment,t),$(_e.$$.fragment,t),$(be.$$.fragment,t),$(we.$$.fragment,t),$($e.$$.fragment,t),$(ye.$$.fragment,t),$(Ee.$$.fragment,t),$(xe.$$.fragment,t),$(oe.$$.fragment,t),$(qe.$$.fragment,t),$(ke.$$.fragment,t),$(Te.$$.fragment,t),$(ne.$$.fragment,t),$(je.$$.fragment,t),$(Pe.$$.fragment,t),$(Ce.$$.fragment,t),$(Ne.$$.fragment,t),$(ie.$$.fragment,t),Qt=!0)},o(t){y(r.$$.fragment,t),y(me.$$.fragment,t),y(fe.$$.fragment,t),y(J.$$.fragment,t),y(ge.$$.fragment,t),y(he.$$.fragment,t),y(ve.$$.fragment,t),y(_e.$$.fragment,t),y(be.$$.fragment,t),y(we.$$.fragment,t),y($e.$$.fragment,t),y(ye.$$.fragment,t),y(Ee.$$.fragment,t),y(xe.$$.fragment,t),y(oe.$$.fragment,t),y(qe.$$.fragment,t),y(ke.$$.fragment,t),y(Te.$$.fragment,t),y(ne.$$.fragment,t),y(je.$$.fragment,t),y(Pe.$$.fragment,t),y(Ce.$$.fragment,t),y(Ne.$$.fragment,t),y(ie.$$.fragment,t),Qt=!1},d(t){a(f),t&&a(q),t&&a(v),E(r),t&&a(jt),t&&a(Ie),t&&a(Pt),t&&a(S),E(me),t&&a(Ct),t&&a(Ae),t&&a(Nt),t&&a(N),E(fe),E(J),t&&a(Dt),t&&a(Me),t&&a(It),t&&a(k),E(ge),E(he),E(ve),E(_e),E(be),E(we),t&&a(At),t&&a(V),E($e),t&&a(Ft),t&&a(B),E(ye),t&&a(Mt),t&&a(I),E(Ee),E(xe),E(oe),t&&a(Lt),t&&a(H),E(qe),t&&a(zt),t&&a(P),E(ke),E(Te),E(ne),E(je),t&&a(Ut),t&&a(W),E(Pe),t&&a(Ot),t&&a(M),E(Ce),E(Ne),E(ie)}}}const Ds={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"},{local:"the-task-specific-evaluators",sections:[{local:"evaluate.ImageClassificationEvaluator",title:"ImageClassificationEvaluator"},{local:"evaluate.QuestionAnsweringEvaluator",title:"QuestionAnsweringEvaluator"},{local:"evaluate.TextClassificationEvaluator",title:"TextClassificationEvaluator"}],title:"The task specific evaluators"}],title:"Evaluator"};function Is(L){return ks(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Us extends ys{constructor(f){super();Es(this,f,Is,Ns,xs,{})}}export{Us as default,Ds as metadata};
