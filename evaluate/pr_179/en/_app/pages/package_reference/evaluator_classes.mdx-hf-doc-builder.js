import{S as Qs,i as zs,s as Os,e as o,k as c,w as _,t as l,M as Us,c as s,d as a,m as d,a as n,x as b,h as i,b as f,G as e,g as v,y as $,q as w,o as y,B as E,v as Ss,L as Lt}from"../../chunks/vendor-hf-doc-builder.js";import{T as Rs}from"../../chunks/Tip-hf-doc-builder.js";import{D as T}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Qt}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as et}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Mt}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Vs(P){let p,x,m,u,q;return u=new Qt({props:{code:`from evaluate import evaluator
# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){p=o("p"),x=l("Examples:"),m=c(),_(u.$$.fragment)},l(r){p=s(r,"P",{});var h=n(p);x=i(h,"Examples:"),h.forEach(a),m=d(r),b(u.$$.fragment,r)},m(r,h){v(r,p,h),e(p,x),v(r,m,h),$(u,r,h),q=!0},p:Lt,i(r){q||(w(u.$$.fragment,r),q=!0)},o(r){y(u.$$.fragment,r),q=!1},d(r){r&&a(p),r&&a(m),E(u,r)}}}function Bs(P){let p,x,m,u,q;return u=new Qt({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("image-classification")
data = load_dataset("beans", split="test[:2]")
results = e.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    metric="accuracy",
    input_column="image",
    label_column="labels",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;image-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;beans&quot;</span>, split=<span class="hljs-string">&quot;test[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;nateraw/vit-base-beans&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;image&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;labels&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&#x27;angular_leaf_spot&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;bean_rust&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>: <span class="hljs-number">2</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){p=o("p"),x=l("Examples:"),m=c(),_(u.$$.fragment)},l(r){p=s(r,"P",{});var h=n(p);x=i(h,"Examples:"),h.forEach(a),m=d(r),b(u.$$.fragment,r)},m(r,h){v(r,p,h),e(p,x),v(r,m,h),$(u,r,h),q=!0},p:Lt,i(r){q||(w(u.$$.fragment,r),q=!0)},o(r){y(u.$$.fragment,r),q=!1},d(r){r&&a(p),r&&a(m),E(u,r)}}}function Hs(P){let p,x,m,u,q;return u=new Qt({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("question-answering")
data = load_dataset("squad", split="validation[:2]")
results = e.compute(
    model_or_pipeline="sshleifer/tiny-distilbert-base-cased-distilled-squad",
    data=data,
    metric="squad",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;sshleifer/tiny-distilbert-base-cased-distilled-squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){p=o("p"),x=l("Examples:"),m=c(),_(u.$$.fragment)},l(r){p=s(r,"P",{});var h=n(p);x=i(h,"Examples:"),h.forEach(a),m=d(r),b(u.$$.fragment,r)},m(r,h){v(r,p,h),e(p,x),v(r,m,h),$(u,r,h),q=!0},p:Lt,i(r){q||(w(u.$$.fragment,r),q=!0)},o(r){y(u.$$.fragment,r),q=!1},d(r){r&&a(p),r&&a(m),E(u,r)}}}function Ws(P){let p,x,m,u,q;return{c(){p=o("p"),x=l(`Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. If using transformers pipeline,
make sure to pass `),m=o("code"),u=l("handle_impossible_answer=True"),q=l(" as an argument to the pipeline.")},l(r){p=s(r,"P",{});var h=n(p);x=i(h,`Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. If using transformers pipeline,
make sure to pass `),m=s(h,"CODE",{});var K=n(m);u=i(K,"handle_impossible_answer=True"),K.forEach(a),q=i(h," as an argument to the pipeline."),h.forEach(a)},m(r,h){v(r,p,h),e(p,x),e(p,m),e(m,u),e(p,q)},d(r){r&&a(p)}}}function Gs(P){let p,x;return p=new Qt({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
from transformers import pipeline
e = evaluator("question-answering")
data = load_dataset("squad_v2", split="validation[:2]")
pipe = pipeline(task="question-answering", model="sshleifer/mrm8488/bert-tiny-finetuned-squadv2", handle_impossible_answer=True)
results = e.compute(
    model_or_pipeline=pipe,
    data=data,
    metric="squad_v2",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad_v2&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(task=<span class="hljs-string">&quot;question-answering&quot;</span>, model=<span class="hljs-string">&quot;sshleifer/mrm8488/bert-tiny-finetuned-squadv2&quot;</span>, handle_impossible_answer=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=pipe,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad_v2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){_(p.$$.fragment)},l(m){b(p.$$.fragment,m)},m(m,u){$(p,m,u),x=!0},p:Lt,i(m){x||(w(p.$$.fragment,m),x=!0)},o(m){y(p.$$.fragment,m),x=!1},d(m){E(p,m)}}}function Js(P){let p,x,m,u,q;return u=new Qt({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("text-classification")
data = load_dataset("imdb", split="test[:2]")
results = e.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    input_column="text",
    label_column="label",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>, split=<span class="hljs-string">&quot;test[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;text&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;label&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){p=o("p"),x=l("Examples:"),m=c(),_(u.$$.fragment)},l(r){p=s(r,"P",{});var h=n(p);x=i(h,"Examples:"),h.forEach(a),m=d(r),b(u.$$.fragment,r)},m(r,h){v(r,p,h),e(p,x),v(r,m,h),$(u,r,h),q=!0},p:Lt,i(r){q||(w(u.$$.fragment,r),q=!0)},o(r){y(u.$$.fragment,r),q=!1},d(r){r&&a(p),r&&a(m),E(u,r)}}}function Ks(P){let p,x,m,u,q,r,h,K,pa,zt,Ue,ua,Ot,V,X,tt,_e,ma,at,fa,Ut,Se,ga,St,I,be,ha,A,va,Re,_a,ba,ot,$a,wa,st,ya,Ea,xa,Y,Rt,Ve,qa,Vt,k,$e,ka,nt,ja,Ta,Z,we,Pa,B,Ca,rt,Da,Na,lt,Ia,Aa,Fa,ee,ye,Ma,it,La,Qa,te,Ee,za,ct,Oa,Ua,ae,xe,Sa,dt,Ra,Va,oe,qe,Ba,pt,Ha,Bt,H,se,ut,ke,Wa,mt,Ga,Ht,W,ne,ft,je,Ja,gt,Ka,Wt,F,Te,Xa,M,Ya,Be,Za,eo,ht,to,ao,vt,oo,so,no,z,Pe,ro,_t,lo,io,re,Gt,G,le,bt,Ce,co,$t,po,Jt,j,De,uo,L,mo,He,fo,go,wt,ho,vo,yt,_o,bo,$o,C,Ne,wo,Et,yo,Eo,ie,xo,ce,qo,de,ko,pe,Ie,jo,Ae,To,xt,Po,Co,Do,ue,Fe,No,qt,Io,Kt,J,me,kt,Me,Ao,jt,Fo,Xt,Q,Le,Mo,N,Lo,We,Qo,zo,Tt,Oo,Uo,Pt,So,Ro,Ct,Vo,Bo,Ho,O,Qe,Wo,Dt,Go,Jo,fe,Yt;return r=new et({}),_e=new et({}),be=new T({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:<ul>
<li><code>&quot;image-classification&quot;</code>: will return a <a href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator">ImageClassificationEvaluator</a>.</li>
<li><code>&quot;question-answering&quot;</code>: will return a <a href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.QuestionAnsweringEvaluator">QuestionAnsweringEvaluator</a>.</li>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/__init__.py#L79",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),Y=new Mt({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[Vs]},$$scope:{ctx:P}}}),$e=new T({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L54"}}),we=new T({props:{name:"compute",anchor:"evaluate.Evaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"**compute_parameters",val:": typing.Dict"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L107"}}),ye=new T({props:{name:"core_compute",anchor:"evaluate.Evaluator.core_compute",parameters:[{name:"references",val:": typing.List"},{name:"predictions",val:": typing.List"},{name:"metric",val:": EvaluationModule"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L220"}}),Ee=new T({props:{name:"prepare_data",anchor:"evaluate.Evaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"input_column",val:": str"},{name:"label_column",val:": str"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_data.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.Evaluator.prepare_data.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.prepare_data.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L125",returnDescription:`
<p>Loaded <code>datasets.Dataset</code>.</p>
`}}),xe=new T({props:{name:"prepare_metric",anchor:"evaluate.Evaluator.prepare_metric",parameters:[{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule]"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_metric.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L196",returnDescription:`
<p>The loaded metric.</p>
`}}),qe=new T({props:{name:"prepare_pipeline",anchor:"evaluate.Evaluator.prepare_pipeline",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"preprocessor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_pipeline.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.prepare_pipeline.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.Evaluator.prepare_pipeline.preprocessor",description:`<strong>preprocessor</strong> (<code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"preprocessor"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L156",returnDescription:`
<p>The initialized pipeline.</p>
`}}),ke=new et({}),je=new et({}),Te=new T({props:{name:"class evaluate.ImageClassificationEvaluator",anchor:"evaluate.ImageClassificationEvaluator",parameters:[{name:"task",val:" = 'image-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/image_classification.py#L39"}}),Pe=new T({props:{name:"compute",anchor:"evaluate.ImageClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"feature_extractor",val:": typing.Union[str, ForwardRef('FeatureExtractionMixin'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'image'"},{name:"label_column",val:": str = 'labels'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"}],parametersDescription:[{anchor:"evaluate.ImageClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.ImageClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>image-classification</code>. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.ImageClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.ImageClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>&#x201D; &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
feature_extractor &#x2014; (<code>str</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default feature extractor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.ImageClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.ImageClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.ImageClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.ImageClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;image&quot;</code>) &#x2014;
the name of the column containing the images as PIL ImageFile in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;labels&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/image_classification.py#L50",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),re=new Mt({props:{anchor:"evaluate.ImageClassificationEvaluator.compute.example",$$slots:{default:[Bs]},$$scope:{ctx:P}}}),Ce=new et({}),De=new T({props:{name:"class evaluate.QuestionAnsweringEvaluator",anchor:"evaluate.QuestionAnsweringEvaluator",parameters:[{name:"task",val:" = 'question-answering'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L38"}}),Ne=new T({props:{name:"compute",anchor:"evaluate.QuestionAnsweringEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"question_column",val:": str = 'question'"},{name:"context_column",val:": str = 'context'"},{name:"id_column",val:": str = 'id'"},{name:"label_column",val:": str = 'answers'"}],parametersDescription:[{anchor:"evaluate.QuestionAnsweringEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>question-answering</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;&#x201D;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.question_column",description:`<strong>question_column</strong> (<code>str</code>, defaults to <code>&quot;question&quot;</code>) &#x2014;
the name of the column containing the question in the dataset specified by <code>data</code>.`,name:"question_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.context_column",description:`<strong>context_column</strong> (<code>str</code>, defaults to <code>&quot;context&quot;</code>) &#x2014;
the name of the column containing the context in the dataset specified by <code>data</code>.`,name:"context_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.id_column",description:`<strong>id_column</strong> (<code>str</code>, defaults to <code>&quot;id&quot;</code>) &#x2014;
the name of the column cointaing the identification field of the question and answer pair in the
dataset specified by <code>data</code>.`,name:"id_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;answers&quot;</code>) &#x2014;
the name of the column containing the answers in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L96",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ie=new Mt({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example",$$slots:{default:[Hs]},$$scope:{ctx:P}}}),ce=new Rs({props:{$$slots:{default:[Ws]},$$scope:{ctx:P}}}),de=new Mt({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example-2",$$slots:{default:[Gs]},$$scope:{ctx:P}}}),Ie=new T({props:{name:"is_squad_v2_schema",anchor:"evaluate.QuestionAnsweringEvaluator.is_squad_v2_schema",parameters:[{name:"data",val:": Dataset"},{name:"label_column",val:": str = 'answers'"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L82"}}),Fe=new T({props:{name:"prepare_data",anchor:"evaluate.QuestionAnsweringEvaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"question_column",val:": str"},{name:"context_column",val:": str"},{name:"id_column",val:": str"},{name:"label_column",val:": str"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L49"}}),Me=new et({}),Le=new T({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/text_classification.py#L39"}}),Qe=new T({props:{name:"compute",anchor:"evaluate.TextClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'text'"},{name:"label_column",val:": str = 'label'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"}],parametersDescription:[{anchor:"evaluate.TextClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.TextClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.TextClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.TextClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;&#x201D;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.TextClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TextClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TextClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TextClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/text_classification.py#L51",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),fe=new Mt({props:{anchor:"evaluate.TextClassificationEvaluator.compute.example",$$slots:{default:[Js]},$$scope:{ctx:P}}}),{c(){p=o("meta"),x=c(),m=o("h1"),u=o("a"),q=o("span"),_(r.$$.fragment),h=c(),K=o("span"),pa=l("Evaluator"),zt=c(),Ue=o("p"),ua=l("The evaluator classes for automatic evaluation."),Ot=c(),V=o("h2"),X=o("a"),tt=o("span"),_(_e.$$.fragment),ma=c(),at=o("span"),fa=l("Evaluator classes"),Ut=c(),Se=o("p"),ga=l("The main entry point for using the evaluator:"),St=c(),I=o("div"),_(be.$$.fragment),ha=c(),A=o("p"),va=l("Utility factory method to build an "),Re=o("a"),_a=l("Evaluator"),ba=l(`.
Evaluators encapsulate a task and a default metric name. They leverage `),ot=o("code"),$a=l("pipeline"),wa=l(" functionalify from "),st=o("code"),ya=l("transformers"),Ea=l(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),xa=c(),_(Y.$$.fragment),Rt=c(),Ve=o("p"),qa=l("The base class for all evaluator classes:"),Vt=c(),k=o("div"),_($e.$$.fragment),ka=c(),nt=o("p"),ja=l(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),Ta=c(),Z=o("div"),_(we.$$.fragment),Pa=c(),B=o("p"),Ca=l("A core method of the "),rt=o("code"),Da=l("Evaluator"),Na=l(` class, computes the metric value for a pipeline and dataset compatible
with the task specified by the `),lt=o("code"),Ia=l("Evaluator"),Aa=l("."),Fa=c(),ee=o("div"),_(ye.$$.fragment),Ma=c(),it=o("p"),La=l("Compute and return metrics."),Qa=c(),te=o("div"),_(Ee.$$.fragment),za=c(),ct=o("p"),Oa=l("Prepare data."),Ua=c(),ae=o("div"),_(xe.$$.fragment),Sa=c(),dt=o("p"),Ra=l("Prepare metric."),Va=c(),oe=o("div"),_(qe.$$.fragment),Ba=c(),pt=o("p"),Ha=l("Prepare pipeline."),Bt=c(),H=o("h2"),se=o("a"),ut=o("span"),_(ke.$$.fragment),Wa=c(),mt=o("span"),Ga=l("The task specific evaluators"),Ht=c(),W=o("h3"),ne=o("a"),ft=o("span"),_(je.$$.fragment),Ja=c(),gt=o("span"),Ka=l("ImageClassificationEvaluator"),Wt=c(),F=o("div"),_(Te.$$.fragment),Xa=c(),M=o("p"),Ya=l(`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Be=o("a"),Za=l("evaluator()"),eo=l(` using the default task name
`),ht=o("code"),to=l("image-classification"),ao=l(`.
Methods in this class assume a data format compatible with the `),vt=o("code"),oo=l("ImageClassificationPipeline"),so=l("."),no=c(),z=o("div"),_(Pe.$$.fragment),ro=c(),_t=o("p"),lo=l("Compute the metric for a given pipeline and dataset combination."),io=c(),_(re.$$.fragment),Gt=c(),G=o("h3"),le=o("a"),bt=o("span"),_(Ce.$$.fragment),co=c(),$t=o("span"),po=l("QuestionAnsweringEvaluator"),Jt=c(),j=o("div"),_(De.$$.fragment),uo=c(),L=o("p"),mo=l(`Question answering evaluator.
This question answering evaluator can currently be loaded from `),He=o("a"),fo=l("evaluator()"),go=l(` using the default task name
`),wt=o("code"),ho=l("question-answering"),vo=l(`.
Methods in this class assume a data format compatible with the `),yt=o("code"),_o=l("QuestionAnsweringPipeline"),bo=l("."),$o=c(),C=o("div"),_(Ne.$$.fragment),wo=c(),Et=o("p"),yo=l("Compute the metric for a given pipeline and dataset combination."),Eo=c(),_(ie.$$.fragment),xo=c(),_(ce.$$.fragment),qo=c(),_(de.$$.fragment),ko=c(),pe=o("div"),_(Ie.$$.fragment),jo=c(),Ae=o("p"),To=l(`Check if the provided dataset follows the squad v2 data schema, namely possible samples where the answer is not in the context.
In this case, the answer text list should be `),xt=o("code"),Po=l("[]"),Co=l("."),Do=c(),ue=o("div"),_(Fe.$$.fragment),No=c(),qt=o("p"),Io=l("Prepare data."),Kt=c(),J=o("h3"),me=o("a"),kt=o("span"),_(Me.$$.fragment),Ao=c(),jt=o("span"),Fo=l("TextClassificationEvaluator"),Xt=c(),Q=o("div"),_(Le.$$.fragment),Mo=c(),N=o("p"),Lo=l(`Text classification evaluator.
This text classification evaluator can currently be loaded from `),We=o("a"),Qo=l("evaluator()"),zo=l(` using the default task name
`),Tt=o("code"),Oo=l("text-classification"),Uo=l(" or with a "),Pt=o("code"),So=l('"sentiment-analysis"'),Ro=l(` alias.
Methods in this class assume a data format compatible with the `),Ct=o("code"),Vo=l("TextClassificationPipeline"),Bo=l(` - a single textual
feature as input and a categorical label as output.`),Ho=c(),O=o("div"),_(Qe.$$.fragment),Wo=c(),Dt=o("p"),Go=l("Compute the metric for a given pipeline and dataset combination."),Jo=c(),_(fe.$$.fragment),this.h()},l(t){const g=Us('[data-svelte="svelte-1phssyn"]',document.head);p=s(g,"META",{name:!0,content:!0}),g.forEach(a),x=d(t),m=s(t,"H1",{class:!0});var ze=n(m);u=s(ze,"A",{id:!0,class:!0,href:!0});var Nt=n(u);q=s(Nt,"SPAN",{});var It=n(q);b(r.$$.fragment,It),It.forEach(a),Nt.forEach(a),h=d(ze),K=s(ze,"SPAN",{});var At=n(K);pa=i(At,"Evaluator"),At.forEach(a),ze.forEach(a),zt=d(t),Ue=s(t,"P",{});var Ft=n(Ue);ua=i(Ft,"The evaluator classes for automatic evaluation."),Ft.forEach(a),Ot=d(t),V=s(t,"H2",{class:!0});var Oe=n(V);X=s(Oe,"A",{id:!0,class:!0,href:!0});var Ko=n(X);tt=s(Ko,"SPAN",{});var Xo=n(tt);b(_e.$$.fragment,Xo),Xo.forEach(a),Ko.forEach(a),ma=d(Oe),at=s(Oe,"SPAN",{});var Yo=n(at);fa=i(Yo,"Evaluator classes"),Yo.forEach(a),Oe.forEach(a),Ut=d(t),Se=s(t,"P",{});var Zo=n(Se);ga=i(Zo,"The main entry point for using the evaluator:"),Zo.forEach(a),St=d(t),I=s(t,"DIV",{class:!0});var Ge=n(I);b(be.$$.fragment,Ge),ha=d(Ge),A=s(Ge,"P",{});var ge=n(A);va=i(ge,"Utility factory method to build an "),Re=s(ge,"A",{href:!0});var es=n(Re);_a=i(es,"Evaluator"),es.forEach(a),ba=i(ge,`.
Evaluators encapsulate a task and a default metric name. They leverage `),ot=s(ge,"CODE",{});var ts=n(ot);$a=i(ts,"pipeline"),ts.forEach(a),wa=i(ge," functionalify from "),st=s(ge,"CODE",{});var as=n(st);ya=i(as,"transformers"),as.forEach(a),Ea=i(ge,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),ge.forEach(a),xa=d(Ge),b(Y.$$.fragment,Ge),Ge.forEach(a),Rt=d(t),Ve=s(t,"P",{});var os=n(Ve);qa=i(os,"The base class for all evaluator classes:"),os.forEach(a),Vt=d(t),k=s(t,"DIV",{class:!0});var D=n(k);b($e.$$.fragment,D),ka=d(D),nt=s(D,"P",{});var ss=n(nt);ja=i(ss,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),ss.forEach(a),Ta=d(D),Z=s(D,"DIV",{class:!0});var Zt=n(Z);b(we.$$.fragment,Zt),Pa=d(Zt),B=s(Zt,"P",{});var Je=n(B);Ca=i(Je,"A core method of the "),rt=s(Je,"CODE",{});var ns=n(rt);Da=i(ns,"Evaluator"),ns.forEach(a),Na=i(Je,` class, computes the metric value for a pipeline and dataset compatible
with the task specified by the `),lt=s(Je,"CODE",{});var rs=n(lt);Ia=i(rs,"Evaluator"),rs.forEach(a),Aa=i(Je,"."),Je.forEach(a),Zt.forEach(a),Fa=d(D),ee=s(D,"DIV",{class:!0});var ea=n(ee);b(ye.$$.fragment,ea),Ma=d(ea),it=s(ea,"P",{});var ls=n(it);La=i(ls,"Compute and return metrics."),ls.forEach(a),ea.forEach(a),Qa=d(D),te=s(D,"DIV",{class:!0});var ta=n(te);b(Ee.$$.fragment,ta),za=d(ta),ct=s(ta,"P",{});var is=n(ct);Oa=i(is,"Prepare data."),is.forEach(a),ta.forEach(a),Ua=d(D),ae=s(D,"DIV",{class:!0});var aa=n(ae);b(xe.$$.fragment,aa),Sa=d(aa),dt=s(aa,"P",{});var cs=n(dt);Ra=i(cs,"Prepare metric."),cs.forEach(a),aa.forEach(a),Va=d(D),oe=s(D,"DIV",{class:!0});var oa=n(oe);b(qe.$$.fragment,oa),Ba=d(oa),pt=s(oa,"P",{});var ds=n(pt);Ha=i(ds,"Prepare pipeline."),ds.forEach(a),oa.forEach(a),D.forEach(a),Bt=d(t),H=s(t,"H2",{class:!0});var sa=n(H);se=s(sa,"A",{id:!0,class:!0,href:!0});var ps=n(se);ut=s(ps,"SPAN",{});var us=n(ut);b(ke.$$.fragment,us),us.forEach(a),ps.forEach(a),Wa=d(sa),mt=s(sa,"SPAN",{});var ms=n(mt);Ga=i(ms,"The task specific evaluators"),ms.forEach(a),sa.forEach(a),Ht=d(t),W=s(t,"H3",{class:!0});var na=n(W);ne=s(na,"A",{id:!0,class:!0,href:!0});var fs=n(ne);ft=s(fs,"SPAN",{});var gs=n(ft);b(je.$$.fragment,gs),gs.forEach(a),fs.forEach(a),Ja=d(na),gt=s(na,"SPAN",{});var hs=n(gt);Ka=i(hs,"ImageClassificationEvaluator"),hs.forEach(a),na.forEach(a),Wt=d(t),F=s(t,"DIV",{class:!0});var Ke=n(F);b(Te.$$.fragment,Ke),Xa=d(Ke),M=s(Ke,"P",{});var he=n(M);Ya=i(he,`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Be=s(he,"A",{href:!0});var vs=n(Be);Za=i(vs,"evaluator()"),vs.forEach(a),eo=i(he,` using the default task name
`),ht=s(he,"CODE",{});var _s=n(ht);to=i(_s,"image-classification"),_s.forEach(a),ao=i(he,`.
Methods in this class assume a data format compatible with the `),vt=s(he,"CODE",{});var bs=n(vt);oo=i(bs,"ImageClassificationPipeline"),bs.forEach(a),so=i(he,"."),he.forEach(a),no=d(Ke),z=s(Ke,"DIV",{class:!0});var Xe=n(z);b(Pe.$$.fragment,Xe),ro=d(Xe),_t=s(Xe,"P",{});var $s=n(_t);lo=i($s,"Compute the metric for a given pipeline and dataset combination."),$s.forEach(a),io=d(Xe),b(re.$$.fragment,Xe),Xe.forEach(a),Ke.forEach(a),Gt=d(t),G=s(t,"H3",{class:!0});var ra=n(G);le=s(ra,"A",{id:!0,class:!0,href:!0});var ws=n(le);bt=s(ws,"SPAN",{});var ys=n(bt);b(Ce.$$.fragment,ys),ys.forEach(a),ws.forEach(a),co=d(ra),$t=s(ra,"SPAN",{});var Es=n($t);po=i(Es,"QuestionAnsweringEvaluator"),Es.forEach(a),ra.forEach(a),Jt=d(t),j=s(t,"DIV",{class:!0});var U=n(j);b(De.$$.fragment,U),uo=d(U),L=s(U,"P",{});var ve=n(L);mo=i(ve,`Question answering evaluator.
This question answering evaluator can currently be loaded from `),He=s(ve,"A",{href:!0});var xs=n(He);fo=i(xs,"evaluator()"),xs.forEach(a),go=i(ve,` using the default task name
`),wt=s(ve,"CODE",{});var qs=n(wt);ho=i(qs,"question-answering"),qs.forEach(a),vo=i(ve,`.
Methods in this class assume a data format compatible with the `),yt=s(ve,"CODE",{});var ks=n(yt);_o=i(ks,"QuestionAnsweringPipeline"),ks.forEach(a),bo=i(ve,"."),ve.forEach(a),$o=d(U),C=s(U,"DIV",{class:!0});var S=n(C);b(Ne.$$.fragment,S),wo=d(S),Et=s(S,"P",{});var js=n(Et);yo=i(js,"Compute the metric for a given pipeline and dataset combination."),js.forEach(a),Eo=d(S),b(ie.$$.fragment,S),xo=d(S),b(ce.$$.fragment,S),qo=d(S),b(de.$$.fragment,S),S.forEach(a),ko=d(U),pe=s(U,"DIV",{class:!0});var la=n(pe);b(Ie.$$.fragment,la),jo=d(la),Ae=s(la,"P",{});var ia=n(Ae);To=i(ia,`Check if the provided dataset follows the squad v2 data schema, namely possible samples where the answer is not in the context.
In this case, the answer text list should be `),xt=s(ia,"CODE",{});var Ts=n(xt);Po=i(Ts,"[]"),Ts.forEach(a),Co=i(ia,"."),ia.forEach(a),la.forEach(a),Do=d(U),ue=s(U,"DIV",{class:!0});var ca=n(ue);b(Fe.$$.fragment,ca),No=d(ca),qt=s(ca,"P",{});var Ps=n(qt);Io=i(Ps,"Prepare data."),Ps.forEach(a),ca.forEach(a),U.forEach(a),Kt=d(t),J=s(t,"H3",{class:!0});var da=n(J);me=s(da,"A",{id:!0,class:!0,href:!0});var Cs=n(me);kt=s(Cs,"SPAN",{});var Ds=n(kt);b(Me.$$.fragment,Ds),Ds.forEach(a),Cs.forEach(a),Ao=d(da),jt=s(da,"SPAN",{});var Ns=n(jt);Fo=i(Ns,"TextClassificationEvaluator"),Ns.forEach(a),da.forEach(a),Xt=d(t),Q=s(t,"DIV",{class:!0});var Ye=n(Q);b(Le.$$.fragment,Ye),Mo=d(Ye),N=s(Ye,"P",{});var R=n(N);Lo=i(R,`Text classification evaluator.
This text classification evaluator can currently be loaded from `),We=s(R,"A",{href:!0});var Is=n(We);Qo=i(Is,"evaluator()"),Is.forEach(a),zo=i(R,` using the default task name
`),Tt=s(R,"CODE",{});var As=n(Tt);Oo=i(As,"text-classification"),As.forEach(a),Uo=i(R," or with a "),Pt=s(R,"CODE",{});var Fs=n(Pt);So=i(Fs,'"sentiment-analysis"'),Fs.forEach(a),Ro=i(R,` alias.
Methods in this class assume a data format compatible with the `),Ct=s(R,"CODE",{});var Ms=n(Ct);Vo=i(Ms,"TextClassificationPipeline"),Ms.forEach(a),Bo=i(R,` - a single textual
feature as input and a categorical label as output.`),R.forEach(a),Ho=d(Ye),O=s(Ye,"DIV",{class:!0});var Ze=n(O);b(Qe.$$.fragment,Ze),Wo=d(Ze),Dt=s(Ze,"P",{});var Ls=n(Dt);Go=i(Ls,"Compute the metric for a given pipeline and dataset combination."),Ls.forEach(a),Jo=d(Ze),b(fe.$$.fragment,Ze),Ze.forEach(a),Ye.forEach(a),this.h()},h(){f(p,"name","hf:doc:metadata"),f(p,"content",JSON.stringify(Xs)),f(u,"id","evaluator"),f(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(u,"href","#evaluator"),f(m,"class","relative group"),f(X,"id","evaluate.evaluator"),f(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(X,"href","#evaluate.evaluator"),f(V,"class","relative group"),f(Re,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.Evaluator"),f(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(se,"id","the-task-specific-evaluators"),f(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(se,"href","#the-task-specific-evaluators"),f(H,"class","relative group"),f(ne,"id","evaluate.ImageClassificationEvaluator"),f(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ne,"href","#evaluate.ImageClassificationEvaluator"),f(W,"class","relative group"),f(Be,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.evaluator"),f(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(le,"id","evaluate.QuestionAnsweringEvaluator"),f(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(le,"href","#evaluate.QuestionAnsweringEvaluator"),f(G,"class","relative group"),f(He,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.evaluator"),f(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(me,"id","evaluate.TextClassificationEvaluator"),f(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(me,"href","#evaluate.TextClassificationEvaluator"),f(J,"class","relative group"),f(We,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.evaluator"),f(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,g){e(document.head,p),v(t,x,g),v(t,m,g),e(m,u),e(u,q),$(r,q,null),e(m,h),e(m,K),e(K,pa),v(t,zt,g),v(t,Ue,g),e(Ue,ua),v(t,Ot,g),v(t,V,g),e(V,X),e(X,tt),$(_e,tt,null),e(V,ma),e(V,at),e(at,fa),v(t,Ut,g),v(t,Se,g),e(Se,ga),v(t,St,g),v(t,I,g),$(be,I,null),e(I,ha),e(I,A),e(A,va),e(A,Re),e(Re,_a),e(A,ba),e(A,ot),e(ot,$a),e(A,wa),e(A,st),e(st,ya),e(A,Ea),e(I,xa),$(Y,I,null),v(t,Rt,g),v(t,Ve,g),e(Ve,qa),v(t,Vt,g),v(t,k,g),$($e,k,null),e(k,ka),e(k,nt),e(nt,ja),e(k,Ta),e(k,Z),$(we,Z,null),e(Z,Pa),e(Z,B),e(B,Ca),e(B,rt),e(rt,Da),e(B,Na),e(B,lt),e(lt,Ia),e(B,Aa),e(k,Fa),e(k,ee),$(ye,ee,null),e(ee,Ma),e(ee,it),e(it,La),e(k,Qa),e(k,te),$(Ee,te,null),e(te,za),e(te,ct),e(ct,Oa),e(k,Ua),e(k,ae),$(xe,ae,null),e(ae,Sa),e(ae,dt),e(dt,Ra),e(k,Va),e(k,oe),$(qe,oe,null),e(oe,Ba),e(oe,pt),e(pt,Ha),v(t,Bt,g),v(t,H,g),e(H,se),e(se,ut),$(ke,ut,null),e(H,Wa),e(H,mt),e(mt,Ga),v(t,Ht,g),v(t,W,g),e(W,ne),e(ne,ft),$(je,ft,null),e(W,Ja),e(W,gt),e(gt,Ka),v(t,Wt,g),v(t,F,g),$(Te,F,null),e(F,Xa),e(F,M),e(M,Ya),e(M,Be),e(Be,Za),e(M,eo),e(M,ht),e(ht,to),e(M,ao),e(M,vt),e(vt,oo),e(M,so),e(F,no),e(F,z),$(Pe,z,null),e(z,ro),e(z,_t),e(_t,lo),e(z,io),$(re,z,null),v(t,Gt,g),v(t,G,g),e(G,le),e(le,bt),$(Ce,bt,null),e(G,co),e(G,$t),e($t,po),v(t,Jt,g),v(t,j,g),$(De,j,null),e(j,uo),e(j,L),e(L,mo),e(L,He),e(He,fo),e(L,go),e(L,wt),e(wt,ho),e(L,vo),e(L,yt),e(yt,_o),e(L,bo),e(j,$o),e(j,C),$(Ne,C,null),e(C,wo),e(C,Et),e(Et,yo),e(C,Eo),$(ie,C,null),e(C,xo),$(ce,C,null),e(C,qo),$(de,C,null),e(j,ko),e(j,pe),$(Ie,pe,null),e(pe,jo),e(pe,Ae),e(Ae,To),e(Ae,xt),e(xt,Po),e(Ae,Co),e(j,Do),e(j,ue),$(Fe,ue,null),e(ue,No),e(ue,qt),e(qt,Io),v(t,Kt,g),v(t,J,g),e(J,me),e(me,kt),$(Me,kt,null),e(J,Ao),e(J,jt),e(jt,Fo),v(t,Xt,g),v(t,Q,g),$(Le,Q,null),e(Q,Mo),e(Q,N),e(N,Lo),e(N,We),e(We,Qo),e(N,zo),e(N,Tt),e(Tt,Oo),e(N,Uo),e(N,Pt),e(Pt,So),e(N,Ro),e(N,Ct),e(Ct,Vo),e(N,Bo),e(Q,Ho),e(Q,O),$(Qe,O,null),e(O,Wo),e(O,Dt),e(Dt,Go),e(O,Jo),$(fe,O,null),Yt=!0},p(t,[g]){const ze={};g&2&&(ze.$$scope={dirty:g,ctx:t}),Y.$set(ze);const Nt={};g&2&&(Nt.$$scope={dirty:g,ctx:t}),re.$set(Nt);const It={};g&2&&(It.$$scope={dirty:g,ctx:t}),ie.$set(It);const At={};g&2&&(At.$$scope={dirty:g,ctx:t}),ce.$set(At);const Ft={};g&2&&(Ft.$$scope={dirty:g,ctx:t}),de.$set(Ft);const Oe={};g&2&&(Oe.$$scope={dirty:g,ctx:t}),fe.$set(Oe)},i(t){Yt||(w(r.$$.fragment,t),w(_e.$$.fragment,t),w(be.$$.fragment,t),w(Y.$$.fragment,t),w($e.$$.fragment,t),w(we.$$.fragment,t),w(ye.$$.fragment,t),w(Ee.$$.fragment,t),w(xe.$$.fragment,t),w(qe.$$.fragment,t),w(ke.$$.fragment,t),w(je.$$.fragment,t),w(Te.$$.fragment,t),w(Pe.$$.fragment,t),w(re.$$.fragment,t),w(Ce.$$.fragment,t),w(De.$$.fragment,t),w(Ne.$$.fragment,t),w(ie.$$.fragment,t),w(ce.$$.fragment,t),w(de.$$.fragment,t),w(Ie.$$.fragment,t),w(Fe.$$.fragment,t),w(Me.$$.fragment,t),w(Le.$$.fragment,t),w(Qe.$$.fragment,t),w(fe.$$.fragment,t),Yt=!0)},o(t){y(r.$$.fragment,t),y(_e.$$.fragment,t),y(be.$$.fragment,t),y(Y.$$.fragment,t),y($e.$$.fragment,t),y(we.$$.fragment,t),y(ye.$$.fragment,t),y(Ee.$$.fragment,t),y(xe.$$.fragment,t),y(qe.$$.fragment,t),y(ke.$$.fragment,t),y(je.$$.fragment,t),y(Te.$$.fragment,t),y(Pe.$$.fragment,t),y(re.$$.fragment,t),y(Ce.$$.fragment,t),y(De.$$.fragment,t),y(Ne.$$.fragment,t),y(ie.$$.fragment,t),y(ce.$$.fragment,t),y(de.$$.fragment,t),y(Ie.$$.fragment,t),y(Fe.$$.fragment,t),y(Me.$$.fragment,t),y(Le.$$.fragment,t),y(Qe.$$.fragment,t),y(fe.$$.fragment,t),Yt=!1},d(t){a(p),t&&a(x),t&&a(m),E(r),t&&a(zt),t&&a(Ue),t&&a(Ot),t&&a(V),E(_e),t&&a(Ut),t&&a(Se),t&&a(St),t&&a(I),E(be),E(Y),t&&a(Rt),t&&a(Ve),t&&a(Vt),t&&a(k),E($e),E(we),E(ye),E(Ee),E(xe),E(qe),t&&a(Bt),t&&a(H),E(ke),t&&a(Ht),t&&a(W),E(je),t&&a(Wt),t&&a(F),E(Te),E(Pe),E(re),t&&a(Gt),t&&a(G),E(Ce),t&&a(Jt),t&&a(j),E(De),E(Ne),E(ie),E(ce),E(de),E(Ie),E(Fe),t&&a(Kt),t&&a(J),E(Me),t&&a(Xt),t&&a(Q),E(Le),E(Qe),E(fe)}}}const Xs={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"},{local:"the-task-specific-evaluators",sections:[{local:"evaluate.ImageClassificationEvaluator",title:"ImageClassificationEvaluator"},{local:"evaluate.QuestionAnsweringEvaluator",title:"QuestionAnsweringEvaluator"},{local:"evaluate.TextClassificationEvaluator",title:"TextClassificationEvaluator"}],title:"The task specific evaluators"}],title:"Evaluator"};function Ys(P){return Ss(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class nn extends Qs{constructor(p){super();zs(this,p,Ys,Ks,Os,{})}}export{nn as default,Xs as metadata};
