import{S as Oo,i as Ro,s as Qo,e as s,k as c,w as b,t as l,M as So,c as n,d as a,m as d,a as r,x as w,h as i,b as f,G as e,g as h,y as $,q as y,o as E,B as x,v as Vo,L as Nt}from"../../chunks/vendor-hf-doc-builder.js";import{D as T}from"../../chunks/Docstring-hf-doc-builder.js";import{C as It}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as no}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Ct}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Bo(L){let u,q,v,p,_;return p=new It({props:{code:`from evaluate import evaluator
# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){u=s("p"),q=l("Examples:"),v=c(),b(p.$$.fragment)},l(o){u=n(o,"P",{});var g=r(u);q=i(g,"Examples:"),g.forEach(a),v=d(o),w(p.$$.fragment,o)},m(o,g){h(o,u,g),e(u,q),h(o,v,g),$(p,o,g),_=!0},p:Nt,i(o){_||(y(p.$$.fragment,o),_=!0)},o(o){E(p.$$.fragment,o),_=!1},d(o){o&&a(u),o&&a(v),x(p,o)}}}function Ho(L){let u,q,v,p,_;return p=new It({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("image-classification")
data =  Dataset.from_dict(load_dataset("beans")["test"][:2])
results = e.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    metric="accuracy",
    input_column="image",
    label_column="labels",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;image-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data =  Dataset.from_dict(load_dataset(<span class="hljs-string">&quot;beans&quot;</span>)[<span class="hljs-string">&quot;test&quot;</span>][:<span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;nateraw/vit-base-beans&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;image&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;labels&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&#x27;angular_leaf_spot&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;bean_rust&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>: <span class="hljs-number">2</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){u=s("p"),q=l("Examples:"),v=c(),b(p.$$.fragment)},l(o){u=n(o,"P",{});var g=r(u);q=i(g,"Examples:"),g.forEach(a),v=d(o),w(p.$$.fragment,o)},m(o,g){h(o,u,g),e(u,q),h(o,v,g),$(p,o,g),_=!0},p:Nt,i(o){_||(y(p.$$.fragment,o),_=!0)},o(o){E(p.$$.fragment,o),_=!1},d(o){o&&a(u),o&&a(v),x(p,o)}}}function Wo(L){let u,q,v,p,_;return p=new It({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("question-answering")
data =  Dataset.from_dict(load_dataset("squad")["validation"][:2])
results = e.compute(
    model_or_pipeline="mrm8488/bert-tiny-finetuned-squadv2",
    data=data,
    metric="squad",
    question_column="question",
    context_column="context",
    label_column="questions",
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data =  Dataset.from_dict(load_dataset(<span class="hljs-string">&quot;squad&quot;</span>)[<span class="hljs-string">&quot;validation&quot;</span>][:<span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;mrm8488/bert-tiny-finetuned-squadv2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    question_column=<span class="hljs-string">&quot;question&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    context_column=<span class="hljs-string">&quot;context&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;questions&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){u=s("p"),q=l("Examples:"),v=c(),b(p.$$.fragment)},l(o){u=n(o,"P",{});var g=r(u);q=i(g,"Examples:"),g.forEach(a),v=d(o),w(p.$$.fragment,o)},m(o,g){h(o,u,g),e(u,q),h(o,v,g),$(p,o,g),_=!0},p:Nt,i(o){_||(y(p.$$.fragment,o),_=!0)},o(o){E(p.$$.fragment,o),_=!1},d(o){o&&a(u),o&&a(v),x(p,o)}}}function Go(L){let u,q,v,p,_;return p=new It({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("text-classification")
data =  Dataset.from_dict(load_dataset("imdb")["test"][:2])
results = e.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    input_column="text",
    label_column="label",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data =  Dataset.from_dict(load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)[<span class="hljs-string">&quot;test&quot;</span>][:<span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;text&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;label&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){u=s("p"),q=l("Examples:"),v=c(),b(p.$$.fragment)},l(o){u=n(o,"P",{});var g=r(u);q=i(g,"Examples:"),g.forEach(a),v=d(o),w(p.$$.fragment,o)},m(o,g){h(o,u,g),e(u,q),h(o,v,g),$(p,o,g),_=!0},p:Nt,i(o){_||(y(p.$$.fragment,o),_=!0)},o(o){E(p.$$.fragment,o),_=!1},d(o){o&&a(u),o&&a(v),x(p,o)}}}function Jo(L){let u,q,v,p,_,o,g,ze,At,ut,xe,Ft,mt,Q,B,Ue,le,Mt,Oe,Lt,ft,qe,zt,gt,C,ie,Ut,N,Ot,je,Rt,Qt,Re,St,Vt,Qe,Bt,Ht,Wt,H,ht,ke,Gt,vt,j,ce,Jt,Se,Kt,Xt,W,de,Yt,S,Zt,Ve,ea,ta,Be,aa,oa,sa,G,pe,na,He,ra,la,J,ue,ia,We,ca,da,K,me,pa,Ge,ua,ma,X,fe,fa,Je,ga,_t,V,Y,Ke,ge,ha,Xe,va,bt,I,he,_a,A,ba,Te,wa,$a,Ye,ya,Ea,Ze,xa,qa,ja,z,ve,ka,et,Ta,Pa,Z,wt,P,_e,Da,F,Ca,Pe,Na,Ia,tt,Aa,Fa,at,Ma,La,za,U,be,Ua,ot,Oa,Ra,ee,Qa,te,we,Sa,st,Va,$t,M,$e,Ba,D,Ha,De,Wa,Ga,nt,Ja,Ka,rt,Xa,Ya,lt,Za,eo,to,O,ye,ao,it,oo,so,ae,yt;return o=new no({}),le=new no({}),ie=new T({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:<ul>
<li><code>&quot;image-classification&quot;</code>: will return a <a href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator">ImageClassificationEvaluator</a>.</li>
<li><code>&quot;question-answering&quot;</code>: will return a <a href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.QuestionAnsweringEvaluator">QuestionAnsweringEvaluator</a>.</li>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/__init__.py#L79",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),H=new Ct({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[Bo]},$$scope:{ctx:L}}}),ce=new T({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L54"}}),de=new T({props:{name:"compute",anchor:"evaluate.Evaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"**compute_parameters",val:": typing.Dict"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L107"}}),pe=new T({props:{name:"core_compute",anchor:"evaluate.Evaluator.core_compute",parameters:[{name:"references",val:": typing.List"},{name:"predictions",val:": typing.List"},{name:"metric",val:": EvaluationModule"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L220"}}),ue=new T({props:{name:"prepare_data",anchor:"evaluate.Evaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"input_column",val:": str"},{name:"label_column",val:": str"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_data.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.Evaluator.prepare_data.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.prepare_data.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L125",returnDescription:`
<p>Loaded <code>datasets.Dataset</code>.</p>
`}}),me=new T({props:{name:"prepare_metric",anchor:"evaluate.Evaluator.prepare_metric",parameters:[{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule]"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_metric.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L196",returnDescription:`
<p>The loaded metric.</p>
`}}),fe=new T({props:{name:"prepare_pipeline",anchor:"evaluate.Evaluator.prepare_pipeline",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"preprocessor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_pipeline.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.prepare_pipeline.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.Evaluator.prepare_pipeline.preprocessor",description:`<strong>preprocessor</strong> (<code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"preprocessor"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L156",returnDescription:`
<p>The initialized pipeline.</p>
`}}),ge=new no({}),he=new T({props:{name:"class evaluate.ImageClassificationEvaluator",anchor:"evaluate.ImageClassificationEvaluator",parameters:[{name:"task",val:" = 'image-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/image_classification.py#L39"}}),ve=new T({props:{name:"compute",anchor:"evaluate.ImageClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"feature_extractor",val:": typing.Union[str, ForwardRef('FeatureExtractionMixin'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'image'"},{name:"label_column",val:": str = 'labels'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"}],parametersDescription:[{anchor:"evaluate.ImageClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.ImageClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>image-classification</code>. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.ImageClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.ImageClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>&#x201D; &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
feature_extractor &#x2014; (<code>str</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default feature extractor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.ImageClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.ImageClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.ImageClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.ImageClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;image&quot;</code>) &#x2014;
the name of the column containing the images as PIL ImageFile in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;labels&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/image_classification.py#L50",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),Z=new Ct({props:{anchor:"evaluate.ImageClassificationEvaluator.compute.example",$$slots:{default:[Ho]},$$scope:{ctx:L}}}),_e=new T({props:{name:"class evaluate.QuestionAnsweringEvaluator",anchor:"evaluate.QuestionAnsweringEvaluator",parameters:[{name:"task",val:" = 'question-answering'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L38"}}),be=new T({props:{name:"compute",anchor:"evaluate.QuestionAnsweringEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"question_column",val:": str = 'question'"},{name:"context_column",val:": str = 'context'"},{name:"id_column",val:": str = 'id'"},{name:"label_column",val:": str = 'answers'"}],parametersDescription:[{anchor:"evaluate.QuestionAnsweringEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>question-answering</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;&#x201D;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.question_column",description:`<strong>question_column</strong> (<code>str</code>, defaults to <code>&quot;question&quot;</code>) &#x2014;
the name of the column containing the question in the dataset specified by <code>data</code>.`,name:"question_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.context_column",description:`<strong>context_column</strong> (<code>str</code>, defaults to <code>&quot;context&quot;</code>) &#x2014;
the name of the column containing the context in the dataset specified by <code>data</code>.`,name:"context_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.id_column",description:`<strong>id_column</strong> (<code>str</code>, defaults to <code>&quot;id&quot;</code>) &#x2014;
the name of the column cointaing the identification field of the question and answer pair in the
dataset specified by <code>data</code>.`,name:"id_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the answers in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L82",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ee=new Ct({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example",$$slots:{default:[Wo]},$$scope:{ctx:L}}}),we=new T({props:{name:"prepare_data",anchor:"evaluate.QuestionAnsweringEvaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"question_column",val:": str"},{name:"context_column",val:": str"},{name:"id_column",val:": str"},{name:"label_column",val:": str"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L49"}}),$e=new T({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/text_classification.py#L39"}}),ye=new T({props:{name:"compute",anchor:"evaluate.TextClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'text'"},{name:"label_column",val:": str = 'label'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"}],parametersDescription:[{anchor:"evaluate.TextClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.TextClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.TextClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.TextClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;&#x201D;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.TextClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TextClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TextClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TextClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/text_classification.py#L51",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ae=new Ct({props:{anchor:"evaluate.TextClassificationEvaluator.compute.example",$$slots:{default:[Go]},$$scope:{ctx:L}}}),{c(){u=s("meta"),q=c(),v=s("h1"),p=s("a"),_=s("span"),b(o.$$.fragment),g=c(),ze=s("span"),At=l("Evaluator"),ut=c(),xe=s("p"),Ft=l("The evaluator classes for automatic evaluation."),mt=c(),Q=s("h2"),B=s("a"),Ue=s("span"),b(le.$$.fragment),Mt=c(),Oe=s("span"),Lt=l("Evaluator classes"),ft=c(),qe=s("p"),zt=l("The main entry point for using the evaluator:"),gt=c(),C=s("div"),b(ie.$$.fragment),Ut=c(),N=s("p"),Ot=l("Utility factory method to build an "),je=s("a"),Rt=l("Evaluator"),Qt=l(`.
Evaluators encapsulate a task and a default metric name. They leverage `),Re=s("code"),St=l("pipeline"),Vt=l(" functionalify from "),Qe=s("code"),Bt=l("transformers"),Ht=l(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),Wt=c(),b(H.$$.fragment),ht=c(),ke=s("p"),Gt=l("The base class for all evaluator classes:"),vt=c(),j=s("div"),b(ce.$$.fragment),Jt=c(),Se=s("p"),Kt=l(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),Xt=c(),W=s("div"),b(de.$$.fragment),Yt=c(),S=s("p"),Zt=l("A core method of the "),Ve=s("code"),ea=l("Evaluator"),ta=l(` class, computes the metric value for a pipeline and dataset compatible
with the task specified by the `),Be=s("code"),aa=l("Evaluator"),oa=l("."),sa=c(),G=s("div"),b(pe.$$.fragment),na=c(),He=s("p"),ra=l("Compute and return metrics."),la=c(),J=s("div"),b(ue.$$.fragment),ia=c(),We=s("p"),ca=l("Prepare data."),da=c(),K=s("div"),b(me.$$.fragment),pa=c(),Ge=s("p"),ua=l("Prepare metric."),ma=c(),X=s("div"),b(fe.$$.fragment),fa=c(),Je=s("p"),ga=l("Prepare pipeline."),_t=c(),V=s("h2"),Y=s("a"),Ke=s("span"),b(ge.$$.fragment),ha=c(),Xe=s("span"),va=l("The task specific evaluators"),bt=c(),I=s("div"),b(he.$$.fragment),_a=c(),A=s("p"),ba=l(`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Te=s("a"),wa=l("evaluator()"),$a=l(` using the default task name
`),Ye=s("code"),ya=l("image-classification"),Ea=l(`.
Methods in this class assume a data format compatible with the `),Ze=s("code"),xa=l("ImageClassificationPipeline"),qa=l("."),ja=c(),z=s("div"),b(ve.$$.fragment),ka=c(),et=s("p"),Ta=l("Compute the metric for a given pipeline and dataset combination."),Pa=c(),b(Z.$$.fragment),wt=c(),P=s("div"),b(_e.$$.fragment),Da=c(),F=s("p"),Ca=l(`Question answering evaluator.
This question answering evaluator can currently be loaded from `),Pe=s("a"),Na=l("evaluator()"),Ia=l(` using the default task name
`),tt=s("code"),Aa=l("question-answering"),Fa=l(`.
Methods in this class assume a data format compatible with the `),at=s("code"),Ma=l("QuestionAnsweringPipeline"),La=l("."),za=c(),U=s("div"),b(be.$$.fragment),Ua=c(),ot=s("p"),Oa=l("Compute the metric for a given pipeline and dataset combination."),Ra=c(),b(ee.$$.fragment),Qa=c(),te=s("div"),b(we.$$.fragment),Sa=c(),st=s("p"),Va=l("Prepare data."),$t=c(),M=s("div"),b($e.$$.fragment),Ba=c(),D=s("p"),Ha=l(`Text classification evaluator.
This text classification evaluator can currently be loaded from `),De=s("a"),Wa=l("evaluator()"),Ga=l(` using the default task name
`),nt=s("code"),Ja=l("text-classification"),Ka=l(" or with a "),rt=s("code"),Xa=l('"sentiment-analysis"'),Ya=l(` alias.
Methods in this class assume a data format compatible with the `),lt=s("code"),Za=l("TextClassificationPipeline"),eo=l(` - a single textual
feature as input and a categorical label as output.`),to=c(),O=s("div"),b(ye.$$.fragment),ao=c(),it=s("p"),oo=l("Compute the metric for a given pipeline and dataset combination."),so=c(),b(ae.$$.fragment),this.h()},l(t){const m=So('[data-svelte="svelte-1phssyn"]',document.head);u=n(m,"META",{name:!0,content:!0}),m.forEach(a),q=d(t),v=n(t,"H1",{class:!0});var Ee=r(v);p=n(Ee,"A",{id:!0,class:!0,href:!0});var ct=r(p);_=n(ct,"SPAN",{});var dt=r(_);w(o.$$.fragment,dt),dt.forEach(a),ct.forEach(a),g=d(Ee),ze=n(Ee,"SPAN",{});var pt=r(ze);At=i(pt,"Evaluator"),pt.forEach(a),Ee.forEach(a),ut=d(t),xe=n(t,"P",{});var ro=r(xe);Ft=i(ro,"The evaluator classes for automatic evaluation."),ro.forEach(a),mt=d(t),Q=n(t,"H2",{class:!0});var Et=r(Q);B=n(Et,"A",{id:!0,class:!0,href:!0});var lo=r(B);Ue=n(lo,"SPAN",{});var io=r(Ue);w(le.$$.fragment,io),io.forEach(a),lo.forEach(a),Mt=d(Et),Oe=n(Et,"SPAN",{});var co=r(Oe);Lt=i(co,"Evaluator classes"),co.forEach(a),Et.forEach(a),ft=d(t),qe=n(t,"P",{});var po=r(qe);zt=i(po,"The main entry point for using the evaluator:"),po.forEach(a),gt=d(t),C=n(t,"DIV",{class:!0});var Ce=r(C);w(ie.$$.fragment,Ce),Ut=d(Ce),N=n(Ce,"P",{});var oe=r(N);Ot=i(oe,"Utility factory method to build an "),je=n(oe,"A",{href:!0});var uo=r(je);Rt=i(uo,"Evaluator"),uo.forEach(a),Qt=i(oe,`.
Evaluators encapsulate a task and a default metric name. They leverage `),Re=n(oe,"CODE",{});var mo=r(Re);St=i(mo,"pipeline"),mo.forEach(a),Vt=i(oe," functionalify from "),Qe=n(oe,"CODE",{});var fo=r(Qe);Bt=i(fo,"transformers"),fo.forEach(a),Ht=i(oe,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),oe.forEach(a),Wt=d(Ce),w(H.$$.fragment,Ce),Ce.forEach(a),ht=d(t),ke=n(t,"P",{});var go=r(ke);Gt=i(go,"The base class for all evaluator classes:"),go.forEach(a),vt=d(t),j=n(t,"DIV",{class:!0});var k=r(j);w(ce.$$.fragment,k),Jt=d(k),Se=n(k,"P",{});var ho=r(Se);Kt=i(ho,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),ho.forEach(a),Xt=d(k),W=n(k,"DIV",{class:!0});var xt=r(W);w(de.$$.fragment,xt),Yt=d(xt),S=n(xt,"P",{});var Ne=r(S);Zt=i(Ne,"A core method of the "),Ve=n(Ne,"CODE",{});var vo=r(Ve);ea=i(vo,"Evaluator"),vo.forEach(a),ta=i(Ne,` class, computes the metric value for a pipeline and dataset compatible
with the task specified by the `),Be=n(Ne,"CODE",{});var _o=r(Be);aa=i(_o,"Evaluator"),_o.forEach(a),oa=i(Ne,"."),Ne.forEach(a),xt.forEach(a),sa=d(k),G=n(k,"DIV",{class:!0});var qt=r(G);w(pe.$$.fragment,qt),na=d(qt),He=n(qt,"P",{});var bo=r(He);ra=i(bo,"Compute and return metrics."),bo.forEach(a),qt.forEach(a),la=d(k),J=n(k,"DIV",{class:!0});var jt=r(J);w(ue.$$.fragment,jt),ia=d(jt),We=n(jt,"P",{});var wo=r(We);ca=i(wo,"Prepare data."),wo.forEach(a),jt.forEach(a),da=d(k),K=n(k,"DIV",{class:!0});var kt=r(K);w(me.$$.fragment,kt),pa=d(kt),Ge=n(kt,"P",{});var $o=r(Ge);ua=i($o,"Prepare metric."),$o.forEach(a),kt.forEach(a),ma=d(k),X=n(k,"DIV",{class:!0});var Tt=r(X);w(fe.$$.fragment,Tt),fa=d(Tt),Je=n(Tt,"P",{});var yo=r(Je);ga=i(yo,"Prepare pipeline."),yo.forEach(a),Tt.forEach(a),k.forEach(a),_t=d(t),V=n(t,"H2",{class:!0});var Pt=r(V);Y=n(Pt,"A",{id:!0,class:!0,href:!0});var Eo=r(Y);Ke=n(Eo,"SPAN",{});var xo=r(Ke);w(ge.$$.fragment,xo),xo.forEach(a),Eo.forEach(a),ha=d(Pt),Xe=n(Pt,"SPAN",{});var qo=r(Xe);va=i(qo,"The task specific evaluators"),qo.forEach(a),Pt.forEach(a),bt=d(t),I=n(t,"DIV",{class:!0});var Ie=r(I);w(he.$$.fragment,Ie),_a=d(Ie),A=n(Ie,"P",{});var se=r(A);ba=i(se,`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Te=n(se,"A",{href:!0});var jo=r(Te);wa=i(jo,"evaluator()"),jo.forEach(a),$a=i(se,` using the default task name
`),Ye=n(se,"CODE",{});var ko=r(Ye);ya=i(ko,"image-classification"),ko.forEach(a),Ea=i(se,`.
Methods in this class assume a data format compatible with the `),Ze=n(se,"CODE",{});var To=r(Ze);xa=i(To,"ImageClassificationPipeline"),To.forEach(a),qa=i(se,"."),se.forEach(a),ja=d(Ie),z=n(Ie,"DIV",{class:!0});var Ae=r(z);w(ve.$$.fragment,Ae),ka=d(Ae),et=n(Ae,"P",{});var Po=r(et);Ta=i(Po,"Compute the metric for a given pipeline and dataset combination."),Po.forEach(a),Pa=d(Ae),w(Z.$$.fragment,Ae),Ae.forEach(a),Ie.forEach(a),wt=d(t),P=n(t,"DIV",{class:!0});var ne=r(P);w(_e.$$.fragment,ne),Da=d(ne),F=n(ne,"P",{});var re=r(F);Ca=i(re,`Question answering evaluator.
This question answering evaluator can currently be loaded from `),Pe=n(re,"A",{href:!0});var Do=r(Pe);Na=i(Do,"evaluator()"),Do.forEach(a),Ia=i(re,` using the default task name
`),tt=n(re,"CODE",{});var Co=r(tt);Aa=i(Co,"question-answering"),Co.forEach(a),Fa=i(re,`.
Methods in this class assume a data format compatible with the `),at=n(re,"CODE",{});var No=r(at);Ma=i(No,"QuestionAnsweringPipeline"),No.forEach(a),La=i(re,"."),re.forEach(a),za=d(ne),U=n(ne,"DIV",{class:!0});var Fe=r(U);w(be.$$.fragment,Fe),Ua=d(Fe),ot=n(Fe,"P",{});var Io=r(ot);Oa=i(Io,"Compute the metric for a given pipeline and dataset combination."),Io.forEach(a),Ra=d(Fe),w(ee.$$.fragment,Fe),Fe.forEach(a),Qa=d(ne),te=n(ne,"DIV",{class:!0});var Dt=r(te);w(we.$$.fragment,Dt),Sa=d(Dt),st=n(Dt,"P",{});var Ao=r(st);Va=i(Ao,"Prepare data."),Ao.forEach(a),Dt.forEach(a),ne.forEach(a),$t=d(t),M=n(t,"DIV",{class:!0});var Me=r(M);w($e.$$.fragment,Me),Ba=d(Me),D=n(Me,"P",{});var R=r(D);Ha=i(R,`Text classification evaluator.
This text classification evaluator can currently be loaded from `),De=n(R,"A",{href:!0});var Fo=r(De);Wa=i(Fo,"evaluator()"),Fo.forEach(a),Ga=i(R,` using the default task name
`),nt=n(R,"CODE",{});var Mo=r(nt);Ja=i(Mo,"text-classification"),Mo.forEach(a),Ka=i(R," or with a "),rt=n(R,"CODE",{});var Lo=r(rt);Xa=i(Lo,'"sentiment-analysis"'),Lo.forEach(a),Ya=i(R,` alias.
Methods in this class assume a data format compatible with the `),lt=n(R,"CODE",{});var zo=r(lt);Za=i(zo,"TextClassificationPipeline"),zo.forEach(a),eo=i(R,` - a single textual
feature as input and a categorical label as output.`),R.forEach(a),to=d(Me),O=n(Me,"DIV",{class:!0});var Le=r(O);w(ye.$$.fragment,Le),ao=d(Le),it=n(Le,"P",{});var Uo=r(it);oo=i(Uo,"Compute the metric for a given pipeline and dataset combination."),Uo.forEach(a),so=d(Le),w(ae.$$.fragment,Le),Le.forEach(a),Me.forEach(a),this.h()},h(){f(u,"name","hf:doc:metadata"),f(u,"content",JSON.stringify(Ko)),f(p,"id","evaluator"),f(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(p,"href","#evaluator"),f(v,"class","relative group"),f(B,"id","evaluate.evaluator"),f(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(B,"href","#evaluate.evaluator"),f(Q,"class","relative group"),f(je,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.Evaluator"),f(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(Y,"id","evaluate.ImageClassificationEvaluator"),f(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Y,"href","#evaluate.ImageClassificationEvaluator"),f(V,"class","relative group"),f(Te,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.evaluator"),f(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(Pe,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.evaluator"),f(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(De,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.evaluator"),f(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),f(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,m){e(document.head,u),h(t,q,m),h(t,v,m),e(v,p),e(p,_),$(o,_,null),e(v,g),e(v,ze),e(ze,At),h(t,ut,m),h(t,xe,m),e(xe,Ft),h(t,mt,m),h(t,Q,m),e(Q,B),e(B,Ue),$(le,Ue,null),e(Q,Mt),e(Q,Oe),e(Oe,Lt),h(t,ft,m),h(t,qe,m),e(qe,zt),h(t,gt,m),h(t,C,m),$(ie,C,null),e(C,Ut),e(C,N),e(N,Ot),e(N,je),e(je,Rt),e(N,Qt),e(N,Re),e(Re,St),e(N,Vt),e(N,Qe),e(Qe,Bt),e(N,Ht),e(C,Wt),$(H,C,null),h(t,ht,m),h(t,ke,m),e(ke,Gt),h(t,vt,m),h(t,j,m),$(ce,j,null),e(j,Jt),e(j,Se),e(Se,Kt),e(j,Xt),e(j,W),$(de,W,null),e(W,Yt),e(W,S),e(S,Zt),e(S,Ve),e(Ve,ea),e(S,ta),e(S,Be),e(Be,aa),e(S,oa),e(j,sa),e(j,G),$(pe,G,null),e(G,na),e(G,He),e(He,ra),e(j,la),e(j,J),$(ue,J,null),e(J,ia),e(J,We),e(We,ca),e(j,da),e(j,K),$(me,K,null),e(K,pa),e(K,Ge),e(Ge,ua),e(j,ma),e(j,X),$(fe,X,null),e(X,fa),e(X,Je),e(Je,ga),h(t,_t,m),h(t,V,m),e(V,Y),e(Y,Ke),$(ge,Ke,null),e(V,ha),e(V,Xe),e(Xe,va),h(t,bt,m),h(t,I,m),$(he,I,null),e(I,_a),e(I,A),e(A,ba),e(A,Te),e(Te,wa),e(A,$a),e(A,Ye),e(Ye,ya),e(A,Ea),e(A,Ze),e(Ze,xa),e(A,qa),e(I,ja),e(I,z),$(ve,z,null),e(z,ka),e(z,et),e(et,Ta),e(z,Pa),$(Z,z,null),h(t,wt,m),h(t,P,m),$(_e,P,null),e(P,Da),e(P,F),e(F,Ca),e(F,Pe),e(Pe,Na),e(F,Ia),e(F,tt),e(tt,Aa),e(F,Fa),e(F,at),e(at,Ma),e(F,La),e(P,za),e(P,U),$(be,U,null),e(U,Ua),e(U,ot),e(ot,Oa),e(U,Ra),$(ee,U,null),e(P,Qa),e(P,te),$(we,te,null),e(te,Sa),e(te,st),e(st,Va),h(t,$t,m),h(t,M,m),$($e,M,null),e(M,Ba),e(M,D),e(D,Ha),e(D,De),e(De,Wa),e(D,Ga),e(D,nt),e(nt,Ja),e(D,Ka),e(D,rt),e(rt,Xa),e(D,Ya),e(D,lt),e(lt,Za),e(D,eo),e(M,to),e(M,O),$(ye,O,null),e(O,ao),e(O,it),e(it,oo),e(O,so),$(ae,O,null),yt=!0},p(t,[m]){const Ee={};m&2&&(Ee.$$scope={dirty:m,ctx:t}),H.$set(Ee);const ct={};m&2&&(ct.$$scope={dirty:m,ctx:t}),Z.$set(ct);const dt={};m&2&&(dt.$$scope={dirty:m,ctx:t}),ee.$set(dt);const pt={};m&2&&(pt.$$scope={dirty:m,ctx:t}),ae.$set(pt)},i(t){yt||(y(o.$$.fragment,t),y(le.$$.fragment,t),y(ie.$$.fragment,t),y(H.$$.fragment,t),y(ce.$$.fragment,t),y(de.$$.fragment,t),y(pe.$$.fragment,t),y(ue.$$.fragment,t),y(me.$$.fragment,t),y(fe.$$.fragment,t),y(ge.$$.fragment,t),y(he.$$.fragment,t),y(ve.$$.fragment,t),y(Z.$$.fragment,t),y(_e.$$.fragment,t),y(be.$$.fragment,t),y(ee.$$.fragment,t),y(we.$$.fragment,t),y($e.$$.fragment,t),y(ye.$$.fragment,t),y(ae.$$.fragment,t),yt=!0)},o(t){E(o.$$.fragment,t),E(le.$$.fragment,t),E(ie.$$.fragment,t),E(H.$$.fragment,t),E(ce.$$.fragment,t),E(de.$$.fragment,t),E(pe.$$.fragment,t),E(ue.$$.fragment,t),E(me.$$.fragment,t),E(fe.$$.fragment,t),E(ge.$$.fragment,t),E(he.$$.fragment,t),E(ve.$$.fragment,t),E(Z.$$.fragment,t),E(_e.$$.fragment,t),E(be.$$.fragment,t),E(ee.$$.fragment,t),E(we.$$.fragment,t),E($e.$$.fragment,t),E(ye.$$.fragment,t),E(ae.$$.fragment,t),yt=!1},d(t){a(u),t&&a(q),t&&a(v),x(o),t&&a(ut),t&&a(xe),t&&a(mt),t&&a(Q),x(le),t&&a(ft),t&&a(qe),t&&a(gt),t&&a(C),x(ie),x(H),t&&a(ht),t&&a(ke),t&&a(vt),t&&a(j),x(ce),x(de),x(pe),x(ue),x(me),x(fe),t&&a(_t),t&&a(V),x(ge),t&&a(bt),t&&a(I),x(he),x(ve),x(Z),t&&a(wt),t&&a(P),x(_e),x(be),x(ee),x(we),t&&a($t),t&&a(M),x($e),x(ye),x(ae)}}}const Ko={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"},{local:"evaluate.ImageClassificationEvaluator",title:"The task specific evaluators"}],title:"Evaluator"};function Xo(L){return Vo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class os extends Oo{constructor(u){super();Ro(this,u,Xo,Jo,Qo,{})}}export{os as default,Ko as metadata};
