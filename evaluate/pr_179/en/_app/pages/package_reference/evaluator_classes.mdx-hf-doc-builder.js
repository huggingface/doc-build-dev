import{S as Fs,i as Ms,s as Ls,e as o,k as d,w as _,t as l,M as zs,c as s,d as a,m as p,a as n,x as b,h as i,b as u,G as e,g as v,y as w,q as $,o as y,B as E,v as Qs,L as Ft}from"../../chunks/vendor-hf-doc-builder.js";import{T as Os}from"../../chunks/Tip-hf-doc-builder.js";import{D as N}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Mt}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Ze}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as It}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Ss(T){let c,x,f,m,q;return m=new Mt({props:{code:`from evaluate import evaluator
# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){c=o("p"),x=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var h=n(c);x=i(h,"Examples:"),h.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,h){v(r,c,h),e(c,x),v(r,f,h),w(m,r,h),q=!0},p:Ft,i(r){q||($(m.$$.fragment,r),q=!0)},o(r){y(m.$$.fragment,r),q=!1},d(r){r&&a(c),r&&a(f),E(m,r)}}}function Us(T){let c,x,f,m,q;return m=new Mt({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("image-classification")
data = load_dataset("beans", split="test[:2]")
results = e.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    metric="accuracy",
    input_column="image",
    label_column="labels",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;image-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;beans&quot;</span>, split=<span class="hljs-string">&quot;test[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;nateraw/vit-base-beans&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;image&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;labels&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&#x27;angular_leaf_spot&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;bean_rust&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>: <span class="hljs-number">2</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){c=o("p"),x=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var h=n(c);x=i(h,"Examples:"),h.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,h){v(r,c,h),e(c,x),v(r,f,h),w(m,r,h),q=!0},p:Ft,i(r){q||($(m.$$.fragment,r),q=!0)},o(r){y(m.$$.fragment,r),q=!1},d(r){r&&a(c),r&&a(f),E(m,r)}}}function Rs(T){let c,x,f,m,q;return m=new Mt({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("question-answering")
data = load_dataset("squad", split="validation[:2]")
results = e.compute(
    model_or_pipeline="sshleifer/tiny-distilbert-base-cased-distilled-squad",
    data=data,
    metric="squad",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;sshleifer/tiny-distilbert-base-cased-distilled-squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){c=o("p"),x=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var h=n(c);x=i(h,"Examples:"),h.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,h){v(r,c,h),e(c,x),v(r,f,h),w(m,r,h),q=!0},p:Ft,i(r){q||($(m.$$.fragment,r),q=!0)},o(r){y(m.$$.fragment,r),q=!1},d(r){r&&a(c),r&&a(f),E(m,r)}}}function Vs(T){let c,x,f,m,q;return{c(){c=o("p"),x=l(`Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. If using transformers pipeline
with models trained on this type of data, make sure to pass `),f=o("code"),m=l("handle_impossible_answer=True"),q=l(" as an argument to the pipeline.")},l(r){c=s(r,"P",{});var h=n(c);x=i(h,`Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. If using transformers pipeline
with models trained on this type of data, make sure to pass `),f=s(h,"CODE",{});var K=n(f);m=i(K,"handle_impossible_answer=True"),K.forEach(a),q=i(h," as an argument to the pipeline."),h.forEach(a)},m(r,h){v(r,c,h),e(c,x),e(c,f),e(f,m),e(c,q)},d(r){r&&a(c)}}}function Bs(T){let c,x;return c=new Mt({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
from transformers import pipeline
e = evaluator("question-answering")
data = load_dataset("squad_v2", split="validation[:2]")
pipe = pipeline(
    task="question-answering",
    model="sshleifer/mrm8488/bert-tiny-finetuned-squadv2",
    handle_impossible_answer=True
)
results = e.compute(
    model_or_pipeline=pipe,
    data=data,
    metric="squad_v2",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad_v2&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(
<span class="hljs-meta">&gt;&gt;&gt; </span>    task=<span class="hljs-string">&quot;question-answering&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    model=<span class="hljs-string">&quot;sshleifer/mrm8488/bert-tiny-finetuned-squadv2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    handle_impossible_answer=<span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=pipe,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad_v2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){_(c.$$.fragment)},l(f){b(c.$$.fragment,f)},m(f,m){w(c,f,m),x=!0},p:Ft,i(f){x||($(c.$$.fragment,f),x=!0)},o(f){y(c.$$.fragment,f),x=!1},d(f){E(c,f)}}}function Hs(T){let c,x,f,m,q;return m=new Mt({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("text-classification")
data = load_dataset("imdb", split="test[:2]")
results = e.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    input_column="text",
    label_column="label",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>, split=<span class="hljs-string">&quot;test[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;text&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;label&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){c=o("p"),x=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var h=n(c);x=i(h,"Examples:"),h.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,h){v(r,c,h),e(c,x),v(r,f,h),w(m,r,h),q=!0},p:Ft,i(r){q||($(m.$$.fragment,r),q=!0)},o(r){y(m.$$.fragment,r),q=!1},d(r){r&&a(c),r&&a(f),E(m,r)}}}function Gs(T){let c,x,f,m,q,r,h,K,ia,Lt,Qe,ca,zt,R,X,et,he,da,tt,pa,Qt,Oe,ua,Ot,A,ve,ma,I,fa,Se,ga,ha,at,va,_a,ot,ba,wa,$a,Y,St,Ue,ya,Ut,k,_e,Ea,st,xa,qa,Z,be,ka,V,ja,nt,Ta,Pa,rt,Ca,Da,Na,ee,we,Aa,lt,Ia,Fa,te,$e,Ma,it,La,za,ae,ye,Qa,ct,Oa,Sa,oe,Ee,Ua,dt,Ra,Rt,B,se,pt,xe,Va,ut,Ba,Vt,H,ne,mt,qe,Ha,ft,Ga,Bt,F,ke,Wa,M,Ja,Re,Ka,Xa,gt,Ya,Za,ht,eo,to,ao,z,je,oo,vt,so,no,re,Ht,G,le,_t,Te,ro,bt,lo,Gt,j,Pe,io,Ce,co,ie,wt,po,uo,mo,fo,W,go,Ve,ho,vo,$t,_o,bo,wo,De,$o,Ne,yt,yo,Eo,xo,P,Ae,qo,Et,ko,jo,ce,To,de,Po,pe,Wt,J,ue,xt,Ie,Co,qt,Do,Jt,L,Fe,No,D,Ao,Be,Io,Fo,kt,Mo,Lo,jt,zo,Qo,Tt,Oo,So,Uo,Q,Me,Ro,Pt,Vo,Bo,me,Kt;return r=new Ze({}),he=new Ze({}),ve=new N({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:<ul>
<li><code>&quot;image-classification&quot;</code>: will return a <a href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator">ImageClassificationEvaluator</a>.</li>
<li><code>&quot;question-answering&quot;</code>: will return a <a href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.QuestionAnsweringEvaluator">QuestionAnsweringEvaluator</a>.</li>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/__init__.py#L79",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),Y=new It({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[Ss]},$$scope:{ctx:T}}}),_e=new N({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L54"}}),be=new N({props:{name:"compute",anchor:"evaluate.Evaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"**compute_parameters",val:": typing.Dict"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L107"}}),we=new N({props:{name:"core_compute",anchor:"evaluate.Evaluator.core_compute",parameters:[{name:"references",val:": typing.List"},{name:"predictions",val:": typing.List"},{name:"metric",val:": EvaluationModule"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L220"}}),$e=new N({props:{name:"prepare_data",anchor:"evaluate.Evaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"input_column",val:": str"},{name:"label_column",val:": str"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_data.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.Evaluator.prepare_data.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.prepare_data.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L125",returnDescription:`
<p>Loaded <code>datasets.Dataset</code>.</p>
`}}),ye=new N({props:{name:"prepare_metric",anchor:"evaluate.Evaluator.prepare_metric",parameters:[{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule]"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_metric.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L196",returnDescription:`
<p>The loaded metric.</p>
`}}),Ee=new N({props:{name:"prepare_pipeline",anchor:"evaluate.Evaluator.prepare_pipeline",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"preprocessor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_pipeline.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.prepare_pipeline.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.Evaluator.prepare_pipeline.preprocessor",description:`<strong>preprocessor</strong> (<code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"preprocessor"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/base.py#L156",returnDescription:`
<p>The initialized pipeline.</p>
`}}),xe=new Ze({}),qe=new Ze({}),ke=new N({props:{name:"class evaluate.ImageClassificationEvaluator",anchor:"evaluate.ImageClassificationEvaluator",parameters:[{name:"task",val:" = 'image-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/image_classification.py#L39"}}),je=new N({props:{name:"compute",anchor:"evaluate.ImageClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"feature_extractor",val:": typing.Union[str, ForwardRef('FeatureExtractionMixin'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'image'"},{name:"label_column",val:": str = 'labels'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"}],parametersDescription:[{anchor:"evaluate.ImageClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.ImageClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>image-classification</code>. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.ImageClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.ImageClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>&#x201D; &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
feature_extractor &#x2014; (<code>str</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default feature extractor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.ImageClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.ImageClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.ImageClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.ImageClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;image&quot;</code>) &#x2014;
the name of the column containing the images as PIL ImageFile in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;labels&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/image_classification.py#L50",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),re=new It({props:{anchor:"evaluate.ImageClassificationEvaluator.compute.example",$$slots:{default:[Us]},$$scope:{ctx:T}}}),Te=new Ze({}),Pe=new N({props:{name:"class evaluate.QuestionAnsweringEvaluator",anchor:"evaluate.QuestionAnsweringEvaluator",parameters:[{name:"task",val:" = 'question-answering'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L38"}}),Ae=new N({props:{name:"compute",anchor:"evaluate.QuestionAnsweringEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"question_column",val:": str = 'question'"},{name:"context_column",val:": str = 'context'"},{name:"id_column",val:": str = 'id'"},{name:"label_column",val:": str = 'answers'"}],parametersDescription:[{anchor:"evaluate.QuestionAnsweringEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>question-answering</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;&#x201D;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.question_column",description:`<strong>question_column</strong> (<code>str</code>, defaults to <code>&quot;question&quot;</code>) &#x2014;
the name of the column containing the question in the dataset specified by <code>data</code>.`,name:"question_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.context_column",description:`<strong>context_column</strong> (<code>str</code>, defaults to <code>&quot;context&quot;</code>) &#x2014;
the name of the column containing the context in the dataset specified by <code>data</code>.`,name:"context_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.id_column",description:`<strong>id_column</strong> (<code>str</code>, defaults to <code>&quot;id&quot;</code>) &#x2014;
the name of the column cointaing the identification field of the question and answer pair in the
dataset specified by <code>data</code>.`,name:"id_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;answers&quot;</code>) &#x2014;
the name of the column containing the answers in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/question_answering.py#L101",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ce=new It({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example",$$slots:{default:[Rs]},$$scope:{ctx:T}}}),de=new Os({props:{$$slots:{default:[Vs]},$$scope:{ctx:T}}}),pe=new It({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example-2",$$slots:{default:[Bs]},$$scope:{ctx:T}}}),Ie=new Ze({}),Fe=new N({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/text_classification.py#L39"}}),Me=new N({props:{name:"compute",anchor:"evaluate.TextClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'text'"},{name:"label_column",val:": str = 'label'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"}],parametersDescription:[{anchor:"evaluate.TextClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.TextClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.TextClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.TextClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;&#x201D;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.TextClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TextClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TextClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TextClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_179/src/evaluate/evaluator/text_classification.py#L51",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),me=new It({props:{anchor:"evaluate.TextClassificationEvaluator.compute.example",$$slots:{default:[Hs]},$$scope:{ctx:T}}}),{c(){c=o("meta"),x=d(),f=o("h1"),m=o("a"),q=o("span"),_(r.$$.fragment),h=d(),K=o("span"),ia=l("Evaluator"),Lt=d(),Qe=o("p"),ca=l("The evaluator classes for automatic evaluation."),zt=d(),R=o("h2"),X=o("a"),et=o("span"),_(he.$$.fragment),da=d(),tt=o("span"),pa=l("Evaluator classes"),Qt=d(),Oe=o("p"),ua=l("The main entry point for using the evaluator:"),Ot=d(),A=o("div"),_(ve.$$.fragment),ma=d(),I=o("p"),fa=l("Utility factory method to build an "),Se=o("a"),ga=l("Evaluator"),ha=l(`.
Evaluators encapsulate a task and a default metric name. They leverage `),at=o("code"),va=l("pipeline"),_a=l(" functionalify from "),ot=o("code"),ba=l("transformers"),wa=l(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),$a=d(),_(Y.$$.fragment),St=d(),Ue=o("p"),ya=l("The base class for all evaluator classes:"),Ut=d(),k=o("div"),_(_e.$$.fragment),Ea=d(),st=o("p"),xa=l(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),qa=d(),Z=o("div"),_(be.$$.fragment),ka=d(),V=o("p"),ja=l("A core method of the "),nt=o("code"),Ta=l("Evaluator"),Pa=l(` class, computes the metric value for a pipeline and dataset compatible
with the task specified by the `),rt=o("code"),Ca=l("Evaluator"),Da=l("."),Na=d(),ee=o("div"),_(we.$$.fragment),Aa=d(),lt=o("p"),Ia=l("Compute and return metrics."),Fa=d(),te=o("div"),_($e.$$.fragment),Ma=d(),it=o("p"),La=l("Prepare data."),za=d(),ae=o("div"),_(ye.$$.fragment),Qa=d(),ct=o("p"),Oa=l("Prepare metric."),Sa=d(),oe=o("div"),_(Ee.$$.fragment),Ua=d(),dt=o("p"),Ra=l("Prepare pipeline."),Rt=d(),B=o("h2"),se=o("a"),pt=o("span"),_(xe.$$.fragment),Va=d(),ut=o("span"),Ba=l("The task specific evaluators"),Vt=d(),H=o("h3"),ne=o("a"),mt=o("span"),_(qe.$$.fragment),Ha=d(),ft=o("span"),Ga=l("ImageClassificationEvaluator"),Bt=d(),F=o("div"),_(ke.$$.fragment),Wa=d(),M=o("p"),Ja=l(`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Re=o("a"),Ka=l("evaluator()"),Xa=l(` using the default task name
`),gt=o("code"),Ya=l("image-classification"),Za=l(`.
Methods in this class assume a data format compatible with the `),ht=o("code"),eo=l("ImageClassificationPipeline"),to=l("."),ao=d(),z=o("div"),_(je.$$.fragment),oo=d(),vt=o("p"),so=l("Compute the metric for a given pipeline and dataset combination."),no=d(),_(re.$$.fragment),Ht=d(),G=o("h3"),le=o("a"),_t=o("span"),_(Te.$$.fragment),ro=d(),bt=o("span"),lo=l("QuestionAnsweringEvaluator"),Gt=d(),j=o("div"),_(Pe.$$.fragment),io=d(),Ce=o("p"),co=l(`Question answering evaluator. This evaluator handles
`),ie=o("a"),wt=o("strong"),po=l("extractive"),uo=l(" question answering"),mo=l(`,
where the answer to the question is extracted from a context.`),fo=d(),W=o("p"),go=l("This question answering evaluator can currently be loaded from "),Ve=o("a"),ho=l("evaluator()"),vo=l(` using the default task name
`),$t=o("code"),_o=l("question-answering"),bo=l("."),wo=d(),De=o("p"),$o=l(`Methods in this class assume a data format compatible with the
`),Ne=o("a"),yt=o("code"),yo=l("QuestionAnsweringPipeline"),Eo=l("."),xo=d(),P=o("div"),_(Ae.$$.fragment),qo=d(),Et=o("p"),ko=l("Compute the metric for a given pipeline and dataset combination."),jo=d(),_(ce.$$.fragment),To=d(),_(de.$$.fragment),Po=d(),_(pe.$$.fragment),Wt=d(),J=o("h3"),ue=o("a"),xt=o("span"),_(Ie.$$.fragment),Co=d(),qt=o("span"),Do=l("TextClassificationEvaluator"),Jt=d(),L=o("div"),_(Fe.$$.fragment),No=d(),D=o("p"),Ao=l(`Text classification evaluator.
This text classification evaluator can currently be loaded from `),Be=o("a"),Io=l("evaluator()"),Fo=l(` using the default task name
`),kt=o("code"),Mo=l("text-classification"),Lo=l(" or with a "),jt=o("code"),zo=l('"sentiment-analysis"'),Qo=l(` alias.
Methods in this class assume a data format compatible with the `),Tt=o("code"),Oo=l("TextClassificationPipeline"),So=l(` - a single textual
feature as input and a categorical label as output.`),Uo=d(),Q=o("div"),_(Me.$$.fragment),Ro=d(),Pt=o("p"),Vo=l("Compute the metric for a given pipeline and dataset combination."),Bo=d(),_(me.$$.fragment),this.h()},l(t){const g=zs('[data-svelte="svelte-1phssyn"]',document.head);c=s(g,"META",{name:!0,content:!0}),g.forEach(a),x=p(t),f=s(t,"H1",{class:!0});var Le=n(f);m=s(Le,"A",{id:!0,class:!0,href:!0});var Ct=n(m);q=s(Ct,"SPAN",{});var Dt=n(q);b(r.$$.fragment,Dt),Dt.forEach(a),Ct.forEach(a),h=p(Le),K=s(Le,"SPAN",{});var Nt=n(K);ia=i(Nt,"Evaluator"),Nt.forEach(a),Le.forEach(a),Lt=p(t),Qe=s(t,"P",{});var At=n(Qe);ca=i(At,"The evaluator classes for automatic evaluation."),At.forEach(a),zt=p(t),R=s(t,"H2",{class:!0});var ze=n(R);X=s(ze,"A",{id:!0,class:!0,href:!0});var Go=n(X);et=s(Go,"SPAN",{});var Wo=n(et);b(he.$$.fragment,Wo),Wo.forEach(a),Go.forEach(a),da=p(ze),tt=s(ze,"SPAN",{});var Jo=n(tt);pa=i(Jo,"Evaluator classes"),Jo.forEach(a),ze.forEach(a),Qt=p(t),Oe=s(t,"P",{});var Ko=n(Oe);ua=i(Ko,"The main entry point for using the evaluator:"),Ko.forEach(a),Ot=p(t),A=s(t,"DIV",{class:!0});var He=n(A);b(ve.$$.fragment,He),ma=p(He),I=s(He,"P",{});var fe=n(I);fa=i(fe,"Utility factory method to build an "),Se=s(fe,"A",{href:!0});var Xo=n(Se);ga=i(Xo,"Evaluator"),Xo.forEach(a),ha=i(fe,`.
Evaluators encapsulate a task and a default metric name. They leverage `),at=s(fe,"CODE",{});var Yo=n(at);va=i(Yo,"pipeline"),Yo.forEach(a),_a=i(fe," functionalify from "),ot=s(fe,"CODE",{});var Zo=n(ot);ba=i(Zo,"transformers"),Zo.forEach(a),wa=i(fe,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),fe.forEach(a),$a=p(He),b(Y.$$.fragment,He),He.forEach(a),St=p(t),Ue=s(t,"P",{});var es=n(Ue);ya=i(es,"The base class for all evaluator classes:"),es.forEach(a),Ut=p(t),k=s(t,"DIV",{class:!0});var C=n(k);b(_e.$$.fragment,C),Ea=p(C),st=s(C,"P",{});var ts=n(st);xa=i(ts,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),ts.forEach(a),qa=p(C),Z=s(C,"DIV",{class:!0});var Xt=n(Z);b(be.$$.fragment,Xt),ka=p(Xt),V=s(Xt,"P",{});var Ge=n(V);ja=i(Ge,"A core method of the "),nt=s(Ge,"CODE",{});var as=n(nt);Ta=i(as,"Evaluator"),as.forEach(a),Pa=i(Ge,` class, computes the metric value for a pipeline and dataset compatible
with the task specified by the `),rt=s(Ge,"CODE",{});var os=n(rt);Ca=i(os,"Evaluator"),os.forEach(a),Da=i(Ge,"."),Ge.forEach(a),Xt.forEach(a),Na=p(C),ee=s(C,"DIV",{class:!0});var Yt=n(ee);b(we.$$.fragment,Yt),Aa=p(Yt),lt=s(Yt,"P",{});var ss=n(lt);Ia=i(ss,"Compute and return metrics."),ss.forEach(a),Yt.forEach(a),Fa=p(C),te=s(C,"DIV",{class:!0});var Zt=n(te);b($e.$$.fragment,Zt),Ma=p(Zt),it=s(Zt,"P",{});var ns=n(it);La=i(ns,"Prepare data."),ns.forEach(a),Zt.forEach(a),za=p(C),ae=s(C,"DIV",{class:!0});var ea=n(ae);b(ye.$$.fragment,ea),Qa=p(ea),ct=s(ea,"P",{});var rs=n(ct);Oa=i(rs,"Prepare metric."),rs.forEach(a),ea.forEach(a),Sa=p(C),oe=s(C,"DIV",{class:!0});var ta=n(oe);b(Ee.$$.fragment,ta),Ua=p(ta),dt=s(ta,"P",{});var ls=n(dt);Ra=i(ls,"Prepare pipeline."),ls.forEach(a),ta.forEach(a),C.forEach(a),Rt=p(t),B=s(t,"H2",{class:!0});var aa=n(B);se=s(aa,"A",{id:!0,class:!0,href:!0});var is=n(se);pt=s(is,"SPAN",{});var cs=n(pt);b(xe.$$.fragment,cs),cs.forEach(a),is.forEach(a),Va=p(aa),ut=s(aa,"SPAN",{});var ds=n(ut);Ba=i(ds,"The task specific evaluators"),ds.forEach(a),aa.forEach(a),Vt=p(t),H=s(t,"H3",{class:!0});var oa=n(H);ne=s(oa,"A",{id:!0,class:!0,href:!0});var ps=n(ne);mt=s(ps,"SPAN",{});var us=n(mt);b(qe.$$.fragment,us),us.forEach(a),ps.forEach(a),Ha=p(oa),ft=s(oa,"SPAN",{});var ms=n(ft);Ga=i(ms,"ImageClassificationEvaluator"),ms.forEach(a),oa.forEach(a),Bt=p(t),F=s(t,"DIV",{class:!0});var We=n(F);b(ke.$$.fragment,We),Wa=p(We),M=s(We,"P",{});var ge=n(M);Ja=i(ge,`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Re=s(ge,"A",{href:!0});var fs=n(Re);Ka=i(fs,"evaluator()"),fs.forEach(a),Xa=i(ge,` using the default task name
`),gt=s(ge,"CODE",{});var gs=n(gt);Ya=i(gs,"image-classification"),gs.forEach(a),Za=i(ge,`.
Methods in this class assume a data format compatible with the `),ht=s(ge,"CODE",{});var hs=n(ht);eo=i(hs,"ImageClassificationPipeline"),hs.forEach(a),to=i(ge,"."),ge.forEach(a),ao=p(We),z=s(We,"DIV",{class:!0});var Je=n(z);b(je.$$.fragment,Je),oo=p(Je),vt=s(Je,"P",{});var vs=n(vt);so=i(vs,"Compute the metric for a given pipeline and dataset combination."),vs.forEach(a),no=p(Je),b(re.$$.fragment,Je),Je.forEach(a),We.forEach(a),Ht=p(t),G=s(t,"H3",{class:!0});var sa=n(G);le=s(sa,"A",{id:!0,class:!0,href:!0});var _s=n(le);_t=s(_s,"SPAN",{});var bs=n(_t);b(Te.$$.fragment,bs),bs.forEach(a),_s.forEach(a),ro=p(sa),bt=s(sa,"SPAN",{});var ws=n(bt);lo=i(ws,"QuestionAnsweringEvaluator"),ws.forEach(a),sa.forEach(a),Gt=p(t),j=s(t,"DIV",{class:!0});var O=n(j);b(Pe.$$.fragment,O),io=p(O),Ce=s(O,"P",{});var na=n(Ce);co=i(na,`Question answering evaluator. This evaluator handles
`),ie=s(na,"A",{href:!0,rel:!0});var Ho=n(ie);wt=s(Ho,"STRONG",{});var $s=n(wt);po=i($s,"extractive"),$s.forEach(a),uo=i(Ho," question answering"),Ho.forEach(a),mo=i(na,`,
where the answer to the question is extracted from a context.`),na.forEach(a),fo=p(O),W=s(O,"P",{});var Ke=n(W);go=i(Ke,"This question answering evaluator can currently be loaded from "),Ve=s(Ke,"A",{href:!0});var ys=n(Ve);ho=i(ys,"evaluator()"),ys.forEach(a),vo=i(Ke,` using the default task name
`),$t=s(Ke,"CODE",{});var Es=n($t);_o=i(Es,"question-answering"),Es.forEach(a),bo=i(Ke,"."),Ke.forEach(a),wo=p(O),De=s(O,"P",{});var ra=n(De);$o=i(ra,`Methods in this class assume a data format compatible with the
`),Ne=s(ra,"A",{href:!0,rel:!0});var xs=n(Ne);yt=s(xs,"CODE",{});var qs=n(yt);yo=i(qs,"QuestionAnsweringPipeline"),qs.forEach(a),xs.forEach(a),Eo=i(ra,"."),ra.forEach(a),xo=p(O),P=s(O,"DIV",{class:!0});var S=n(P);b(Ae.$$.fragment,S),qo=p(S),Et=s(S,"P",{});var ks=n(Et);ko=i(ks,"Compute the metric for a given pipeline and dataset combination."),ks.forEach(a),jo=p(S),b(ce.$$.fragment,S),To=p(S),b(de.$$.fragment,S),Po=p(S),b(pe.$$.fragment,S),S.forEach(a),O.forEach(a),Wt=p(t),J=s(t,"H3",{class:!0});var la=n(J);ue=s(la,"A",{id:!0,class:!0,href:!0});var js=n(ue);xt=s(js,"SPAN",{});var Ts=n(xt);b(Ie.$$.fragment,Ts),Ts.forEach(a),js.forEach(a),Co=p(la),qt=s(la,"SPAN",{});var Ps=n(qt);Do=i(Ps,"TextClassificationEvaluator"),Ps.forEach(a),la.forEach(a),Jt=p(t),L=s(t,"DIV",{class:!0});var Xe=n(L);b(Fe.$$.fragment,Xe),No=p(Xe),D=s(Xe,"P",{});var U=n(D);Ao=i(U,`Text classification evaluator.
This text classification evaluator can currently be loaded from `),Be=s(U,"A",{href:!0});var Cs=n(Be);Io=i(Cs,"evaluator()"),Cs.forEach(a),Fo=i(U,` using the default task name
`),kt=s(U,"CODE",{});var Ds=n(kt);Mo=i(Ds,"text-classification"),Ds.forEach(a),Lo=i(U," or with a "),jt=s(U,"CODE",{});var Ns=n(jt);zo=i(Ns,'"sentiment-analysis"'),Ns.forEach(a),Qo=i(U,` alias.
Methods in this class assume a data format compatible with the `),Tt=s(U,"CODE",{});var As=n(Tt);Oo=i(As,"TextClassificationPipeline"),As.forEach(a),So=i(U,` - a single textual
feature as input and a categorical label as output.`),U.forEach(a),Uo=p(Xe),Q=s(Xe,"DIV",{class:!0});var Ye=n(Q);b(Me.$$.fragment,Ye),Ro=p(Ye),Pt=s(Ye,"P",{});var Is=n(Pt);Vo=i(Is,"Compute the metric for a given pipeline and dataset combination."),Is.forEach(a),Bo=p(Ye),b(me.$$.fragment,Ye),Ye.forEach(a),Xe.forEach(a),this.h()},h(){u(c,"name","hf:doc:metadata"),u(c,"content",JSON.stringify(Ws)),u(m,"id","evaluator"),u(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(m,"href","#evaluator"),u(f,"class","relative group"),u(X,"id","evaluate.evaluator"),u(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(X,"href","#evaluate.evaluator"),u(R,"class","relative group"),u(Se,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.Evaluator"),u(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(se,"id","the-task-specific-evaluators"),u(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(se,"href","#the-task-specific-evaluators"),u(B,"class","relative group"),u(ne,"id","evaluate.ImageClassificationEvaluator"),u(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ne,"href","#evaluate.ImageClassificationEvaluator"),u(H,"class","relative group"),u(Re,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.evaluator"),u(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(le,"id","evaluate.QuestionAnsweringEvaluator"),u(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(le,"href","#evaluate.QuestionAnsweringEvaluator"),u(G,"class","relative group"),u(ie,"href","https://huggingface.co/docs/transformers/task_summary#extractive-question-answering"),u(ie,"rel","nofollow"),u(Ve,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.evaluator"),u(Ne,"href","https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline"),u(Ne,"rel","nofollow"),u(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ue,"id","evaluate.TextClassificationEvaluator"),u(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ue,"href","#evaluate.TextClassificationEvaluator"),u(J,"class","relative group"),u(Be,"href","/docs/evaluate/pr_179/en/package_reference/evaluator_classes#evaluate.evaluator"),u(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,g){e(document.head,c),v(t,x,g),v(t,f,g),e(f,m),e(m,q),w(r,q,null),e(f,h),e(f,K),e(K,ia),v(t,Lt,g),v(t,Qe,g),e(Qe,ca),v(t,zt,g),v(t,R,g),e(R,X),e(X,et),w(he,et,null),e(R,da),e(R,tt),e(tt,pa),v(t,Qt,g),v(t,Oe,g),e(Oe,ua),v(t,Ot,g),v(t,A,g),w(ve,A,null),e(A,ma),e(A,I),e(I,fa),e(I,Se),e(Se,ga),e(I,ha),e(I,at),e(at,va),e(I,_a),e(I,ot),e(ot,ba),e(I,wa),e(A,$a),w(Y,A,null),v(t,St,g),v(t,Ue,g),e(Ue,ya),v(t,Ut,g),v(t,k,g),w(_e,k,null),e(k,Ea),e(k,st),e(st,xa),e(k,qa),e(k,Z),w(be,Z,null),e(Z,ka),e(Z,V),e(V,ja),e(V,nt),e(nt,Ta),e(V,Pa),e(V,rt),e(rt,Ca),e(V,Da),e(k,Na),e(k,ee),w(we,ee,null),e(ee,Aa),e(ee,lt),e(lt,Ia),e(k,Fa),e(k,te),w($e,te,null),e(te,Ma),e(te,it),e(it,La),e(k,za),e(k,ae),w(ye,ae,null),e(ae,Qa),e(ae,ct),e(ct,Oa),e(k,Sa),e(k,oe),w(Ee,oe,null),e(oe,Ua),e(oe,dt),e(dt,Ra),v(t,Rt,g),v(t,B,g),e(B,se),e(se,pt),w(xe,pt,null),e(B,Va),e(B,ut),e(ut,Ba),v(t,Vt,g),v(t,H,g),e(H,ne),e(ne,mt),w(qe,mt,null),e(H,Ha),e(H,ft),e(ft,Ga),v(t,Bt,g),v(t,F,g),w(ke,F,null),e(F,Wa),e(F,M),e(M,Ja),e(M,Re),e(Re,Ka),e(M,Xa),e(M,gt),e(gt,Ya),e(M,Za),e(M,ht),e(ht,eo),e(M,to),e(F,ao),e(F,z),w(je,z,null),e(z,oo),e(z,vt),e(vt,so),e(z,no),w(re,z,null),v(t,Ht,g),v(t,G,g),e(G,le),e(le,_t),w(Te,_t,null),e(G,ro),e(G,bt),e(bt,lo),v(t,Gt,g),v(t,j,g),w(Pe,j,null),e(j,io),e(j,Ce),e(Ce,co),e(Ce,ie),e(ie,wt),e(wt,po),e(ie,uo),e(Ce,mo),e(j,fo),e(j,W),e(W,go),e(W,Ve),e(Ve,ho),e(W,vo),e(W,$t),e($t,_o),e(W,bo),e(j,wo),e(j,De),e(De,$o),e(De,Ne),e(Ne,yt),e(yt,yo),e(De,Eo),e(j,xo),e(j,P),w(Ae,P,null),e(P,qo),e(P,Et),e(Et,ko),e(P,jo),w(ce,P,null),e(P,To),w(de,P,null),e(P,Po),w(pe,P,null),v(t,Wt,g),v(t,J,g),e(J,ue),e(ue,xt),w(Ie,xt,null),e(J,Co),e(J,qt),e(qt,Do),v(t,Jt,g),v(t,L,g),w(Fe,L,null),e(L,No),e(L,D),e(D,Ao),e(D,Be),e(Be,Io),e(D,Fo),e(D,kt),e(kt,Mo),e(D,Lo),e(D,jt),e(jt,zo),e(D,Qo),e(D,Tt),e(Tt,Oo),e(D,So),e(L,Uo),e(L,Q),w(Me,Q,null),e(Q,Ro),e(Q,Pt),e(Pt,Vo),e(Q,Bo),w(me,Q,null),Kt=!0},p(t,[g]){const Le={};g&2&&(Le.$$scope={dirty:g,ctx:t}),Y.$set(Le);const Ct={};g&2&&(Ct.$$scope={dirty:g,ctx:t}),re.$set(Ct);const Dt={};g&2&&(Dt.$$scope={dirty:g,ctx:t}),ce.$set(Dt);const Nt={};g&2&&(Nt.$$scope={dirty:g,ctx:t}),de.$set(Nt);const At={};g&2&&(At.$$scope={dirty:g,ctx:t}),pe.$set(At);const ze={};g&2&&(ze.$$scope={dirty:g,ctx:t}),me.$set(ze)},i(t){Kt||($(r.$$.fragment,t),$(he.$$.fragment,t),$(ve.$$.fragment,t),$(Y.$$.fragment,t),$(_e.$$.fragment,t),$(be.$$.fragment,t),$(we.$$.fragment,t),$($e.$$.fragment,t),$(ye.$$.fragment,t),$(Ee.$$.fragment,t),$(xe.$$.fragment,t),$(qe.$$.fragment,t),$(ke.$$.fragment,t),$(je.$$.fragment,t),$(re.$$.fragment,t),$(Te.$$.fragment,t),$(Pe.$$.fragment,t),$(Ae.$$.fragment,t),$(ce.$$.fragment,t),$(de.$$.fragment,t),$(pe.$$.fragment,t),$(Ie.$$.fragment,t),$(Fe.$$.fragment,t),$(Me.$$.fragment,t),$(me.$$.fragment,t),Kt=!0)},o(t){y(r.$$.fragment,t),y(he.$$.fragment,t),y(ve.$$.fragment,t),y(Y.$$.fragment,t),y(_e.$$.fragment,t),y(be.$$.fragment,t),y(we.$$.fragment,t),y($e.$$.fragment,t),y(ye.$$.fragment,t),y(Ee.$$.fragment,t),y(xe.$$.fragment,t),y(qe.$$.fragment,t),y(ke.$$.fragment,t),y(je.$$.fragment,t),y(re.$$.fragment,t),y(Te.$$.fragment,t),y(Pe.$$.fragment,t),y(Ae.$$.fragment,t),y(ce.$$.fragment,t),y(de.$$.fragment,t),y(pe.$$.fragment,t),y(Ie.$$.fragment,t),y(Fe.$$.fragment,t),y(Me.$$.fragment,t),y(me.$$.fragment,t),Kt=!1},d(t){a(c),t&&a(x),t&&a(f),E(r),t&&a(Lt),t&&a(Qe),t&&a(zt),t&&a(R),E(he),t&&a(Qt),t&&a(Oe),t&&a(Ot),t&&a(A),E(ve),E(Y),t&&a(St),t&&a(Ue),t&&a(Ut),t&&a(k),E(_e),E(be),E(we),E($e),E(ye),E(Ee),t&&a(Rt),t&&a(B),E(xe),t&&a(Vt),t&&a(H),E(qe),t&&a(Bt),t&&a(F),E(ke),E(je),E(re),t&&a(Ht),t&&a(G),E(Te),t&&a(Gt),t&&a(j),E(Pe),E(Ae),E(ce),E(de),E(pe),t&&a(Wt),t&&a(J),E(Ie),t&&a(Jt),t&&a(L),E(Fe),E(Me),E(me)}}}const Ws={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"},{local:"the-task-specific-evaluators",sections:[{local:"evaluate.ImageClassificationEvaluator",title:"ImageClassificationEvaluator"},{local:"evaluate.QuestionAnsweringEvaluator",title:"QuestionAnsweringEvaluator"},{local:"evaluate.TextClassificationEvaluator",title:"TextClassificationEvaluator"}],title:"The task specific evaluators"}],title:"Evaluator"};function Js(T){return Qs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class an extends Fs{constructor(c){super();Ms(this,c,Js,Gs,Ls,{})}}export{an as default,Ws as metadata};
