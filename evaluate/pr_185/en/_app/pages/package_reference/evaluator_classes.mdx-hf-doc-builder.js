import{S as no,i as ro,s as lo,e as s,k as c,w as $,t as l,M as io,c as n,d as t,m as d,a as r,x as y,h as i,b as g,G as e,g as f,y as E,q as w,o as x,B as q,v as co,L as kt}from"../../chunks/vendor-hf-doc-builder.js";import{D as P}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Pt}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as so}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as jt}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function po(z){let u,T,h,p,b;return p=new Pt({props:{code:`from evaluate import evaluator
# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){u=s("p"),T=l("Examples:"),h=c(),$(p.$$.fragment)},l(o){u=n(o,"P",{});var v=r(u);T=i(v,"Examples:"),v.forEach(t),h=d(o),y(p.$$.fragment,o)},m(o,v){f(o,u,v),e(u,T),f(o,h,v),E(p,o,v),b=!0},p:kt,i(o){b||(w(p.$$.fragment,o),b=!0)},o(o){x(p.$$.fragment,o),b=!1},d(o){o&&t(u),o&&t(h),q(p,o)}}}function uo(z){let u,T,h,p,b;return p=new Pt({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("text-classification")
data =  Dataset.from_dict(load_dataset("imdb")["test"][:2])
results = e.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    input_column="text",
    label_column="label",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data =  Dataset.from_dict(load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)[<span class="hljs-string">&quot;test&quot;</span>][:<span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;text&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;label&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){u=s("p"),T=l("Examples:"),h=c(),$(p.$$.fragment)},l(o){u=n(o,"P",{});var v=r(u);T=i(v,"Examples:"),v.forEach(t),h=d(o),y(p.$$.fragment,o)},m(o,v){f(o,u,v),e(u,T),f(o,h,v),E(p,o,v),b=!0},p:kt,i(o){b||(w(p.$$.fragment,o),b=!0)},o(o){x(p.$$.fragment,o),b=!1},d(o){o&&t(u),o&&t(h),q(p,o)}}}function mo(z){let u,T,h,p,b;return p=new Pt({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("image-classification")
data =  Dataset.from_dict(load_dataset("beans")["test"][:2])
results = e.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    metric="accuracy",
    input_column="image",
    label_column="labels",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;image-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data =  Dataset.from_dict(load_dataset(<span class="hljs-string">&quot;beans&quot;</span>)[<span class="hljs-string">&quot;test&quot;</span>][:<span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;nateraw/vit-base-beans&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;image&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;labels&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&#x27;angular_leaf_spot&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;bean_rust&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>: <span class="hljs-number">2</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){u=s("p"),T=l("Examples:"),h=c(),$(p.$$.fragment)},l(o){u=n(o,"P",{});var v=r(u);T=i(v,"Examples:"),v.forEach(t),h=d(o),y(p.$$.fragment,o)},m(o,v){f(o,u,v),e(u,T),f(o,h,v),E(p,o,v),b=!0},p:kt,i(o){b||(w(p.$$.fragment,o),b=!0)},o(o){x(p.$$.fragment,o),b=!1},d(o){o&&t(u),o&&t(h),q(p,o)}}}function fo(z){let u,T,h,p,b,o,v,je,ma,Ke,fe,fa,Qe,O,R,ke,Z,ga,Pe,ha,Xe,ge,va,Ye,C,ee,_a,D,ba,he,$a,ya,Ce,Ea,wa,De,xa,qa,Ta,S,Ze,ve,ja,ea,_,ae,ka,Ne,Pa,Ca,V,te,Da,U,Na,Ie,Ia,Fa,Fe,Ma,La,Aa,B,oe,Oa,Me,Ua,za,H,se,Ra,ne,Sa,Le,Va,Ba,Ha,W,re,Wa,Ae,Ga,Ja,G,le,Ka,Oe,Qa,Xa,J,ie,Ya,Ue,Za,aa,_e,et,ta,N,ce,at,k,tt,be,ot,st,ze,nt,rt,Re,lt,it,Se,ct,dt,pt,M,de,ut,Ve,mt,ft,K,oa,I,pe,gt,F,ht,$e,vt,_t,Be,bt,$t,He,yt,Et,wt,L,ue,xt,We,qt,Tt,Q,sa;return o=new so({}),Z=new so({}),ee=new P({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:<ul>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/pr_185/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
<li><code>&quot;image-classification&quot;</code>: will return a <a href="/docs/evaluate/pr_185/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator">ImageClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/vr_185/src/evaluate/evaluator/__init__.py#L73",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/pr_185/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),S=new jt({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[po]},$$scope:{ctx:z}}}),ae=new P({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_185/src/evaluate/evaluator/base.py#L54"}}),te=new P({props:{name:"compute",anchor:"evaluate.Evaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"**compute_parameters",val:": typing.Dict"}],source:"https://github.com/huggingface/evaluate/blob/vr_185/src/evaluate/evaluator/base.py#L118"}}),oe=new P({props:{name:"compute_metric",anchor:"evaluate.Evaluator.compute_metric",parameters:[{name:"metric",val:": EvaluationModule"},{name:"metric_inputs",val:""},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_185/src/evaluate/evaluator/base.py#L236"}}),se=new P({props:{name:"predictions_processor",anchor:"evaluate.Evaluator.predictions_processor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/evaluate/blob/vr_185/src/evaluate/evaluator/base.py#L111"}}),re=new P({props:{name:"prepare_data",anchor:"evaluate.Evaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"input_column",val:": str"},{name:"label_column",val:": str"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_data.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.Evaluator.prepare_data.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.prepare_data.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/vr_185/src/evaluate/evaluator/base.py#L136",returnDescription:`
<p>Loaded <code>datasets.Dataset</code>.</p>
`}}),le=new P({props:{name:"prepare_metric",anchor:"evaluate.Evaluator.prepare_metric",parameters:[{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule]"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_metric.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"}],source:"https://github.com/huggingface/evaluate/blob/vr_185/src/evaluate/evaluator/base.py#L208",returnDescription:`
<p>The loaded metric.</p>
`}}),ie=new P({props:{name:"prepare_pipeline",anchor:"evaluate.Evaluator.prepare_pipeline",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"preprocessor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_pipeline.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.prepare_pipeline.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.Evaluator.prepare_pipeline.preprocessor",description:`<strong>preprocessor</strong> (<code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"preprocessor"}],source:"https://github.com/huggingface/evaluate/blob/vr_185/src/evaluate/evaluator/base.py#L167",returnDescription:`
<p>The initialized pipeline.</p>
`}}),ce=new P({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_185/src/evaluate/evaluator/text_classification.py#L39"}}),de=new P({props:{name:"compute",anchor:"evaluate.TextClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'text'"},{name:"label_column",val:": str = 'label'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"}],parametersDescription:[{anchor:"evaluate.TextClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.TextClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.TextClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.TextClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;&#x201D;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.TextClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TextClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TextClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TextClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_185/src/evaluate/evaluator/text_classification.py#L58",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),K=new jt({props:{anchor:"evaluate.TextClassificationEvaluator.compute.example",$$slots:{default:[uo]},$$scope:{ctx:z}}}),pe=new P({props:{name:"class evaluate.ImageClassificationEvaluator",anchor:"evaluate.ImageClassificationEvaluator",parameters:[{name:"task",val:" = 'image-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/vr_185/src/evaluate/evaluator/image_classification.py#L39"}}),ue=new P({props:{name:"compute",anchor:"evaluate.ImageClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"feature_extractor",val:": typing.Union[str, ForwardRef('FeatureExtractionMixin'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'image'"},{name:"label_column",val:": str = 'labels'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"}],parametersDescription:[{anchor:"evaluate.ImageClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.ImageClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>image-classification</code>. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.ImageClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.ImageClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>&#x201D; &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
feature_extractor &#x2014; (<code>str</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default feature extractor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.ImageClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.ImageClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.ImageClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.ImageClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;image&quot;</code>) &#x2014;
the name of the column containing the images as PIL ImageFile in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;labels&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/vr_185/src/evaluate/evaluator/image_classification.py#L54",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),Q=new jt({props:{anchor:"evaluate.ImageClassificationEvaluator.compute.example",$$slots:{default:[mo]},$$scope:{ctx:z}}}),{c(){u=s("meta"),T=c(),h=s("h1"),p=s("a"),b=s("span"),$(o.$$.fragment),v=c(),je=s("span"),ma=l("Evaluator"),Ke=c(),fe=s("p"),fa=l("The evaluator classes for automatic evaluation."),Qe=c(),O=s("h2"),R=s("a"),ke=s("span"),$(Z.$$.fragment),ga=c(),Pe=s("span"),ha=l("Evaluator classes"),Xe=c(),ge=s("p"),va=l("The main entry point for using the evaluator:"),Ye=c(),C=s("div"),$(ee.$$.fragment),_a=c(),D=s("p"),ba=l("Utility factory method to build an "),he=s("a"),$a=l("Evaluator"),ya=l(`.
Evaluators encapsulate a task and a default metric name. They leverage `),Ce=s("code"),Ea=l("pipeline"),wa=l(" functionalify from "),De=s("code"),xa=l("transformers"),qa=l(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),Ta=c(),$(S.$$.fragment),Ze=c(),ve=s("p"),ja=l("The base class for all evaluator classes:"),ea=c(),_=s("div"),$(ae.$$.fragment),ka=c(),Ne=s("p"),Pa=l(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),Ca=c(),V=s("div"),$(te.$$.fragment),Da=c(),U=s("p"),Na=l("A core method of the "),Ie=s("code"),Ia=l("Evaluator"),Fa=l(` class, computes the metric value for a pipeline and dataset compatible
with the task specified by the `),Fe=s("code"),Ma=l("Evaluator"),La=l("."),Aa=c(),B=s("div"),$(oe.$$.fragment),Oa=c(),Me=s("p"),Ua=l("Compute and return metrics."),za=c(),H=s("div"),$(se.$$.fragment),Ra=c(),ne=s("p"),Sa=l("A core method of the "),Le=s("code"),Va=l("Evaluator"),Ba=l(" class, which processes the pipeline outputs for compatibility with the metric."),Ha=c(),W=s("div"),$(re.$$.fragment),Wa=c(),Ae=s("p"),Ga=l("Prepare data."),Ja=c(),G=s("div"),$(le.$$.fragment),Ka=c(),Oe=s("p"),Qa=l("Prepare metric."),Xa=c(),J=s("div"),$(ie.$$.fragment),Ya=c(),Ue=s("p"),Za=l("Prepare pipeline."),aa=c(),_e=s("p"),et=l("The class for text classification evaluation:"),ta=c(),N=s("div"),$(ce.$$.fragment),at=c(),k=s("p"),tt=l(`Text classification evaluator.
This text classification evaluator can currently be loaded from `),be=s("a"),ot=l("evaluator()"),st=l(` using the default task name
`),ze=s("code"),nt=l("text-classification"),rt=l(" or with a "),Re=s("code"),lt=l('"sentiment-analysis"'),it=l(` alias.
Methods in this class assume a data format compatible with the `),Se=s("code"),ct=l("TextClassificationPipeline"),dt=l(` - a single textual
feature as input and a categorical label as output.`),pt=c(),M=s("div"),$(de.$$.fragment),ut=c(),Ve=s("p"),mt=l("Compute the metric for a given pipeline and dataset combination."),ft=c(),$(K.$$.fragment),oa=c(),I=s("div"),$(pe.$$.fragment),gt=c(),F=s("p"),ht=l(`Image classification evaluator.
This image classification evaluator can currently be loaded from `),$e=s("a"),vt=l("evaluator()"),_t=l(` using the default task name
`),Be=s("code"),bt=l("image-classification"),$t=l(`.
Methods in this class assume a data format compatible with the `),He=s("code"),yt=l("ImageClassificationPipeline"),Et=l("."),wt=c(),L=s("div"),$(ue.$$.fragment),xt=c(),We=s("p"),qt=l("Compute the metric for a given pipeline and dataset combination."),Tt=c(),$(Q.$$.fragment),this.h()},l(a){const m=io('[data-svelte="svelte-1phssyn"]',document.head);u=n(m,"META",{name:!0,content:!0}),m.forEach(t),T=d(a),h=n(a,"H1",{class:!0});var me=r(h);p=n(me,"A",{id:!0,class:!0,href:!0});var Ge=r(p);b=n(Ge,"SPAN",{});var Je=r(b);y(o.$$.fragment,Je),Je.forEach(t),Ge.forEach(t),v=d(me),je=n(me,"SPAN",{});var Ct=r(je);ma=i(Ct,"Evaluator"),Ct.forEach(t),me.forEach(t),Ke=d(a),fe=n(a,"P",{});var Dt=r(fe);fa=i(Dt,"The evaluator classes for automatic evaluation."),Dt.forEach(t),Qe=d(a),O=n(a,"H2",{class:!0});var na=r(O);R=n(na,"A",{id:!0,class:!0,href:!0});var Nt=r(R);ke=n(Nt,"SPAN",{});var It=r(ke);y(Z.$$.fragment,It),It.forEach(t),Nt.forEach(t),ga=d(na),Pe=n(na,"SPAN",{});var Ft=r(Pe);ha=i(Ft,"Evaluator classes"),Ft.forEach(t),na.forEach(t),Xe=d(a),ge=n(a,"P",{});var Mt=r(ge);va=i(Mt,"The main entry point for using the evaluator:"),Mt.forEach(t),Ye=d(a),C=n(a,"DIV",{class:!0});var ye=r(C);y(ee.$$.fragment,ye),_a=d(ye),D=n(ye,"P",{});var X=r(D);ba=i(X,"Utility factory method to build an "),he=n(X,"A",{href:!0});var Lt=r(he);$a=i(Lt,"Evaluator"),Lt.forEach(t),ya=i(X,`.
Evaluators encapsulate a task and a default metric name. They leverage `),Ce=n(X,"CODE",{});var At=r(Ce);Ea=i(At,"pipeline"),At.forEach(t),wa=i(X," functionalify from "),De=n(X,"CODE",{});var Ot=r(De);xa=i(Ot,"transformers"),Ot.forEach(t),qa=i(X,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),X.forEach(t),Ta=d(ye),y(S.$$.fragment,ye),ye.forEach(t),Ze=d(a),ve=n(a,"P",{});var Ut=r(ve);ja=i(Ut,"The base class for all evaluator classes:"),Ut.forEach(t),ea=d(a),_=n(a,"DIV",{class:!0});var j=r(_);y(ae.$$.fragment,j),ka=d(j),Ne=n(j,"P",{});var zt=r(Ne);Pa=i(zt,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),zt.forEach(t),Ca=d(j),V=n(j,"DIV",{class:!0});var ra=r(V);y(te.$$.fragment,ra),Da=d(ra),U=n(ra,"P",{});var Ee=r(U);Na=i(Ee,"A core method of the "),Ie=n(Ee,"CODE",{});var Rt=r(Ie);Ia=i(Rt,"Evaluator"),Rt.forEach(t),Fa=i(Ee,` class, computes the metric value for a pipeline and dataset compatible
with the task specified by the `),Fe=n(Ee,"CODE",{});var St=r(Fe);Ma=i(St,"Evaluator"),St.forEach(t),La=i(Ee,"."),Ee.forEach(t),ra.forEach(t),Aa=d(j),B=n(j,"DIV",{class:!0});var la=r(B);y(oe.$$.fragment,la),Oa=d(la),Me=n(la,"P",{});var Vt=r(Me);Ua=i(Vt,"Compute and return metrics."),Vt.forEach(t),la.forEach(t),za=d(j),H=n(j,"DIV",{class:!0});var ia=r(H);y(se.$$.fragment,ia),Ra=d(ia),ne=n(ia,"P",{});var ca=r(ne);Sa=i(ca,"A core method of the "),Le=n(ca,"CODE",{});var Bt=r(Le);Va=i(Bt,"Evaluator"),Bt.forEach(t),Ba=i(ca," class, which processes the pipeline outputs for compatibility with the metric."),ca.forEach(t),ia.forEach(t),Ha=d(j),W=n(j,"DIV",{class:!0});var da=r(W);y(re.$$.fragment,da),Wa=d(da),Ae=n(da,"P",{});var Ht=r(Ae);Ga=i(Ht,"Prepare data."),Ht.forEach(t),da.forEach(t),Ja=d(j),G=n(j,"DIV",{class:!0});var pa=r(G);y(le.$$.fragment,pa),Ka=d(pa),Oe=n(pa,"P",{});var Wt=r(Oe);Qa=i(Wt,"Prepare metric."),Wt.forEach(t),pa.forEach(t),Xa=d(j),J=n(j,"DIV",{class:!0});var ua=r(J);y(ie.$$.fragment,ua),Ya=d(ua),Ue=n(ua,"P",{});var Gt=r(Ue);Za=i(Gt,"Prepare pipeline."),Gt.forEach(t),ua.forEach(t),j.forEach(t),aa=d(a),_e=n(a,"P",{});var Jt=r(_e);et=i(Jt,"The class for text classification evaluation:"),Jt.forEach(t),ta=d(a),N=n(a,"DIV",{class:!0});var we=r(N);y(ce.$$.fragment,we),at=d(we),k=n(we,"P",{});var A=r(k);tt=i(A,`Text classification evaluator.
This text classification evaluator can currently be loaded from `),be=n(A,"A",{href:!0});var Kt=r(be);ot=i(Kt,"evaluator()"),Kt.forEach(t),st=i(A,` using the default task name
`),ze=n(A,"CODE",{});var Qt=r(ze);nt=i(Qt,"text-classification"),Qt.forEach(t),rt=i(A," or with a "),Re=n(A,"CODE",{});var Xt=r(Re);lt=i(Xt,'"sentiment-analysis"'),Xt.forEach(t),it=i(A,` alias.
Methods in this class assume a data format compatible with the `),Se=n(A,"CODE",{});var Yt=r(Se);ct=i(Yt,"TextClassificationPipeline"),Yt.forEach(t),dt=i(A,` - a single textual
feature as input and a categorical label as output.`),A.forEach(t),pt=d(we),M=n(we,"DIV",{class:!0});var xe=r(M);y(de.$$.fragment,xe),ut=d(xe),Ve=n(xe,"P",{});var Zt=r(Ve);mt=i(Zt,"Compute the metric for a given pipeline and dataset combination."),Zt.forEach(t),ft=d(xe),y(K.$$.fragment,xe),xe.forEach(t),we.forEach(t),oa=d(a),I=n(a,"DIV",{class:!0});var qe=r(I);y(pe.$$.fragment,qe),gt=d(qe),F=n(qe,"P",{});var Y=r(F);ht=i(Y,`Image classification evaluator.
This image classification evaluator can currently be loaded from `),$e=n(Y,"A",{href:!0});var eo=r($e);vt=i(eo,"evaluator()"),eo.forEach(t),_t=i(Y,` using the default task name
`),Be=n(Y,"CODE",{});var ao=r(Be);bt=i(ao,"image-classification"),ao.forEach(t),$t=i(Y,`.
Methods in this class assume a data format compatible with the `),He=n(Y,"CODE",{});var to=r(He);yt=i(to,"ImageClassificationPipeline"),to.forEach(t),Et=i(Y,"."),Y.forEach(t),wt=d(qe),L=n(qe,"DIV",{class:!0});var Te=r(L);y(ue.$$.fragment,Te),xt=d(Te),We=n(Te,"P",{});var oo=r(We);qt=i(oo,"Compute the metric for a given pipeline and dataset combination."),oo.forEach(t),Tt=d(Te),y(Q.$$.fragment,Te),Te.forEach(t),qe.forEach(t),this.h()},h(){g(u,"name","hf:doc:metadata"),g(u,"content",JSON.stringify(go)),g(p,"id","evaluator"),g(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(p,"href","#evaluator"),g(h,"class","relative group"),g(R,"id","evaluate.evaluator"),g(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(R,"href","#evaluate.evaluator"),g(O,"class","relative group"),g(he,"href","/docs/evaluate/pr_185/en/package_reference/evaluator_classes#evaluate.Evaluator"),g(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(_,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(be,"href","/docs/evaluate/pr_185/en/package_reference/evaluator_classes#evaluate.evaluator"),g(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g($e,"href","/docs/evaluate/pr_185/en/package_reference/evaluator_classes#evaluate.evaluator"),g(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(a,m){e(document.head,u),f(a,T,m),f(a,h,m),e(h,p),e(p,b),E(o,b,null),e(h,v),e(h,je),e(je,ma),f(a,Ke,m),f(a,fe,m),e(fe,fa),f(a,Qe,m),f(a,O,m),e(O,R),e(R,ke),E(Z,ke,null),e(O,ga),e(O,Pe),e(Pe,ha),f(a,Xe,m),f(a,ge,m),e(ge,va),f(a,Ye,m),f(a,C,m),E(ee,C,null),e(C,_a),e(C,D),e(D,ba),e(D,he),e(he,$a),e(D,ya),e(D,Ce),e(Ce,Ea),e(D,wa),e(D,De),e(De,xa),e(D,qa),e(C,Ta),E(S,C,null),f(a,Ze,m),f(a,ve,m),e(ve,ja),f(a,ea,m),f(a,_,m),E(ae,_,null),e(_,ka),e(_,Ne),e(Ne,Pa),e(_,Ca),e(_,V),E(te,V,null),e(V,Da),e(V,U),e(U,Na),e(U,Ie),e(Ie,Ia),e(U,Fa),e(U,Fe),e(Fe,Ma),e(U,La),e(_,Aa),e(_,B),E(oe,B,null),e(B,Oa),e(B,Me),e(Me,Ua),e(_,za),e(_,H),E(se,H,null),e(H,Ra),e(H,ne),e(ne,Sa),e(ne,Le),e(Le,Va),e(ne,Ba),e(_,Ha),e(_,W),E(re,W,null),e(W,Wa),e(W,Ae),e(Ae,Ga),e(_,Ja),e(_,G),E(le,G,null),e(G,Ka),e(G,Oe),e(Oe,Qa),e(_,Xa),e(_,J),E(ie,J,null),e(J,Ya),e(J,Ue),e(Ue,Za),f(a,aa,m),f(a,_e,m),e(_e,et),f(a,ta,m),f(a,N,m),E(ce,N,null),e(N,at),e(N,k),e(k,tt),e(k,be),e(be,ot),e(k,st),e(k,ze),e(ze,nt),e(k,rt),e(k,Re),e(Re,lt),e(k,it),e(k,Se),e(Se,ct),e(k,dt),e(N,pt),e(N,M),E(de,M,null),e(M,ut),e(M,Ve),e(Ve,mt),e(M,ft),E(K,M,null),f(a,oa,m),f(a,I,m),E(pe,I,null),e(I,gt),e(I,F),e(F,ht),e(F,$e),e($e,vt),e(F,_t),e(F,Be),e(Be,bt),e(F,$t),e(F,He),e(He,yt),e(F,Et),e(I,wt),e(I,L),E(ue,L,null),e(L,xt),e(L,We),e(We,qt),e(L,Tt),E(Q,L,null),sa=!0},p(a,[m]){const me={};m&2&&(me.$$scope={dirty:m,ctx:a}),S.$set(me);const Ge={};m&2&&(Ge.$$scope={dirty:m,ctx:a}),K.$set(Ge);const Je={};m&2&&(Je.$$scope={dirty:m,ctx:a}),Q.$set(Je)},i(a){sa||(w(o.$$.fragment,a),w(Z.$$.fragment,a),w(ee.$$.fragment,a),w(S.$$.fragment,a),w(ae.$$.fragment,a),w(te.$$.fragment,a),w(oe.$$.fragment,a),w(se.$$.fragment,a),w(re.$$.fragment,a),w(le.$$.fragment,a),w(ie.$$.fragment,a),w(ce.$$.fragment,a),w(de.$$.fragment,a),w(K.$$.fragment,a),w(pe.$$.fragment,a),w(ue.$$.fragment,a),w(Q.$$.fragment,a),sa=!0)},o(a){x(o.$$.fragment,a),x(Z.$$.fragment,a),x(ee.$$.fragment,a),x(S.$$.fragment,a),x(ae.$$.fragment,a),x(te.$$.fragment,a),x(oe.$$.fragment,a),x(se.$$.fragment,a),x(re.$$.fragment,a),x(le.$$.fragment,a),x(ie.$$.fragment,a),x(ce.$$.fragment,a),x(de.$$.fragment,a),x(K.$$.fragment,a),x(pe.$$.fragment,a),x(ue.$$.fragment,a),x(Q.$$.fragment,a),sa=!1},d(a){t(u),a&&t(T),a&&t(h),q(o),a&&t(Ke),a&&t(fe),a&&t(Qe),a&&t(O),q(Z),a&&t(Xe),a&&t(ge),a&&t(Ye),a&&t(C),q(ee),q(S),a&&t(Ze),a&&t(ve),a&&t(ea),a&&t(_),q(ae),q(te),q(oe),q(se),q(re),q(le),q(ie),a&&t(aa),a&&t(_e),a&&t(ta),a&&t(N),q(ce),q(de),q(K),a&&t(oa),a&&t(I),q(pe),q(ue),q(Q)}}}const go={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"}],title:"Evaluator"};function ho(z){return co(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Eo extends no{constructor(u){super();ro(this,u,ho,fo,lo,{})}}export{Eo as default,go as metadata};
