import{S as aV,i as nV,s as rV,e as r,k as p,w as _,t as i,M as oV,c as o,d as t,m as h,a as l,x as v,h as u,b as f,N as sV,G as e,g as m,y,q as E,o as w,B as b,v as lV,L as O}from"../chunks/vendor-hf-doc-builder.js";import{T as K}from"../chunks/Tip-hf-doc-builder.js";import{I as U}from"../chunks/IconCopyLink-hf-doc-builder.js";import{I as L,M as P,C as R}from"../chunks/InferenceApi-hf-doc-builder.js";function iV(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("bert-base-uncased"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"bert-base-uncased"),D.forEach(t),A=u(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/bert-base-uncased"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function uV(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/bert-base-uncased"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is [MASK]."})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is [MASK].&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function cV(q){let n,c;return n=new P({props:{$$slots:{default:[uV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function fV(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/bert-base-uncased",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is [MASK]."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;The answer to the universe is [MASK].&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:<span class="hljs-number">0.16963955760002136</span>,&quot;token&quot;:<span class="hljs-number">2053</span>,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:<span class="hljs-number">0.07344776391983032</span>,&quot;token&quot;:<span class="hljs-number">2498</span>,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:<span class="hljs-number">0.05803241208195686</span>,&quot;token&quot;:<span class="hljs-number">2748</span>,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:<span class="hljs-number">0.043957844376564026</span>,&quot;token&quot;:<span class="hljs-number">4242</span>,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:<span class="hljs-number">0.04015745222568512</span>,&quot;token&quot;:<span class="hljs-number">3722</span>,&quot;token_str&quot;:&quot;simple&quot;}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function pV(q){let n,c;return n=new P({props:{$$slots:{default:[fV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function hV(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is [MASK]."}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is [MASK].&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:0.16963955760002136,&quot;token&quot;:2053,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:0.07344776391983032,&quot;token&quot;:2498,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:0.05803241208195686,&quot;token&quot;:2748,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:0.043957844376564026,&quot;token&quot;:4242,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:0.04015745222568512,&quot;token&quot;:3722,&quot;token_str&quot;:&quot;simple&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function dV(q){let n,c;return n=new P({props:{$$slots:{default:[hV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function gV(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "sequence": "the answer to the universe is no.",
            "score": 0.1696,
            "token": 2053,
            "token_str": "no",
        },
        {
            "sequence": "the answer to the universe is nothing.",
            "score": 0.0734,
            "token": 2498,
            "token_str": "nothing",
        },
        {
            "sequence": "the answer to the universe is yes.",
            "score": 0.0580,
            "token": 2748,
            "token_str": "yes",
        },
        {
            "sequence": "the answer to the universe is unknown.",
            "score": 0.044,
            "token": 4242,
            "token_str": "unknown",
        },
        {
            "sequence": "the answer to the universe is simple.",
            "score": 0.0402,
            "token": 3722,
            "token_str": "simple",
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is no.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.1696</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2053</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;no&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is nothing.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0734</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2498</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;nothing&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is yes.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0580</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2748</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;yes&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is unknown.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.044</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">4242</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;unknown&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is simple.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0402</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">3722</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;simple&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function mV(q){let n,c;return n=new P({props:{$$slots:{default:[gV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function $V(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/bart-large-cnn"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/bart-large-cnn"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/bart-large-cnn"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function qV(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",
        "parameters": {"do_sample": False},
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "summary_text": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;do_sample&quot;</span>: <span class="hljs-literal">False</span>},
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;summary_text&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function _V(q){let n,c;return n=new P({props:{$$slots:{default:[qV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function vV(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-cnn",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres."}]`,highlighted:`import fetch from <span class="hljs-comment">&quot;node-fetch&quot;</span>;
async function query(data) {
    const response = await fetch(
        <span class="hljs-comment">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>,
        {
            headers: { <span class="hljs-type">Authorization</span>: \`<span class="hljs-type">Bearer</span> <span class="hljs-string">\${</span><span class="hljs-type">API_TOKEN</span>}\` },
            method: <span class="hljs-comment">&quot;POST&quot;</span>,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: <span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>}).then((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{<span class="hljs-comment">&quot;summary_text&quot;</span>:<span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres.&quot;</span>}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function yV(q){let n,c;return n=new P({props:{$$slots:{default:[vV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function EV(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d '{"inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.", "parameters": {"do_sample": false}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world."}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;, &quot;parameters&quot;: {&quot;do_sample&quot;: false}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;summary_text&quot;:&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function wV(q){let n,c;return n=new P({props:{$$slots:{default:[EV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function bV(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("deepset/roberta-base-squad2"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"deepset/roberta-base-squad2"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/deepset/roberta-base-squad2"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function TV(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "question": "What's my name?",
            "context": "My name is Clara and I live in Berkeley.",
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;question&quot;</span>: <span class="hljs-string">&quot;What&#x27;s my name?&quot;</span>,
            <span class="hljs-string">&quot;context&quot;</span>: <span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>,
        }
    }
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function jV(q){let n,c;return n=new P({props:{$$slots:{default:[TV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function kV(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{question:"What is my name?",context:"My name is Clara and I live in Berkeley."}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:{<span class="hljs-attr">question</span>:<span class="hljs-string">&quot;What is my name?&quot;</span>,<span class="hljs-attr">context</span>:<span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function AV(q){let n,c;return n=new P({props:{$$slots:{default:[kV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function DV(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d '{"inputs":{"question":"What is my name?","context":"My name is Clara and I live in Berkeley."}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;question&quot;:&quot;What is my name?&quot;,&quot;context&quot;:&quot;My name is Clara and I live in Berkeley.&quot;}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function OV(q){let n,c;return n=new P({props:{$$slots:{default:[DV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function PV(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data),
    {"score": 0.9327, "start": 11, "end": 16, "answer": "Clara"},
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9327</span>, <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;Clara&quot;</span>},
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function RV(q){let n,c;return n=new P({props:{$$slots:{default:[PV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function SV(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("google/tapas-base-finetuned-wtq"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"google/tapas-base-finetuned-wtq"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/google/tapas-base-finetuned-wtq"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function NV(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "query": "How many stars does the transformers repository have?",
            "table": {
                "Repository": ["Transformers", "Datasets", "Tokenizers"],
                "Stars": ["36542", "4512", "3934"],
                "Contributors": ["651", "77", "34"],
                "Programming language": [
                    "Python",
                    "Python",
                    "Rust, Python and NodeJS",
                ],
            },
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;query&quot;</span>: <span class="hljs-string">&quot;How many stars does the transformers repository have?&quot;</span>,
            <span class="hljs-string">&quot;table&quot;</span>: {
                <span class="hljs-string">&quot;Repository&quot;</span>: [<span class="hljs-string">&quot;Transformers&quot;</span>, <span class="hljs-string">&quot;Datasets&quot;</span>, <span class="hljs-string">&quot;Tokenizers&quot;</span>],
                <span class="hljs-string">&quot;Stars&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>, <span class="hljs-string">&quot;4512&quot;</span>, <span class="hljs-string">&quot;3934&quot;</span>],
                <span class="hljs-string">&quot;Contributors&quot;</span>: [<span class="hljs-string">&quot;651&quot;</span>, <span class="hljs-string">&quot;77&quot;</span>, <span class="hljs-string">&quot;34&quot;</span>],
                <span class="hljs-string">&quot;Programming language&quot;</span>: [
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Rust, Python and NodeJS&quot;</span>,
                ],
            },
        }
    }
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function xV(q){let n,c;return n=new P({props:{$$slots:{default:[NV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function IV(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{query:"How many stars does the transformers repository have?",table:{Repository:["Transformers","Datasets","Tokenizers"],Stars:["36542","4512","3934"],Contributors:["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:{query:&quot;How many stars does the transformers repository have?&quot;,<span class="hljs-keyword">table</span>:{Repository:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],Stars:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],Contributors:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function HV(q){let n,c;return n=new P({props:{$$slots:{default:[IV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function BV(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d '{"inputs":{"query":"How many stars does the transformers repository have?","table":{"Repository":["Transformers","Datasets","Tokenizers"],"Stars":["36542","4512","3934"],"Contributors":["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;query&quot;:&quot;How many stars does the transformers repository have?&quot;,&quot;table&quot;:{&quot;Repository&quot;:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],&quot;Stars&quot;:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],&quot;Contributors&quot;:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[0,1]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function CV(q){let n,c;return n=new P({props:{$$slots:{default:[BV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function GV(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    data,
    {
        "answer": "AVERAGE > 36542",
        "coordinates": [[0, 1]],
        "cells": ["36542"],
        "aggregator": "AVERAGE",
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;AVERAGE &gt; 36542&quot;</span>,
        <span class="hljs-string">&quot;coordinates&quot;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]],
        <span class="hljs-string">&quot;cells&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>],
        <span class="hljs-string">&quot;aggregator&quot;</span>: <span class="hljs-string">&quot;AVERAGE&quot;</span>,
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function LV(q){let n,c;return n=new P({props:{$$slots:{default:[GV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function MV(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("sentence-transformers/all-MiniLM-L6-v2"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"sentence-transformers/all-MiniLM-L6-v2"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function UV(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "source_sentence": "That is a happy person",
            "sentences": ["That is a happy dog", "That is a very happy person", "Today is a sunny day"],
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;source_sentence&quot;</span>: <span class="hljs-string">&quot;That is a happy person&quot;</span>,
            <span class="hljs-string">&quot;sentences&quot;</span>: [<span class="hljs-string">&quot;That is a happy dog&quot;</span>, <span class="hljs-string">&quot;That is a very happy person&quot;</span>, <span class="hljs-string">&quot;Today is a sunny day&quot;</span>],
        }
    }
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function zV(q){let n,c;return n=new P({props:{$$slots:{default:[UV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function KV(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{
    source_sentence: "That is a happy person",
    sentences: [
        "That is a happy dog",
        "That is a very happy person",
        "Today is a sunny day"
    ]
}}).then((response) => {
    console.log(JSON.stringify(response));
});
// [0.6945773363113403,0.9429150819778442,0.2568760812282562]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:{
    <span class="hljs-attr">source_sentence</span>: <span class="hljs-string">&quot;That is a happy person&quot;</span>,
    <span class="hljs-attr">sentences</span>: [
        <span class="hljs-string">&quot;That is a happy dog&quot;</span>,
        <span class="hljs-string">&quot;That is a very happy person&quot;</span>,
        <span class="hljs-string">&quot;Today is a sunny day&quot;</span>
    ]
}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [0.6945773363113403,0.9429150819778442,0.2568760812282562]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function FV(q){let n,c;return n=new P({props:{$$slots:{default:[KV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function JV(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2 \\
        -X POST \\
        -d '{"inputs":{"source_sentence": "That is a happy person", "sentences": ["That is a happy dog","That is a very happy person","Today is a sunny day"]}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [0.6945773363113403,0.9429150819778442,0.2568760812282562]`,highlighted:`curl https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;source_sentence&quot;: &quot;That is a happy person&quot;, &quot;sentences&quot;: [&quot;That is a happy dog&quot;,&quot;That is a very happy person&quot;,&quot;Today is a sunny day&quot;]}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [0.6945773363113403,0.9429150819778442,0.2568760812282562]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function WV(q){let n,c;return n=new P({props:{$$slots:{default:[JV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function YV(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    data,
    [0.6945773363113403, 0.9429150819778442, 0.2568760812282562],
)`,highlighted:`self.assertEqual(
    data,
    [<span class="hljs-number">0.6945773363113403</span>, <span class="hljs-number">0.9429150819778442</span>, <span class="hljs-number">0.2568760812282562</span>],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function VV(q){let n,c;return n=new P({props:{$$slots:{default:[YV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function XV(q){let n,c,s,d,$,k;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("distilbert-base-uncased-finetuned-sst-2-english"),this.h()},l(A){n=o(A,"P",{});var j=l(n);c=o(j,"STRONG",{});var T=l(c);s=u(T,"Recommended model"),T.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var S=l($);k=u(S,"distilbert-base-uncased-finetuned-sst-2-english"),S.forEach(t),j.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),f($,"rel","nofollow")},m(A,j){m(A,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k)},d(A){A&&t(n)}}}function QV(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "I like you. I love you"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;I like you. I love you&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function ZV(q){let n,c;return n=new P({props:{$$slots:{default:[QV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function eX(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"I like you. I love you"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [[{"label":"POSITIVE","score":0.9998738765716553},{"label":"NEGATIVE","score":0.0001261125144083053}]]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;I like you. I love you&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [[{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553},{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053}]]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function tX(q){let n,c;return n=new P({props:{$$slots:{default:[eX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function sX(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d '{"inputs":"I like you. I love you"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [[{"label":"POSITIVE","score":0.9998738765716553},{"label":"NEGATIVE","score":0.0001261125144083053}]]`,highlighted:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;I like you. I love you&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [[{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553},{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053}]]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function aX(q){let n,c;return n=new P({props:{$$slots:{default:[sX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function nX(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data),
    [
        [
            {"label": "POSITIVE", "score": 0.9999},
            {"label": "NEGATIVE", "score": 0.0001},
        ]
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        [
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9999</span>},
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;NEGATIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0001</span>},
        ]
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function rX(q){let n,c;return n=new P({props:{$$slots:{default:[nX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function oX(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(": "),$=r("a"),k=i("gpt2"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,": "),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"gpt2"),D.forEach(t),A=u(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/gpt2"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function lX(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/gpt2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function iX(q){let n,c;return n=new P({props:{$$slots:{default:[lX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function uX(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/gpt2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;The answer to the universe is&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function cX(q){let n,c;return n=new P({props:{$$slots:{default:[uX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function fX(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function pX(q){let n,c;return n=new P({props:{$$slots:{default:[fX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function hX(q){let n,c;return n=new R({props:{code:`data == [
    {
        "generated_text": 'The answer to the universe is that we are the creation of the entire universe," says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.'
    }
]`,highlighted:`data == [
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&#x27;The answer to the universe is that we are the creation of the entire universe,&quot; says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.&#x27;</span>
    }
]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function dX(q){let n,c;return n=new P({props:{$$slots:{default:[hX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function gX(q){let n,c,s,d,$,k;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("dbmdz/bert-large-cased-finetuned-conll03-english"),this.h()},l(A){n=o(A,"P",{});var j=l(n);c=o(j,"STRONG",{});var T=l(c);s=u(T,"Recommended model"),T.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var S=l($);k=u(S,"dbmdz/bert-large-cased-finetuned-conll03-english"),S.forEach(t),j.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"),f($,"rel","nofollow")},m(A,j){m(A,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k)},d(A){A&&t(n)}}}function mX(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "My name is Sarah Jessica Parker but you can call me Jessica"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function $X(q){let n,c;return n=new P({props:{$$slots:{default:[mX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function qX(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"My name is Sarah Jessica Parker but you can call me Jessica"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9991337060928345</span>,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:<span class="hljs-number">11</span>,&quot;end&quot;:<span class="hljs-number">31</span>},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9979912042617798</span>,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:<span class="hljs-number">52</span>,&quot;end&quot;:<span class="hljs-number">59</span>}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function _X(q){let n,c;return n=new P({props:{$$slots:{default:[qX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function vX(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d '{"inputs":"My name is Sarah Jessica Parker but you can call me Jessica"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9991337060928345,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:11,&quot;end&quot;:31},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9979912042617798,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:52,&quot;end&quot;:59}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function yX(q){let n,c;return n=new P({props:{$$slots:{default:[vX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function EX(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "entity_group": "PER",
            "score": 0.9991,
            "word": "Sarah Jessica Parker",
            "start": 11,
            "end": 31,
        },
        {
            "entity_group": "PER",
            "score": 0.998,
            "word": "Jessica",
            "start": 52,
            "end": 59,
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9991</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Sarah Jessica Parker&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">31</span>,
        },
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.998</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Jessica&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">52</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">59</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function wX(q){let n,c;return n=new P({props:{$$slots:{default:[EX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function bX(q){let n,c,s,d,$,k,A,j,T,S,D,le,Ne;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("Helsinki-NLP/opus-mt-ru-en"),A=i(`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=r("strong"),T=i("Recommended model"),S=i(": "),D=r("a"),le=i("t5-base"),Ne=i("."),this.h()},l(ee){n=o(ee,"P",{});var V=l(n);c=o(V,"STRONG",{});var pt=l(c);s=u(pt,"Recommended model"),pt.forEach(t),d=u(V,`:
`),$=o(V,"A",{href:!0,rel:!0});var _u=l($);k=u(_u,"Helsinki-NLP/opus-mt-ru-en"),_u.forEach(t),A=u(V,`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=o(V,"STRONG",{});var qn=l(j);T=u(qn,"Recommended model"),qn.forEach(t),S=u(V,": "),D=o(V,"A",{href:!0,rel:!0});var xe=l(D);le=u(xe,"t5-base"),xe.forEach(t),Ne=u(V,"."),V.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/Helsinki-NLP/opus-mt-ru-en"),f($,"rel","nofollow"),f(D,"href","https://huggingface.co/t5-base"),f(D,"rel","nofollow")},m(ee,V){m(ee,n,V),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A),e(n,j),e(j,T),e(n,S),e(n,D),e(D,le),e(n,Ne)},d(ee){ee&&t(n)}}}function TX(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435",
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "translation_text": "My name is Wolfgang and I live in Berlin.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>,
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;translation_text&quot;</span>: <span class="hljs-string">&quot;My name is Wolfgang and I live in Berlin.&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function jX(q){let n,c;return n=new P({props:{$$slots:{default:[TX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function kX(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function AX(q){let n,c;return n=new P({props:{$$slots:{default:[kX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function DX(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d '{"inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function OX(q){let n,c;return n=new P({props:{$$slots:{default:[DX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function PX(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/bart-large-mnli"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/bart-large-mnli"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/bart-large-mnli"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function RX(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-mnli"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "parameters": {"candidate_labels": ["refund", "legal", "faq"]},
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;candidate_labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]},
    }
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function SX(q){let n,c;return n=new P({props:{$$slots:{default:[RX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function NX(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-mnli",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", parameters: {candidate_labels: ["refund", "legal", "faq"]}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>, <span class="hljs-attr">parameters</span>: {<span class="hljs-attr">candidate_labels</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function xX(q){let n,c;return n=new P({props:{$$slots:{default:[NX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function IX(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d '{"inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;, &quot;parameters&quot;: {&quot;candidate_labels&quot;: [&quot;refund&quot;, &quot;legal&quot;, &quot;faq&quot;]}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function HX(q){let n,c;return n=new P({props:{$$slots:{default:[IX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function BX(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data),
    {
        "sequence": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "labels": ["refund", "faq", "legal"],
        "scores": [
            # 88% refund
            0.8778,
            0.1052,
            0.017,
        ],
    },
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {
        <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>],
        <span class="hljs-string">&quot;scores&quot;</span>: [
            <span class="hljs-comment"># 88% refund</span>
            <span class="hljs-number">0.8778</span>,
            <span class="hljs-number">0.1052</span>,
            <span class="hljs-number">0.017</span>,
        ],
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function CX(q){let n,c;return n=new P({props:{$$slots:{default:[BX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function GX(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("microsoft/DialoGPT-large"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"microsoft/DialoGPT-large"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/microsoft/DialoGPT-large"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function LX(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "past_user_inputs": ["Which movie is the best ?"],
            "generated_responses": ["It's Die Hard for sure."],
            "text": "Can you explain why ?",
        },
    }
)
# Response
self.assertEqual(
    data,
    {
        "generated_text": "It's the best movie ever.",
        "conversation": {
            "past_user_inputs": [
                "Which movie is the best ?",
                "Can you explain why ?",
            ],
            "generated_responses": [
                "It's Die Hard for sure.",
                "It's the best movie ever.",
            ],
        },
        "warnings": ["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."],
    },
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [<span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>],
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
        },
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
        <span class="hljs-string">&quot;conversation&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [
                <span class="hljs-string">&quot;Which movie is the best ?&quot;</span>,
                <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
            ],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [
                <span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>,
                <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
            ],
        },
        <span class="hljs-string">&quot;warnings&quot;</span>: [<span class="hljs-string">&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;</span>],
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function MX(q){let n,c;return n=new P({props:{$$slots:{default:[LX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function UX(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: {past_user_inputs: ["Which movie is the best ?"], generated_responses: ["It is Die Hard for sure."], text:"Can you explain why ?"}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs: {past_user_inputs: [&quot;Which movie is the best ?&quot;], generated_responses: [&quot;It is Die Hard for sure.&quot;], <span class="hljs-type">text</span>:&quot;Can you explain why ?&quot;}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;]}`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function zX(q){let n,c;return n=new P({props:{$$slots:{default:[UX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function KX(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\
        -X POST \\
        -d '{"inputs": {"past_user_inputs": ["Which movie is the best ?"], "generated_responses": ["It is Die Hard for sure."], "text":"Can you explain why ?"}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:'curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\\n        -X POST \\\n        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: {&quot;past_user_inputs&quot;: [&quot;Which movie is the best ?&quot;], &quot;generated_responses&quot;: [&quot;It is Die Hard for sure.&quot;], &quot;text&quot;:&quot;Can you explain why ?&quot;}}&#x27;</span> \\\n        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">${HF_API_TOKEN}</span>&quot;</span>\n<span class="hljs-comment"># {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.&quot;]}</span>'}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function FX(q){let n,c;return n=new P({props:{$$slots:{default:[KX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function JX(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("Sentence-transformers"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"Sentence-transformers"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function WX(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(": "),$=r("a"),k=i(`Check your
langage`),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,": "),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,`Check your
langage`),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function YX(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("English"),d=i(`:
`),$=r("a"),k=i("facebook/wav2vec2-large-960h-lv60-self"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"English"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/wav2vec2-large-960h-lv60-self"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function VX(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("sample1.flac")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function XX(q){let n,c;return n=new P({props:{$$slots:{default:[VX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function QX(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("sample1.flac").then((response) => {
    console.log(JSON.stringify(response));
});
// {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;fs&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>) {
    <span class="hljs-keyword">const</span> data = fs.<span class="hljs-title function_">readFileSync</span>(filename);
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: data,
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>(<span class="hljs-string">&quot;sample1.flac&quot;</span>).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function ZX(q){let n,c;return n=new P({props:{$$slots:{default:[QX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function eQ(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
        -X POST \\
        --data-binary '@sample1.flac' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function tQ(q){let n,c;return n=new P({props:{$$slots:{default:[eQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function sQ(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    data,
    {
        "text": "GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;</span>
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function aQ(q){let n,c;return n=new P({props:{$$slots:{default:[sQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function nQ(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("superb/hubert-large-superb-er"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"superb/hubert-large-superb-er"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/superb/hubert-large-superb-er"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function rQ(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("sample1.flac")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function oQ(q){let n,c;return n=new P({props:{$$slots:{default:[rQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function lQ(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("sample1.flac").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;sample1.flac&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.5927661657333374</span>,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:<span class="hljs-number">0.2002529799938202</span>,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:<span class="hljs-number">0.12795612215995789</span>,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:<span class="hljs-number">0.07902472466230392</span>,&quot;label&quot;:&quot;sad&quot;}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function iQ(q){let n,c;return n=new P({props:{$$slots:{default:[lQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function uQ(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
        -X POST \\
        --data-binary '@sample1.flac' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.5927661657333374,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:0.2002529799938202,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:0.12795612215995789,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:0.07902472466230392,&quot;label&quot;:&quot;sad&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function cQ(q){let n,c;return n=new P({props:{$$slots:{default:[uQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function fQ(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {"score": 0.5928, "label": "neu"},
        {"score": 0.2003, "label": "hap"},
        {"score": 0.128, "label": "ang"},
        {"score": 0.079, "label": "sad"},
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.5928</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;neu&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.2003</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;hap&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.128</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;ang&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.079</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;sad&quot;</span>},
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function pQ(q){let n,c;return n=new P({props:{$$slots:{default:[fQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function hQ(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("google/vit-base-patch16-224"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"google/vit-base-patch16-224"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/google/vit-base-patch16-224"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function dQ(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/vit-base-patch16-224"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/vit-base-patch16-224&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function gQ(q){let n,c;return n=new P({props:{$$slots:{default:[dQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function mQ(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/vit-base-patch16-224",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.9374412894248962,"label":"Egyptian cat"},{"score":0.03844260051846504,"label":"tabby, tabby cat"},{"score":0.014411412179470062,"label":"tiger cat"},{"score":0.003274323185905814,"label":"lynx, catamount"},{"score":0.0006795919616706669,"label":"Siamese cat, Siamese"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/vit-base-patch16-224&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.9374412894248962</span>,&quot;label&quot;:&quot;Egyptian cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.03844260051846504</span>,&quot;label&quot;:&quot;tabby, tabby cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.014411412179470062</span>,&quot;label&quot;:&quot;tiger cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.003274323185905814</span>,&quot;label&quot;:&quot;lynx, catamount&quot;},{&quot;score&quot;:<span class="hljs-number">0.0006795919616706669</span>,&quot;label&quot;:&quot;Siamese cat, Siamese&quot;}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function $Q(q){let n,c;return n=new P({props:{$$slots:{default:[mQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function qQ(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/google/vit-base-patch16-224 \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.9374412894248962,"label":"Egyptian cat"},{"score":0.03844260051846504,"label":"tabby, tabby cat"},{"score":0.014411412179470062,"label":"tiger cat"},{"score":0.003274323185905814,"label":"lynx, catamount"},{"score":0.0006795919616706669,"label":"Siamese cat, Siamese"}]`,highlighted:`curl https://api-inference.huggingface.co/models/google/vit-base-patch16-224 \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.9374412894248962,&quot;label&quot;:&quot;Egyptian cat&quot;},{&quot;score&quot;:0.03844260051846504,&quot;label&quot;:&quot;tabby, tabby cat&quot;},{&quot;score&quot;:0.014411412179470062,&quot;label&quot;:&quot;tiger cat&quot;},{&quot;score&quot;:0.003274323185905814,&quot;label&quot;:&quot;lynx, catamount&quot;},{&quot;score&quot;:0.0006795919616706669,&quot;label&quot;:&quot;Siamese cat, Siamese&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function _Q(q){let n,c;return n=new P({props:{$$slots:{default:[qQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function vQ(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {"score": 0.9374, "label": "Egyptian cat"},
        {"score": 0.0384, "label": "tabby, tabby cat"},
        {"score": 0.0144, "label": "tiger cat"},
        {"score": 0.0033, "label": "lynx, catamount"},
        {"score": 0.0007, "label": "Siamese cat, Siamese"},
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9374</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;Egyptian cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0384</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;tabby, tabby cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0144</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;tiger cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0033</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;lynx, catamount&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0007</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;Siamese cat, Siamese&quot;</span>},
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function yQ(q){let n,c;return n=new P({props:{$$slots:{default:[vQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function EQ(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/detr-resnet-50"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/detr-resnet-50"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/detr-resnet-50"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function wQ(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function bQ(q){let n,c;return n=new P({props:{$$slots:{default:[wQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function TQ(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/detr-resnet-50",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.9982201457023621</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">40</span>,&quot;ymin&quot;:<span class="hljs-number">70</span>,&quot;xmax&quot;:<span class="hljs-number">175</span>,&quot;ymax&quot;:<span class="hljs-number">117</span>}},{&quot;score&quot;:<span class="hljs-number">0.9960021376609802</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">333</span>,&quot;ymin&quot;:<span class="hljs-number">72</span>,&quot;xmax&quot;:<span class="hljs-number">368</span>,&quot;ymax&quot;:<span class="hljs-number">187</span>}},{&quot;score&quot;:<span class="hljs-number">0.9954745173454285</span>,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">0</span>,&quot;ymin&quot;:<span class="hljs-number">1</span>,&quot;xmax&quot;:<span class="hljs-number">639</span>,&quot;ymax&quot;:<span class="hljs-number">473</span>}},{&quot;score&quot;:<span class="hljs-number">0.9988006353378296</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">13</span>,&quot;ymin&quot;:<span class="hljs-number">52</span>,&quot;xmax&quot;:<span class="hljs-number">314</span>,&quot;ymax&quot;:<span class="hljs-number">470</span>}},{&quot;score&quot;:<span class="hljs-number">0.9986783862113953</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">345</span>,&quot;ymin&quot;:<span class="hljs-number">23</span>,&quot;xmax&quot;:<span class="hljs-number">640</span>,&quot;ymax&quot;:<span class="hljs-number">368</span>}}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function jQ(q){let n,c;return n=new P({props:{$$slots:{default:[TQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function kQ(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.9982201457023621,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:40,&quot;ymin&quot;:70,&quot;xmax&quot;:175,&quot;ymax&quot;:117}},{&quot;score&quot;:0.9960021376609802,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:333,&quot;ymin&quot;:72,&quot;xmax&quot;:368,&quot;ymax&quot;:187}},{&quot;score&quot;:0.9954745173454285,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:0,&quot;ymin&quot;:1,&quot;xmax&quot;:639,&quot;ymax&quot;:473}},{&quot;score&quot;:0.9988006353378296,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:13,&quot;ymin&quot;:52,&quot;xmax&quot;:314,&quot;ymax&quot;:470}},{&quot;score&quot;:0.9986783862113953,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:345,&quot;ymin&quot;:23,&quot;xmax&quot;:640,&quot;ymax&quot;:368}}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function AQ(q){let n,c;return n=new P({props:{$$slots:{default:[kQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function DQ(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {
            "score": 0.9982,
            "label": "remote",
            "box": {"xmin": 40, "ymin": 70, "xmax": 175, "ymax": 117},
        },
        {
            "score": 0.9960,
            "label": "remote",
            "box": {"xmin": 333, "ymin": 72, "xmax": 368, "ymax": 187},
        },
        {
            "score": 0.9955,
            "label": "couch",
            "box": {"xmin": 0, "ymin": 1, "xmax": 639, "ymax": 473},
        },
        {
            "score": 0.9988,
            "label": "cat",
            "box": {"xmin": 13, "ymin": 52, "xmax": 314, "ymax": 470},
        },
        {
            "score": 0.9987,
            "label": "cat",
            "box": {"xmin": 345, "ymin": 23, "xmax": 640, "ymax": 368},
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9982</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">40</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">70</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">175</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">117</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9960</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">333</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">72</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">368</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">187</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9955</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;couch&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">639</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">473</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9988</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">52</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">314</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">470</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9987</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">345</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">23</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">640</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">368</span>},
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function OQ(q){let n,c;return n=new P({props:{$$slots:{default:[DQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function PQ(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/detr-resnet-50-panoptic"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/detr-resnet-50-panoptic"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/detr-resnet-50-panoptic"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function RQ(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function SQ(q){let n,c;return n=new P({props:{$$slots:{default:[RQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function NQ(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score": 0.9094282388687134, "label": "blanket", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9940965175628662, "label": "cat", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9986692667007446, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994757771492004, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9722068309783936, "label": "couch", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994235038757324, "label": "cat", "mask": "BASE64ENCODED_MASK"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;: <span class="hljs-number">0.9094282388687134</span>, &quot;label&quot;: &quot;blanket&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9940965175628662</span>, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9986692667007446</span>, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9994757771492004</span>, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9722068309783936</span>, &quot;label&quot;: &quot;couch&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9994235038757324</span>, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function xQ(q){let n,c;return n=new P({props:{$$slots:{default:[NQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function IQ(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score": 0.9094282388687134, "label": "blanket", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9940965175628662, "label": "cat", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9986692667007446, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994757771492004, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9722068309783936, "label": "couch", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994235038757324, "label": "cat", "mask": "BASE64ENCODED_MASK"}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;: 0.9094282388687134, &quot;label&quot;: &quot;blanket&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9940965175628662, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9986692667007446, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9994757771492004, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9722068309783936, &quot;label&quot;: &quot;couch&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9994235038757324, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function HQ(q){let n,c;return n=new P({props:{$$slots:{default:[IQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function BQ(q){let n,c;return n=new R({props:{code:`import base64
from io import BytesIO
from PIL import Image
with Image.open("cats.jpg") as img:
    masks = [d["mask"] for d in data]
    self.assertEqual(img.size, (640, 480))
    mask_imgs = [Image.open(BytesIO(base64.b64decode(mask))) for mask in masks]
    for mask_img in mask_imgs:
        self.assertEqual(mask_img.size, img.size)
        self.assertEqual(mask_img.mode, "L")  # L (8-bit pixels, black and white)
    first_mask_img = mask_imgs[0]
    min_pxl_val, max_pxl_val = first_mask_img.getextrema()
    self.assertGreaterEqual(min_pxl_val, 0)
    self.assertLessEqual(max_pxl_val, 255)`,highlighted:`<span class="hljs-keyword">import</span> base64
<span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">with</span> Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;cats.jpg&quot;</span>) <span class="hljs-keyword">as</span> img:
    masks = [d[<span class="hljs-string">&quot;mask&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> data]
    self.assertEqual(img.size, (<span class="hljs-number">640</span>, <span class="hljs-number">480</span>))
    mask_imgs = [Image.<span class="hljs-built_in">open</span>(BytesIO(base64.b64decode(mask))) <span class="hljs-keyword">for</span> mask <span class="hljs-keyword">in</span> masks]
    <span class="hljs-keyword">for</span> mask_img <span class="hljs-keyword">in</span> mask_imgs:
        self.assertEqual(mask_img.size, img.size)
        self.assertEqual(mask_img.mode, <span class="hljs-string">&quot;L&quot;</span>)  <span class="hljs-comment"># L (8-bit pixels, black and white)</span>
    first_mask_img = mask_imgs[<span class="hljs-number">0</span>]
    min_pxl_val, max_pxl_val = first_mask_img.getextrema()
    self.assertGreaterEqual(min_pxl_val, <span class="hljs-number">0</span>)
    self.assertLessEqual(max_pxl_val, <span class="hljs-number">255</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function CQ(q){let n,c;return n=new P({props:{$$slots:{default:[BQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function GQ(q){let n,c,s,d,$,k,A,j,T,S,D,le,Ne,ee,V,pt,_u,qn,xe,WT,dv,vu,YT,gv,ht,dC,mv,dt,gC,$v,Ie,gt,fg,_n,VT,pg,XT,qv,He,mt,hg,vn,QT,dg,ZT,_v,yu,ej,vv,$t,yv,yn,tj,En,sj,Ev,Eu,aj,wv,qt,bv,wu,nj,Tv,_t,gg,wn,bu,rj,oj,mg,lj,te,bn,Tn,$g,ij,uj,cj,Tu,fj,pj,jn,ju,qg,hj,dj,ku,gj,mj,kn,Au,$j,qj,vt,_j,_g,vj,yj,Ej,An,Du,wj,bj,yt,Tj,vg,jj,kj,Aj,Dn,Ou,Dj,Oj,Et,Pj,yg,Rj,Sj,jv,Pu,Nj,kv,wt,Av,bt,Eg,On,Ru,xj,Ij,wg,Hj,de,Pn,Su,bg,Bj,Cj,Nu,Gj,Lj,Rn,xu,Tg,Mj,Uj,Iu,zj,Kj,Sn,Hu,jg,Fj,Jj,Bu,Wj,Yj,Nn,Cu,kg,Vj,Xj,Gu,Qj,Dv,Be,Tt,Ag,xn,Zj,Dg,e4,Ov,jt,t4,Lu,s4,a4,Pv,kt,Rv,In,n4,Hn,r4,Sv,Mu,o4,Nv,At,xv,Uu,l4,Iv,Dt,Og,Bn,zu,i4,u4,Pg,c4,G,Cn,Gn,Rg,f4,p4,h4,Ku,d4,g4,Ln,Fu,Sg,m4,$4,Ju,q4,_4,Mn,Wu,v4,y4,_e,E4,Ng,w4,b4,xg,T4,j4,k4,Un,Yu,A4,D4,ve,O4,Ig,P4,R4,Hg,S4,N4,x4,zn,Vu,I4,H4,ye,B4,Bg,C4,G4,Cg,L4,M4,U4,Kn,Xu,z4,K4,ie,F4,Gg,J4,W4,Lg,Y4,V4,Mg,X4,Q4,Z4,Fn,Qu,e5,t5,ue,s5,Ug,a5,n5,zg,r5,o5,Kg,l5,i5,u5,Jn,Zu,c5,f5,Ot,p5,Fg,h5,d5,g5,Wn,ec,m5,$5,Pt,q5,Jg,_5,v5,y5,Yn,tc,Wg,E5,w5,sc,b5,T5,Vn,ac,j5,k5,Rt,A5,Yg,D5,O5,P5,Xn,nc,R5,S5,St,N5,Vg,x5,I5,H5,Qn,rc,B5,C5,Nt,G5,Xg,L5,M5,Hv,oc,U5,Bv,xt,Qg,Zn,lc,z5,K5,Zg,F5,em,er,ic,tm,J5,W5,uc,Y5,Cv,Ce,It,sm,tr,V5,am,X5,Gv,cc,Q5,Lv,Ht,Mv,Ge,Z5,sr,ek,tk,ar,sk,Uv,fc,ak,zv,Bt,Kv,pc,nk,Fv,hc,rk,Jv,Ct,Wv,Gt,nm,nr,dc,ok,lk,rm,ik,ge,rr,gc,om,uk,ck,mc,fk,pk,or,$c,lm,hk,dk,qc,gk,mk,lr,_c,im,$k,qk,Lt,_k,um,vk,yk,Ek,ir,vc,cm,wk,bk,Mt,Tk,fm,jk,kk,Yv,Le,Ut,pm,ur,Ak,hm,Dk,Vv,yc,Ok,Xv,zt,Qv,cr,Pk,fr,Rk,Zv,Ec,Sk,e2,Kt,t2,wc,Nk,s2,Ft,dm,pr,bc,xk,Ik,gm,Hk,F,hr,dr,mm,Bk,Ck,Gk,$m,Lk,gr,Tc,Mk,Uk,jc,zk,Kk,mr,kc,Fk,Jk,Ac,Wk,Yk,$r,Dc,qm,Vk,Xk,Oc,Qk,Zk,qr,Pc,e6,t6,Jt,s6,_m,a6,n6,r6,_r,Rc,o6,l6,Wt,i6,vm,u6,c6,f6,vr,Sc,p6,h6,Yt,d6,ym,g6,m6,a2,Nc,$6,n2,Vt,r2,Xt,Em,yr,xc,q6,_6,wm,v6,me,Er,Ic,bm,y6,E6,Hc,w6,b6,wr,Bc,Tm,T6,j6,Cc,k6,A6,br,Gc,jm,D6,O6,Lc,P6,R6,Tr,Mc,km,S6,N6,Uc,x6,o2,Me,Qt,Am,jr,I6,Dm,H6,l2,zc,B6,i2,Zt,u2,Ue,C6,kr,G6,L6,Ar,M6,c2,Kc,U6,f2,es,p2,Fc,z6,h2,ts,Om,Dr,Jc,K6,F6,Pm,J6,J,Or,Pr,Rm,W6,Y6,V6,Sm,X6,Rr,Wc,Q6,Z6,Yc,e7,t7,Sr,Vc,s7,a7,Xc,n7,r7,Nr,Qc,Nm,o7,l7,Zc,i7,u7,xr,ef,c7,f7,ss,p7,xm,h7,d7,g7,Ir,tf,m7,$7,as,q7,Im,_7,v7,y7,Hr,sf,E7,w7,ns,b7,Hm,T7,j7,d2,af,k7,g2,rs,m2,os,Bm,Br,nf,A7,D7,Cm,O7,Gm,Cr,rf,Lm,P7,R7,of,S7,$2,ze,ls,Mm,Gr,N7,Um,x7,q2,lf,I7,_2,is,v2,Lr,H7,Mr,B7,y2,uf,C7,E2,us,w2,cf,G7,b2,cs,zm,Ur,ff,L7,M7,Km,U7,se,zr,Kr,Fm,z7,K7,F7,pf,J7,W7,Fr,hf,Jm,Y7,V7,df,X7,Q7,Jr,gf,Z7,e9,fs,t9,Wm,s9,a9,n9,Wr,mf,r9,o9,ps,l9,Ym,i9,u9,c9,Yr,$f,f9,p9,hs,h9,Vm,d9,g9,T2,qf,m9,j2,ds,k2,gs,Xm,Vr,_f,$9,q9,Qm,_9,Xr,Qr,vf,Zm,v9,y9,yf,E9,w9,Zr,Ef,e$,b9,T9,wf,j9,A2,Ke,ms,t$,eo,k9,s$,A9,D2,bf,D9,O2,$s,P2,to,O9,so,P9,R2,Tf,R9,S2,qs,N2,jf,S9,x2,_s,a$,ao,kf,N9,x9,n$,I9,I,no,ro,r$,H9,B9,C9,Af,G9,L9,oo,Df,o$,M9,U9,Of,z9,K9,lo,Pf,F9,J9,Ee,W9,l$,Y9,V9,i$,X9,Q9,Z9,io,Rf,e8,t8,ce,s8,u$,a8,n8,c$,r8,o8,f$,l8,i8,u8,uo,Sf,c8,f8,fe,p8,p$,h8,d8,h$,g8,m8,d$,$8,q8,_8,co,Nf,v8,y8,vs,E8,g$,w8,b8,T8,fo,xf,j8,k8,we,A8,m$,D8,O8,$$,P8,R8,S8,po,If,N8,x8,be,I8,q$,H8,B8,_$,C8,G8,L8,ho,Hf,M8,U8,Te,z8,v$,K8,F8,y$,J8,W8,Y8,go,Bf,V8,X8,ys,Q8,E$,Z8,eA,tA,mo,Cf,sA,aA,Es,nA,w$,rA,oA,lA,$o,Gf,b$,iA,uA,Lf,cA,fA,qo,Mf,pA,hA,ws,dA,T$,gA,mA,$A,_o,Uf,qA,_A,bs,vA,j$,yA,EA,wA,vo,zf,bA,TA,Ts,jA,k$,kA,AA,I2,Kf,DA,H2,js,B2,ks,A$,yo,Ff,OA,PA,D$,RA,O$,Eo,Jf,P$,SA,NA,Wf,xA,C2,Fe,As,R$,wo,IA,S$,HA,G2,Ds,BA,Yf,CA,GA,L2,Je,Os,N$,bo,LA,x$,MA,M2,Vf,UA,U2,Ps,z2,We,zA,To,KA,FA,jo,JA,K2,Xf,WA,F2,Rs,J2,Qf,YA,W2,Ss,I$,ko,Zf,VA,XA,H$,QA,W,Ao,Do,B$,ZA,eD,tD,ep,sD,aD,Oo,tp,C$,nD,rD,sp,oD,lD,Po,ap,iD,uD,x,cD,G$,fD,pD,hD,dD,L$,gD,mD,$D,qD,M$,_D,vD,yD,ED,U$,wD,bD,z$,TD,jD,kD,AD,K$,DD,OD,F$,PD,RD,SD,ND,J$,xD,ID,W$,HD,BD,CD,Ro,np,Y$,GD,LD,rp,MD,UD,So,op,zD,KD,Ns,FD,V$,JD,WD,YD,No,lp,VD,XD,xs,QD,X$,ZD,eO,tO,xo,ip,sO,aO,Is,nO,Q$,rO,oO,Y2,up,lO,V2,Hs,X2,Bs,Z$,Io,cp,iO,uO,eq,cO,ae,Ho,fp,tq,fO,pO,pp,hO,dO,Bo,hp,sq,gO,mO,dp,$O,qO,Co,gp,aq,_O,vO,mp,yO,EO,Go,$p,nq,wO,bO,Cs,TO,rq,jO,kO,AO,Lo,qp,oq,DO,OO,Gs,PO,lq,RO,SO,Q2,Ye,Ls,iq,Mo,NO,uq,xO,Z2,Uo,IO,_p,HO,ey,Ve,Ms,cq,zo,BO,fq,CO,ty,vp,GO,sy,Us,ay,Ko,LO,Fo,MO,ny,yp,UO,ry,zs,oy,Ep,zO,ly,Ks,pq,Jo,wp,KO,FO,hq,JO,ne,Wo,Yo,dq,WO,YO,VO,bp,XO,QO,Vo,Tp,gq,ZO,eP,jp,tP,sP,Xo,kp,aP,nP,Fs,rP,mq,oP,lP,iP,Qo,Ap,uP,cP,Js,fP,$q,pP,hP,dP,Zo,Dp,gP,mP,Ws,$P,qq,qP,_P,iy,Op,vP,uy,Ys,_q,el,Pp,yP,EP,vq,wP,yq,tl,Rp,Eq,bP,TP,Sp,jP,cy,Xe,Vs,wq,sl,kP,bq,AP,fy,Np,DP,py,Xs,hy,al,OP,nl,PP,dy,xp,RP,gy,Qs,my,Ip,SP,$y,Zs,Tq,rl,Hp,NP,xP,jq,IP,z,ol,ll,kq,HP,BP,CP,Bp,GP,LP,il,ul,Aq,MP,UP,zP,Cp,KP,FP,cl,Gp,JP,WP,je,YP,Dq,VP,XP,Oq,QP,ZP,eR,fl,Lp,tR,sR,ea,aR,Pq,nR,rR,oR,pl,Mp,Rq,lR,iR,Up,uR,cR,hl,zp,fR,pR,ta,hR,Sq,dR,gR,mR,dl,Kp,$R,qR,sa,_R,Nq,vR,yR,ER,gl,Fp,wR,bR,aa,TR,xq,jR,kR,qy,Jp,AR,_y,Wp,DR,vy,na,yy,ra,Iq,ml,Yp,OR,PR,Hq,RR,Qe,$l,Vp,Bq,SR,NR,Xp,xR,IR,ql,Qp,Cq,HR,BR,Zp,CR,GR,_l,eh,Gq,LR,MR,oa,UR,Lq,zR,KR,Ey,Ze,la,Mq,vl,FR,Uq,JR,wy,th,WR,by,ia,Ty,yl,YR,El,VR,jy,sh,XR,ky,ua,Ay,ah,QR,Dy,ca,zq,wl,nh,ZR,eS,Kq,tS,N,bl,Tl,Fq,sS,aS,nS,Jq,rS,jl,rh,oS,lS,oh,iS,uS,kl,lh,cS,fS,ih,pS,hS,Al,uh,dS,gS,fa,mS,Wq,$S,qS,_S,Dl,ch,Yq,vS,yS,fh,ES,wS,Ol,ph,bS,TS,ke,jS,Vq,kS,AS,Xq,DS,OS,PS,Pl,hh,RS,SS,Ae,NS,Qq,xS,IS,Zq,HS,BS,CS,Rl,dh,GS,LS,De,MS,e_,US,zS,t_,KS,FS,JS,Sl,gh,WS,YS,pe,VS,s_,XS,QS,a_,ZS,eN,n_,tN,sN,aN,Nl,mh,nN,rN,he,oN,r_,lN,iN,o_,uN,cN,l_,fN,pN,hN,xl,$h,dN,gN,pa,mN,i_,$N,qN,_N,Il,qh,vN,yN,ha,EN,u_,wN,bN,TN,Hl,_h,c_,jN,kN,vh,AN,DN,Bl,yh,ON,PN,da,RN,f_,SN,NN,xN,Cl,Eh,IN,HN,ga,BN,p_,CN,GN,LN,Gl,wh,MN,UN,ma,zN,h_,KN,FN,Oy,bh,JN,Py,$a,d_,Ll,Th,WN,YN,g_,VN,$e,Ml,jh,m_,XN,QN,kh,ZN,ex,Ul,Ah,$_,tx,sx,Dh,ax,nx,zl,Oh,rx,ox,Ph,lx,ix,Kl,Rh,ux,cx,Sh,fx,Ry,et,qa,q_,Fl,px,__,hx,Sy,Nh,dx,Ny,_a,xy,tt,gx,Jl,mx,$x,Wl,qx,Iy,xh,_x,Hy,va,v_,Yl,Ih,vx,yx,y_,Ex,re,Vl,Xl,E_,wx,bx,Tx,Hh,jx,kx,Ql,Bh,w_,Ax,Dx,Ch,Ox,Px,Zl,Gh,Rx,Sx,ya,Nx,b_,xx,Ix,Hx,ei,Lh,Bx,Cx,Ea,Gx,T_,Lx,Mx,Ux,ti,Mh,zx,Kx,wa,Fx,j_,Jx,Wx,By,Uh,Yx,Cy,ba,k_,si,zh,Vx,Xx,A_,Qx,D_,ai,Kh,O_,Zx,eI,Fh,tI,Gy,Jh,sI,Ly,st,Ta,P_,ni,aI,R_,nI,My,at,ja,S_,ri,rI,N_,oI,Uy,Wh,lI,zy,ka,Ky,Aa,Fy,qe,iI,oi,uI,cI,li,fI,pI,ii,hI,Jy,Yh,dI,Wy,Da,Yy,Vh,gI,Vy,Oa,x_,ui,Xh,mI,$I,I_,qI,H_,ci,fi,B_,_I,vI,yI,Qh,EI,Xy,Zh,wI,Qy,ed,bI,Zy,Pa,eE,Ra,C_,pi,td,TI,jI,G_,kI,L_,hi,sd,M_,AI,DI,ad,OI,tE,nt,Sa,U_,di,PI,z_,RI,sE,nd,SI,aE,Na,nE,rt,NI,gi,xI,II,mi,HI,rE,rd,BI,oE,xa,lE,od,CI,iE,Ia,K_,$i,ld,GI,LI,F_,MI,J_,qi,_i,W_,UI,zI,KI,id,FI,uE,ud,JI,cE,Ha,fE,Ba,Y_,vi,cd,WI,YI,V_,VI,yi,Ei,fd,X_,XI,QI,pd,ZI,eH,wi,hd,Q_,tH,sH,dd,aH,pE,ot,Ca,Z_,bi,nH,e1,rH,hE,lt,Ga,t1,Ti,oH,s1,lH,dE,gd,iH,gE,La,mE,ji,uH,ki,cH,$E,md,fH,qE,Ma,_E,Ua,pH,Ai,hH,dH,vE,za,a1,Di,$d,gH,mH,n1,$H,r1,Oi,Pi,o1,qH,_H,vH,qd,yH,yE,_d,EH,EE,Ka,wE,Fa,l1,Ri,vd,wH,bH,i1,TH,Si,Ni,yd,u1,jH,kH,Ed,AH,DH,xi,wd,c1,OH,PH,bd,RH,bE,it,Ja,f1,Ii,SH,p1,NH,TE,Td,xH,jE,Wa,kE,Hi,IH,Bi,HH,AE,jd,BH,DE,Ya,OE,Va,CH,Ci,GH,LH,PE,Xa,h1,Gi,kd,MH,UH,d1,zH,g1,Li,Mi,m1,KH,FH,JH,Ad,WH,RE,Dd,YH,SE,Qa,NE,Za,$1,Ui,Od,VH,XH,q1,QH,ut,zi,Pd,_1,ZH,eB,Rd,tB,sB,Ki,Sd,v1,aB,nB,Nd,rB,oB,Fi,xd,y1,lB,iB,Id,uB,xE,ct,en,E1,Ji,cB,w1,fB,IE,Hd,pB,HE,tn,BE,Wi,hB,Yi,dB,CE,Bd,gB,GE,sn,LE,an,mB,Vi,$B,qB,ME,nn,b1,Xi,Cd,_B,vB,T1,yB,j1,Qi,Zi,k1,EB,wB,bB,Gd,TB,UE,Ld,jB,zE,rn,KE,on,A1,eu,Md,kB,AB,D1,DB,ft,tu,Ud,O1,OB,PB,zd,RB,SB,su,Kd,P1,NB,xB,Fd,IB,HB,au,Jd,R1,BB,CB,Wd,GB,FE;return k=new U({}),ee=new U({}),_n=new U({}),vn=new U({}),$t=new K({props:{$$slots:{default:[iV]},$$scope:{ctx:q}}}),qt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[dV],js:[pV],python:[cV]},$$scope:{ctx:q}}}),wt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[mV]},$$scope:{ctx:q}}}),xn=new U({}),kt=new K({props:{$$slots:{default:[$V]},$$scope:{ctx:q}}}),At=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[wV],js:[yV],python:[_V]},$$scope:{ctx:q}}}),tr=new U({}),Ht=new K({props:{$$slots:{default:[bV]},$$scope:{ctx:q}}}),Bt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[OV],js:[AV],python:[jV]},$$scope:{ctx:q}}}),Ct=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[RV]},$$scope:{ctx:q}}}),ur=new U({}),zt=new K({props:{$$slots:{default:[SV]},$$scope:{ctx:q}}}),Kt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[CV],js:[HV],python:[xV]},$$scope:{ctx:q}}}),Vt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[LV]},$$scope:{ctx:q}}}),jr=new U({}),Zt=new K({props:{$$slots:{default:[MV]},$$scope:{ctx:q}}}),es=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[WV],js:[FV],python:[zV]},$$scope:{ctx:q}}}),rs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[VV]},$$scope:{ctx:q}}}),Gr=new U({}),is=new K({props:{$$slots:{default:[XV]},$$scope:{ctx:q}}}),us=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[aX],js:[tX],python:[ZV]},$$scope:{ctx:q}}}),ds=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[rX]},$$scope:{ctx:q}}}),eo=new U({}),$s=new K({props:{$$slots:{default:[oX]},$$scope:{ctx:q}}}),qs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[pX],js:[cX],python:[iX]},$$scope:{ctx:q}}}),js=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[dX]},$$scope:{ctx:q}}}),wo=new U({}),bo=new U({}),Ps=new K({props:{$$slots:{default:[gX]},$$scope:{ctx:q}}}),Rs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[yX],js:[_X],python:[$X]},$$scope:{ctx:q}}}),Hs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[wX]},$$scope:{ctx:q}}}),Mo=new U({}),zo=new U({}),Us=new K({props:{$$slots:{default:[bX]},$$scope:{ctx:q}}}),zs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[OX],js:[AX],python:[jX]},$$scope:{ctx:q}}}),sl=new U({}),Xs=new K({props:{$$slots:{default:[PX]},$$scope:{ctx:q}}}),Qs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[HX],js:[xX],python:[SX]},$$scope:{ctx:q}}}),na=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[CX]},$$scope:{ctx:q}}}),vl=new U({}),ia=new K({props:{$$slots:{default:[GX]},$$scope:{ctx:q}}}),ua=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[FX],js:[zX],python:[MX]},$$scope:{ctx:q}}}),Fl=new U({}),_a=new K({props:{$$slots:{default:[JX]},$$scope:{ctx:q}}}),ni=new U({}),ri=new U({}),ka=new K({props:{$$slots:{default:[WX]},$$scope:{ctx:q}}}),Aa=new K({props:{$$slots:{default:[YX]},$$scope:{ctx:q}}}),Da=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[tQ],js:[ZX],python:[XX]},$$scope:{ctx:q}}}),Pa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[aQ]},$$scope:{ctx:q}}}),di=new U({}),Na=new K({props:{$$slots:{default:[nQ]},$$scope:{ctx:q}}}),xa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[cQ],js:[iQ],python:[oQ]},$$scope:{ctx:q}}}),Ha=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[pQ]},$$scope:{ctx:q}}}),bi=new U({}),Ti=new U({}),La=new K({props:{$$slots:{default:[hQ]},$$scope:{ctx:q}}}),Ma=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[_Q],js:[$Q],python:[gQ]},$$scope:{ctx:q}}}),Ka=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[yQ]},$$scope:{ctx:q}}}),Ii=new U({}),Wa=new K({props:{$$slots:{default:[EQ]},$$scope:{ctx:q}}}),Ya=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[AQ],js:[jQ],python:[bQ]},$$scope:{ctx:q}}}),Qa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[OQ]},$$scope:{ctx:q}}}),Ji=new U({}),tn=new K({props:{$$slots:{default:[PQ]},$$scope:{ctx:q}}}),sn=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[HQ],js:[xQ],python:[SQ]},$$scope:{ctx:q}}}),rn=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[CQ]},$$scope:{ctx:q}}}),{c(){n=r("meta"),c=p(),s=r("h1"),d=r("a"),$=r("span"),_(k.$$.fragment),A=p(),j=r("span"),T=i("Detailed parameters"),S=p(),D=r("h2"),le=r("a"),Ne=r("span"),_(ee.$$.fragment),V=p(),pt=r("span"),_u=i("Which task is used by this model ?"),qn=p(),xe=r("p"),WT=i(`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),dv=p(),vu=r("p"),YT=i("The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),gv=p(),ht=r("img"),mv=p(),dt=r("img"),$v=p(),Ie=r("h2"),gt=r("a"),fg=r("span"),_(_n.$$.fragment),VT=p(),pg=r("span"),XT=i("Natural Language Processing"),qv=p(),He=r("h3"),mt=r("a"),hg=r("span"),_(vn.$$.fragment),QT=p(),dg=r("span"),ZT=i("Fill Mask task"),_v=p(),yu=r("p"),ej=i(`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),vv=p(),_($t.$$.fragment),yv=p(),yn=r("p"),tj=i("Available with: "),En=r("a"),sj=i("\u{1F917} Transformers"),Ev=p(),Eu=r("p"),aj=i("Example:"),wv=p(),_(qt.$$.fragment),bv=p(),wu=r("p"),nj=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Tv=p(),_t=r("table"),gg=r("thead"),wn=r("tr"),bu=r("th"),rj=i("All parameters"),oj=p(),mg=r("th"),lj=p(),te=r("tbody"),bn=r("tr"),Tn=r("td"),$g=r("strong"),ij=i("inputs"),uj=i(" (required):"),cj=p(),Tu=r("td"),fj=i("a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),pj=p(),jn=r("tr"),ju=r("td"),qg=r("strong"),hj=i("options"),dj=p(),ku=r("td"),gj=i("a dict containing the following keys:"),mj=p(),kn=r("tr"),Au=r("td"),$j=i("use_gpu"),qj=p(),vt=r("td"),_j=i("(Default: "),_g=r("code"),vj=i("false"),yj=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Ej=p(),An=r("tr"),Du=r("td"),wj=i("use_cache"),bj=p(),yt=r("td"),Tj=i("(Default: "),vg=r("code"),jj=i("true"),kj=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Aj=p(),Dn=r("tr"),Ou=r("td"),Dj=i("wait_for_model"),Oj=p(),Et=r("td"),Pj=i("(Default: "),yg=r("code"),Rj=i("false"),Sj=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),jv=p(),Pu=r("p"),Nj=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),kv=p(),_(wt.$$.fragment),Av=p(),bt=r("table"),Eg=r("thead"),On=r("tr"),Ru=r("th"),xj=i("Returned values"),Ij=p(),wg=r("th"),Hj=p(),de=r("tbody"),Pn=r("tr"),Su=r("td"),bg=r("strong"),Bj=i("sequence"),Cj=p(),Nu=r("td"),Gj=i("The actual sequence of tokens that ran against the model (may contain special tokens)"),Lj=p(),Rn=r("tr"),xu=r("td"),Tg=r("strong"),Mj=i("score"),Uj=p(),Iu=r("td"),zj=i("The probability for this token."),Kj=p(),Sn=r("tr"),Hu=r("td"),jg=r("strong"),Fj=i("token"),Jj=p(),Bu=r("td"),Wj=i("The id of the token"),Yj=p(),Nn=r("tr"),Cu=r("td"),kg=r("strong"),Vj=i("token_str"),Xj=p(),Gu=r("td"),Qj=i("The string representation of the token"),Dv=p(),Be=r("h3"),Tt=r("a"),Ag=r("span"),_(xn.$$.fragment),Zj=p(),Dg=r("span"),e4=i("Summarization task"),Ov=p(),jt=r("p"),t4=i(`This task is well known to summarize longer text into shorter text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss your summarization needs,
please get in touch with us: <`),Lu=r("a"),s4=i("api-enterprise@huggingface.co"),a4=i(">"),Pv=p(),_(kt.$$.fragment),Rv=p(),In=r("p"),n4=i("Available with: "),Hn=r("a"),r4=i("\u{1F917} Transformers"),Sv=p(),Mu=r("p"),o4=i("Example:"),Nv=p(),_(At.$$.fragment),xv=p(),Uu=r("p"),l4=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Iv=p(),Dt=r("table"),Og=r("thead"),Bn=r("tr"),zu=r("th"),i4=i("All parameters"),u4=p(),Pg=r("th"),c4=p(),G=r("tbody"),Cn=r("tr"),Gn=r("td"),Rg=r("strong"),f4=i("inputs"),p4=i(" (required)"),h4=p(),Ku=r("td"),d4=i("a string to be summarized"),g4=p(),Ln=r("tr"),Fu=r("td"),Sg=r("strong"),m4=i("parameters"),$4=p(),Ju=r("td"),q4=i("a dict containing the following keys:"),_4=p(),Mn=r("tr"),Wu=r("td"),v4=i("min_length"),y4=p(),_e=r("td"),E4=i("(Default: "),Ng=r("code"),w4=i("None"),b4=i("). Integer to define the minimum length "),xg=r("strong"),T4=i("in tokens"),j4=i(" of the output summary."),k4=p(),Un=r("tr"),Yu=r("td"),A4=i("max_length"),D4=p(),ve=r("td"),O4=i("(Default: "),Ig=r("code"),P4=i("None"),R4=i("). Integer to define the maximum length "),Hg=r("strong"),S4=i("in tokens"),N4=i(" of the output summary."),x4=p(),zn=r("tr"),Vu=r("td"),I4=i("top_k"),H4=p(),ye=r("td"),B4=i("(Default: "),Bg=r("code"),C4=i("None"),G4=i("). Integer to define the top tokens considered within the "),Cg=r("code"),L4=i("sample"),M4=i(" operation to create new text."),U4=p(),Kn=r("tr"),Xu=r("td"),z4=i("top_p"),K4=p(),ie=r("td"),F4=i("(Default: "),Gg=r("code"),J4=i("None"),W4=i("). Float to define the tokens that are within the "),Lg=r("code"),Y4=i("sample"),V4=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Mg=r("code"),X4=i("top_p"),Q4=i("."),Z4=p(),Fn=r("tr"),Qu=r("td"),e5=i("temperature"),t5=p(),ue=r("td"),s5=i("(Default: "),Ug=r("code"),a5=i("1.0"),n5=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),zg=r("code"),r5=i("0"),o5=i(" means always take the highest score, "),Kg=r("code"),l5=i("100.0"),i5=i(" is getting closer to uniform probability."),u5=p(),Jn=r("tr"),Zu=r("td"),c5=i("repetition_penalty"),f5=p(),Ot=r("td"),p5=i("(Default: "),Fg=r("code"),h5=i("None"),d5=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),g5=p(),Wn=r("tr"),ec=r("td"),m5=i("max_time"),$5=p(),Pt=r("td"),q5=i("(Default: "),Jg=r("code"),_5=i("None"),v5=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),y5=p(),Yn=r("tr"),tc=r("td"),Wg=r("strong"),E5=i("options"),w5=p(),sc=r("td"),b5=i("a dict containing the following keys:"),T5=p(),Vn=r("tr"),ac=r("td"),j5=i("use_gpu"),k5=p(),Rt=r("td"),A5=i("(Default: "),Yg=r("code"),D5=i("false"),O5=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),P5=p(),Xn=r("tr"),nc=r("td"),R5=i("use_cache"),S5=p(),St=r("td"),N5=i("(Default: "),Vg=r("code"),x5=i("true"),I5=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),H5=p(),Qn=r("tr"),rc=r("td"),B5=i("wait_for_model"),C5=p(),Nt=r("td"),G5=i("(Default: "),Xg=r("code"),L5=i("false"),M5=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Hv=p(),oc=r("p"),U5=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Bv=p(),xt=r("table"),Qg=r("thead"),Zn=r("tr"),lc=r("th"),z5=i("Returned values"),K5=p(),Zg=r("th"),F5=p(),em=r("tbody"),er=r("tr"),ic=r("td"),tm=r("strong"),J5=i("summarization_text"),W5=p(),uc=r("td"),Y5=i("The string after translation"),Cv=p(),Ce=r("h3"),It=r("a"),sm=r("span"),_(tr.$$.fragment),V5=p(),am=r("span"),X5=i("Question Answering task"),Gv=p(),cc=r("p"),Q5=i("Want to have a nice know-it-all bot that can answer any question?"),Lv=p(),_(Ht.$$.fragment),Mv=p(),Ge=r("p"),Z5=i("Available with: "),sr=r("a"),ek=i("\u{1F917}Transformers"),tk=i(` and
`),ar=r("a"),sk=i("AllenNLP"),Uv=p(),fc=r("p"),ak=i("Example:"),zv=p(),_(Bt.$$.fragment),Kv=p(),pc=r("p"),nk=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Fv=p(),hc=r("p"),rk=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Jv=p(),_(Ct.$$.fragment),Wv=p(),Gt=r("table"),nm=r("thead"),nr=r("tr"),dc=r("th"),ok=i("Returned values"),lk=p(),rm=r("th"),ik=p(),ge=r("tbody"),rr=r("tr"),gc=r("td"),om=r("strong"),uk=i("answer"),ck=p(),mc=r("td"),fk=i("A string that\u2019s the answer within the text."),pk=p(),or=r("tr"),$c=r("td"),lm=r("strong"),hk=i("score"),dk=p(),qc=r("td"),gk=i("A float that represents how likely that the answer is correct"),mk=p(),lr=r("tr"),_c=r("td"),im=r("strong"),$k=i("start"),qk=p(),Lt=r("td"),_k=i("The index (string wise) of the start of the answer within "),um=r("code"),vk=i("context"),yk=i("."),Ek=p(),ir=r("tr"),vc=r("td"),cm=r("strong"),wk=i("stop"),bk=p(),Mt=r("td"),Tk=i("The index (string wise) of the stop of the answer within "),fm=r("code"),jk=i("context"),kk=i("."),Yv=p(),Le=r("h3"),Ut=r("a"),pm=r("span"),_(ur.$$.fragment),Ak=p(),hm=r("span"),Dk=i("Table Question Answering task"),Vv=p(),yc=r("p"),Ok=i(`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),Xv=p(),_(zt.$$.fragment),Qv=p(),cr=r("p"),Pk=i("Available with: "),fr=r("a"),Rk=i("\u{1F917} Transformers"),Zv=p(),Ec=r("p"),Sk=i("Example:"),e2=p(),_(Kt.$$.fragment),t2=p(),wc=r("p"),Nk=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),s2=p(),Ft=r("table"),dm=r("thead"),pr=r("tr"),bc=r("th"),xk=i("All parameters"),Ik=p(),gm=r("th"),Hk=p(),F=r("tbody"),hr=r("tr"),dr=r("td"),mm=r("strong"),Bk=i("inputs"),Ck=i(" (required)"),Gk=p(),$m=r("td"),Lk=p(),gr=r("tr"),Tc=r("td"),Mk=i("query (required)"),Uk=p(),jc=r("td"),zk=i("The query in plain text that you want to ask the table"),Kk=p(),mr=r("tr"),kc=r("td"),Fk=i("table (required)"),Jk=p(),Ac=r("td"),Wk=i("A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),Yk=p(),$r=r("tr"),Dc=r("td"),qm=r("strong"),Vk=i("options"),Xk=p(),Oc=r("td"),Qk=i("a dict containing the following keys:"),Zk=p(),qr=r("tr"),Pc=r("td"),e6=i("use_gpu"),t6=p(),Jt=r("td"),s6=i("(Default: "),_m=r("code"),a6=i("false"),n6=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),r6=p(),_r=r("tr"),Rc=r("td"),o6=i("use_cache"),l6=p(),Wt=r("td"),i6=i("(Default: "),vm=r("code"),u6=i("true"),c6=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),f6=p(),vr=r("tr"),Sc=r("td"),p6=i("wait_for_model"),h6=p(),Yt=r("td"),d6=i("(Default: "),ym=r("code"),g6=i("false"),m6=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),a2=p(),Nc=r("p"),$6=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),n2=p(),_(Vt.$$.fragment),r2=p(),Xt=r("table"),Em=r("thead"),yr=r("tr"),xc=r("th"),q6=i("Returned values"),_6=p(),wm=r("th"),v6=p(),me=r("tbody"),Er=r("tr"),Ic=r("td"),bm=r("strong"),y6=i("answer"),E6=p(),Hc=r("td"),w6=i("The plaintext answer"),b6=p(),wr=r("tr"),Bc=r("td"),Tm=r("strong"),T6=i("coordinates"),j6=p(),Cc=r("td"),k6=i("a list of coordinates of the cells referenced in the answer"),A6=p(),br=r("tr"),Gc=r("td"),jm=r("strong"),D6=i("cells"),O6=p(),Lc=r("td"),P6=i("a list of coordinates of the cells contents"),R6=p(),Tr=r("tr"),Mc=r("td"),km=r("strong"),S6=i("aggregator"),N6=p(),Uc=r("td"),x6=i("The aggregator used to get the answer"),o2=p(),Me=r("h3"),Qt=r("a"),Am=r("span"),_(jr.$$.fragment),I6=p(),Dm=r("span"),H6=i("Sentence Similarity task"),l2=p(),zc=r("p"),B6=i("Calculate the semantic similarity for two different texts by comparing their embeddings."),i2=p(),_(Zt.$$.fragment),u2=p(),Ue=r("p"),C6=i("Available with: "),kr=r("a"),G6=i("\u{1F917}Transformers"),L6=i(` and
`),Ar=r("a"),M6=i("Sentence Transformers"),c2=p(),Kc=r("p"),U6=i("Example:"),f2=p(),_(es.$$.fragment),p2=p(),Fc=r("p"),z6=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),h2=p(),ts=r("table"),Om=r("thead"),Dr=r("tr"),Jc=r("th"),K6=i("All parameters"),F6=p(),Pm=r("th"),J6=p(),J=r("tbody"),Or=r("tr"),Pr=r("td"),Rm=r("strong"),W6=i("inputs"),Y6=i(" (required)"),V6=p(),Sm=r("td"),X6=p(),Rr=r("tr"),Wc=r("td"),Q6=i("source_sentence (required)"),Z6=p(),Yc=r("td"),e7=i("The query in plain text that you want to ask the table"),t7=p(),Sr=r("tr"),Vc=r("td"),s7=i("sentences (required)"),a7=p(),Xc=r("td"),n7=i("A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),r7=p(),Nr=r("tr"),Qc=r("td"),Nm=r("strong"),o7=i("options"),l7=p(),Zc=r("td"),i7=i("a dict containing the following keys:"),u7=p(),xr=r("tr"),ef=r("td"),c7=i("use_gpu"),f7=p(),ss=r("td"),p7=i("(Default: "),xm=r("code"),h7=i("false"),d7=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),g7=p(),Ir=r("tr"),tf=r("td"),m7=i("use_cache"),$7=p(),as=r("td"),q7=i("(Default: "),Im=r("code"),_7=i("true"),v7=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),y7=p(),Hr=r("tr"),sf=r("td"),E7=i("wait_for_model"),w7=p(),ns=r("td"),b7=i("(Default: "),Hm=r("code"),T7=i("false"),j7=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),d2=p(),af=r("p"),k7=i("The return value is a list of similarity scores, given as floats."),g2=p(),_(rs.$$.fragment),m2=p(),os=r("table"),Bm=r("thead"),Br=r("tr"),nf=r("th"),A7=i("Returned values"),D7=p(),Cm=r("th"),O7=p(),Gm=r("tbody"),Cr=r("tr"),rf=r("td"),Lm=r("strong"),P7=i("Scores"),R7=p(),of=r("td"),S7=i("The associated similarity score for each of the given strings"),$2=p(),ze=r("h3"),ls=r("a"),Mm=r("span"),_(Gr.$$.fragment),N7=p(),Um=r("span"),x7=i("Text Classification task"),q2=p(),lf=r("p"),I7=i(`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),_2=p(),_(is.$$.fragment),v2=p(),Lr=r("p"),H7=i("Available with: "),Mr=r("a"),B7=i("\u{1F917} Transformers"),y2=p(),uf=r("p"),C7=i("Example:"),E2=p(),_(us.$$.fragment),w2=p(),cf=r("p"),G7=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),b2=p(),cs=r("table"),zm=r("thead"),Ur=r("tr"),ff=r("th"),L7=i("All parameters"),M7=p(),Km=r("th"),U7=p(),se=r("tbody"),zr=r("tr"),Kr=r("td"),Fm=r("strong"),z7=i("inputs"),K7=i(" (required)"),F7=p(),pf=r("td"),J7=i("a string to be classified"),W7=p(),Fr=r("tr"),hf=r("td"),Jm=r("strong"),Y7=i("options"),V7=p(),df=r("td"),X7=i("a dict containing the following keys:"),Q7=p(),Jr=r("tr"),gf=r("td"),Z7=i("use_gpu"),e9=p(),fs=r("td"),t9=i("(Default: "),Wm=r("code"),s9=i("false"),a9=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),n9=p(),Wr=r("tr"),mf=r("td"),r9=i("use_cache"),o9=p(),ps=r("td"),l9=i("(Default: "),Ym=r("code"),i9=i("true"),u9=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),c9=p(),Yr=r("tr"),$f=r("td"),f9=i("wait_for_model"),p9=p(),hs=r("td"),h9=i("(Default: "),Vm=r("code"),d9=i("false"),g9=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),T2=p(),qf=r("p"),m9=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),j2=p(),_(ds.$$.fragment),k2=p(),gs=r("table"),Xm=r("thead"),Vr=r("tr"),_f=r("th"),$9=i("Returned values"),q9=p(),Qm=r("th"),_9=p(),Xr=r("tbody"),Qr=r("tr"),vf=r("td"),Zm=r("strong"),v9=i("label"),y9=p(),yf=r("td"),E9=i("The label for the class (model specific)"),w9=p(),Zr=r("tr"),Ef=r("td"),e$=r("strong"),b9=i("score"),T9=p(),wf=r("td"),j9=i("A floats that represents how likely is that the text belongs the this class."),A2=p(),Ke=r("h3"),ms=r("a"),t$=r("span"),_(eo.$$.fragment),k9=p(),s$=r("span"),A9=i("Text Generation task"),D2=p(),bf=r("p"),D9=i("Use to continue text from a prompt. This is a very generic task."),O2=p(),_($s.$$.fragment),P2=p(),to=r("p"),O9=i("Available with: "),so=r("a"),P9=i("\u{1F917} Transformers"),R2=p(),Tf=r("p"),R9=i("Example:"),S2=p(),_(qs.$$.fragment),N2=p(),jf=r("p"),S9=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),x2=p(),_s=r("table"),a$=r("thead"),ao=r("tr"),kf=r("th"),N9=i("All parameters"),x9=p(),n$=r("th"),I9=p(),I=r("tbody"),no=r("tr"),ro=r("td"),r$=r("strong"),H9=i("inputs"),B9=i(" (required):"),C9=p(),Af=r("td"),G9=i("a string to be generated from"),L9=p(),oo=r("tr"),Df=r("td"),o$=r("strong"),M9=i("parameters"),U9=p(),Of=r("td"),z9=i("dict containing the following keys:"),K9=p(),lo=r("tr"),Pf=r("td"),F9=i("top_k"),J9=p(),Ee=r("td"),W9=i("(Default: "),l$=r("code"),Y9=i("None"),V9=i("). Integer to define the top tokens considered within the "),i$=r("code"),X9=i("sample"),Q9=i(" operation to create new text."),Z9=p(),io=r("tr"),Rf=r("td"),e8=i("top_p"),t8=p(),ce=r("td"),s8=i("(Default: "),u$=r("code"),a8=i("None"),n8=i("). Float to define the tokens that are within the "),c$=r("code"),r8=i("sample"),o8=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),f$=r("code"),l8=i("top_p"),i8=i("."),u8=p(),uo=r("tr"),Sf=r("td"),c8=i("temperature"),f8=p(),fe=r("td"),p8=i("(Default: "),p$=r("code"),h8=i("1.0"),d8=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),h$=r("code"),g8=i("0"),m8=i(" means always take the highest score, "),d$=r("code"),$8=i("100.0"),q8=i(" is getting closer to uniform probability."),_8=p(),co=r("tr"),Nf=r("td"),v8=i("repetition_penalty"),y8=p(),vs=r("td"),E8=i("(Default: "),g$=r("code"),w8=i("None"),b8=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),T8=p(),fo=r("tr"),xf=r("td"),j8=i("max_new_tokens"),k8=p(),we=r("td"),A8=i("(Default: "),m$=r("code"),D8=i("None"),O8=i("). Int (0-250). The amount of new tokens to be generated, this does "),$$=r("strong"),P8=i("not"),R8=i(" include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),S8=p(),po=r("tr"),If=r("td"),N8=i("max_time"),x8=p(),be=r("td"),I8=i("(Default: "),q$=r("code"),H8=i("None"),B8=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),_$=r("code"),C8=i("max_new_tokens"),G8=i(" for best results."),L8=p(),ho=r("tr"),Hf=r("td"),M8=i("return_full_text"),U8=p(),Te=r("td"),z8=i("(Default: "),v$=r("code"),K8=i("True"),F8=i("). Bool. If set to False, the return results will "),y$=r("strong"),J8=i("not"),W8=i(" contain the original query making it easier for prompting."),Y8=p(),go=r("tr"),Bf=r("td"),V8=i("num_return_sequences"),X8=p(),ys=r("td"),Q8=i("(Default: "),E$=r("code"),Z8=i("1"),eA=i("). Integer. The number of proposition you want to be returned."),tA=p(),mo=r("tr"),Cf=r("td"),sA=i("do_sample"),aA=p(),Es=r("td"),nA=i("(Optional: "),w$=r("code"),rA=i("True"),oA=i("). Bool. Whether or not to use sampling, use greedy decoding otherwise."),lA=p(),$o=r("tr"),Gf=r("td"),b$=r("strong"),iA=i("options"),uA=p(),Lf=r("td"),cA=i("a dict containing the following keys:"),fA=p(),qo=r("tr"),Mf=r("td"),pA=i("use_gpu"),hA=p(),ws=r("td"),dA=i("(Default: "),T$=r("code"),gA=i("false"),mA=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),$A=p(),_o=r("tr"),Uf=r("td"),qA=i("use_cache"),_A=p(),bs=r("td"),vA=i("(Default: "),j$=r("code"),yA=i("true"),EA=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),wA=p(),vo=r("tr"),zf=r("td"),bA=i("wait_for_model"),TA=p(),Ts=r("td"),jA=i("(Default: "),k$=r("code"),kA=i("false"),AA=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),I2=p(),Kf=r("p"),DA=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),H2=p(),_(js.$$.fragment),B2=p(),ks=r("table"),A$=r("thead"),yo=r("tr"),Ff=r("th"),OA=i("Returned values"),PA=p(),D$=r("th"),RA=p(),O$=r("tbody"),Eo=r("tr"),Jf=r("td"),P$=r("strong"),SA=i("generated_text"),NA=p(),Wf=r("td"),xA=i("The continuated string"),C2=p(),Fe=r("h3"),As=r("a"),R$=r("span"),_(wo.$$.fragment),IA=p(),S$=r("span"),HA=i("Text2Text Generation task"),G2=p(),Ds=r("p"),BA=i("Essentially "),Yf=r("a"),CA=i("Text-generation task"),GA=i(`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),L2=p(),Je=r("h3"),Os=r("a"),N$=r("span"),_(bo.$$.fragment),LA=p(),x$=r("span"),MA=i("Token Classification task"),M2=p(),Vf=r("p"),UA=i(`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),U2=p(),_(Ps.$$.fragment),z2=p(),We=r("p"),zA=i("Available with: "),To=r("a"),KA=i("\u{1F917} Transformers"),FA=i(`,
`),jo=r("a"),JA=i("Flair"),K2=p(),Xf=r("p"),WA=i("Example:"),F2=p(),_(Rs.$$.fragment),J2=p(),Qf=r("p"),YA=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),W2=p(),Ss=r("table"),I$=r("thead"),ko=r("tr"),Zf=r("th"),VA=i("All parameters"),XA=p(),H$=r("th"),QA=p(),W=r("tbody"),Ao=r("tr"),Do=r("td"),B$=r("strong"),ZA=i("inputs"),eD=i(" (required)"),tD=p(),ep=r("td"),sD=i("a string to be classified"),aD=p(),Oo=r("tr"),tp=r("td"),C$=r("strong"),nD=i("parameters"),rD=p(),sp=r("td"),oD=i("a dict containing the following key:"),lD=p(),Po=r("tr"),ap=r("td"),iD=i("aggregation_strategy"),uD=p(),x=r("td"),cD=i("(Default: "),G$=r("code"),fD=i("simple"),pD=i("). There are several aggregation strategies: "),hD=r("br"),dD=p(),L$=r("code"),gD=i("none"),mD=i(": Every token gets classified without further aggregation. "),$D=r("br"),qD=p(),M$=r("code"),_D=i("simple"),vD=i(": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),yD=r("br"),ED=p(),U$=r("code"),wD=i("first"),bD=i(": Same as the "),z$=r("code"),TD=i("simple"),jD=i(" strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),kD=r("br"),AD=p(),K$=r("code"),DD=i("average"),OD=i(": Same as the "),F$=r("code"),PD=i("simple"),RD=i(" strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),SD=r("br"),ND=p(),J$=r("code"),xD=i("max"),ID=i(": Same as the "),W$=r("code"),HD=i("simple"),BD=i(" strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),CD=p(),Ro=r("tr"),np=r("td"),Y$=r("strong"),GD=i("options"),LD=p(),rp=r("td"),MD=i("a dict containing the following keys:"),UD=p(),So=r("tr"),op=r("td"),zD=i("use_gpu"),KD=p(),Ns=r("td"),FD=i("(Default: "),V$=r("code"),JD=i("false"),WD=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),YD=p(),No=r("tr"),lp=r("td"),VD=i("use_cache"),XD=p(),xs=r("td"),QD=i("(Default: "),X$=r("code"),ZD=i("true"),eO=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),tO=p(),xo=r("tr"),ip=r("td"),sO=i("wait_for_model"),aO=p(),Is=r("td"),nO=i("(Default: "),Q$=r("code"),rO=i("false"),oO=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Y2=p(),up=r("p"),lO=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),V2=p(),_(Hs.$$.fragment),X2=p(),Bs=r("table"),Z$=r("thead"),Io=r("tr"),cp=r("th"),iO=i("Returned values"),uO=p(),eq=r("th"),cO=p(),ae=r("tbody"),Ho=r("tr"),fp=r("td"),tq=r("strong"),fO=i("entity_group"),pO=p(),pp=r("td"),hO=i("The type for the entity being recognized (model specific)."),dO=p(),Bo=r("tr"),hp=r("td"),sq=r("strong"),gO=i("score"),mO=p(),dp=r("td"),$O=i("How likely the entity was recognized."),qO=p(),Co=r("tr"),gp=r("td"),aq=r("strong"),_O=i("word"),vO=p(),mp=r("td"),yO=i("The string that was captured"),EO=p(),Go=r("tr"),$p=r("td"),nq=r("strong"),wO=i("start"),bO=p(),Cs=r("td"),TO=i("The offset stringwise where the answer is located. Useful to disambiguate if "),rq=r("code"),jO=i("word"),kO=i(" occurs multiple times."),AO=p(),Lo=r("tr"),qp=r("td"),oq=r("strong"),DO=i("end"),OO=p(),Gs=r("td"),PO=i("The offset stringwise where the answer is located. Useful to disambiguate if "),lq=r("code"),RO=i("word"),SO=i(" occurs multiple times."),Q2=p(),Ye=r("h3"),Ls=r("a"),iq=r("span"),_(Mo.$$.fragment),NO=p(),uq=r("span"),xO=i("Named Entity Recognition (NER) task"),Z2=p(),Uo=r("p"),IO=i("See "),_p=r("a"),HO=i("Token-classification task"),ey=p(),Ve=r("h3"),Ms=r("a"),cq=r("span"),_(zo.$$.fragment),BO=p(),fq=r("span"),CO=i("Translation task"),ty=p(),vp=r("p"),GO=i("This task is well known to translate text from one language to another"),sy=p(),_(Us.$$.fragment),ay=p(),Ko=r("p"),LO=i("Available with: "),Fo=r("a"),MO=i("\u{1F917} Transformers"),ny=p(),yp=r("p"),UO=i("Example:"),ry=p(),_(zs.$$.fragment),oy=p(),Ep=r("p"),zO=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),ly=p(),Ks=r("table"),pq=r("thead"),Jo=r("tr"),wp=r("th"),KO=i("All parameters"),FO=p(),hq=r("th"),JO=p(),ne=r("tbody"),Wo=r("tr"),Yo=r("td"),dq=r("strong"),WO=i("inputs"),YO=i(" (required)"),VO=p(),bp=r("td"),XO=i("a string to be translated in the original languages"),QO=p(),Vo=r("tr"),Tp=r("td"),gq=r("strong"),ZO=i("options"),eP=p(),jp=r("td"),tP=i("a dict containing the following keys:"),sP=p(),Xo=r("tr"),kp=r("td"),aP=i("use_gpu"),nP=p(),Fs=r("td"),rP=i("(Default: "),mq=r("code"),oP=i("false"),lP=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),iP=p(),Qo=r("tr"),Ap=r("td"),uP=i("use_cache"),cP=p(),Js=r("td"),fP=i("(Default: "),$q=r("code"),pP=i("true"),hP=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),dP=p(),Zo=r("tr"),Dp=r("td"),gP=i("wait_for_model"),mP=p(),Ws=r("td"),$P=i("(Default: "),qq=r("code"),qP=i("false"),_P=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),iy=p(),Op=r("p"),vP=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),uy=p(),Ys=r("table"),_q=r("thead"),el=r("tr"),Pp=r("th"),yP=i("Returned values"),EP=p(),vq=r("th"),wP=p(),yq=r("tbody"),tl=r("tr"),Rp=r("td"),Eq=r("strong"),bP=i("translation_text"),TP=p(),Sp=r("td"),jP=i("The string after translation"),cy=p(),Xe=r("h3"),Vs=r("a"),wq=r("span"),_(sl.$$.fragment),kP=p(),bq=r("span"),AP=i("Zero-Shot Classification task"),fy=p(),Np=r("p"),DP=i(`This task is super useful to try out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence, and you get a result.`),py=p(),_(Xs.$$.fragment),hy=p(),al=r("p"),OP=i("Available with: "),nl=r("a"),PP=i("\u{1F917} Transformers"),dy=p(),xp=r("p"),RP=i("Request:"),gy=p(),_(Qs.$$.fragment),my=p(),Ip=r("p"),SP=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),$y=p(),Zs=r("table"),Tq=r("thead"),rl=r("tr"),Hp=r("th"),NP=i("All parameters"),xP=p(),jq=r("th"),IP=p(),z=r("tbody"),ol=r("tr"),ll=r("td"),kq=r("strong"),HP=i("inputs"),BP=i(" (required)"),CP=p(),Bp=r("td"),GP=i("a string or list of strings"),LP=p(),il=r("tr"),ul=r("td"),Aq=r("strong"),MP=i("parameters"),UP=i(" (required)"),zP=p(),Cp=r("td"),KP=i("a dict containing the following keys:"),FP=p(),cl=r("tr"),Gp=r("td"),JP=i("candidate_labels (required)"),WP=p(),je=r("td"),YP=i("a list of strings that are potential classes for "),Dq=r("code"),VP=i("inputs"),XP=i(". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),Oq=r("code"),QP=i("multi_label=True"),ZP=i(" and do the scaling on your end. )"),eR=p(),fl=r("tr"),Lp=r("td"),tR=i("multi_label"),sR=p(),ea=r("td"),aR=i("(Default: "),Pq=r("code"),nR=i("false"),rR=i(") Boolean that is set to True if classes can overlap"),oR=p(),pl=r("tr"),Mp=r("td"),Rq=r("strong"),lR=i("options"),iR=p(),Up=r("td"),uR=i("a dict containing the following keys:"),cR=p(),hl=r("tr"),zp=r("td"),fR=i("use_gpu"),pR=p(),ta=r("td"),hR=i("(Default: "),Sq=r("code"),dR=i("false"),gR=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),mR=p(),dl=r("tr"),Kp=r("td"),$R=i("use_cache"),qR=p(),sa=r("td"),_R=i("(Default: "),Nq=r("code"),vR=i("true"),yR=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),ER=p(),gl=r("tr"),Fp=r("td"),wR=i("wait_for_model"),bR=p(),aa=r("td"),TR=i("(Default: "),xq=r("code"),jR=i("false"),kR=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),qy=p(),Jp=r("p"),AR=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),_y=p(),Wp=r("p"),DR=i("Response:"),vy=p(),_(na.$$.fragment),yy=p(),ra=r("table"),Iq=r("thead"),ml=r("tr"),Yp=r("th"),OR=i("Returned values"),PR=p(),Hq=r("th"),RR=p(),Qe=r("tbody"),$l=r("tr"),Vp=r("td"),Bq=r("strong"),SR=i("sequence"),NR=p(),Xp=r("td"),xR=i("The string sent as an input"),IR=p(),ql=r("tr"),Qp=r("td"),Cq=r("strong"),HR=i("labels"),BR=p(),Zp=r("td"),CR=i("The list of strings for labels that you sent (in order)"),GR=p(),_l=r("tr"),eh=r("td"),Gq=r("strong"),LR=i("scores"),MR=p(),oa=r("td"),UR=i("a list of floats that correspond the the probability of label, in the same order as "),Lq=r("code"),zR=i("labels"),KR=i("."),Ey=p(),Ze=r("h3"),la=r("a"),Mq=r("span"),_(vl.$$.fragment),FR=p(),Uq=r("span"),JR=i("Conversational task"),wy=p(),th=r("p"),WR=i(`This task corresponds to any chatbot like structure. Models tend to have
shorter max_length, so please check with caution when using a given
model if you need long range dependency or not.`),by=p(),_(ia.$$.fragment),Ty=p(),yl=r("p"),YR=i("Available with: "),El=r("a"),VR=i("\u{1F917} Transformers"),jy=p(),sh=r("p"),XR=i("Example:"),ky=p(),_(ua.$$.fragment),Ay=p(),ah=r("p"),QR=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Dy=p(),ca=r("table"),zq=r("thead"),wl=r("tr"),nh=r("th"),ZR=i("All parameters"),eS=p(),Kq=r("th"),tS=p(),N=r("tbody"),bl=r("tr"),Tl=r("td"),Fq=r("strong"),sS=i("inputs"),aS=i(" (required)"),nS=p(),Jq=r("td"),rS=p(),jl=r("tr"),rh=r("td"),oS=i("text (required)"),lS=p(),oh=r("td"),iS=i("The last input from the user in the conversation."),uS=p(),kl=r("tr"),lh=r("td"),cS=i("generated_responses"),fS=p(),ih=r("td"),pS=i("A list of strings corresponding to the earlier replies from the model."),hS=p(),Al=r("tr"),uh=r("td"),dS=i("past_user_inputs"),gS=p(),fa=r("td"),mS=i("A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),Wq=r("code"),$S=i("generated_responses"),qS=i("."),_S=p(),Dl=r("tr"),ch=r("td"),Yq=r("strong"),vS=i("parameters"),yS=p(),fh=r("td"),ES=i("a dict containing the following keys:"),wS=p(),Ol=r("tr"),ph=r("td"),bS=i("min_length"),TS=p(),ke=r("td"),jS=i("(Default: "),Vq=r("code"),kS=i("None"),AS=i("). Integer to define the minimum length "),Xq=r("strong"),DS=i("in tokens"),OS=i(" of the output summary."),PS=p(),Pl=r("tr"),hh=r("td"),RS=i("max_length"),SS=p(),Ae=r("td"),NS=i("(Default: "),Qq=r("code"),xS=i("None"),IS=i("). Integer to define the maximum length "),Zq=r("strong"),HS=i("in tokens"),BS=i(" of the output summary."),CS=p(),Rl=r("tr"),dh=r("td"),GS=i("top_k"),LS=p(),De=r("td"),MS=i("(Default: "),e_=r("code"),US=i("None"),zS=i("). Integer to define the top tokens considered within the "),t_=r("code"),KS=i("sample"),FS=i(" operation to create new text."),JS=p(),Sl=r("tr"),gh=r("td"),WS=i("top_p"),YS=p(),pe=r("td"),VS=i("(Default: "),s_=r("code"),XS=i("None"),QS=i("). Float to define the tokens that are within the "),a_=r("code"),ZS=i("sample"),eN=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),n_=r("code"),tN=i("top_p"),sN=i("."),aN=p(),Nl=r("tr"),mh=r("td"),nN=i("temperature"),rN=p(),he=r("td"),oN=i("(Default: "),r_=r("code"),lN=i("1.0"),iN=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),o_=r("code"),uN=i("0"),cN=i(" means always take the highest score, "),l_=r("code"),fN=i("100.0"),pN=i(" is getting closer to uniform probability."),hN=p(),xl=r("tr"),$h=r("td"),dN=i("repetition_penalty"),gN=p(),pa=r("td"),mN=i("(Default: "),i_=r("code"),$N=i("None"),qN=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),_N=p(),Il=r("tr"),qh=r("td"),vN=i("max_time"),yN=p(),ha=r("td"),EN=i("(Default: "),u_=r("code"),wN=i("None"),bN=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),TN=p(),Hl=r("tr"),_h=r("td"),c_=r("strong"),jN=i("options"),kN=p(),vh=r("td"),AN=i("a dict containing the following keys:"),DN=p(),Bl=r("tr"),yh=r("td"),ON=i("use_gpu"),PN=p(),da=r("td"),RN=i("(Default: "),f_=r("code"),SN=i("false"),NN=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),xN=p(),Cl=r("tr"),Eh=r("td"),IN=i("use_cache"),HN=p(),ga=r("td"),BN=i("(Default: "),p_=r("code"),CN=i("true"),GN=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),LN=p(),Gl=r("tr"),wh=r("td"),MN=i("wait_for_model"),UN=p(),ma=r("td"),zN=i("(Default: "),h_=r("code"),KN=i("false"),FN=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Oy=p(),bh=r("p"),JN=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Py=p(),$a=r("table"),d_=r("thead"),Ll=r("tr"),Th=r("th"),WN=i("Returned values"),YN=p(),g_=r("th"),VN=p(),$e=r("tbody"),Ml=r("tr"),jh=r("td"),m_=r("strong"),XN=i("generated_text"),QN=p(),kh=r("td"),ZN=i("The answer of the bot"),ex=p(),Ul=r("tr"),Ah=r("td"),$_=r("strong"),tx=i("conversation"),sx=p(),Dh=r("td"),ax=i("A facility dictionnary to send back for the next input (with the new user input addition)."),nx=p(),zl=r("tr"),Oh=r("td"),rx=i("past_user_inputs"),ox=p(),Ph=r("td"),lx=i("List of strings. The last inputs from the user in the conversation, <em>after the model has run."),ix=p(),Kl=r("tr"),Rh=r("td"),ux=i("generated_responses"),cx=p(),Sh=r("td"),fx=i("List of strings. The last outputs from the model in the conversation, <em>after the model has run."),Ry=p(),et=r("h3"),qa=r("a"),q_=r("span"),_(Fl.$$.fragment),px=p(),__=r("span"),hx=i("Feature Extraction task"),Sy=p(),Nh=r("p"),dx=i(`This task reads some text and outputs raw float values, that are usually
consumed as part of a semantic database/semantic search.`),Ny=p(),_(_a.$$.fragment),xy=p(),tt=r("p"),gx=i("Available with: "),Jl=r("a"),mx=i("\u{1F917} Transformers"),$x=p(),Wl=r("a"),qx=i("Sentence-transformers"),Iy=p(),xh=r("p"),_x=i("Request:"),Hy=p(),va=r("table"),v_=r("thead"),Yl=r("tr"),Ih=r("th"),vx=i("All parameters"),yx=p(),y_=r("th"),Ex=p(),re=r("tbody"),Vl=r("tr"),Xl=r("td"),E_=r("strong"),wx=i("inputs"),bx=i(" (required):"),Tx=p(),Hh=r("td"),jx=i("a string or a list of strings to get the features from."),kx=p(),Ql=r("tr"),Bh=r("td"),w_=r("strong"),Ax=i("options"),Dx=p(),Ch=r("td"),Ox=i("a dict containing the following keys:"),Px=p(),Zl=r("tr"),Gh=r("td"),Rx=i("use_gpu"),Sx=p(),ya=r("td"),Nx=i("(Default: "),b_=r("code"),xx=i("false"),Ix=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Hx=p(),ei=r("tr"),Lh=r("td"),Bx=i("use_cache"),Cx=p(),Ea=r("td"),Gx=i("(Default: "),T_=r("code"),Lx=i("true"),Mx=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Ux=p(),ti=r("tr"),Mh=r("td"),zx=i("wait_for_model"),Kx=p(),wa=r("td"),Fx=i("(Default: "),j_=r("code"),Jx=i("false"),Wx=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),By=p(),Uh=r("p"),Yx=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Cy=p(),ba=r("table"),k_=r("thead"),si=r("tr"),zh=r("th"),Vx=i("Returned values"),Xx=p(),A_=r("th"),Qx=p(),D_=r("tbody"),ai=r("tr"),Kh=r("td"),O_=r("strong"),Zx=i("A list of float (or list of list of floats)"),eI=p(),Fh=r("td"),tI=i("The numbers that are the representation features of the input."),Gy=p(),Jh=r("small"),sI=i(`Returned values are a list of floats, or a list of list of floats (depending
  on if you sent a string or a list of string, and if the automatic reduction,
  usually mean_pooling for instance was applied for you or not. This should be
  explained on the model's README.`),Ly=p(),st=r("h2"),Ta=r("a"),P_=r("span"),_(ni.$$.fragment),aI=p(),R_=r("span"),nI=i("Audio"),My=p(),at=r("h3"),ja=r("a"),S_=r("span"),_(ri.$$.fragment),rI=p(),N_=r("span"),oI=i("Automatic Speech Recognition task"),Uy=p(),Wh=r("p"),lI=i(`This task reads some audio input and outputs the said words within the
audio files.`),zy=p(),_(ka.$$.fragment),Ky=p(),_(Aa.$$.fragment),Fy=p(),qe=r("p"),iI=i("Available with: "),oi=r("a"),uI=i("\u{1F917} Transformers"),cI=p(),li=r("a"),fI=i("ESPnet"),pI=i(` and
`),ii=r("a"),hI=i("SpeechBrain"),Jy=p(),Yh=r("p"),dI=i("Request:"),Wy=p(),_(Da.$$.fragment),Yy=p(),Vh=r("p"),gI=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),Vy=p(),Oa=r("table"),x_=r("thead"),ui=r("tr"),Xh=r("th"),mI=i("All parameters"),$I=p(),I_=r("th"),qI=p(),H_=r("tbody"),ci=r("tr"),fi=r("td"),B_=r("strong"),_I=i("no parameter"),vI=i(" (required)"),yI=p(),Qh=r("td"),EI=i("a binary representation of the audio file. No other parameters are currently allowed."),Xy=p(),Zh=r("p"),wI=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Qy=p(),ed=r("p"),bI=i("Response:"),Zy=p(),_(Pa.$$.fragment),eE=p(),Ra=r("table"),C_=r("thead"),pi=r("tr"),td=r("th"),TI=i("Returned values"),jI=p(),G_=r("th"),kI=p(),L_=r("tbody"),hi=r("tr"),sd=r("td"),M_=r("strong"),AI=i("text"),DI=p(),ad=r("td"),OI=i("The string that was recognized within the audio file."),tE=p(),nt=r("h3"),Sa=r("a"),U_=r("span"),_(di.$$.fragment),PI=p(),z_=r("span"),RI=i("Audio Classification task"),sE=p(),nd=r("p"),SI=i("This task reads some audio input and outputs the likelihood of classes."),aE=p(),_(Na.$$.fragment),nE=p(),rt=r("p"),NI=i("Available with: "),gi=r("a"),xI=i("\u{1F917} Transformers"),II=p(),mi=r("a"),HI=i("SpeechBrain"),rE=p(),rd=r("p"),BI=i("Request:"),oE=p(),_(xa.$$.fragment),lE=p(),od=r("p"),CI=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),iE=p(),Ia=r("table"),K_=r("thead"),$i=r("tr"),ld=r("th"),GI=i("All parameters"),LI=p(),F_=r("th"),MI=p(),J_=r("tbody"),qi=r("tr"),_i=r("td"),W_=r("strong"),UI=i("no parameter"),zI=i(" (required)"),KI=p(),id=r("td"),FI=i("a binary representation of the audio file. No other parameters are currently allowed."),uE=p(),ud=r("p"),JI=i("Return value is a dict"),cE=p(),_(Ha.$$.fragment),fE=p(),Ba=r("table"),Y_=r("thead"),vi=r("tr"),cd=r("th"),WI=i("Returned values"),YI=p(),V_=r("th"),VI=p(),yi=r("tbody"),Ei=r("tr"),fd=r("td"),X_=r("strong"),XI=i("label"),QI=p(),pd=r("td"),ZI=i("The label for the class (model specific)"),eH=p(),wi=r("tr"),hd=r("td"),Q_=r("strong"),tH=i("score"),sH=p(),dd=r("td"),aH=i("A float that represents how likely it is that the audio file belongs to this class."),pE=p(),ot=r("h2"),Ca=r("a"),Z_=r("span"),_(bi.$$.fragment),nH=p(),e1=r("span"),rH=i("Computer Vision"),hE=p(),lt=r("h3"),Ga=r("a"),t1=r("span"),_(Ti.$$.fragment),oH=p(),s1=r("span"),lH=i("Image Classification task"),dE=p(),gd=r("p"),iH=i("This task reads some image input and outputs the likelihood of classes."),gE=p(),_(La.$$.fragment),mE=p(),ji=r("p"),uH=i("Available with: "),ki=r("a"),cH=i("\u{1F917} Transformers"),$E=p(),md=r("p"),fH=i("Request:"),qE=p(),_(Ma.$$.fragment),_E=p(),Ua=r("p"),pH=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Ai=r("a"),hH=i(`Pillow
supports`),dH=i("."),vE=p(),za=r("table"),a1=r("thead"),Di=r("tr"),$d=r("th"),gH=i("All parameters"),mH=p(),n1=r("th"),$H=p(),r1=r("tbody"),Oi=r("tr"),Pi=r("td"),o1=r("strong"),qH=i("no parameter"),_H=i(" (required)"),vH=p(),qd=r("td"),yH=i("a binary representation of the image file. No other parameters are currently allowed."),yE=p(),_d=r("p"),EH=i("Return value is a dict"),EE=p(),_(Ka.$$.fragment),wE=p(),Fa=r("table"),l1=r("thead"),Ri=r("tr"),vd=r("th"),wH=i("Returned values"),bH=p(),i1=r("th"),TH=p(),Si=r("tbody"),Ni=r("tr"),yd=r("td"),u1=r("strong"),jH=i("label"),kH=p(),Ed=r("td"),AH=i("The label for the class (model specific)"),DH=p(),xi=r("tr"),wd=r("td"),c1=r("strong"),OH=i("score"),PH=p(),bd=r("td"),RH=i("A float that represents how likely it is that the image file belongs to this class."),bE=p(),it=r("h3"),Ja=r("a"),f1=r("span"),_(Ii.$$.fragment),SH=p(),p1=r("span"),NH=i("Object Detection task"),TE=p(),Td=r("p"),xH=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),jE=p(),_(Wa.$$.fragment),kE=p(),Hi=r("p"),IH=i("Available with: "),Bi=r("a"),HH=i("\u{1F917} Transformers"),AE=p(),jd=r("p"),BH=i("Request:"),DE=p(),_(Ya.$$.fragment),OE=p(),Va=r("p"),CH=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Ci=r("a"),GH=i(`Pillow
supports`),LH=i("."),PE=p(),Xa=r("table"),h1=r("thead"),Gi=r("tr"),kd=r("th"),MH=i("All parameters"),UH=p(),d1=r("th"),zH=p(),g1=r("tbody"),Li=r("tr"),Mi=r("td"),m1=r("strong"),KH=i("no parameter"),FH=i(" (required)"),JH=p(),Ad=r("td"),WH=i("a binary representation of the image file. No other parameters are currently allowed."),RE=p(),Dd=r("p"),YH=i("Return value is a dict"),SE=p(),_(Qa.$$.fragment),NE=p(),Za=r("table"),$1=r("thead"),Ui=r("tr"),Od=r("th"),VH=i("Returned values"),XH=p(),q1=r("th"),QH=p(),ut=r("tbody"),zi=r("tr"),Pd=r("td"),_1=r("strong"),ZH=i("label"),eB=p(),Rd=r("td"),tB=i("The label for the class (model specific) of a detected object."),sB=p(),Ki=r("tr"),Sd=r("td"),v1=r("strong"),aB=i("score"),nB=p(),Nd=r("td"),rB=i("A float that represents how likely it is that the detected object belongs to the given class."),oB=p(),Fi=r("tr"),xd=r("td"),y1=r("strong"),lB=i("box"),iB=p(),Id=r("td"),uB=i("A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),xE=p(),ct=r("h3"),en=r("a"),E1=r("span"),_(Ji.$$.fragment),cB=p(),w1=r("span"),fB=i("Image Segmentation task"),IE=p(),Hd=r("p"),pB=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),HE=p(),_(tn.$$.fragment),BE=p(),Wi=r("p"),hB=i("Available with: "),Yi=r("a"),dB=i("\u{1F917} Transformers"),CE=p(),Bd=r("p"),gB=i("Request:"),GE=p(),_(sn.$$.fragment),LE=p(),an=r("p"),mB=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Vi=r("a"),$B=i(`Pillow
supports`),qB=i("."),ME=p(),nn=r("table"),b1=r("thead"),Xi=r("tr"),Cd=r("th"),_B=i("All parameters"),vB=p(),T1=r("th"),yB=p(),j1=r("tbody"),Qi=r("tr"),Zi=r("td"),k1=r("strong"),EB=i("no parameter"),wB=i(" (required)"),bB=p(),Gd=r("td"),TB=i("a binary representation of the image file. No other parameters are currently allowed."),UE=p(),Ld=r("p"),jB=i("Return value is a dict"),zE=p(),_(rn.$$.fragment),KE=p(),on=r("table"),A1=r("thead"),eu=r("tr"),Md=r("th"),kB=i("Returned values"),AB=p(),D1=r("th"),DB=p(),ft=r("tbody"),tu=r("tr"),Ud=r("td"),O1=r("strong"),OB=i("label"),PB=p(),zd=r("td"),RB=i("The label for the class (model specific) of a segment."),SB=p(),su=r("tr"),Kd=r("td"),P1=r("strong"),NB=i("score"),xB=p(),Fd=r("td"),IB=i("A float that represents how likely it is that the segment belongs to the given class."),HB=p(),au=r("tr"),Jd=r("td"),R1=r("strong"),BB=i("mask"),CB=p(),Wd=r("td"),GB=i("A str (base64 str of a single channel black-and-white img) representing the mask of a segment."),this.h()},l(a){const g=oV('[data-svelte="svelte-1phssyn"]',document.head);n=o(g,"META",{name:!0,content:!0}),g.forEach(t),c=h(a),s=o(a,"H1",{class:!0});var nu=l(s);d=o(nu,"A",{id:!0,class:!0,href:!0});var S1=l(d);$=o(S1,"SPAN",{});var N1=l($);v(k.$$.fragment,N1),N1.forEach(t),S1.forEach(t),A=h(nu),j=o(nu,"SPAN",{});var x1=l(j);T=u(x1,"Detailed parameters"),x1.forEach(t),nu.forEach(t),S=h(a),D=o(a,"H2",{class:!0});var ru=l(D);le=o(ru,"A",{id:!0,class:!0,href:!0});var I1=l(le);Ne=o(I1,"SPAN",{});var H1=l(Ne);v(ee.$$.fragment,H1),H1.forEach(t),I1.forEach(t),V=h(ru),pt=o(ru,"SPAN",{});var B1=l(pt);_u=u(B1,"Which task is used by this model ?"),B1.forEach(t),ru.forEach(t),qn=h(a),xe=o(a,"P",{});var C1=l(xe);WT=u(C1,`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),C1.forEach(t),dv=h(a),vu=o(a,"P",{});var G1=l(vu);YT=u(G1,"The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),G1.forEach(t),gv=h(a),ht=o(a,"IMG",{class:!0,src:!0,width:!0}),mv=h(a),dt=o(a,"IMG",{class:!0,src:!0,width:!0}),$v=h(a),Ie=o(a,"H2",{class:!0});var ou=l(Ie);gt=o(ou,"A",{id:!0,class:!0,href:!0});var L1=l(gt);fg=o(L1,"SPAN",{});var M1=l(fg);v(_n.$$.fragment,M1),M1.forEach(t),L1.forEach(t),VT=h(ou),pg=o(ou,"SPAN",{});var U1=l(pg);XT=u(U1,"Natural Language Processing"),U1.forEach(t),ou.forEach(t),qv=h(a),He=o(a,"H3",{class:!0});var lu=l(He);mt=o(lu,"A",{id:!0,class:!0,href:!0});var z1=l(mt);hg=o(z1,"SPAN",{});var K1=l(hg);v(vn.$$.fragment,K1),K1.forEach(t),z1.forEach(t),QT=h(lu),dg=o(lu,"SPAN",{});var F1=l(dg);ZT=u(F1,"Fill Mask task"),F1.forEach(t),lu.forEach(t),_v=h(a),yu=o(a,"P",{});var J1=l(yu);ej=u(J1,`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),J1.forEach(t),vv=h(a),v($t.$$.fragment,a),yv=h(a),yn=o(a,"P",{});var Yd=l(yn);tj=u(Yd,"Available with: "),En=o(Yd,"A",{href:!0,rel:!0});var W1=l(En);sj=u(W1,"\u{1F917} Transformers"),W1.forEach(t),Yd.forEach(t),Ev=h(a),Eu=o(a,"P",{});var Y1=l(Eu);aj=u(Y1,"Example:"),Y1.forEach(t),wv=h(a),v(qt.$$.fragment,a),bv=h(a),wu=o(a,"P",{});var V1=l(wu);nj=u(V1,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),V1.forEach(t),Tv=h(a),_t=o(a,"TABLE",{});var iu=l(_t);gg=o(iu,"THEAD",{});var X1=l(gg);wn=o(X1,"TR",{});var uu=l(wn);bu=o(uu,"TH",{align:!0});var Q1=l(bu);rj=u(Q1,"All parameters"),Q1.forEach(t),oj=h(uu),mg=o(uu,"TH",{align:!0}),l(mg).forEach(t),uu.forEach(t),X1.forEach(t),lj=h(iu),te=o(iu,"TBODY",{});var oe=l(te);bn=o(oe,"TR",{});var cu=l(bn);Tn=o(cu,"TD",{align:!0});var Vd=l(Tn);$g=o(Vd,"STRONG",{});var Z1=l($g);ij=u(Z1,"inputs"),Z1.forEach(t),uj=u(Vd," (required):"),Vd.forEach(t),cj=h(cu),Tu=o(cu,"TD",{align:!0});var ev=l(Tu);fj=u(ev,"a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),ev.forEach(t),cu.forEach(t),pj=h(oe),jn=o(oe,"TR",{});var fu=l(jn);ju=o(fu,"TD",{align:!0});var tv=l(ju);qg=o(tv,"STRONG",{});var sv=l(qg);hj=u(sv,"options"),sv.forEach(t),tv.forEach(t),dj=h(fu),ku=o(fu,"TD",{align:!0});var av=l(ku);gj=u(av,"a dict containing the following keys:"),av.forEach(t),fu.forEach(t),mj=h(oe),kn=o(oe,"TR",{});var pu=l(kn);Au=o(pu,"TD",{align:!0});var nv=l(Au);$j=u(nv,"use_gpu"),nv.forEach(t),qj=h(pu),vt=o(pu,"TD",{align:!0});var hu=l(vt);_j=u(hu,"(Default: "),_g=o(hu,"CODE",{});var rv=l(_g);vj=u(rv,"false"),rv.forEach(t),yj=u(hu,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),hu.forEach(t),pu.forEach(t),Ej=h(oe),An=o(oe,"TR",{});var du=l(An);Du=o(du,"TD",{align:!0});var ov=l(Du);wj=u(ov,"use_cache"),ov.forEach(t),bj=h(du),yt=o(du,"TD",{align:!0});var gu=l(yt);Tj=u(gu,"(Default: "),vg=o(gu,"CODE",{});var lv=l(vg);jj=u(lv,"true"),lv.forEach(t),kj=u(gu,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),gu.forEach(t),du.forEach(t),Aj=h(oe),Dn=o(oe,"TR",{});var mu=l(Dn);Ou=o(mu,"TD",{align:!0});var iv=l(Ou);Dj=u(iv,"wait_for_model"),iv.forEach(t),Oj=h(mu),Et=o(mu,"TD",{align:!0});var $u=l(Et);Pj=u($u,"(Default: "),yg=o($u,"CODE",{});var mC=l(yg);Rj=u(mC,"false"),mC.forEach(t),Sj=u($u,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),$u.forEach(t),mu.forEach(t),oe.forEach(t),iu.forEach(t),jv=h(a),Pu=o(a,"P",{});var $C=l(Pu);Nj=u($C,"Return value is either a dict or a list of dicts if you sent a list of inputs"),$C.forEach(t),kv=h(a),v(wt.$$.fragment,a),Av=h(a),bt=o(a,"TABLE",{});var JE=l(bt);Eg=o(JE,"THEAD",{});var qC=l(Eg);On=o(qC,"TR",{});var WE=l(On);Ru=o(WE,"TH",{align:!0});var _C=l(Ru);xj=u(_C,"Returned values"),_C.forEach(t),Ij=h(WE),wg=o(WE,"TH",{align:!0}),l(wg).forEach(t),WE.forEach(t),qC.forEach(t),Hj=h(JE),de=o(JE,"TBODY",{});var ln=l(de);Pn=o(ln,"TR",{});var YE=l(Pn);Su=o(YE,"TD",{align:!0});var vC=l(Su);bg=o(vC,"STRONG",{});var yC=l(bg);Bj=u(yC,"sequence"),yC.forEach(t),vC.forEach(t),Cj=h(YE),Nu=o(YE,"TD",{align:!0});var EC=l(Nu);Gj=u(EC,"The actual sequence of tokens that ran against the model (may contain special tokens)"),EC.forEach(t),YE.forEach(t),Lj=h(ln),Rn=o(ln,"TR",{});var VE=l(Rn);xu=o(VE,"TD",{align:!0});var wC=l(xu);Tg=o(wC,"STRONG",{});var bC=l(Tg);Mj=u(bC,"score"),bC.forEach(t),wC.forEach(t),Uj=h(VE),Iu=o(VE,"TD",{align:!0});var TC=l(Iu);zj=u(TC,"The probability for this token."),TC.forEach(t),VE.forEach(t),Kj=h(ln),Sn=o(ln,"TR",{});var XE=l(Sn);Hu=o(XE,"TD",{align:!0});var jC=l(Hu);jg=o(jC,"STRONG",{});var kC=l(jg);Fj=u(kC,"token"),kC.forEach(t),jC.forEach(t),Jj=h(XE),Bu=o(XE,"TD",{align:!0});var AC=l(Bu);Wj=u(AC,"The id of the token"),AC.forEach(t),XE.forEach(t),Yj=h(ln),Nn=o(ln,"TR",{});var QE=l(Nn);Cu=o(QE,"TD",{align:!0});var DC=l(Cu);kg=o(DC,"STRONG",{});var OC=l(kg);Vj=u(OC,"token_str"),OC.forEach(t),DC.forEach(t),Xj=h(QE),Gu=o(QE,"TD",{align:!0});var PC=l(Gu);Qj=u(PC,"The string representation of the token"),PC.forEach(t),QE.forEach(t),ln.forEach(t),JE.forEach(t),Dv=h(a),Be=o(a,"H3",{class:!0});var ZE=l(Be);Tt=o(ZE,"A",{id:!0,class:!0,href:!0});var RC=l(Tt);Ag=o(RC,"SPAN",{});var SC=l(Ag);v(xn.$$.fragment,SC),SC.forEach(t),RC.forEach(t),Zj=h(ZE),Dg=o(ZE,"SPAN",{});var NC=l(Dg);e4=u(NC,"Summarization task"),NC.forEach(t),ZE.forEach(t),Ov=h(a),jt=o(a,"P",{});var ew=l(jt);t4=u(ew,`This task is well known to summarize longer text into shorter text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss your summarization needs,
please get in touch with us: <`),Lu=o(ew,"A",{href:!0});var xC=l(Lu);s4=u(xC,"api-enterprise@huggingface.co"),xC.forEach(t),a4=u(ew,">"),ew.forEach(t),Pv=h(a),v(kt.$$.fragment,a),Rv=h(a),In=o(a,"P",{});var LB=l(In);n4=u(LB,"Available with: "),Hn=o(LB,"A",{href:!0,rel:!0});var IC=l(Hn);r4=u(IC,"\u{1F917} Transformers"),IC.forEach(t),LB.forEach(t),Sv=h(a),Mu=o(a,"P",{});var HC=l(Mu);o4=u(HC,"Example:"),HC.forEach(t),Nv=h(a),v(At.$$.fragment,a),xv=h(a),Uu=o(a,"P",{});var BC=l(Uu);l4=u(BC,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),BC.forEach(t),Iv=h(a),Dt=o(a,"TABLE",{});var tw=l(Dt);Og=o(tw,"THEAD",{});var CC=l(Og);Bn=o(CC,"TR",{});var sw=l(Bn);zu=o(sw,"TH",{align:!0});var GC=l(zu);i4=u(GC,"All parameters"),GC.forEach(t),u4=h(sw),Pg=o(sw,"TH",{align:!0}),l(Pg).forEach(t),sw.forEach(t),CC.forEach(t),c4=h(tw),G=o(tw,"TBODY",{});var M=l(G);Cn=o(M,"TR",{});var aw=l(Cn);Gn=o(aw,"TD",{align:!0});var MB=l(Gn);Rg=o(MB,"STRONG",{});var LC=l(Rg);f4=u(LC,"inputs"),LC.forEach(t),p4=u(MB," (required)"),MB.forEach(t),h4=h(aw),Ku=o(aw,"TD",{align:!0});var MC=l(Ku);d4=u(MC,"a string to be summarized"),MC.forEach(t),aw.forEach(t),g4=h(M),Ln=o(M,"TR",{});var nw=l(Ln);Fu=o(nw,"TD",{align:!0});var UC=l(Fu);Sg=o(UC,"STRONG",{});var zC=l(Sg);m4=u(zC,"parameters"),zC.forEach(t),UC.forEach(t),$4=h(nw),Ju=o(nw,"TD",{align:!0});var KC=l(Ju);q4=u(KC,"a dict containing the following keys:"),KC.forEach(t),nw.forEach(t),_4=h(M),Mn=o(M,"TR",{});var rw=l(Mn);Wu=o(rw,"TD",{align:!0});var FC=l(Wu);v4=u(FC,"min_length"),FC.forEach(t),y4=h(rw),_e=o(rw,"TD",{align:!0});var Xd=l(_e);E4=u(Xd,"(Default: "),Ng=o(Xd,"CODE",{});var JC=l(Ng);w4=u(JC,"None"),JC.forEach(t),b4=u(Xd,"). Integer to define the minimum length "),xg=o(Xd,"STRONG",{});var WC=l(xg);T4=u(WC,"in tokens"),WC.forEach(t),j4=u(Xd," of the output summary."),Xd.forEach(t),rw.forEach(t),k4=h(M),Un=o(M,"TR",{});var ow=l(Un);Yu=o(ow,"TD",{align:!0});var YC=l(Yu);A4=u(YC,"max_length"),YC.forEach(t),D4=h(ow),ve=o(ow,"TD",{align:!0});var Qd=l(ve);O4=u(Qd,"(Default: "),Ig=o(Qd,"CODE",{});var VC=l(Ig);P4=u(VC,"None"),VC.forEach(t),R4=u(Qd,"). Integer to define the maximum length "),Hg=o(Qd,"STRONG",{});var XC=l(Hg);S4=u(XC,"in tokens"),XC.forEach(t),N4=u(Qd," of the output summary."),Qd.forEach(t),ow.forEach(t),x4=h(M),zn=o(M,"TR",{});var lw=l(zn);Vu=o(lw,"TD",{align:!0});var QC=l(Vu);I4=u(QC,"top_k"),QC.forEach(t),H4=h(lw),ye=o(lw,"TD",{align:!0});var Zd=l(ye);B4=u(Zd,"(Default: "),Bg=o(Zd,"CODE",{});var ZC=l(Bg);C4=u(ZC,"None"),ZC.forEach(t),G4=u(Zd,"). Integer to define the top tokens considered within the "),Cg=o(Zd,"CODE",{});var eG=l(Cg);L4=u(eG,"sample"),eG.forEach(t),M4=u(Zd," operation to create new text."),Zd.forEach(t),lw.forEach(t),U4=h(M),Kn=o(M,"TR",{});var iw=l(Kn);Xu=o(iw,"TD",{align:!0});var tG=l(Xu);z4=u(tG,"top_p"),tG.forEach(t),K4=h(iw),ie=o(iw,"TD",{align:!0});var un=l(ie);F4=u(un,"(Default: "),Gg=o(un,"CODE",{});var sG=l(Gg);J4=u(sG,"None"),sG.forEach(t),W4=u(un,"). Float to define the tokens that are within the "),Lg=o(un,"CODE",{});var aG=l(Lg);Y4=u(aG,"sample"),aG.forEach(t),V4=u(un," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Mg=o(un,"CODE",{});var nG=l(Mg);X4=u(nG,"top_p"),nG.forEach(t),Q4=u(un,"."),un.forEach(t),iw.forEach(t),Z4=h(M),Fn=o(M,"TR",{});var uw=l(Fn);Qu=o(uw,"TD",{align:!0});var rG=l(Qu);e5=u(rG,"temperature"),rG.forEach(t),t5=h(uw),ue=o(uw,"TD",{align:!0});var cn=l(ue);s5=u(cn,"(Default: "),Ug=o(cn,"CODE",{});var oG=l(Ug);a5=u(oG,"1.0"),oG.forEach(t),n5=u(cn,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),zg=o(cn,"CODE",{});var lG=l(zg);r5=u(lG,"0"),lG.forEach(t),o5=u(cn," means always take the highest score, "),Kg=o(cn,"CODE",{});var iG=l(Kg);l5=u(iG,"100.0"),iG.forEach(t),i5=u(cn," is getting closer to uniform probability."),cn.forEach(t),uw.forEach(t),u5=h(M),Jn=o(M,"TR",{});var cw=l(Jn);Zu=o(cw,"TD",{align:!0});var uG=l(Zu);c5=u(uG,"repetition_penalty"),uG.forEach(t),f5=h(cw),Ot=o(cw,"TD",{align:!0});var fw=l(Ot);p5=u(fw,"(Default: "),Fg=o(fw,"CODE",{});var cG=l(Fg);h5=u(cG,"None"),cG.forEach(t),d5=u(fw,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),fw.forEach(t),cw.forEach(t),g5=h(M),Wn=o(M,"TR",{});var pw=l(Wn);ec=o(pw,"TD",{align:!0});var fG=l(ec);m5=u(fG,"max_time"),fG.forEach(t),$5=h(pw),Pt=o(pw,"TD",{align:!0});var hw=l(Pt);q5=u(hw,"(Default: "),Jg=o(hw,"CODE",{});var pG=l(Jg);_5=u(pG,"None"),pG.forEach(t),v5=u(hw,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),hw.forEach(t),pw.forEach(t),y5=h(M),Yn=o(M,"TR",{});var dw=l(Yn);tc=o(dw,"TD",{align:!0});var hG=l(tc);Wg=o(hG,"STRONG",{});var dG=l(Wg);E5=u(dG,"options"),dG.forEach(t),hG.forEach(t),w5=h(dw),sc=o(dw,"TD",{align:!0});var gG=l(sc);b5=u(gG,"a dict containing the following keys:"),gG.forEach(t),dw.forEach(t),T5=h(M),Vn=o(M,"TR",{});var gw=l(Vn);ac=o(gw,"TD",{align:!0});var mG=l(ac);j5=u(mG,"use_gpu"),mG.forEach(t),k5=h(gw),Rt=o(gw,"TD",{align:!0});var mw=l(Rt);A5=u(mw,"(Default: "),Yg=o(mw,"CODE",{});var $G=l(Yg);D5=u($G,"false"),$G.forEach(t),O5=u(mw,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),mw.forEach(t),gw.forEach(t),P5=h(M),Xn=o(M,"TR",{});var $w=l(Xn);nc=o($w,"TD",{align:!0});var qG=l(nc);R5=u(qG,"use_cache"),qG.forEach(t),S5=h($w),St=o($w,"TD",{align:!0});var qw=l(St);N5=u(qw,"(Default: "),Vg=o(qw,"CODE",{});var _G=l(Vg);x5=u(_G,"true"),_G.forEach(t),I5=u(qw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),qw.forEach(t),$w.forEach(t),H5=h(M),Qn=o(M,"TR",{});var _w=l(Qn);rc=o(_w,"TD",{align:!0});var vG=l(rc);B5=u(vG,"wait_for_model"),vG.forEach(t),C5=h(_w),Nt=o(_w,"TD",{align:!0});var vw=l(Nt);G5=u(vw,"(Default: "),Xg=o(vw,"CODE",{});var yG=l(Xg);L5=u(yG,"false"),yG.forEach(t),M5=u(vw,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),vw.forEach(t),_w.forEach(t),M.forEach(t),tw.forEach(t),Hv=h(a),oc=o(a,"P",{});var EG=l(oc);U5=u(EG,"Return value is either a dict or a list of dicts if you sent a list of inputs"),EG.forEach(t),Bv=h(a),xt=o(a,"TABLE",{});var yw=l(xt);Qg=o(yw,"THEAD",{});var wG=l(Qg);Zn=o(wG,"TR",{});var Ew=l(Zn);lc=o(Ew,"TH",{align:!0});var bG=l(lc);z5=u(bG,"Returned values"),bG.forEach(t),K5=h(Ew),Zg=o(Ew,"TH",{align:!0}),l(Zg).forEach(t),Ew.forEach(t),wG.forEach(t),F5=h(yw),em=o(yw,"TBODY",{});var TG=l(em);er=o(TG,"TR",{});var ww=l(er);ic=o(ww,"TD",{align:!0});var jG=l(ic);tm=o(jG,"STRONG",{});var kG=l(tm);J5=u(kG,"summarization_text"),kG.forEach(t),jG.forEach(t),W5=h(ww),uc=o(ww,"TD",{align:!0});var AG=l(uc);Y5=u(AG,"The string after translation"),AG.forEach(t),ww.forEach(t),TG.forEach(t),yw.forEach(t),Cv=h(a),Ce=o(a,"H3",{class:!0});var bw=l(Ce);It=o(bw,"A",{id:!0,class:!0,href:!0});var DG=l(It);sm=o(DG,"SPAN",{});var OG=l(sm);v(tr.$$.fragment,OG),OG.forEach(t),DG.forEach(t),V5=h(bw),am=o(bw,"SPAN",{});var PG=l(am);X5=u(PG,"Question Answering task"),PG.forEach(t),bw.forEach(t),Gv=h(a),cc=o(a,"P",{});var RG=l(cc);Q5=u(RG,"Want to have a nice know-it-all bot that can answer any question?"),RG.forEach(t),Lv=h(a),v(Ht.$$.fragment,a),Mv=h(a),Ge=o(a,"P",{});var uv=l(Ge);Z5=u(uv,"Available with: "),sr=o(uv,"A",{href:!0,rel:!0});var SG=l(sr);ek=u(SG,"\u{1F917}Transformers"),SG.forEach(t),tk=u(uv,` and
`),ar=o(uv,"A",{href:!0,rel:!0});var NG=l(ar);sk=u(NG,"AllenNLP"),NG.forEach(t),uv.forEach(t),Uv=h(a),fc=o(a,"P",{});var xG=l(fc);ak=u(xG,"Example:"),xG.forEach(t),zv=h(a),v(Bt.$$.fragment,a),Kv=h(a),pc=o(a,"P",{});var IG=l(pc);nk=u(IG,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),IG.forEach(t),Fv=h(a),hc=o(a,"P",{});var HG=l(hc);rk=u(HG,"Return value is either a dict or a list of dicts if you sent a list of inputs"),HG.forEach(t),Jv=h(a),v(Ct.$$.fragment,a),Wv=h(a),Gt=o(a,"TABLE",{});var Tw=l(Gt);nm=o(Tw,"THEAD",{});var BG=l(nm);nr=o(BG,"TR",{});var jw=l(nr);dc=o(jw,"TH",{align:!0});var CG=l(dc);ok=u(CG,"Returned values"),CG.forEach(t),lk=h(jw),rm=o(jw,"TH",{align:!0}),l(rm).forEach(t),jw.forEach(t),BG.forEach(t),ik=h(Tw),ge=o(Tw,"TBODY",{});var fn=l(ge);rr=o(fn,"TR",{});var kw=l(rr);gc=o(kw,"TD",{align:!0});var GG=l(gc);om=o(GG,"STRONG",{});var LG=l(om);uk=u(LG,"answer"),LG.forEach(t),GG.forEach(t),ck=h(kw),mc=o(kw,"TD",{align:!0});var MG=l(mc);fk=u(MG,"A string that\u2019s the answer within the text."),MG.forEach(t),kw.forEach(t),pk=h(fn),or=o(fn,"TR",{});var Aw=l(or);$c=o(Aw,"TD",{align:!0});var UG=l($c);lm=o(UG,"STRONG",{});var zG=l(lm);hk=u(zG,"score"),zG.forEach(t),UG.forEach(t),dk=h(Aw),qc=o(Aw,"TD",{align:!0});var KG=l(qc);gk=u(KG,"A float that represents how likely that the answer is correct"),KG.forEach(t),Aw.forEach(t),mk=h(fn),lr=o(fn,"TR",{});var Dw=l(lr);_c=o(Dw,"TD",{align:!0});var FG=l(_c);im=o(FG,"STRONG",{});var JG=l(im);$k=u(JG,"start"),JG.forEach(t),FG.forEach(t),qk=h(Dw),Lt=o(Dw,"TD",{align:!0});var Ow=l(Lt);_k=u(Ow,"The index (string wise) of the start of the answer within "),um=o(Ow,"CODE",{});var WG=l(um);vk=u(WG,"context"),WG.forEach(t),yk=u(Ow,"."),Ow.forEach(t),Dw.forEach(t),Ek=h(fn),ir=o(fn,"TR",{});var Pw=l(ir);vc=o(Pw,"TD",{align:!0});var YG=l(vc);cm=o(YG,"STRONG",{});var VG=l(cm);wk=u(VG,"stop"),VG.forEach(t),YG.forEach(t),bk=h(Pw),Mt=o(Pw,"TD",{align:!0});var Rw=l(Mt);Tk=u(Rw,"The index (string wise) of the stop of the answer within "),fm=o(Rw,"CODE",{});var XG=l(fm);jk=u(XG,"context"),XG.forEach(t),kk=u(Rw,"."),Rw.forEach(t),Pw.forEach(t),fn.forEach(t),Tw.forEach(t),Yv=h(a),Le=o(a,"H3",{class:!0});var Sw=l(Le);Ut=o(Sw,"A",{id:!0,class:!0,href:!0});var QG=l(Ut);pm=o(QG,"SPAN",{});var ZG=l(pm);v(ur.$$.fragment,ZG),ZG.forEach(t),QG.forEach(t),Ak=h(Sw),hm=o(Sw,"SPAN",{});var eL=l(hm);Dk=u(eL,"Table Question Answering task"),eL.forEach(t),Sw.forEach(t),Vv=h(a),yc=o(a,"P",{});var tL=l(yc);Ok=u(tL,`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),tL.forEach(t),Xv=h(a),v(zt.$$.fragment,a),Qv=h(a),cr=o(a,"P",{});var UB=l(cr);Pk=u(UB,"Available with: "),fr=o(UB,"A",{href:!0,rel:!0});var sL=l(fr);Rk=u(sL,"\u{1F917} Transformers"),sL.forEach(t),UB.forEach(t),Zv=h(a),Ec=o(a,"P",{});var aL=l(Ec);Sk=u(aL,"Example:"),aL.forEach(t),e2=h(a),v(Kt.$$.fragment,a),t2=h(a),wc=o(a,"P",{});var nL=l(wc);Nk=u(nL,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),nL.forEach(t),s2=h(a),Ft=o(a,"TABLE",{});var Nw=l(Ft);dm=o(Nw,"THEAD",{});var rL=l(dm);pr=o(rL,"TR",{});var xw=l(pr);bc=o(xw,"TH",{align:!0});var oL=l(bc);xk=u(oL,"All parameters"),oL.forEach(t),Ik=h(xw),gm=o(xw,"TH",{align:!0}),l(gm).forEach(t),xw.forEach(t),rL.forEach(t),Hk=h(Nw),F=o(Nw,"TBODY",{});var X=l(F);hr=o(X,"TR",{});var Iw=l(hr);dr=o(Iw,"TD",{align:!0});var zB=l(dr);mm=o(zB,"STRONG",{});var lL=l(mm);Bk=u(lL,"inputs"),lL.forEach(t),Ck=u(zB," (required)"),zB.forEach(t),Gk=h(Iw),$m=o(Iw,"TD",{align:!0}),l($m).forEach(t),Iw.forEach(t),Lk=h(X),gr=o(X,"TR",{});var Hw=l(gr);Tc=o(Hw,"TD",{align:!0});var iL=l(Tc);Mk=u(iL,"query (required)"),iL.forEach(t),Uk=h(Hw),jc=o(Hw,"TD",{align:!0});var uL=l(jc);zk=u(uL,"The query in plain text that you want to ask the table"),uL.forEach(t),Hw.forEach(t),Kk=h(X),mr=o(X,"TR",{});var Bw=l(mr);kc=o(Bw,"TD",{align:!0});var cL=l(kc);Fk=u(cL,"table (required)"),cL.forEach(t),Jk=h(Bw),Ac=o(Bw,"TD",{align:!0});var fL=l(Ac);Wk=u(fL,"A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),fL.forEach(t),Bw.forEach(t),Yk=h(X),$r=o(X,"TR",{});var Cw=l($r);Dc=o(Cw,"TD",{align:!0});var pL=l(Dc);qm=o(pL,"STRONG",{});var hL=l(qm);Vk=u(hL,"options"),hL.forEach(t),pL.forEach(t),Xk=h(Cw),Oc=o(Cw,"TD",{align:!0});var dL=l(Oc);Qk=u(dL,"a dict containing the following keys:"),dL.forEach(t),Cw.forEach(t),Zk=h(X),qr=o(X,"TR",{});var Gw=l(qr);Pc=o(Gw,"TD",{align:!0});var gL=l(Pc);e6=u(gL,"use_gpu"),gL.forEach(t),t6=h(Gw),Jt=o(Gw,"TD",{align:!0});var Lw=l(Jt);s6=u(Lw,"(Default: "),_m=o(Lw,"CODE",{});var mL=l(_m);a6=u(mL,"false"),mL.forEach(t),n6=u(Lw,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Lw.forEach(t),Gw.forEach(t),r6=h(X),_r=o(X,"TR",{});var Mw=l(_r);Rc=o(Mw,"TD",{align:!0});var $L=l(Rc);o6=u($L,"use_cache"),$L.forEach(t),l6=h(Mw),Wt=o(Mw,"TD",{align:!0});var Uw=l(Wt);i6=u(Uw,"(Default: "),vm=o(Uw,"CODE",{});var qL=l(vm);u6=u(qL,"true"),qL.forEach(t),c6=u(Uw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Uw.forEach(t),Mw.forEach(t),f6=h(X),vr=o(X,"TR",{});var zw=l(vr);Sc=o(zw,"TD",{align:!0});var _L=l(Sc);p6=u(_L,"wait_for_model"),_L.forEach(t),h6=h(zw),Yt=o(zw,"TD",{align:!0});var Kw=l(Yt);d6=u(Kw,"(Default: "),ym=o(Kw,"CODE",{});var vL=l(ym);g6=u(vL,"false"),vL.forEach(t),m6=u(Kw,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Kw.forEach(t),zw.forEach(t),X.forEach(t),Nw.forEach(t),a2=h(a),Nc=o(a,"P",{});var yL=l(Nc);$6=u(yL,"Return value is either a dict or a list of dicts if you sent a list of inputs"),yL.forEach(t),n2=h(a),v(Vt.$$.fragment,a),r2=h(a),Xt=o(a,"TABLE",{});var Fw=l(Xt);Em=o(Fw,"THEAD",{});var EL=l(Em);yr=o(EL,"TR",{});var Jw=l(yr);xc=o(Jw,"TH",{align:!0});var wL=l(xc);q6=u(wL,"Returned values"),wL.forEach(t),_6=h(Jw),wm=o(Jw,"TH",{align:!0}),l(wm).forEach(t),Jw.forEach(t),EL.forEach(t),v6=h(Fw),me=o(Fw,"TBODY",{});var pn=l(me);Er=o(pn,"TR",{});var Ww=l(Er);Ic=o(Ww,"TD",{align:!0});var bL=l(Ic);bm=o(bL,"STRONG",{});var TL=l(bm);y6=u(TL,"answer"),TL.forEach(t),bL.forEach(t),E6=h(Ww),Hc=o(Ww,"TD",{align:!0});var jL=l(Hc);w6=u(jL,"The plaintext answer"),jL.forEach(t),Ww.forEach(t),b6=h(pn),wr=o(pn,"TR",{});var Yw=l(wr);Bc=o(Yw,"TD",{align:!0});var kL=l(Bc);Tm=o(kL,"STRONG",{});var AL=l(Tm);T6=u(AL,"coordinates"),AL.forEach(t),kL.forEach(t),j6=h(Yw),Cc=o(Yw,"TD",{align:!0});var DL=l(Cc);k6=u(DL,"a list of coordinates of the cells referenced in the answer"),DL.forEach(t),Yw.forEach(t),A6=h(pn),br=o(pn,"TR",{});var Vw=l(br);Gc=o(Vw,"TD",{align:!0});var OL=l(Gc);jm=o(OL,"STRONG",{});var PL=l(jm);D6=u(PL,"cells"),PL.forEach(t),OL.forEach(t),O6=h(Vw),Lc=o(Vw,"TD",{align:!0});var RL=l(Lc);P6=u(RL,"a list of coordinates of the cells contents"),RL.forEach(t),Vw.forEach(t),R6=h(pn),Tr=o(pn,"TR",{});var Xw=l(Tr);Mc=o(Xw,"TD",{align:!0});var SL=l(Mc);km=o(SL,"STRONG",{});var NL=l(km);S6=u(NL,"aggregator"),NL.forEach(t),SL.forEach(t),N6=h(Xw),Uc=o(Xw,"TD",{align:!0});var xL=l(Uc);x6=u(xL,"The aggregator used to get the answer"),xL.forEach(t),Xw.forEach(t),pn.forEach(t),Fw.forEach(t),o2=h(a),Me=o(a,"H3",{class:!0});var Qw=l(Me);Qt=o(Qw,"A",{id:!0,class:!0,href:!0});var IL=l(Qt);Am=o(IL,"SPAN",{});var HL=l(Am);v(jr.$$.fragment,HL),HL.forEach(t),IL.forEach(t),I6=h(Qw),Dm=o(Qw,"SPAN",{});var BL=l(Dm);H6=u(BL,"Sentence Similarity task"),BL.forEach(t),Qw.forEach(t),l2=h(a),zc=o(a,"P",{});var CL=l(zc);B6=u(CL,"Calculate the semantic similarity for two different texts by comparing their embeddings."),CL.forEach(t),i2=h(a),v(Zt.$$.fragment,a),u2=h(a),Ue=o(a,"P",{});var cv=l(Ue);C6=u(cv,"Available with: "),kr=o(cv,"A",{href:!0,rel:!0});var GL=l(kr);G6=u(GL,"\u{1F917}Transformers"),GL.forEach(t),L6=u(cv,` and
`),Ar=o(cv,"A",{href:!0,rel:!0});var LL=l(Ar);M6=u(LL,"Sentence Transformers"),LL.forEach(t),cv.forEach(t),c2=h(a),Kc=o(a,"P",{});var ML=l(Kc);U6=u(ML,"Example:"),ML.forEach(t),f2=h(a),v(es.$$.fragment,a),p2=h(a),Fc=o(a,"P",{});var UL=l(Fc);z6=u(UL,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),UL.forEach(t),h2=h(a),ts=o(a,"TABLE",{});var Zw=l(ts);Om=o(Zw,"THEAD",{});var zL=l(Om);Dr=o(zL,"TR",{});var e0=l(Dr);Jc=o(e0,"TH",{align:!0});var KL=l(Jc);K6=u(KL,"All parameters"),KL.forEach(t),F6=h(e0),Pm=o(e0,"TH",{align:!0}),l(Pm).forEach(t),e0.forEach(t),zL.forEach(t),J6=h(Zw),J=o(Zw,"TBODY",{});var Q=l(J);Or=o(Q,"TR",{});var t0=l(Or);Pr=o(t0,"TD",{align:!0});var KB=l(Pr);Rm=o(KB,"STRONG",{});var FL=l(Rm);W6=u(FL,"inputs"),FL.forEach(t),Y6=u(KB," (required)"),KB.forEach(t),V6=h(t0),Sm=o(t0,"TD",{align:!0}),l(Sm).forEach(t),t0.forEach(t),X6=h(Q),Rr=o(Q,"TR",{});var s0=l(Rr);Wc=o(s0,"TD",{align:!0});var JL=l(Wc);Q6=u(JL,"source_sentence (required)"),JL.forEach(t),Z6=h(s0),Yc=o(s0,"TD",{align:!0});var WL=l(Yc);e7=u(WL,"The query in plain text that you want to ask the table"),WL.forEach(t),s0.forEach(t),t7=h(Q),Sr=o(Q,"TR",{});var a0=l(Sr);Vc=o(a0,"TD",{align:!0});var YL=l(Vc);s7=u(YL,"sentences (required)"),YL.forEach(t),a7=h(a0),Xc=o(a0,"TD",{align:!0});var VL=l(Xc);n7=u(VL,"A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),VL.forEach(t),a0.forEach(t),r7=h(Q),Nr=o(Q,"TR",{});var n0=l(Nr);Qc=o(n0,"TD",{align:!0});var XL=l(Qc);Nm=o(XL,"STRONG",{});var QL=l(Nm);o7=u(QL,"options"),QL.forEach(t),XL.forEach(t),l7=h(n0),Zc=o(n0,"TD",{align:!0});var ZL=l(Zc);i7=u(ZL,"a dict containing the following keys:"),ZL.forEach(t),n0.forEach(t),u7=h(Q),xr=o(Q,"TR",{});var r0=l(xr);ef=o(r0,"TD",{align:!0});var eM=l(ef);c7=u(eM,"use_gpu"),eM.forEach(t),f7=h(r0),ss=o(r0,"TD",{align:!0});var o0=l(ss);p7=u(o0,"(Default: "),xm=o(o0,"CODE",{});var tM=l(xm);h7=u(tM,"false"),tM.forEach(t),d7=u(o0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),o0.forEach(t),r0.forEach(t),g7=h(Q),Ir=o(Q,"TR",{});var l0=l(Ir);tf=o(l0,"TD",{align:!0});var sM=l(tf);m7=u(sM,"use_cache"),sM.forEach(t),$7=h(l0),as=o(l0,"TD",{align:!0});var i0=l(as);q7=u(i0,"(Default: "),Im=o(i0,"CODE",{});var aM=l(Im);_7=u(aM,"true"),aM.forEach(t),v7=u(i0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),i0.forEach(t),l0.forEach(t),y7=h(Q),Hr=o(Q,"TR",{});var u0=l(Hr);sf=o(u0,"TD",{align:!0});var nM=l(sf);E7=u(nM,"wait_for_model"),nM.forEach(t),w7=h(u0),ns=o(u0,"TD",{align:!0});var c0=l(ns);b7=u(c0,"(Default: "),Hm=o(c0,"CODE",{});var rM=l(Hm);T7=u(rM,"false"),rM.forEach(t),j7=u(c0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),c0.forEach(t),u0.forEach(t),Q.forEach(t),Zw.forEach(t),d2=h(a),af=o(a,"P",{});var oM=l(af);k7=u(oM,"The return value is a list of similarity scores, given as floats."),oM.forEach(t),g2=h(a),v(rs.$$.fragment,a),m2=h(a),os=o(a,"TABLE",{});var f0=l(os);Bm=o(f0,"THEAD",{});var lM=l(Bm);Br=o(lM,"TR",{});var p0=l(Br);nf=o(p0,"TH",{align:!0});var iM=l(nf);A7=u(iM,"Returned values"),iM.forEach(t),D7=h(p0),Cm=o(p0,"TH",{align:!0}),l(Cm).forEach(t),p0.forEach(t),lM.forEach(t),O7=h(f0),Gm=o(f0,"TBODY",{});var uM=l(Gm);Cr=o(uM,"TR",{});var h0=l(Cr);rf=o(h0,"TD",{align:!0});var cM=l(rf);Lm=o(cM,"STRONG",{});var fM=l(Lm);P7=u(fM,"Scores"),fM.forEach(t),cM.forEach(t),R7=h(h0),of=o(h0,"TD",{align:!0});var pM=l(of);S7=u(pM,"The associated similarity score for each of the given strings"),pM.forEach(t),h0.forEach(t),uM.forEach(t),f0.forEach(t),$2=h(a),ze=o(a,"H3",{class:!0});var d0=l(ze);ls=o(d0,"A",{id:!0,class:!0,href:!0});var hM=l(ls);Mm=o(hM,"SPAN",{});var dM=l(Mm);v(Gr.$$.fragment,dM),dM.forEach(t),hM.forEach(t),N7=h(d0),Um=o(d0,"SPAN",{});var gM=l(Um);x7=u(gM,"Text Classification task"),gM.forEach(t),d0.forEach(t),q2=h(a),lf=o(a,"P",{});var mM=l(lf);I7=u(mM,`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),mM.forEach(t),_2=h(a),v(is.$$.fragment,a),v2=h(a),Lr=o(a,"P",{});var FB=l(Lr);H7=u(FB,"Available with: "),Mr=o(FB,"A",{href:!0,rel:!0});var $M=l(Mr);B7=u($M,"\u{1F917} Transformers"),$M.forEach(t),FB.forEach(t),y2=h(a),uf=o(a,"P",{});var qM=l(uf);C7=u(qM,"Example:"),qM.forEach(t),E2=h(a),v(us.$$.fragment,a),w2=h(a),cf=o(a,"P",{});var _M=l(cf);G7=u(_M,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),_M.forEach(t),b2=h(a),cs=o(a,"TABLE",{});var g0=l(cs);zm=o(g0,"THEAD",{});var vM=l(zm);Ur=o(vM,"TR",{});var m0=l(Ur);ff=o(m0,"TH",{align:!0});var yM=l(ff);L7=u(yM,"All parameters"),yM.forEach(t),M7=h(m0),Km=o(m0,"TH",{align:!0}),l(Km).forEach(t),m0.forEach(t),vM.forEach(t),U7=h(g0),se=o(g0,"TBODY",{});var Oe=l(se);zr=o(Oe,"TR",{});var $0=l(zr);Kr=o($0,"TD",{align:!0});var JB=l(Kr);Fm=o(JB,"STRONG",{});var EM=l(Fm);z7=u(EM,"inputs"),EM.forEach(t),K7=u(JB," (required)"),JB.forEach(t),F7=h($0),pf=o($0,"TD",{align:!0});var wM=l(pf);J7=u(wM,"a string to be classified"),wM.forEach(t),$0.forEach(t),W7=h(Oe),Fr=o(Oe,"TR",{});var q0=l(Fr);hf=o(q0,"TD",{align:!0});var bM=l(hf);Jm=o(bM,"STRONG",{});var TM=l(Jm);Y7=u(TM,"options"),TM.forEach(t),bM.forEach(t),V7=h(q0),df=o(q0,"TD",{align:!0});var jM=l(df);X7=u(jM,"a dict containing the following keys:"),jM.forEach(t),q0.forEach(t),Q7=h(Oe),Jr=o(Oe,"TR",{});var _0=l(Jr);gf=o(_0,"TD",{align:!0});var kM=l(gf);Z7=u(kM,"use_gpu"),kM.forEach(t),e9=h(_0),fs=o(_0,"TD",{align:!0});var v0=l(fs);t9=u(v0,"(Default: "),Wm=o(v0,"CODE",{});var AM=l(Wm);s9=u(AM,"false"),AM.forEach(t),a9=u(v0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),v0.forEach(t),_0.forEach(t),n9=h(Oe),Wr=o(Oe,"TR",{});var y0=l(Wr);mf=o(y0,"TD",{align:!0});var DM=l(mf);r9=u(DM,"use_cache"),DM.forEach(t),o9=h(y0),ps=o(y0,"TD",{align:!0});var E0=l(ps);l9=u(E0,"(Default: "),Ym=o(E0,"CODE",{});var OM=l(Ym);i9=u(OM,"true"),OM.forEach(t),u9=u(E0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),E0.forEach(t),y0.forEach(t),c9=h(Oe),Yr=o(Oe,"TR",{});var w0=l(Yr);$f=o(w0,"TD",{align:!0});var PM=l($f);f9=u(PM,"wait_for_model"),PM.forEach(t),p9=h(w0),hs=o(w0,"TD",{align:!0});var b0=l(hs);h9=u(b0,"(Default: "),Vm=o(b0,"CODE",{});var RM=l(Vm);d9=u(RM,"false"),RM.forEach(t),g9=u(b0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),b0.forEach(t),w0.forEach(t),Oe.forEach(t),g0.forEach(t),T2=h(a),qf=o(a,"P",{});var SM=l(qf);m9=u(SM,"Return value is either a dict or a list of dicts if you sent a list of inputs"),SM.forEach(t),j2=h(a),v(ds.$$.fragment,a),k2=h(a),gs=o(a,"TABLE",{});var T0=l(gs);Xm=o(T0,"THEAD",{});var NM=l(Xm);Vr=o(NM,"TR",{});var j0=l(Vr);_f=o(j0,"TH",{align:!0});var xM=l(_f);$9=u(xM,"Returned values"),xM.forEach(t),q9=h(j0),Qm=o(j0,"TH",{align:!0}),l(Qm).forEach(t),j0.forEach(t),NM.forEach(t),_9=h(T0),Xr=o(T0,"TBODY",{});var k0=l(Xr);Qr=o(k0,"TR",{});var A0=l(Qr);vf=o(A0,"TD",{align:!0});var IM=l(vf);Zm=o(IM,"STRONG",{});var HM=l(Zm);v9=u(HM,"label"),HM.forEach(t),IM.forEach(t),y9=h(A0),yf=o(A0,"TD",{align:!0});var BM=l(yf);E9=u(BM,"The label for the class (model specific)"),BM.forEach(t),A0.forEach(t),w9=h(k0),Zr=o(k0,"TR",{});var D0=l(Zr);Ef=o(D0,"TD",{align:!0});var CM=l(Ef);e$=o(CM,"STRONG",{});var GM=l(e$);b9=u(GM,"score"),GM.forEach(t),CM.forEach(t),T9=h(D0),wf=o(D0,"TD",{align:!0});var LM=l(wf);j9=u(LM,"A floats that represents how likely is that the text belongs the this class."),LM.forEach(t),D0.forEach(t),k0.forEach(t),T0.forEach(t),A2=h(a),Ke=o(a,"H3",{class:!0});var O0=l(Ke);ms=o(O0,"A",{id:!0,class:!0,href:!0});var MM=l(ms);t$=o(MM,"SPAN",{});var UM=l(t$);v(eo.$$.fragment,UM),UM.forEach(t),MM.forEach(t),k9=h(O0),s$=o(O0,"SPAN",{});var zM=l(s$);A9=u(zM,"Text Generation task"),zM.forEach(t),O0.forEach(t),D2=h(a),bf=o(a,"P",{});var KM=l(bf);D9=u(KM,"Use to continue text from a prompt. This is a very generic task."),KM.forEach(t),O2=h(a),v($s.$$.fragment,a),P2=h(a),to=o(a,"P",{});var WB=l(to);O9=u(WB,"Available with: "),so=o(WB,"A",{href:!0,rel:!0});var FM=l(so);P9=u(FM,"\u{1F917} Transformers"),FM.forEach(t),WB.forEach(t),R2=h(a),Tf=o(a,"P",{});var JM=l(Tf);R9=u(JM,"Example:"),JM.forEach(t),S2=h(a),v(qs.$$.fragment,a),N2=h(a),jf=o(a,"P",{});var WM=l(jf);S9=u(WM,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),WM.forEach(t),x2=h(a),_s=o(a,"TABLE",{});var P0=l(_s);a$=o(P0,"THEAD",{});var YM=l(a$);ao=o(YM,"TR",{});var R0=l(ao);kf=o(R0,"TH",{align:!0});var VM=l(kf);N9=u(VM,"All parameters"),VM.forEach(t),x9=h(R0),n$=o(R0,"TH",{align:!0}),l(n$).forEach(t),R0.forEach(t),YM.forEach(t),I9=h(P0),I=o(P0,"TBODY",{});var B=l(I);no=o(B,"TR",{});var S0=l(no);ro=o(S0,"TD",{align:!0});var YB=l(ro);r$=o(YB,"STRONG",{});var XM=l(r$);H9=u(XM,"inputs"),XM.forEach(t),B9=u(YB," (required):"),YB.forEach(t),C9=h(S0),Af=o(S0,"TD",{align:!0});var QM=l(Af);G9=u(QM,"a string to be generated from"),QM.forEach(t),S0.forEach(t),L9=h(B),oo=o(B,"TR",{});var N0=l(oo);Df=o(N0,"TD",{align:!0});var ZM=l(Df);o$=o(ZM,"STRONG",{});var eU=l(o$);M9=u(eU,"parameters"),eU.forEach(t),ZM.forEach(t),U9=h(N0),Of=o(N0,"TD",{align:!0});var tU=l(Of);z9=u(tU,"dict containing the following keys:"),tU.forEach(t),N0.forEach(t),K9=h(B),lo=o(B,"TR",{});var x0=l(lo);Pf=o(x0,"TD",{align:!0});var sU=l(Pf);F9=u(sU,"top_k"),sU.forEach(t),J9=h(x0),Ee=o(x0,"TD",{align:!0});var eg=l(Ee);W9=u(eg,"(Default: "),l$=o(eg,"CODE",{});var aU=l(l$);Y9=u(aU,"None"),aU.forEach(t),V9=u(eg,"). Integer to define the top tokens considered within the "),i$=o(eg,"CODE",{});var nU=l(i$);X9=u(nU,"sample"),nU.forEach(t),Q9=u(eg," operation to create new text."),eg.forEach(t),x0.forEach(t),Z9=h(B),io=o(B,"TR",{});var I0=l(io);Rf=o(I0,"TD",{align:!0});var rU=l(Rf);e8=u(rU,"top_p"),rU.forEach(t),t8=h(I0),ce=o(I0,"TD",{align:!0});var hn=l(ce);s8=u(hn,"(Default: "),u$=o(hn,"CODE",{});var oU=l(u$);a8=u(oU,"None"),oU.forEach(t),n8=u(hn,"). Float to define the tokens that are within the "),c$=o(hn,"CODE",{});var lU=l(c$);r8=u(lU,"sample"),lU.forEach(t),o8=u(hn," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),f$=o(hn,"CODE",{});var iU=l(f$);l8=u(iU,"top_p"),iU.forEach(t),i8=u(hn,"."),hn.forEach(t),I0.forEach(t),u8=h(B),uo=o(B,"TR",{});var H0=l(uo);Sf=o(H0,"TD",{align:!0});var uU=l(Sf);c8=u(uU,"temperature"),uU.forEach(t),f8=h(H0),fe=o(H0,"TD",{align:!0});var dn=l(fe);p8=u(dn,"(Default: "),p$=o(dn,"CODE",{});var cU=l(p$);h8=u(cU,"1.0"),cU.forEach(t),d8=u(dn,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),h$=o(dn,"CODE",{});var fU=l(h$);g8=u(fU,"0"),fU.forEach(t),m8=u(dn," means always take the highest score, "),d$=o(dn,"CODE",{});var pU=l(d$);$8=u(pU,"100.0"),pU.forEach(t),q8=u(dn," is getting closer to uniform probability."),dn.forEach(t),H0.forEach(t),_8=h(B),co=o(B,"TR",{});var B0=l(co);Nf=o(B0,"TD",{align:!0});var hU=l(Nf);v8=u(hU,"repetition_penalty"),hU.forEach(t),y8=h(B0),vs=o(B0,"TD",{align:!0});var C0=l(vs);E8=u(C0,"(Default: "),g$=o(C0,"CODE",{});var dU=l(g$);w8=u(dU,"None"),dU.forEach(t),b8=u(C0,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),C0.forEach(t),B0.forEach(t),T8=h(B),fo=o(B,"TR",{});var G0=l(fo);xf=o(G0,"TD",{align:!0});var gU=l(xf);j8=u(gU,"max_new_tokens"),gU.forEach(t),k8=h(G0),we=o(G0,"TD",{align:!0});var tg=l(we);A8=u(tg,"(Default: "),m$=o(tg,"CODE",{});var mU=l(m$);D8=u(mU,"None"),mU.forEach(t),O8=u(tg,"). Int (0-250). The amount of new tokens to be generated, this does "),$$=o(tg,"STRONG",{});var $U=l($$);P8=u($U,"not"),$U.forEach(t),R8=u(tg," include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),tg.forEach(t),G0.forEach(t),S8=h(B),po=o(B,"TR",{});var L0=l(po);If=o(L0,"TD",{align:!0});var qU=l(If);N8=u(qU,"max_time"),qU.forEach(t),x8=h(L0),be=o(L0,"TD",{align:!0});var sg=l(be);I8=u(sg,"(Default: "),q$=o(sg,"CODE",{});var _U=l(q$);H8=u(_U,"None"),_U.forEach(t),B8=u(sg,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),_$=o(sg,"CODE",{});var vU=l(_$);C8=u(vU,"max_new_tokens"),vU.forEach(t),G8=u(sg," for best results."),sg.forEach(t),L0.forEach(t),L8=h(B),ho=o(B,"TR",{});var M0=l(ho);Hf=o(M0,"TD",{align:!0});var yU=l(Hf);M8=u(yU,"return_full_text"),yU.forEach(t),U8=h(M0),Te=o(M0,"TD",{align:!0});var ag=l(Te);z8=u(ag,"(Default: "),v$=o(ag,"CODE",{});var EU=l(v$);K8=u(EU,"True"),EU.forEach(t),F8=u(ag,"). Bool. If set to False, the return results will "),y$=o(ag,"STRONG",{});var wU=l(y$);J8=u(wU,"not"),wU.forEach(t),W8=u(ag," contain the original query making it easier for prompting."),ag.forEach(t),M0.forEach(t),Y8=h(B),go=o(B,"TR",{});var U0=l(go);Bf=o(U0,"TD",{align:!0});var bU=l(Bf);V8=u(bU,"num_return_sequences"),bU.forEach(t),X8=h(U0),ys=o(U0,"TD",{align:!0});var z0=l(ys);Q8=u(z0,"(Default: "),E$=o(z0,"CODE",{});var TU=l(E$);Z8=u(TU,"1"),TU.forEach(t),eA=u(z0,"). Integer. The number of proposition you want to be returned."),z0.forEach(t),U0.forEach(t),tA=h(B),mo=o(B,"TR",{});var K0=l(mo);Cf=o(K0,"TD",{align:!0});var jU=l(Cf);sA=u(jU,"do_sample"),jU.forEach(t),aA=h(K0),Es=o(K0,"TD",{align:!0});var F0=l(Es);nA=u(F0,"(Optional: "),w$=o(F0,"CODE",{});var kU=l(w$);rA=u(kU,"True"),kU.forEach(t),oA=u(F0,"). Bool. Whether or not to use sampling, use greedy decoding otherwise."),F0.forEach(t),K0.forEach(t),lA=h(B),$o=o(B,"TR",{});var J0=l($o);Gf=o(J0,"TD",{align:!0});var AU=l(Gf);b$=o(AU,"STRONG",{});var DU=l(b$);iA=u(DU,"options"),DU.forEach(t),AU.forEach(t),uA=h(J0),Lf=o(J0,"TD",{align:!0});var OU=l(Lf);cA=u(OU,"a dict containing the following keys:"),OU.forEach(t),J0.forEach(t),fA=h(B),qo=o(B,"TR",{});var W0=l(qo);Mf=o(W0,"TD",{align:!0});var PU=l(Mf);pA=u(PU,"use_gpu"),PU.forEach(t),hA=h(W0),ws=o(W0,"TD",{align:!0});var Y0=l(ws);dA=u(Y0,"(Default: "),T$=o(Y0,"CODE",{});var RU=l(T$);gA=u(RU,"false"),RU.forEach(t),mA=u(Y0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Y0.forEach(t),W0.forEach(t),$A=h(B),_o=o(B,"TR",{});var V0=l(_o);Uf=o(V0,"TD",{align:!0});var SU=l(Uf);qA=u(SU,"use_cache"),SU.forEach(t),_A=h(V0),bs=o(V0,"TD",{align:!0});var X0=l(bs);vA=u(X0,"(Default: "),j$=o(X0,"CODE",{});var NU=l(j$);yA=u(NU,"true"),NU.forEach(t),EA=u(X0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),X0.forEach(t),V0.forEach(t),wA=h(B),vo=o(B,"TR",{});var Q0=l(vo);zf=o(Q0,"TD",{align:!0});var xU=l(zf);bA=u(xU,"wait_for_model"),xU.forEach(t),TA=h(Q0),Ts=o(Q0,"TD",{align:!0});var Z0=l(Ts);jA=u(Z0,"(Default: "),k$=o(Z0,"CODE",{});var IU=l(k$);kA=u(IU,"false"),IU.forEach(t),AA=u(Z0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Z0.forEach(t),Q0.forEach(t),B.forEach(t),P0.forEach(t),I2=h(a),Kf=o(a,"P",{});var HU=l(Kf);DA=u(HU,"Return value is either a dict or a list of dicts if you sent a list of inputs"),HU.forEach(t),H2=h(a),v(js.$$.fragment,a),B2=h(a),ks=o(a,"TABLE",{});var eb=l(ks);A$=o(eb,"THEAD",{});var BU=l(A$);yo=o(BU,"TR",{});var tb=l(yo);Ff=o(tb,"TH",{align:!0});var CU=l(Ff);OA=u(CU,"Returned values"),CU.forEach(t),PA=h(tb),D$=o(tb,"TH",{align:!0}),l(D$).forEach(t),tb.forEach(t),BU.forEach(t),RA=h(eb),O$=o(eb,"TBODY",{});var GU=l(O$);Eo=o(GU,"TR",{});var sb=l(Eo);Jf=o(sb,"TD",{align:!0});var LU=l(Jf);P$=o(LU,"STRONG",{});var MU=l(P$);SA=u(MU,"generated_text"),MU.forEach(t),LU.forEach(t),NA=h(sb),Wf=o(sb,"TD",{align:!0});var UU=l(Wf);xA=u(UU,"The continuated string"),UU.forEach(t),sb.forEach(t),GU.forEach(t),eb.forEach(t),C2=h(a),Fe=o(a,"H3",{class:!0});var ab=l(Fe);As=o(ab,"A",{id:!0,class:!0,href:!0});var zU=l(As);R$=o(zU,"SPAN",{});var KU=l(R$);v(wo.$$.fragment,KU),KU.forEach(t),zU.forEach(t),IA=h(ab),S$=o(ab,"SPAN",{});var FU=l(S$);HA=u(FU,"Text2Text Generation task"),FU.forEach(t),ab.forEach(t),G2=h(a),Ds=o(a,"P",{});var nb=l(Ds);BA=u(nb,"Essentially "),Yf=o(nb,"A",{href:!0});var JU=l(Yf);CA=u(JU,"Text-generation task"),JU.forEach(t),GA=u(nb,`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),nb.forEach(t),L2=h(a),Je=o(a,"H3",{class:!0});var rb=l(Je);Os=o(rb,"A",{id:!0,class:!0,href:!0});var WU=l(Os);N$=o(WU,"SPAN",{});var YU=l(N$);v(bo.$$.fragment,YU),YU.forEach(t),WU.forEach(t),LA=h(rb),x$=o(rb,"SPAN",{});var VU=l(x$);MA=u(VU,"Token Classification task"),VU.forEach(t),rb.forEach(t),M2=h(a),Vf=o(a,"P",{});var XU=l(Vf);UA=u(XU,`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),XU.forEach(t),U2=h(a),v(Ps.$$.fragment,a),z2=h(a),We=o(a,"P",{});var fv=l(We);zA=u(fv,"Available with: "),To=o(fv,"A",{href:!0,rel:!0});var QU=l(To);KA=u(QU,"\u{1F917} Transformers"),QU.forEach(t),FA=u(fv,`,
`),jo=o(fv,"A",{href:!0,rel:!0});var ZU=l(jo);JA=u(ZU,"Flair"),ZU.forEach(t),fv.forEach(t),K2=h(a),Xf=o(a,"P",{});var ez=l(Xf);WA=u(ez,"Example:"),ez.forEach(t),F2=h(a),v(Rs.$$.fragment,a),J2=h(a),Qf=o(a,"P",{});var tz=l(Qf);YA=u(tz,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),tz.forEach(t),W2=h(a),Ss=o(a,"TABLE",{});var ob=l(Ss);I$=o(ob,"THEAD",{});var sz=l(I$);ko=o(sz,"TR",{});var lb=l(ko);Zf=o(lb,"TH",{align:!0});var az=l(Zf);VA=u(az,"All parameters"),az.forEach(t),XA=h(lb),H$=o(lb,"TH",{align:!0}),l(H$).forEach(t),lb.forEach(t),sz.forEach(t),QA=h(ob),W=o(ob,"TBODY",{});var Z=l(W);Ao=o(Z,"TR",{});var ib=l(Ao);Do=o(ib,"TD",{align:!0});var VB=l(Do);B$=o(VB,"STRONG",{});var nz=l(B$);ZA=u(nz,"inputs"),nz.forEach(t),eD=u(VB," (required)"),VB.forEach(t),tD=h(ib),ep=o(ib,"TD",{align:!0});var rz=l(ep);sD=u(rz,"a string to be classified"),rz.forEach(t),ib.forEach(t),aD=h(Z),Oo=o(Z,"TR",{});var ub=l(Oo);tp=o(ub,"TD",{align:!0});var oz=l(tp);C$=o(oz,"STRONG",{});var lz=l(C$);nD=u(lz,"parameters"),lz.forEach(t),oz.forEach(t),rD=h(ub),sp=o(ub,"TD",{align:!0});var iz=l(sp);oD=u(iz,"a dict containing the following key:"),iz.forEach(t),ub.forEach(t),lD=h(Z),Po=o(Z,"TR",{});var cb=l(Po);ap=o(cb,"TD",{align:!0});var uz=l(ap);iD=u(uz,"aggregation_strategy"),uz.forEach(t),uD=h(cb),x=o(cb,"TD",{align:!0});var C=l(x);cD=u(C,"(Default: "),G$=o(C,"CODE",{});var cz=l(G$);fD=u(cz,"simple"),cz.forEach(t),pD=u(C,"). There are several aggregation strategies: "),hD=o(C,"BR",{}),dD=h(C),L$=o(C,"CODE",{});var fz=l(L$);gD=u(fz,"none"),fz.forEach(t),mD=u(C,": Every token gets classified without further aggregation. "),$D=o(C,"BR",{}),qD=h(C),M$=o(C,"CODE",{});var pz=l(M$);_D=u(pz,"simple"),pz.forEach(t),vD=u(C,": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),yD=o(C,"BR",{}),ED=h(C),U$=o(C,"CODE",{});var hz=l(U$);wD=u(hz,"first"),hz.forEach(t),bD=u(C,": Same as the "),z$=o(C,"CODE",{});var dz=l(z$);TD=u(dz,"simple"),dz.forEach(t),jD=u(C," strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),kD=o(C,"BR",{}),AD=h(C),K$=o(C,"CODE",{});var gz=l(K$);DD=u(gz,"average"),gz.forEach(t),OD=u(C,": Same as the "),F$=o(C,"CODE",{});var mz=l(F$);PD=u(mz,"simple"),mz.forEach(t),RD=u(C," strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),SD=o(C,"BR",{}),ND=h(C),J$=o(C,"CODE",{});var $z=l(J$);xD=u($z,"max"),$z.forEach(t),ID=u(C,": Same as the "),W$=o(C,"CODE",{});var qz=l(W$);HD=u(qz,"simple"),qz.forEach(t),BD=u(C," strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),C.forEach(t),cb.forEach(t),CD=h(Z),Ro=o(Z,"TR",{});var fb=l(Ro);np=o(fb,"TD",{align:!0});var _z=l(np);Y$=o(_z,"STRONG",{});var vz=l(Y$);GD=u(vz,"options"),vz.forEach(t),_z.forEach(t),LD=h(fb),rp=o(fb,"TD",{align:!0});var yz=l(rp);MD=u(yz,"a dict containing the following keys:"),yz.forEach(t),fb.forEach(t),UD=h(Z),So=o(Z,"TR",{});var pb=l(So);op=o(pb,"TD",{align:!0});var Ez=l(op);zD=u(Ez,"use_gpu"),Ez.forEach(t),KD=h(pb),Ns=o(pb,"TD",{align:!0});var hb=l(Ns);FD=u(hb,"(Default: "),V$=o(hb,"CODE",{});var wz=l(V$);JD=u(wz,"false"),wz.forEach(t),WD=u(hb,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),hb.forEach(t),pb.forEach(t),YD=h(Z),No=o(Z,"TR",{});var db=l(No);lp=o(db,"TD",{align:!0});var bz=l(lp);VD=u(bz,"use_cache"),bz.forEach(t),XD=h(db),xs=o(db,"TD",{align:!0});var gb=l(xs);QD=u(gb,"(Default: "),X$=o(gb,"CODE",{});var Tz=l(X$);ZD=u(Tz,"true"),Tz.forEach(t),eO=u(gb,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),gb.forEach(t),db.forEach(t),tO=h(Z),xo=o(Z,"TR",{});var mb=l(xo);ip=o(mb,"TD",{align:!0});var jz=l(ip);sO=u(jz,"wait_for_model"),jz.forEach(t),aO=h(mb),Is=o(mb,"TD",{align:!0});var $b=l(Is);nO=u($b,"(Default: "),Q$=o($b,"CODE",{});var kz=l(Q$);rO=u(kz,"false"),kz.forEach(t),oO=u($b,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),$b.forEach(t),mb.forEach(t),Z.forEach(t),ob.forEach(t),Y2=h(a),up=o(a,"P",{});var Az=l(up);lO=u(Az,"Return value is either a dict or a list of dicts if you sent a list of inputs"),Az.forEach(t),V2=h(a),v(Hs.$$.fragment,a),X2=h(a),Bs=o(a,"TABLE",{});var qb=l(Bs);Z$=o(qb,"THEAD",{});var Dz=l(Z$);Io=o(Dz,"TR",{});var _b=l(Io);cp=o(_b,"TH",{align:!0});var Oz=l(cp);iO=u(Oz,"Returned values"),Oz.forEach(t),uO=h(_b),eq=o(_b,"TH",{align:!0}),l(eq).forEach(t),_b.forEach(t),Dz.forEach(t),cO=h(qb),ae=o(qb,"TBODY",{});var Pe=l(ae);Ho=o(Pe,"TR",{});var vb=l(Ho);fp=o(vb,"TD",{align:!0});var Pz=l(fp);tq=o(Pz,"STRONG",{});var Rz=l(tq);fO=u(Rz,"entity_group"),Rz.forEach(t),Pz.forEach(t),pO=h(vb),pp=o(vb,"TD",{align:!0});var Sz=l(pp);hO=u(Sz,"The type for the entity being recognized (model specific)."),Sz.forEach(t),vb.forEach(t),dO=h(Pe),Bo=o(Pe,"TR",{});var yb=l(Bo);hp=o(yb,"TD",{align:!0});var Nz=l(hp);sq=o(Nz,"STRONG",{});var xz=l(sq);gO=u(xz,"score"),xz.forEach(t),Nz.forEach(t),mO=h(yb),dp=o(yb,"TD",{align:!0});var Iz=l(dp);$O=u(Iz,"How likely the entity was recognized."),Iz.forEach(t),yb.forEach(t),qO=h(Pe),Co=o(Pe,"TR",{});var Eb=l(Co);gp=o(Eb,"TD",{align:!0});var Hz=l(gp);aq=o(Hz,"STRONG",{});var Bz=l(aq);_O=u(Bz,"word"),Bz.forEach(t),Hz.forEach(t),vO=h(Eb),mp=o(Eb,"TD",{align:!0});var Cz=l(mp);yO=u(Cz,"The string that was captured"),Cz.forEach(t),Eb.forEach(t),EO=h(Pe),Go=o(Pe,"TR",{});var wb=l(Go);$p=o(wb,"TD",{align:!0});var Gz=l($p);nq=o(Gz,"STRONG",{});var Lz=l(nq);wO=u(Lz,"start"),Lz.forEach(t),Gz.forEach(t),bO=h(wb),Cs=o(wb,"TD",{align:!0});var bb=l(Cs);TO=u(bb,"The offset stringwise where the answer is located. Useful to disambiguate if "),rq=o(bb,"CODE",{});var Mz=l(rq);jO=u(Mz,"word"),Mz.forEach(t),kO=u(bb," occurs multiple times."),bb.forEach(t),wb.forEach(t),AO=h(Pe),Lo=o(Pe,"TR",{});var Tb=l(Lo);qp=o(Tb,"TD",{align:!0});var Uz=l(qp);oq=o(Uz,"STRONG",{});var zz=l(oq);DO=u(zz,"end"),zz.forEach(t),Uz.forEach(t),OO=h(Tb),Gs=o(Tb,"TD",{align:!0});var jb=l(Gs);PO=u(jb,"The offset stringwise where the answer is located. Useful to disambiguate if "),lq=o(jb,"CODE",{});var Kz=l(lq);RO=u(Kz,"word"),Kz.forEach(t),SO=u(jb," occurs multiple times."),jb.forEach(t),Tb.forEach(t),Pe.forEach(t),qb.forEach(t),Q2=h(a),Ye=o(a,"H3",{class:!0});var kb=l(Ye);Ls=o(kb,"A",{id:!0,class:!0,href:!0});var Fz=l(Ls);iq=o(Fz,"SPAN",{});var Jz=l(iq);v(Mo.$$.fragment,Jz),Jz.forEach(t),Fz.forEach(t),NO=h(kb),uq=o(kb,"SPAN",{});var Wz=l(uq);xO=u(Wz,"Named Entity Recognition (NER) task"),Wz.forEach(t),kb.forEach(t),Z2=h(a),Uo=o(a,"P",{});var XB=l(Uo);IO=u(XB,"See "),_p=o(XB,"A",{href:!0});var Yz=l(_p);HO=u(Yz,"Token-classification task"),Yz.forEach(t),XB.forEach(t),ey=h(a),Ve=o(a,"H3",{class:!0});var Ab=l(Ve);Ms=o(Ab,"A",{id:!0,class:!0,href:!0});var Vz=l(Ms);cq=o(Vz,"SPAN",{});var Xz=l(cq);v(zo.$$.fragment,Xz),Xz.forEach(t),Vz.forEach(t),BO=h(Ab),fq=o(Ab,"SPAN",{});var Qz=l(fq);CO=u(Qz,"Translation task"),Qz.forEach(t),Ab.forEach(t),ty=h(a),vp=o(a,"P",{});var Zz=l(vp);GO=u(Zz,"This task is well known to translate text from one language to another"),Zz.forEach(t),sy=h(a),v(Us.$$.fragment,a),ay=h(a),Ko=o(a,"P",{});var QB=l(Ko);LO=u(QB,"Available with: "),Fo=o(QB,"A",{href:!0,rel:!0});var eK=l(Fo);MO=u(eK,"\u{1F917} Transformers"),eK.forEach(t),QB.forEach(t),ny=h(a),yp=o(a,"P",{});var tK=l(yp);UO=u(tK,"Example:"),tK.forEach(t),ry=h(a),v(zs.$$.fragment,a),oy=h(a),Ep=o(a,"P",{});var sK=l(Ep);zO=u(sK,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),sK.forEach(t),ly=h(a),Ks=o(a,"TABLE",{});var Db=l(Ks);pq=o(Db,"THEAD",{});var aK=l(pq);Jo=o(aK,"TR",{});var Ob=l(Jo);wp=o(Ob,"TH",{align:!0});var nK=l(wp);KO=u(nK,"All parameters"),nK.forEach(t),FO=h(Ob),hq=o(Ob,"TH",{align:!0}),l(hq).forEach(t),Ob.forEach(t),aK.forEach(t),JO=h(Db),ne=o(Db,"TBODY",{});var Re=l(ne);Wo=o(Re,"TR",{});var Pb=l(Wo);Yo=o(Pb,"TD",{align:!0});var ZB=l(Yo);dq=o(ZB,"STRONG",{});var rK=l(dq);WO=u(rK,"inputs"),rK.forEach(t),YO=u(ZB," (required)"),ZB.forEach(t),VO=h(Pb),bp=o(Pb,"TD",{align:!0});var oK=l(bp);XO=u(oK,"a string to be translated in the original languages"),oK.forEach(t),Pb.forEach(t),QO=h(Re),Vo=o(Re,"TR",{});var Rb=l(Vo);Tp=o(Rb,"TD",{align:!0});var lK=l(Tp);gq=o(lK,"STRONG",{});var iK=l(gq);ZO=u(iK,"options"),iK.forEach(t),lK.forEach(t),eP=h(Rb),jp=o(Rb,"TD",{align:!0});var uK=l(jp);tP=u(uK,"a dict containing the following keys:"),uK.forEach(t),Rb.forEach(t),sP=h(Re),Xo=o(Re,"TR",{});var Sb=l(Xo);kp=o(Sb,"TD",{align:!0});var cK=l(kp);aP=u(cK,"use_gpu"),cK.forEach(t),nP=h(Sb),Fs=o(Sb,"TD",{align:!0});var Nb=l(Fs);rP=u(Nb,"(Default: "),mq=o(Nb,"CODE",{});var fK=l(mq);oP=u(fK,"false"),fK.forEach(t),lP=u(Nb,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Nb.forEach(t),Sb.forEach(t),iP=h(Re),Qo=o(Re,"TR",{});var xb=l(Qo);Ap=o(xb,"TD",{align:!0});var pK=l(Ap);uP=u(pK,"use_cache"),pK.forEach(t),cP=h(xb),Js=o(xb,"TD",{align:!0});var Ib=l(Js);fP=u(Ib,"(Default: "),$q=o(Ib,"CODE",{});var hK=l($q);pP=u(hK,"true"),hK.forEach(t),hP=u(Ib,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Ib.forEach(t),xb.forEach(t),dP=h(Re),Zo=o(Re,"TR",{});var Hb=l(Zo);Dp=o(Hb,"TD",{align:!0});var dK=l(Dp);gP=u(dK,"wait_for_model"),dK.forEach(t),mP=h(Hb),Ws=o(Hb,"TD",{align:!0});var Bb=l(Ws);$P=u(Bb,"(Default: "),qq=o(Bb,"CODE",{});var gK=l(qq);qP=u(gK,"false"),gK.forEach(t),_P=u(Bb,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Bb.forEach(t),Hb.forEach(t),Re.forEach(t),Db.forEach(t),iy=h(a),Op=o(a,"P",{});var mK=l(Op);vP=u(mK,"Return value is either a dict or a list of dicts if you sent a list of inputs"),mK.forEach(t),uy=h(a),Ys=o(a,"TABLE",{});var Cb=l(Ys);_q=o(Cb,"THEAD",{});var $K=l(_q);el=o($K,"TR",{});var Gb=l(el);Pp=o(Gb,"TH",{align:!0});var qK=l(Pp);yP=u(qK,"Returned values"),qK.forEach(t),EP=h(Gb),vq=o(Gb,"TH",{align:!0}),l(vq).forEach(t),Gb.forEach(t),$K.forEach(t),wP=h(Cb),yq=o(Cb,"TBODY",{});var _K=l(yq);tl=o(_K,"TR",{});var Lb=l(tl);Rp=o(Lb,"TD",{align:!0});var vK=l(Rp);Eq=o(vK,"STRONG",{});var yK=l(Eq);bP=u(yK,"translation_text"),yK.forEach(t),vK.forEach(t),TP=h(Lb),Sp=o(Lb,"TD",{align:!0});var EK=l(Sp);jP=u(EK,"The string after translation"),EK.forEach(t),Lb.forEach(t),_K.forEach(t),Cb.forEach(t),cy=h(a),Xe=o(a,"H3",{class:!0});var Mb=l(Xe);Vs=o(Mb,"A",{id:!0,class:!0,href:!0});var wK=l(Vs);wq=o(wK,"SPAN",{});var bK=l(wq);v(sl.$$.fragment,bK),bK.forEach(t),wK.forEach(t),kP=h(Mb),bq=o(Mb,"SPAN",{});var TK=l(bq);AP=u(TK,"Zero-Shot Classification task"),TK.forEach(t),Mb.forEach(t),fy=h(a),Np=o(a,"P",{});var jK=l(Np);DP=u(jK,`This task is super useful to try out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence, and you get a result.`),jK.forEach(t),py=h(a),v(Xs.$$.fragment,a),hy=h(a),al=o(a,"P",{});var eC=l(al);OP=u(eC,"Available with: "),nl=o(eC,"A",{href:!0,rel:!0});var kK=l(nl);PP=u(kK,"\u{1F917} Transformers"),kK.forEach(t),eC.forEach(t),dy=h(a),xp=o(a,"P",{});var AK=l(xp);RP=u(AK,"Request:"),AK.forEach(t),gy=h(a),v(Qs.$$.fragment,a),my=h(a),Ip=o(a,"P",{});var DK=l(Ip);SP=u(DK,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),DK.forEach(t),$y=h(a),Zs=o(a,"TABLE",{});var Ub=l(Zs);Tq=o(Ub,"THEAD",{});var OK=l(Tq);rl=o(OK,"TR",{});var zb=l(rl);Hp=o(zb,"TH",{align:!0});var PK=l(Hp);NP=u(PK,"All parameters"),PK.forEach(t),xP=h(zb),jq=o(zb,"TH",{align:!0}),l(jq).forEach(t),zb.forEach(t),OK.forEach(t),IP=h(Ub),z=o(Ub,"TBODY",{});var Y=l(z);ol=o(Y,"TR",{});var Kb=l(ol);ll=o(Kb,"TD",{align:!0});var tC=l(ll);kq=o(tC,"STRONG",{});var RK=l(kq);HP=u(RK,"inputs"),RK.forEach(t),BP=u(tC," (required)"),tC.forEach(t),CP=h(Kb),Bp=o(Kb,"TD",{align:!0});var SK=l(Bp);GP=u(SK,"a string or list of strings"),SK.forEach(t),Kb.forEach(t),LP=h(Y),il=o(Y,"TR",{});var Fb=l(il);ul=o(Fb,"TD",{align:!0});var sC=l(ul);Aq=o(sC,"STRONG",{});var NK=l(Aq);MP=u(NK,"parameters"),NK.forEach(t),UP=u(sC," (required)"),sC.forEach(t),zP=h(Fb),Cp=o(Fb,"TD",{align:!0});var xK=l(Cp);KP=u(xK,"a dict containing the following keys:"),xK.forEach(t),Fb.forEach(t),FP=h(Y),cl=o(Y,"TR",{});var Jb=l(cl);Gp=o(Jb,"TD",{align:!0});var IK=l(Gp);JP=u(IK,"candidate_labels (required)"),IK.forEach(t),WP=h(Jb),je=o(Jb,"TD",{align:!0});var ng=l(je);YP=u(ng,"a list of strings that are potential classes for "),Dq=o(ng,"CODE",{});var HK=l(Dq);VP=u(HK,"inputs"),HK.forEach(t),XP=u(ng,". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),Oq=o(ng,"CODE",{});var BK=l(Oq);QP=u(BK,"multi_label=True"),BK.forEach(t),ZP=u(ng," and do the scaling on your end. )"),ng.forEach(t),Jb.forEach(t),eR=h(Y),fl=o(Y,"TR",{});var Wb=l(fl);Lp=o(Wb,"TD",{align:!0});var CK=l(Lp);tR=u(CK,"multi_label"),CK.forEach(t),sR=h(Wb),ea=o(Wb,"TD",{align:!0});var Yb=l(ea);aR=u(Yb,"(Default: "),Pq=o(Yb,"CODE",{});var GK=l(Pq);nR=u(GK,"false"),GK.forEach(t),rR=u(Yb,") Boolean that is set to True if classes can overlap"),Yb.forEach(t),Wb.forEach(t),oR=h(Y),pl=o(Y,"TR",{});var Vb=l(pl);Mp=o(Vb,"TD",{align:!0});var LK=l(Mp);Rq=o(LK,"STRONG",{});var MK=l(Rq);lR=u(MK,"options"),MK.forEach(t),LK.forEach(t),iR=h(Vb),Up=o(Vb,"TD",{align:!0});var UK=l(Up);uR=u(UK,"a dict containing the following keys:"),UK.forEach(t),Vb.forEach(t),cR=h(Y),hl=o(Y,"TR",{});var Xb=l(hl);zp=o(Xb,"TD",{align:!0});var zK=l(zp);fR=u(zK,"use_gpu"),zK.forEach(t),pR=h(Xb),ta=o(Xb,"TD",{align:!0});var Qb=l(ta);hR=u(Qb,"(Default: "),Sq=o(Qb,"CODE",{});var KK=l(Sq);dR=u(KK,"false"),KK.forEach(t),gR=u(Qb,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Qb.forEach(t),Xb.forEach(t),mR=h(Y),dl=o(Y,"TR",{});var Zb=l(dl);Kp=o(Zb,"TD",{align:!0});var FK=l(Kp);$R=u(FK,"use_cache"),FK.forEach(t),qR=h(Zb),sa=o(Zb,"TD",{align:!0});var e3=l(sa);_R=u(e3,"(Default: "),Nq=o(e3,"CODE",{});var JK=l(Nq);vR=u(JK,"true"),JK.forEach(t),yR=u(e3,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),e3.forEach(t),Zb.forEach(t),ER=h(Y),gl=o(Y,"TR",{});var t3=l(gl);Fp=o(t3,"TD",{align:!0});var WK=l(Fp);wR=u(WK,"wait_for_model"),WK.forEach(t),bR=h(t3),aa=o(t3,"TD",{align:!0});var s3=l(aa);TR=u(s3,"(Default: "),xq=o(s3,"CODE",{});var YK=l(xq);jR=u(YK,"false"),YK.forEach(t),kR=u(s3,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),s3.forEach(t),t3.forEach(t),Y.forEach(t),Ub.forEach(t),qy=h(a),Jp=o(a,"P",{});var VK=l(Jp);AR=u(VK,"Return value is either a dict or a list of dicts if you sent a list of inputs"),VK.forEach(t),_y=h(a),Wp=o(a,"P",{});var XK=l(Wp);DR=u(XK,"Response:"),XK.forEach(t),vy=h(a),v(na.$$.fragment,a),yy=h(a),ra=o(a,"TABLE",{});var a3=l(ra);Iq=o(a3,"THEAD",{});var QK=l(Iq);ml=o(QK,"TR",{});var n3=l(ml);Yp=o(n3,"TH",{align:!0});var ZK=l(Yp);OR=u(ZK,"Returned values"),ZK.forEach(t),PR=h(n3),Hq=o(n3,"TH",{align:!0}),l(Hq).forEach(t),n3.forEach(t),QK.forEach(t),RR=h(a3),Qe=o(a3,"TBODY",{});var rg=l(Qe);$l=o(rg,"TR",{});var r3=l($l);Vp=o(r3,"TD",{align:!0});var eF=l(Vp);Bq=o(eF,"STRONG",{});var tF=l(Bq);SR=u(tF,"sequence"),tF.forEach(t),eF.forEach(t),NR=h(r3),Xp=o(r3,"TD",{align:!0});var sF=l(Xp);xR=u(sF,"The string sent as an input"),sF.forEach(t),r3.forEach(t),IR=h(rg),ql=o(rg,"TR",{});var o3=l(ql);Qp=o(o3,"TD",{align:!0});var aF=l(Qp);Cq=o(aF,"STRONG",{});var nF=l(Cq);HR=u(nF,"labels"),nF.forEach(t),aF.forEach(t),BR=h(o3),Zp=o(o3,"TD",{align:!0});var rF=l(Zp);CR=u(rF,"The list of strings for labels that you sent (in order)"),rF.forEach(t),o3.forEach(t),GR=h(rg),_l=o(rg,"TR",{});var l3=l(_l);eh=o(l3,"TD",{align:!0});var oF=l(eh);Gq=o(oF,"STRONG",{});var lF=l(Gq);LR=u(lF,"scores"),lF.forEach(t),oF.forEach(t),MR=h(l3),oa=o(l3,"TD",{align:!0});var i3=l(oa);UR=u(i3,"a list of floats that correspond the the probability of label, in the same order as "),Lq=o(i3,"CODE",{});var iF=l(Lq);zR=u(iF,"labels"),iF.forEach(t),KR=u(i3,"."),i3.forEach(t),l3.forEach(t),rg.forEach(t),a3.forEach(t),Ey=h(a),Ze=o(a,"H3",{class:!0});var u3=l(Ze);la=o(u3,"A",{id:!0,class:!0,href:!0});var uF=l(la);Mq=o(uF,"SPAN",{});var cF=l(Mq);v(vl.$$.fragment,cF),cF.forEach(t),uF.forEach(t),FR=h(u3),Uq=o(u3,"SPAN",{});var fF=l(Uq);JR=u(fF,"Conversational task"),fF.forEach(t),u3.forEach(t),wy=h(a),th=o(a,"P",{});var pF=l(th);WR=u(pF,`This task corresponds to any chatbot like structure. Models tend to have
shorter max_length, so please check with caution when using a given
model if you need long range dependency or not.`),pF.forEach(t),by=h(a),v(ia.$$.fragment,a),Ty=h(a),yl=o(a,"P",{});var aC=l(yl);YR=u(aC,"Available with: "),El=o(aC,"A",{href:!0,rel:!0});var hF=l(El);VR=u(hF,"\u{1F917} Transformers"),hF.forEach(t),aC.forEach(t),jy=h(a),sh=o(a,"P",{});var dF=l(sh);XR=u(dF,"Example:"),dF.forEach(t),ky=h(a),v(ua.$$.fragment,a),Ay=h(a),ah=o(a,"P",{});var gF=l(ah);QR=u(gF,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),gF.forEach(t),Dy=h(a),ca=o(a,"TABLE",{});var c3=l(ca);zq=o(c3,"THEAD",{});var mF=l(zq);wl=o(mF,"TR",{});var f3=l(wl);nh=o(f3,"TH",{align:!0});var $F=l(nh);ZR=u($F,"All parameters"),$F.forEach(t),eS=h(f3),Kq=o(f3,"TH",{align:!0}),l(Kq).forEach(t),f3.forEach(t),mF.forEach(t),tS=h(c3),N=o(c3,"TBODY",{});var H=l(N);bl=o(H,"TR",{});var p3=l(bl);Tl=o(p3,"TD",{align:!0});var nC=l(Tl);Fq=o(nC,"STRONG",{});var qF=l(Fq);sS=u(qF,"inputs"),qF.forEach(t),aS=u(nC," (required)"),nC.forEach(t),nS=h(p3),Jq=o(p3,"TD",{align:!0}),l(Jq).forEach(t),p3.forEach(t),rS=h(H),jl=o(H,"TR",{});var h3=l(jl);rh=o(h3,"TD",{align:!0});var _F=l(rh);oS=u(_F,"text (required)"),_F.forEach(t),lS=h(h3),oh=o(h3,"TD",{align:!0});var vF=l(oh);iS=u(vF,"The last input from the user in the conversation."),vF.forEach(t),h3.forEach(t),uS=h(H),kl=o(H,"TR",{});var d3=l(kl);lh=o(d3,"TD",{align:!0});var yF=l(lh);cS=u(yF,"generated_responses"),yF.forEach(t),fS=h(d3),ih=o(d3,"TD",{align:!0});var EF=l(ih);pS=u(EF,"A list of strings corresponding to the earlier replies from the model."),EF.forEach(t),d3.forEach(t),hS=h(H),Al=o(H,"TR",{});var g3=l(Al);uh=o(g3,"TD",{align:!0});var wF=l(uh);dS=u(wF,"past_user_inputs"),wF.forEach(t),gS=h(g3),fa=o(g3,"TD",{align:!0});var m3=l(fa);mS=u(m3,"A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),Wq=o(m3,"CODE",{});var bF=l(Wq);$S=u(bF,"generated_responses"),bF.forEach(t),qS=u(m3,"."),m3.forEach(t),g3.forEach(t),_S=h(H),Dl=o(H,"TR",{});var $3=l(Dl);ch=o($3,"TD",{align:!0});var TF=l(ch);Yq=o(TF,"STRONG",{});var jF=l(Yq);vS=u(jF,"parameters"),jF.forEach(t),TF.forEach(t),yS=h($3),fh=o($3,"TD",{align:!0});var kF=l(fh);ES=u(kF,"a dict containing the following keys:"),kF.forEach(t),$3.forEach(t),wS=h(H),Ol=o(H,"TR",{});var q3=l(Ol);ph=o(q3,"TD",{align:!0});var AF=l(ph);bS=u(AF,"min_length"),AF.forEach(t),TS=h(q3),ke=o(q3,"TD",{align:!0});var og=l(ke);jS=u(og,"(Default: "),Vq=o(og,"CODE",{});var DF=l(Vq);kS=u(DF,"None"),DF.forEach(t),AS=u(og,"). Integer to define the minimum length "),Xq=o(og,"STRONG",{});var OF=l(Xq);DS=u(OF,"in tokens"),OF.forEach(t),OS=u(og," of the output summary."),og.forEach(t),q3.forEach(t),PS=h(H),Pl=o(H,"TR",{});var _3=l(Pl);hh=o(_3,"TD",{align:!0});var PF=l(hh);RS=u(PF,"max_length"),PF.forEach(t),SS=h(_3),Ae=o(_3,"TD",{align:!0});var lg=l(Ae);NS=u(lg,"(Default: "),Qq=o(lg,"CODE",{});var RF=l(Qq);xS=u(RF,"None"),RF.forEach(t),IS=u(lg,"). Integer to define the maximum length "),Zq=o(lg,"STRONG",{});var SF=l(Zq);HS=u(SF,"in tokens"),SF.forEach(t),BS=u(lg," of the output summary."),lg.forEach(t),_3.forEach(t),CS=h(H),Rl=o(H,"TR",{});var v3=l(Rl);dh=o(v3,"TD",{align:!0});var NF=l(dh);GS=u(NF,"top_k"),NF.forEach(t),LS=h(v3),De=o(v3,"TD",{align:!0});var ig=l(De);MS=u(ig,"(Default: "),e_=o(ig,"CODE",{});var xF=l(e_);US=u(xF,"None"),xF.forEach(t),zS=u(ig,"). Integer to define the top tokens considered within the "),t_=o(ig,"CODE",{});var IF=l(t_);KS=u(IF,"sample"),IF.forEach(t),FS=u(ig," operation to create new text."),ig.forEach(t),v3.forEach(t),JS=h(H),Sl=o(H,"TR",{});var y3=l(Sl);gh=o(y3,"TD",{align:!0});var HF=l(gh);WS=u(HF,"top_p"),HF.forEach(t),YS=h(y3),pe=o(y3,"TD",{align:!0});var gn=l(pe);VS=u(gn,"(Default: "),s_=o(gn,"CODE",{});var BF=l(s_);XS=u(BF,"None"),BF.forEach(t),QS=u(gn,"). Float to define the tokens that are within the "),a_=o(gn,"CODE",{});var CF=l(a_);ZS=u(CF,"sample"),CF.forEach(t),eN=u(gn," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),n_=o(gn,"CODE",{});var GF=l(n_);tN=u(GF,"top_p"),GF.forEach(t),sN=u(gn,"."),gn.forEach(t),y3.forEach(t),aN=h(H),Nl=o(H,"TR",{});var E3=l(Nl);mh=o(E3,"TD",{align:!0});var LF=l(mh);nN=u(LF,"temperature"),LF.forEach(t),rN=h(E3),he=o(E3,"TD",{align:!0});var mn=l(he);oN=u(mn,"(Default: "),r_=o(mn,"CODE",{});var MF=l(r_);lN=u(MF,"1.0"),MF.forEach(t),iN=u(mn,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),o_=o(mn,"CODE",{});var UF=l(o_);uN=u(UF,"0"),UF.forEach(t),cN=u(mn," means always take the highest score, "),l_=o(mn,"CODE",{});var zF=l(l_);fN=u(zF,"100.0"),zF.forEach(t),pN=u(mn," is getting closer to uniform probability."),mn.forEach(t),E3.forEach(t),hN=h(H),xl=o(H,"TR",{});var w3=l(xl);$h=o(w3,"TD",{align:!0});var KF=l($h);dN=u(KF,"repetition_penalty"),KF.forEach(t),gN=h(w3),pa=o(w3,"TD",{align:!0});var b3=l(pa);mN=u(b3,"(Default: "),i_=o(b3,"CODE",{});var FF=l(i_);$N=u(FF,"None"),FF.forEach(t),qN=u(b3,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),b3.forEach(t),w3.forEach(t),_N=h(H),Il=o(H,"TR",{});var T3=l(Il);qh=o(T3,"TD",{align:!0});var JF=l(qh);vN=u(JF,"max_time"),JF.forEach(t),yN=h(T3),ha=o(T3,"TD",{align:!0});var j3=l(ha);EN=u(j3,"(Default: "),u_=o(j3,"CODE",{});var WF=l(u_);wN=u(WF,"None"),WF.forEach(t),bN=u(j3,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),j3.forEach(t),T3.forEach(t),TN=h(H),Hl=o(H,"TR",{});var k3=l(Hl);_h=o(k3,"TD",{align:!0});var YF=l(_h);c_=o(YF,"STRONG",{});var VF=l(c_);jN=u(VF,"options"),VF.forEach(t),YF.forEach(t),kN=h(k3),vh=o(k3,"TD",{align:!0});var XF=l(vh);AN=u(XF,"a dict containing the following keys:"),XF.forEach(t),k3.forEach(t),DN=h(H),Bl=o(H,"TR",{});var A3=l(Bl);yh=o(A3,"TD",{align:!0});var QF=l(yh);ON=u(QF,"use_gpu"),QF.forEach(t),PN=h(A3),da=o(A3,"TD",{align:!0});var D3=l(da);RN=u(D3,"(Default: "),f_=o(D3,"CODE",{});var ZF=l(f_);SN=u(ZF,"false"),ZF.forEach(t),NN=u(D3,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),D3.forEach(t),A3.forEach(t),xN=h(H),Cl=o(H,"TR",{});var O3=l(Cl);Eh=o(O3,"TD",{align:!0});var eJ=l(Eh);IN=u(eJ,"use_cache"),eJ.forEach(t),HN=h(O3),ga=o(O3,"TD",{align:!0});var P3=l(ga);BN=u(P3,"(Default: "),p_=o(P3,"CODE",{});var tJ=l(p_);CN=u(tJ,"true"),tJ.forEach(t),GN=u(P3,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),P3.forEach(t),O3.forEach(t),LN=h(H),Gl=o(H,"TR",{});var R3=l(Gl);wh=o(R3,"TD",{align:!0});var sJ=l(wh);MN=u(sJ,"wait_for_model"),sJ.forEach(t),UN=h(R3),ma=o(R3,"TD",{align:!0});var S3=l(ma);zN=u(S3,"(Default: "),h_=o(S3,"CODE",{});var aJ=l(h_);KN=u(aJ,"false"),aJ.forEach(t),FN=u(S3,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),S3.forEach(t),R3.forEach(t),H.forEach(t),c3.forEach(t),Oy=h(a),bh=o(a,"P",{});var nJ=l(bh);JN=u(nJ,"Return value is either a dict or a list of dicts if you sent a list of inputs"),nJ.forEach(t),Py=h(a),$a=o(a,"TABLE",{});var N3=l($a);d_=o(N3,"THEAD",{});var rJ=l(d_);Ll=o(rJ,"TR",{});var x3=l(Ll);Th=o(x3,"TH",{align:!0});var oJ=l(Th);WN=u(oJ,"Returned values"),oJ.forEach(t),YN=h(x3),g_=o(x3,"TH",{align:!0}),l(g_).forEach(t),x3.forEach(t),rJ.forEach(t),VN=h(N3),$e=o(N3,"TBODY",{});var $n=l($e);Ml=o($n,"TR",{});var I3=l(Ml);jh=o(I3,"TD",{align:!0});var lJ=l(jh);m_=o(lJ,"STRONG",{});var iJ=l(m_);XN=u(iJ,"generated_text"),iJ.forEach(t),lJ.forEach(t),QN=h(I3),kh=o(I3,"TD",{align:!0});var uJ=l(kh);ZN=u(uJ,"The answer of the bot"),uJ.forEach(t),I3.forEach(t),ex=h($n),Ul=o($n,"TR",{});var H3=l(Ul);Ah=o(H3,"TD",{align:!0});var cJ=l(Ah);$_=o(cJ,"STRONG",{});var fJ=l($_);tx=u(fJ,"conversation"),fJ.forEach(t),cJ.forEach(t),sx=h(H3),Dh=o(H3,"TD",{align:!0});var pJ=l(Dh);ax=u(pJ,"A facility dictionnary to send back for the next input (with the new user input addition)."),pJ.forEach(t),H3.forEach(t),nx=h($n),zl=o($n,"TR",{});var B3=l(zl);Oh=o(B3,"TD",{align:!0});var hJ=l(Oh);rx=u(hJ,"past_user_inputs"),hJ.forEach(t),ox=h(B3),Ph=o(B3,"TD",{align:!0});var dJ=l(Ph);lx=u(dJ,"List of strings. The last inputs from the user in the conversation, <em>after the model has run."),dJ.forEach(t),B3.forEach(t),ix=h($n),Kl=o($n,"TR",{});var C3=l(Kl);Rh=o(C3,"TD",{align:!0});var gJ=l(Rh);ux=u(gJ,"generated_responses"),gJ.forEach(t),cx=h(C3),Sh=o(C3,"TD",{align:!0});var mJ=l(Sh);fx=u(mJ,"List of strings. The last outputs from the model in the conversation, <em>after the model has run."),mJ.forEach(t),C3.forEach(t),$n.forEach(t),N3.forEach(t),Ry=h(a),et=o(a,"H3",{class:!0});var G3=l(et);qa=o(G3,"A",{id:!0,class:!0,href:!0});var $J=l(qa);q_=o($J,"SPAN",{});var qJ=l(q_);v(Fl.$$.fragment,qJ),qJ.forEach(t),$J.forEach(t),px=h(G3),__=o(G3,"SPAN",{});var _J=l(__);hx=u(_J,"Feature Extraction task"),_J.forEach(t),G3.forEach(t),Sy=h(a),Nh=o(a,"P",{});var vJ=l(Nh);dx=u(vJ,`This task reads some text and outputs raw float values, that are usually
consumed as part of a semantic database/semantic search.`),vJ.forEach(t),Ny=h(a),v(_a.$$.fragment,a),xy=h(a),tt=o(a,"P",{});var pv=l(tt);gx=u(pv,"Available with: "),Jl=o(pv,"A",{href:!0,rel:!0});var yJ=l(Jl);mx=u(yJ,"\u{1F917} Transformers"),yJ.forEach(t),$x=h(pv),Wl=o(pv,"A",{href:!0,rel:!0});var EJ=l(Wl);qx=u(EJ,"Sentence-transformers"),EJ.forEach(t),pv.forEach(t),Iy=h(a),xh=o(a,"P",{});var wJ=l(xh);_x=u(wJ,"Request:"),wJ.forEach(t),Hy=h(a),va=o(a,"TABLE",{});var L3=l(va);v_=o(L3,"THEAD",{});var bJ=l(v_);Yl=o(bJ,"TR",{});var M3=l(Yl);Ih=o(M3,"TH",{align:!0});var TJ=l(Ih);vx=u(TJ,"All parameters"),TJ.forEach(t),yx=h(M3),y_=o(M3,"TH",{align:!0}),l(y_).forEach(t),M3.forEach(t),bJ.forEach(t),Ex=h(L3),re=o(L3,"TBODY",{});var Se=l(re);Vl=o(Se,"TR",{});var U3=l(Vl);Xl=o(U3,"TD",{align:!0});var rC=l(Xl);E_=o(rC,"STRONG",{});var jJ=l(E_);wx=u(jJ,"inputs"),jJ.forEach(t),bx=u(rC," (required):"),rC.forEach(t),Tx=h(U3),Hh=o(U3,"TD",{align:!0});var kJ=l(Hh);jx=u(kJ,"a string or a list of strings to get the features from."),kJ.forEach(t),U3.forEach(t),kx=h(Se),Ql=o(Se,"TR",{});var z3=l(Ql);Bh=o(z3,"TD",{align:!0});var AJ=l(Bh);w_=o(AJ,"STRONG",{});var DJ=l(w_);Ax=u(DJ,"options"),DJ.forEach(t),AJ.forEach(t),Dx=h(z3),Ch=o(z3,"TD",{align:!0});var OJ=l(Ch);Ox=u(OJ,"a dict containing the following keys:"),OJ.forEach(t),z3.forEach(t),Px=h(Se),Zl=o(Se,"TR",{});var K3=l(Zl);Gh=o(K3,"TD",{align:!0});var PJ=l(Gh);Rx=u(PJ,"use_gpu"),PJ.forEach(t),Sx=h(K3),ya=o(K3,"TD",{align:!0});var F3=l(ya);Nx=u(F3,"(Default: "),b_=o(F3,"CODE",{});var RJ=l(b_);xx=u(RJ,"false"),RJ.forEach(t),Ix=u(F3,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),F3.forEach(t),K3.forEach(t),Hx=h(Se),ei=o(Se,"TR",{});var J3=l(ei);Lh=o(J3,"TD",{align:!0});var SJ=l(Lh);Bx=u(SJ,"use_cache"),SJ.forEach(t),Cx=h(J3),Ea=o(J3,"TD",{align:!0});var W3=l(Ea);Gx=u(W3,"(Default: "),T_=o(W3,"CODE",{});var NJ=l(T_);Lx=u(NJ,"true"),NJ.forEach(t),Mx=u(W3,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),W3.forEach(t),J3.forEach(t),Ux=h(Se),ti=o(Se,"TR",{});var Y3=l(ti);Mh=o(Y3,"TD",{align:!0});var xJ=l(Mh);zx=u(xJ,"wait_for_model"),xJ.forEach(t),Kx=h(Y3),wa=o(Y3,"TD",{align:!0});var V3=l(wa);Fx=u(V3,"(Default: "),j_=o(V3,"CODE",{});var IJ=l(j_);Jx=u(IJ,"false"),IJ.forEach(t),Wx=u(V3,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),V3.forEach(t),Y3.forEach(t),Se.forEach(t),L3.forEach(t),By=h(a),Uh=o(a,"P",{});var HJ=l(Uh);Yx=u(HJ,"Return value is either a dict or a list of dicts if you sent a list of inputs"),HJ.forEach(t),Cy=h(a),ba=o(a,"TABLE",{});var X3=l(ba);k_=o(X3,"THEAD",{});var BJ=l(k_);si=o(BJ,"TR",{});var Q3=l(si);zh=o(Q3,"TH",{align:!0});var CJ=l(zh);Vx=u(CJ,"Returned values"),CJ.forEach(t),Xx=h(Q3),A_=o(Q3,"TH",{align:!0}),l(A_).forEach(t),Q3.forEach(t),BJ.forEach(t),Qx=h(X3),D_=o(X3,"TBODY",{});var GJ=l(D_);ai=o(GJ,"TR",{});var Z3=l(ai);Kh=o(Z3,"TD",{align:!0});var LJ=l(Kh);O_=o(LJ,"STRONG",{});var MJ=l(O_);Zx=u(MJ,"A list of float (or list of list of floats)"),MJ.forEach(t),LJ.forEach(t),eI=h(Z3),Fh=o(Z3,"TD",{align:!0});var UJ=l(Fh);tI=u(UJ,"The numbers that are the representation features of the input."),UJ.forEach(t),Z3.forEach(t),GJ.forEach(t),X3.forEach(t),Gy=h(a),Jh=o(a,"SMALL",{});var zJ=l(Jh);sI=u(zJ,`Returned values are a list of floats, or a list of list of floats (depending
  on if you sent a string or a list of string, and if the automatic reduction,
  usually mean_pooling for instance was applied for you or not. This should be
  explained on the model's README.`),zJ.forEach(t),Ly=h(a),st=o(a,"H2",{class:!0});var eT=l(st);Ta=o(eT,"A",{id:!0,class:!0,href:!0});var KJ=l(Ta);P_=o(KJ,"SPAN",{});var FJ=l(P_);v(ni.$$.fragment,FJ),FJ.forEach(t),KJ.forEach(t),aI=h(eT),R_=o(eT,"SPAN",{});var JJ=l(R_);nI=u(JJ,"Audio"),JJ.forEach(t),eT.forEach(t),My=h(a),at=o(a,"H3",{class:!0});var tT=l(at);ja=o(tT,"A",{id:!0,class:!0,href:!0});var WJ=l(ja);S_=o(WJ,"SPAN",{});var YJ=l(S_);v(ri.$$.fragment,YJ),YJ.forEach(t),WJ.forEach(t),rI=h(tT),N_=o(tT,"SPAN",{});var VJ=l(N_);oI=u(VJ,"Automatic Speech Recognition task"),VJ.forEach(t),tT.forEach(t),Uy=h(a),Wh=o(a,"P",{});var XJ=l(Wh);lI=u(XJ,`This task reads some audio input and outputs the said words within the
audio files.`),XJ.forEach(t),zy=h(a),v(ka.$$.fragment,a),Ky=h(a),v(Aa.$$.fragment,a),Fy=h(a),qe=o(a,"P",{});var qu=l(qe);iI=u(qu,"Available with: "),oi=o(qu,"A",{href:!0,rel:!0});var QJ=l(oi);uI=u(QJ,"\u{1F917} Transformers"),QJ.forEach(t),cI=h(qu),li=o(qu,"A",{href:!0,rel:!0});var ZJ=l(li);fI=u(ZJ,"ESPnet"),ZJ.forEach(t),pI=u(qu,` and
`),ii=o(qu,"A",{href:!0,rel:!0});var eW=l(ii);hI=u(eW,"SpeechBrain"),eW.forEach(t),qu.forEach(t),Jy=h(a),Yh=o(a,"P",{});var tW=l(Yh);dI=u(tW,"Request:"),tW.forEach(t),Wy=h(a),v(Da.$$.fragment,a),Yy=h(a),Vh=o(a,"P",{});var sW=l(Vh);gI=u(sW,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),sW.forEach(t),Vy=h(a),Oa=o(a,"TABLE",{});var sT=l(Oa);x_=o(sT,"THEAD",{});var aW=l(x_);ui=o(aW,"TR",{});var aT=l(ui);Xh=o(aT,"TH",{align:!0});var nW=l(Xh);mI=u(nW,"All parameters"),nW.forEach(t),$I=h(aT),I_=o(aT,"TH",{align:!0}),l(I_).forEach(t),aT.forEach(t),aW.forEach(t),qI=h(sT),H_=o(sT,"TBODY",{});var rW=l(H_);ci=o(rW,"TR",{});var nT=l(ci);fi=o(nT,"TD",{align:!0});var oC=l(fi);B_=o(oC,"STRONG",{});var oW=l(B_);_I=u(oW,"no parameter"),oW.forEach(t),vI=u(oC," (required)"),oC.forEach(t),yI=h(nT),Qh=o(nT,"TD",{align:!0});var lW=l(Qh);EI=u(lW,"a binary representation of the audio file. No other parameters are currently allowed."),lW.forEach(t),nT.forEach(t),rW.forEach(t),sT.forEach(t),Xy=h(a),Zh=o(a,"P",{});var iW=l(Zh);wI=u(iW,"Return value is either a dict or a list of dicts if you sent a list of inputs"),iW.forEach(t),Qy=h(a),ed=o(a,"P",{});var uW=l(ed);bI=u(uW,"Response:"),uW.forEach(t),Zy=h(a),v(Pa.$$.fragment,a),eE=h(a),Ra=o(a,"TABLE",{});var rT=l(Ra);C_=o(rT,"THEAD",{});var cW=l(C_);pi=o(cW,"TR",{});var oT=l(pi);td=o(oT,"TH",{align:!0});var fW=l(td);TI=u(fW,"Returned values"),fW.forEach(t),jI=h(oT),G_=o(oT,"TH",{align:!0}),l(G_).forEach(t),oT.forEach(t),cW.forEach(t),kI=h(rT),L_=o(rT,"TBODY",{});var pW=l(L_);hi=o(pW,"TR",{});var lT=l(hi);sd=o(lT,"TD",{align:!0});var hW=l(sd);M_=o(hW,"STRONG",{});var dW=l(M_);AI=u(dW,"text"),dW.forEach(t),hW.forEach(t),DI=h(lT),ad=o(lT,"TD",{align:!0});var gW=l(ad);OI=u(gW,"The string that was recognized within the audio file."),gW.forEach(t),lT.forEach(t),pW.forEach(t),rT.forEach(t),tE=h(a),nt=o(a,"H3",{class:!0});var iT=l(nt);Sa=o(iT,"A",{id:!0,class:!0,href:!0});var mW=l(Sa);U_=o(mW,"SPAN",{});var $W=l(U_);v(di.$$.fragment,$W),$W.forEach(t),mW.forEach(t),PI=h(iT),z_=o(iT,"SPAN",{});var qW=l(z_);RI=u(qW,"Audio Classification task"),qW.forEach(t),iT.forEach(t),sE=h(a),nd=o(a,"P",{});var _W=l(nd);SI=u(_W,"This task reads some audio input and outputs the likelihood of classes."),_W.forEach(t),aE=h(a),v(Na.$$.fragment,a),nE=h(a),rt=o(a,"P",{});var hv=l(rt);NI=u(hv,"Available with: "),gi=o(hv,"A",{href:!0,rel:!0});var vW=l(gi);xI=u(vW,"\u{1F917} Transformers"),vW.forEach(t),II=h(hv),mi=o(hv,"A",{href:!0,rel:!0});var yW=l(mi);HI=u(yW,"SpeechBrain"),yW.forEach(t),hv.forEach(t),rE=h(a),rd=o(a,"P",{});var EW=l(rd);BI=u(EW,"Request:"),EW.forEach(t),oE=h(a),v(xa.$$.fragment,a),lE=h(a),od=o(a,"P",{});var wW=l(od);CI=u(wW,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),wW.forEach(t),iE=h(a),Ia=o(a,"TABLE",{});var uT=l(Ia);K_=o(uT,"THEAD",{});var bW=l(K_);$i=o(bW,"TR",{});var cT=l($i);ld=o(cT,"TH",{align:!0});var TW=l(ld);GI=u(TW,"All parameters"),TW.forEach(t),LI=h(cT),F_=o(cT,"TH",{align:!0}),l(F_).forEach(t),cT.forEach(t),bW.forEach(t),MI=h(uT),J_=o(uT,"TBODY",{});var jW=l(J_);qi=o(jW,"TR",{});var fT=l(qi);_i=o(fT,"TD",{align:!0});var lC=l(_i);W_=o(lC,"STRONG",{});var kW=l(W_);UI=u(kW,"no parameter"),kW.forEach(t),zI=u(lC," (required)"),lC.forEach(t),KI=h(fT),id=o(fT,"TD",{align:!0});var AW=l(id);FI=u(AW,"a binary representation of the audio file. No other parameters are currently allowed."),AW.forEach(t),fT.forEach(t),jW.forEach(t),uT.forEach(t),uE=h(a),ud=o(a,"P",{});var DW=l(ud);JI=u(DW,"Return value is a dict"),DW.forEach(t),cE=h(a),v(Ha.$$.fragment,a),fE=h(a),Ba=o(a,"TABLE",{});var pT=l(Ba);Y_=o(pT,"THEAD",{});var OW=l(Y_);vi=o(OW,"TR",{});var hT=l(vi);cd=o(hT,"TH",{align:!0});var PW=l(cd);WI=u(PW,"Returned values"),PW.forEach(t),YI=h(hT),V_=o(hT,"TH",{align:!0}),l(V_).forEach(t),hT.forEach(t),OW.forEach(t),VI=h(pT),yi=o(pT,"TBODY",{});var dT=l(yi);Ei=o(dT,"TR",{});var gT=l(Ei);fd=o(gT,"TD",{align:!0});var RW=l(fd);X_=o(RW,"STRONG",{});var SW=l(X_);XI=u(SW,"label"),SW.forEach(t),RW.forEach(t),QI=h(gT),pd=o(gT,"TD",{align:!0});var NW=l(pd);ZI=u(NW,"The label for the class (model specific)"),NW.forEach(t),gT.forEach(t),eH=h(dT),wi=o(dT,"TR",{});var mT=l(wi);hd=o(mT,"TD",{align:!0});var xW=l(hd);Q_=o(xW,"STRONG",{});var IW=l(Q_);tH=u(IW,"score"),IW.forEach(t),xW.forEach(t),sH=h(mT),dd=o(mT,"TD",{align:!0});var HW=l(dd);aH=u(HW,"A float that represents how likely it is that the audio file belongs to this class."),HW.forEach(t),mT.forEach(t),dT.forEach(t),pT.forEach(t),pE=h(a),ot=o(a,"H2",{class:!0});var $T=l(ot);Ca=o($T,"A",{id:!0,class:!0,href:!0});var BW=l(Ca);Z_=o(BW,"SPAN",{});var CW=l(Z_);v(bi.$$.fragment,CW),CW.forEach(t),BW.forEach(t),nH=h($T),e1=o($T,"SPAN",{});var GW=l(e1);rH=u(GW,"Computer Vision"),GW.forEach(t),$T.forEach(t),hE=h(a),lt=o(a,"H3",{class:!0});var qT=l(lt);Ga=o(qT,"A",{id:!0,class:!0,href:!0});var LW=l(Ga);t1=o(LW,"SPAN",{});var MW=l(t1);v(Ti.$$.fragment,MW),MW.forEach(t),LW.forEach(t),oH=h(qT),s1=o(qT,"SPAN",{});var UW=l(s1);lH=u(UW,"Image Classification task"),UW.forEach(t),qT.forEach(t),dE=h(a),gd=o(a,"P",{});var zW=l(gd);iH=u(zW,"This task reads some image input and outputs the likelihood of classes."),zW.forEach(t),gE=h(a),v(La.$$.fragment,a),mE=h(a),ji=o(a,"P",{});var iC=l(ji);uH=u(iC,"Available with: "),ki=o(iC,"A",{href:!0,rel:!0});var KW=l(ki);cH=u(KW,"\u{1F917} Transformers"),KW.forEach(t),iC.forEach(t),$E=h(a),md=o(a,"P",{});var FW=l(md);fH=u(FW,"Request:"),FW.forEach(t),qE=h(a),v(Ma.$$.fragment,a),_E=h(a),Ua=o(a,"P",{});var _T=l(Ua);pH=u(_T,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Ai=o(_T,"A",{href:!0,rel:!0});var JW=l(Ai);hH=u(JW,`Pillow
supports`),JW.forEach(t),dH=u(_T,"."),_T.forEach(t),vE=h(a),za=o(a,"TABLE",{});var vT=l(za);a1=o(vT,"THEAD",{});var WW=l(a1);Di=o(WW,"TR",{});var yT=l(Di);$d=o(yT,"TH",{align:!0});var YW=l($d);gH=u(YW,"All parameters"),YW.forEach(t),mH=h(yT),n1=o(yT,"TH",{align:!0}),l(n1).forEach(t),yT.forEach(t),WW.forEach(t),$H=h(vT),r1=o(vT,"TBODY",{});var VW=l(r1);Oi=o(VW,"TR",{});var ET=l(Oi);Pi=o(ET,"TD",{align:!0});var uC=l(Pi);o1=o(uC,"STRONG",{});var XW=l(o1);qH=u(XW,"no parameter"),XW.forEach(t),_H=u(uC," (required)"),uC.forEach(t),vH=h(ET),qd=o(ET,"TD",{align:!0});var QW=l(qd);yH=u(QW,"a binary representation of the image file. No other parameters are currently allowed."),QW.forEach(t),ET.forEach(t),VW.forEach(t),vT.forEach(t),yE=h(a),_d=o(a,"P",{});var ZW=l(_d);EH=u(ZW,"Return value is a dict"),ZW.forEach(t),EE=h(a),v(Ka.$$.fragment,a),wE=h(a),Fa=o(a,"TABLE",{});var wT=l(Fa);l1=o(wT,"THEAD",{});var eY=l(l1);Ri=o(eY,"TR",{});var bT=l(Ri);vd=o(bT,"TH",{align:!0});var tY=l(vd);wH=u(tY,"Returned values"),tY.forEach(t),bH=h(bT),i1=o(bT,"TH",{align:!0}),l(i1).forEach(t),bT.forEach(t),eY.forEach(t),TH=h(wT),Si=o(wT,"TBODY",{});var TT=l(Si);Ni=o(TT,"TR",{});var jT=l(Ni);yd=o(jT,"TD",{align:!0});var sY=l(yd);u1=o(sY,"STRONG",{});var aY=l(u1);jH=u(aY,"label"),aY.forEach(t),sY.forEach(t),kH=h(jT),Ed=o(jT,"TD",{align:!0});var nY=l(Ed);AH=u(nY,"The label for the class (model specific)"),nY.forEach(t),jT.forEach(t),DH=h(TT),xi=o(TT,"TR",{});var kT=l(xi);wd=o(kT,"TD",{align:!0});var rY=l(wd);c1=o(rY,"STRONG",{});var oY=l(c1);OH=u(oY,"score"),oY.forEach(t),rY.forEach(t),PH=h(kT),bd=o(kT,"TD",{align:!0});var lY=l(bd);RH=u(lY,"A float that represents how likely it is that the image file belongs to this class."),lY.forEach(t),kT.forEach(t),TT.forEach(t),wT.forEach(t),bE=h(a),it=o(a,"H3",{class:!0});var AT=l(it);Ja=o(AT,"A",{id:!0,class:!0,href:!0});var iY=l(Ja);f1=o(iY,"SPAN",{});var uY=l(f1);v(Ii.$$.fragment,uY),uY.forEach(t),iY.forEach(t),SH=h(AT),p1=o(AT,"SPAN",{});var cY=l(p1);NH=u(cY,"Object Detection task"),cY.forEach(t),AT.forEach(t),TE=h(a),Td=o(a,"P",{});var fY=l(Td);xH=u(fY,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),fY.forEach(t),jE=h(a),v(Wa.$$.fragment,a),kE=h(a),Hi=o(a,"P",{});var cC=l(Hi);IH=u(cC,"Available with: "),Bi=o(cC,"A",{href:!0,rel:!0});var pY=l(Bi);HH=u(pY,"\u{1F917} Transformers"),pY.forEach(t),cC.forEach(t),AE=h(a),jd=o(a,"P",{});var hY=l(jd);BH=u(hY,"Request:"),hY.forEach(t),DE=h(a),v(Ya.$$.fragment,a),OE=h(a),Va=o(a,"P",{});var DT=l(Va);CH=u(DT,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Ci=o(DT,"A",{href:!0,rel:!0});var dY=l(Ci);GH=u(dY,`Pillow
supports`),dY.forEach(t),LH=u(DT,"."),DT.forEach(t),PE=h(a),Xa=o(a,"TABLE",{});var OT=l(Xa);h1=o(OT,"THEAD",{});var gY=l(h1);Gi=o(gY,"TR",{});var PT=l(Gi);kd=o(PT,"TH",{align:!0});var mY=l(kd);MH=u(mY,"All parameters"),mY.forEach(t),UH=h(PT),d1=o(PT,"TH",{align:!0}),l(d1).forEach(t),PT.forEach(t),gY.forEach(t),zH=h(OT),g1=o(OT,"TBODY",{});var $Y=l(g1);Li=o($Y,"TR",{});var RT=l(Li);Mi=o(RT,"TD",{align:!0});var fC=l(Mi);m1=o(fC,"STRONG",{});var qY=l(m1);KH=u(qY,"no parameter"),qY.forEach(t),FH=u(fC," (required)"),fC.forEach(t),JH=h(RT),Ad=o(RT,"TD",{align:!0});var _Y=l(Ad);WH=u(_Y,"a binary representation of the image file. No other parameters are currently allowed."),_Y.forEach(t),RT.forEach(t),$Y.forEach(t),OT.forEach(t),RE=h(a),Dd=o(a,"P",{});var vY=l(Dd);YH=u(vY,"Return value is a dict"),vY.forEach(t),SE=h(a),v(Qa.$$.fragment,a),NE=h(a),Za=o(a,"TABLE",{});var ST=l(Za);$1=o(ST,"THEAD",{});var yY=l($1);Ui=o(yY,"TR",{});var NT=l(Ui);Od=o(NT,"TH",{align:!0});var EY=l(Od);VH=u(EY,"Returned values"),EY.forEach(t),XH=h(NT),q1=o(NT,"TH",{align:!0}),l(q1).forEach(t),NT.forEach(t),yY.forEach(t),QH=h(ST),ut=o(ST,"TBODY",{});var ug=l(ut);zi=o(ug,"TR",{});var xT=l(zi);Pd=o(xT,"TD",{align:!0});var wY=l(Pd);_1=o(wY,"STRONG",{});var bY=l(_1);ZH=u(bY,"label"),bY.forEach(t),wY.forEach(t),eB=h(xT),Rd=o(xT,"TD",{align:!0});var TY=l(Rd);tB=u(TY,"The label for the class (model specific) of a detected object."),TY.forEach(t),xT.forEach(t),sB=h(ug),Ki=o(ug,"TR",{});var IT=l(Ki);Sd=o(IT,"TD",{align:!0});var jY=l(Sd);v1=o(jY,"STRONG",{});var kY=l(v1);aB=u(kY,"score"),kY.forEach(t),jY.forEach(t),nB=h(IT),Nd=o(IT,"TD",{align:!0});var AY=l(Nd);rB=u(AY,"A float that represents how likely it is that the detected object belongs to the given class."),AY.forEach(t),IT.forEach(t),oB=h(ug),Fi=o(ug,"TR",{});var HT=l(Fi);xd=o(HT,"TD",{align:!0});var DY=l(xd);y1=o(DY,"STRONG",{});var OY=l(y1);lB=u(OY,"box"),OY.forEach(t),DY.forEach(t),iB=h(HT),Id=o(HT,"TD",{align:!0});var PY=l(Id);uB=u(PY,"A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),PY.forEach(t),HT.forEach(t),ug.forEach(t),ST.forEach(t),xE=h(a),ct=o(a,"H3",{class:!0});var BT=l(ct);en=o(BT,"A",{id:!0,class:!0,href:!0});var RY=l(en);E1=o(RY,"SPAN",{});var SY=l(E1);v(Ji.$$.fragment,SY),SY.forEach(t),RY.forEach(t),cB=h(BT),w1=o(BT,"SPAN",{});var NY=l(w1);fB=u(NY,"Image Segmentation task"),NY.forEach(t),BT.forEach(t),IE=h(a),Hd=o(a,"P",{});var xY=l(Hd);pB=u(xY,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),xY.forEach(t),HE=h(a),v(tn.$$.fragment,a),BE=h(a),Wi=o(a,"P",{});var pC=l(Wi);hB=u(pC,"Available with: "),Yi=o(pC,"A",{href:!0,rel:!0});var IY=l(Yi);dB=u(IY,"\u{1F917} Transformers"),IY.forEach(t),pC.forEach(t),CE=h(a),Bd=o(a,"P",{});var HY=l(Bd);gB=u(HY,"Request:"),HY.forEach(t),GE=h(a),v(sn.$$.fragment,a),LE=h(a),an=o(a,"P",{});var CT=l(an);mB=u(CT,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Vi=o(CT,"A",{href:!0,rel:!0});var BY=l(Vi);$B=u(BY,`Pillow
supports`),BY.forEach(t),qB=u(CT,"."),CT.forEach(t),ME=h(a),nn=o(a,"TABLE",{});var GT=l(nn);b1=o(GT,"THEAD",{});var CY=l(b1);Xi=o(CY,"TR",{});var LT=l(Xi);Cd=o(LT,"TH",{align:!0});var GY=l(Cd);_B=u(GY,"All parameters"),GY.forEach(t),vB=h(LT),T1=o(LT,"TH",{align:!0}),l(T1).forEach(t),LT.forEach(t),CY.forEach(t),yB=h(GT),j1=o(GT,"TBODY",{});var LY=l(j1);Qi=o(LY,"TR",{});var MT=l(Qi);Zi=o(MT,"TD",{align:!0});var hC=l(Zi);k1=o(hC,"STRONG",{});var MY=l(k1);EB=u(MY,"no parameter"),MY.forEach(t),wB=u(hC," (required)"),hC.forEach(t),bB=h(MT),Gd=o(MT,"TD",{align:!0});var UY=l(Gd);TB=u(UY,"a binary representation of the image file. No other parameters are currently allowed."),UY.forEach(t),MT.forEach(t),LY.forEach(t),GT.forEach(t),UE=h(a),Ld=o(a,"P",{});var zY=l(Ld);jB=u(zY,"Return value is a dict"),zY.forEach(t),zE=h(a),v(rn.$$.fragment,a),KE=h(a),on=o(a,"TABLE",{});var UT=l(on);A1=o(UT,"THEAD",{});var KY=l(A1);eu=o(KY,"TR",{});var zT=l(eu);Md=o(zT,"TH",{align:!0});var FY=l(Md);kB=u(FY,"Returned values"),FY.forEach(t),AB=h(zT),D1=o(zT,"TH",{align:!0}),l(D1).forEach(t),zT.forEach(t),KY.forEach(t),DB=h(UT),ft=o(UT,"TBODY",{});var cg=l(ft);tu=o(cg,"TR",{});var KT=l(tu);Ud=o(KT,"TD",{align:!0});var JY=l(Ud);O1=o(JY,"STRONG",{});var WY=l(O1);OB=u(WY,"label"),WY.forEach(t),JY.forEach(t),PB=h(KT),zd=o(KT,"TD",{align:!0});var YY=l(zd);RB=u(YY,"The label for the class (model specific) of a segment."),YY.forEach(t),KT.forEach(t),SB=h(cg),su=o(cg,"TR",{});var FT=l(su);Kd=o(FT,"TD",{align:!0});var VY=l(Kd);P1=o(VY,"STRONG",{});var XY=l(P1);NB=u(XY,"score"),XY.forEach(t),VY.forEach(t),xB=h(FT),Fd=o(FT,"TD",{align:!0});var QY=l(Fd);IB=u(QY,"A float that represents how likely it is that the segment belongs to the given class."),QY.forEach(t),FT.forEach(t),HB=h(cg),au=o(cg,"TR",{});var JT=l(au);Jd=o(JT,"TD",{align:!0});var ZY=l(Jd);R1=o(ZY,"STRONG",{});var eV=l(R1);BB=u(eV,"mask"),eV.forEach(t),ZY.forEach(t),CB=h(JT),Wd=o(JT,"TD",{align:!0});var tV=l(Wd);GB=u(tV,"A str (base64 str of a single channel black-and-white img) representing the mask of a segment."),tV.forEach(t),JT.forEach(t),cg.forEach(t),UT.forEach(t),this.h()},h(){f(n,"name","hf:doc:metadata"),f(n,"content",JSON.stringify(LQ)),f(d,"id","detailed-parameters"),f(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(d,"href","#detailed-parameters"),f(s,"class","relative group"),f(le,"id","which-task-is-used-by-this-model"),f(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(le,"href","#which-task-is-used-by-this-model"),f(D,"class","relative group"),f(ht,"class","block dark:hidden"),sV(ht.src,dC="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task.png")||f(ht,"src",dC),f(ht,"width","300"),f(dt,"class","hidden dark:block invert"),sV(dt.src,gC="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task-dark.png")||f(dt,"src",gC),f(dt,"width","300"),f(gt,"id","natural-language-processing"),f(gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(gt,"href","#natural-language-processing"),f(Ie,"class","relative group"),f(mt,"id","fill-mask-task"),f(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(mt,"href","#fill-mask-task"),f(He,"class","relative group"),f(En,"href","https://github.com/huggingface/transformers"),f(En,"rel","nofollow"),f(bu,"align","left"),f(mg,"align","left"),f(Tn,"align","left"),f(Tu,"align","left"),f(ju,"align","left"),f(ku,"align","left"),f(Au,"align","left"),f(vt,"align","left"),f(Du,"align","left"),f(yt,"align","left"),f(Ou,"align","left"),f(Et,"align","left"),f(Ru,"align","left"),f(wg,"align","left"),f(Su,"align","left"),f(Nu,"align","left"),f(xu,"align","left"),f(Iu,"align","left"),f(Hu,"align","left"),f(Bu,"align","left"),f(Cu,"align","left"),f(Gu,"align","left"),f(Tt,"id","summarization-task"),f(Tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Tt,"href","#summarization-task"),f(Be,"class","relative group"),f(Lu,"href","mailto:api-enterprise@huggingface.co"),f(Hn,"href","https://github.com/huggingface/transformers"),f(Hn,"rel","nofollow"),f(zu,"align","left"),f(Pg,"align","left"),f(Gn,"align","left"),f(Ku,"align","left"),f(Fu,"align","left"),f(Ju,"align","left"),f(Wu,"align","left"),f(_e,"align","left"),f(Yu,"align","left"),f(ve,"align","left"),f(Vu,"align","left"),f(ye,"align","left"),f(Xu,"align","left"),f(ie,"align","left"),f(Qu,"align","left"),f(ue,"align","left"),f(Zu,"align","left"),f(Ot,"align","left"),f(ec,"align","left"),f(Pt,"align","left"),f(tc,"align","left"),f(sc,"align","left"),f(ac,"align","left"),f(Rt,"align","left"),f(nc,"align","left"),f(St,"align","left"),f(rc,"align","left"),f(Nt,"align","left"),f(lc,"align","left"),f(Zg,"align","left"),f(ic,"align","left"),f(uc,"align","left"),f(It,"id","question-answering-task"),f(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(It,"href","#question-answering-task"),f(Ce,"class","relative group"),f(sr,"href","https://github.com/huggingface/transformers"),f(sr,"rel","nofollow"),f(ar,"href","https://github.com/allenai/allennlp"),f(ar,"rel","nofollow"),f(dc,"align","left"),f(rm,"align","left"),f(gc,"align","left"),f(mc,"align","left"),f($c,"align","left"),f(qc,"align","left"),f(_c,"align","left"),f(Lt,"align","left"),f(vc,"align","left"),f(Mt,"align","left"),f(Ut,"id","table-question-answering-task"),f(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ut,"href","#table-question-answering-task"),f(Le,"class","relative group"),f(fr,"href","https://github.com/huggingface/transformers"),f(fr,"rel","nofollow"),f(bc,"align","left"),f(gm,"align","left"),f(dr,"align","left"),f($m,"align","left"),f(Tc,"align","left"),f(jc,"align","left"),f(kc,"align","left"),f(Ac,"align","left"),f(Dc,"align","left"),f(Oc,"align","left"),f(Pc,"align","left"),f(Jt,"align","left"),f(Rc,"align","left"),f(Wt,"align","left"),f(Sc,"align","left"),f(Yt,"align","left"),f(xc,"align","left"),f(wm,"align","left"),f(Ic,"align","left"),f(Hc,"align","left"),f(Bc,"align","left"),f(Cc,"align","left"),f(Gc,"align","left"),f(Lc,"align","left"),f(Mc,"align","left"),f(Uc,"align","left"),f(Qt,"id","sentence-similarity-task"),f(Qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Qt,"href","#sentence-similarity-task"),f(Me,"class","relative group"),f(kr,"href","https://github.com/huggingface/transformers"),f(kr,"rel","nofollow"),f(Ar,"href","https://www.sbert.net/index.html"),f(Ar,"rel","nofollow"),f(Jc,"align","left"),f(Pm,"align","left"),f(Pr,"align","left"),f(Sm,"align","left"),f(Wc,"align","left"),f(Yc,"align","left"),f(Vc,"align","left"),f(Xc,"align","left"),f(Qc,"align","left"),f(Zc,"align","left"),f(ef,"align","left"),f(ss,"align","left"),f(tf,"align","left"),f(as,"align","left"),f(sf,"align","left"),f(ns,"align","left"),f(nf,"align","left"),f(Cm,"align","left"),f(rf,"align","left"),f(of,"align","left"),f(ls,"id","text-classification-task"),f(ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ls,"href","#text-classification-task"),f(ze,"class","relative group"),f(Mr,"href","https://github.com/huggingface/transformers"),f(Mr,"rel","nofollow"),f(ff,"align","left"),f(Km,"align","left"),f(Kr,"align","left"),f(pf,"align","left"),f(hf,"align","left"),f(df,"align","left"),f(gf,"align","left"),f(fs,"align","left"),f(mf,"align","left"),f(ps,"align","left"),f($f,"align","left"),f(hs,"align","left"),f(_f,"align","left"),f(Qm,"align","left"),f(vf,"align","left"),f(yf,"align","left"),f(Ef,"align","left"),f(wf,"align","left"),f(ms,"id","text-generation-task"),f(ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ms,"href","#text-generation-task"),f(Ke,"class","relative group"),f(so,"href","https://github.com/huggingface/transformers"),f(so,"rel","nofollow"),f(kf,"align","left"),f(n$,"align","left"),f(ro,"align","left"),f(Af,"align","left"),f(Df,"align","left"),f(Of,"align","left"),f(Pf,"align","left"),f(Ee,"align","left"),f(Rf,"align","left"),f(ce,"align","left"),f(Sf,"align","left"),f(fe,"align","left"),f(Nf,"align","left"),f(vs,"align","left"),f(xf,"align","left"),f(we,"align","left"),f(If,"align","left"),f(be,"align","left"),f(Hf,"align","left"),f(Te,"align","left"),f(Bf,"align","left"),f(ys,"align","left"),f(Cf,"align","left"),f(Es,"align","left"),f(Gf,"align","left"),f(Lf,"align","left"),f(Mf,"align","left"),f(ws,"align","left"),f(Uf,"align","left"),f(bs,"align","left"),f(zf,"align","left"),f(Ts,"align","left"),f(Ff,"align","left"),f(D$,"align","left"),f(Jf,"align","left"),f(Wf,"align","left"),f(As,"id","text2text-generation-task"),f(As,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(As,"href","#text2text-generation-task"),f(Fe,"class","relative group"),f(Yf,"href","#text-generation-task"),f(Os,"id","token-classification-task"),f(Os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Os,"href","#token-classification-task"),f(Je,"class","relative group"),f(To,"href","https://github.com/huggingface/transformers"),f(To,"rel","nofollow"),f(jo,"href","https://github.com/flairNLP/flair"),f(jo,"rel","nofollow"),f(Zf,"align","left"),f(H$,"align","left"),f(Do,"align","left"),f(ep,"align","left"),f(tp,"align","left"),f(sp,"align","left"),f(ap,"align","left"),f(x,"align","left"),f(np,"align","left"),f(rp,"align","left"),f(op,"align","left"),f(Ns,"align","left"),f(lp,"align","left"),f(xs,"align","left"),f(ip,"align","left"),f(Is,"align","left"),f(cp,"align","left"),f(eq,"align","left"),f(fp,"align","left"),f(pp,"align","left"),f(hp,"align","left"),f(dp,"align","left"),f(gp,"align","left"),f(mp,"align","left"),f($p,"align","left"),f(Cs,"align","left"),f(qp,"align","left"),f(Gs,"align","left"),f(Ls,"id","named-entity-recognition-ner-task"),f(Ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ls,"href","#named-entity-recognition-ner-task"),f(Ye,"class","relative group"),f(_p,"href","#token-classification-task"),f(Ms,"id","translation-task"),f(Ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ms,"href","#translation-task"),f(Ve,"class","relative group"),f(Fo,"href","https://github.com/huggingface/transformers"),f(Fo,"rel","nofollow"),f(wp,"align","left"),f(hq,"align","left"),f(Yo,"align","left"),f(bp,"align","left"),f(Tp,"align","left"),f(jp,"align","left"),f(kp,"align","left"),f(Fs,"align","left"),f(Ap,"align","left"),f(Js,"align","left"),f(Dp,"align","left"),f(Ws,"align","left"),f(Pp,"align","left"),f(vq,"align","left"),f(Rp,"align","left"),f(Sp,"align","left"),f(Vs,"id","zeroshot-classification-task"),f(Vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Vs,"href","#zeroshot-classification-task"),f(Xe,"class","relative group"),f(nl,"href","https://github.com/huggingface/transformers"),f(nl,"rel","nofollow"),f(Hp,"align","left"),f(jq,"align","left"),f(ll,"align","left"),f(Bp,"align","left"),f(ul,"align","left"),f(Cp,"align","left"),f(Gp,"align","left"),f(je,"align","left"),f(Lp,"align","left"),f(ea,"align","left"),f(Mp,"align","left"),f(Up,"align","left"),f(zp,"align","left"),f(ta,"align","left"),f(Kp,"align","left"),f(sa,"align","left"),f(Fp,"align","left"),f(aa,"align","left"),f(Yp,"align","left"),f(Hq,"align","left"),f(Vp,"align","left"),f(Xp,"align","left"),f(Qp,"align","left"),f(Zp,"align","left"),f(eh,"align","left"),f(oa,"align","left"),f(la,"id","conversational-task"),f(la,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(la,"href","#conversational-task"),f(Ze,"class","relative group"),f(El,"href","https://github.com/huggingface/transformers"),f(El,"rel","nofollow"),f(nh,"align","left"),f(Kq,"align","left"),f(Tl,"align","left"),f(Jq,"align","left"),f(rh,"align","left"),f(oh,"align","left"),f(lh,"align","left"),f(ih,"align","left"),f(uh,"align","left"),f(fa,"align","left"),f(ch,"align","left"),f(fh,"align","left"),f(ph,"align","left"),f(ke,"align","left"),f(hh,"align","left"),f(Ae,"align","left"),f(dh,"align","left"),f(De,"align","left"),f(gh,"align","left"),f(pe,"align","left"),f(mh,"align","left"),f(he,"align","left"),f($h,"align","left"),f(pa,"align","left"),f(qh,"align","left"),f(ha,"align","left"),f(_h,"align","left"),f(vh,"align","left"),f(yh,"align","left"),f(da,"align","left"),f(Eh,"align","left"),f(ga,"align","left"),f(wh,"align","left"),f(ma,"align","left"),f(Th,"align","left"),f(g_,"align","left"),f(jh,"align","left"),f(kh,"align","left"),f(Ah,"align","left"),f(Dh,"align","left"),f(Oh,"align","left"),f(Ph,"align","left"),f(Rh,"align","left"),f(Sh,"align","left"),f(qa,"id","feature-extraction-task"),f(qa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(qa,"href","#feature-extraction-task"),f(et,"class","relative group"),f(Jl,"href","https://github.com/huggingface/transformers"),f(Jl,"rel","nofollow"),f(Wl,"href","https://github.com/UKPLab/sentence-transformers"),f(Wl,"rel","nofollow"),f(Ih,"align","left"),f(y_,"align","left"),f(Xl,"align","left"),f(Hh,"align","left"),f(Bh,"align","left"),f(Ch,"align","left"),f(Gh,"align","left"),f(ya,"align","left"),f(Lh,"align","left"),f(Ea,"align","left"),f(Mh,"align","left"),f(wa,"align","left"),f(zh,"align","left"),f(A_,"align","left"),f(Kh,"align","left"),f(Fh,"align","left"),f(Ta,"id","audio"),f(Ta,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ta,"href","#audio"),f(st,"class","relative group"),f(ja,"id","automatic-speech-recognition-task"),f(ja,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ja,"href","#automatic-speech-recognition-task"),f(at,"class","relative group"),f(oi,"href","https://github.com/huggingface/transformers"),f(oi,"rel","nofollow"),f(li,"href","https://github.com/espnet/espnet"),f(li,"rel","nofollow"),f(ii,"href","https://github.com/speechbrain/speechbrain"),f(ii,"rel","nofollow"),f(Xh,"align","left"),f(I_,"align","left"),f(fi,"align","left"),f(Qh,"align","left"),f(td,"align","left"),f(G_,"align","left"),f(sd,"align","left"),f(ad,"align","left"),f(Sa,"id","audio-classification-task"),f(Sa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Sa,"href","#audio-classification-task"),f(nt,"class","relative group"),f(gi,"href","https://github.com/huggingface/transformers"),f(gi,"rel","nofollow"),f(mi,"href","https://github.com/speechbrain/speechbrain"),f(mi,"rel","nofollow"),f(ld,"align","left"),f(F_,"align","left"),f(_i,"align","left"),f(id,"align","left"),f(cd,"align","left"),f(V_,"align","left"),f(fd,"align","left"),f(pd,"align","left"),f(hd,"align","left"),f(dd,"align","left"),f(Ca,"id","computer-vision"),f(Ca,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ca,"href","#computer-vision"),f(ot,"class","relative group"),f(Ga,"id","image-classification-task"),f(Ga,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ga,"href","#image-classification-task"),f(lt,"class","relative group"),f(ki,"href","https://github.com/huggingface/transformers"),f(ki,"rel","nofollow"),f(Ai,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),f(Ai,"rel","nofollow"),f($d,"align","left"),f(n1,"align","left"),f(Pi,"align","left"),f(qd,"align","left"),f(vd,"align","left"),f(i1,"align","left"),f(yd,"align","left"),f(Ed,"align","left"),f(wd,"align","left"),f(bd,"align","left"),f(Ja,"id","object-detection-task"),f(Ja,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ja,"href","#object-detection-task"),f(it,"class","relative group"),f(Bi,"href","https://github.com/huggingface/transformers"),f(Bi,"rel","nofollow"),f(Ci,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),f(Ci,"rel","nofollow"),f(kd,"align","left"),f(d1,"align","left"),f(Mi,"align","left"),f(Ad,"align","left"),f(Od,"align","left"),f(q1,"align","left"),f(Pd,"align","left"),f(Rd,"align","left"),f(Sd,"align","left"),f(Nd,"align","left"),f(xd,"align","left"),f(Id,"align","left"),f(en,"id","image-segmentation-task"),f(en,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(en,"href","#image-segmentation-task"),f(ct,"class","relative group"),f(Yi,"href","https://github.com/huggingface/transformers"),f(Yi,"rel","nofollow"),f(Vi,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),f(Vi,"rel","nofollow"),f(Cd,"align","left"),f(T1,"align","left"),f(Zi,"align","left"),f(Gd,"align","left"),f(Md,"align","left"),f(D1,"align","left"),f(Ud,"align","left"),f(zd,"align","left"),f(Kd,"align","left"),f(Fd,"align","left"),f(Jd,"align","left"),f(Wd,"align","left")},m(a,g){e(document.head,n),m(a,c,g),m(a,s,g),e(s,d),e(d,$),y(k,$,null),e(s,A),e(s,j),e(j,T),m(a,S,g),m(a,D,g),e(D,le),e(le,Ne),y(ee,Ne,null),e(D,V),e(D,pt),e(pt,_u),m(a,qn,g),m(a,xe,g),e(xe,WT),m(a,dv,g),m(a,vu,g),e(vu,YT),m(a,gv,g),m(a,ht,g),m(a,mv,g),m(a,dt,g),m(a,$v,g),m(a,Ie,g),e(Ie,gt),e(gt,fg),y(_n,fg,null),e(Ie,VT),e(Ie,pg),e(pg,XT),m(a,qv,g),m(a,He,g),e(He,mt),e(mt,hg),y(vn,hg,null),e(He,QT),e(He,dg),e(dg,ZT),m(a,_v,g),m(a,yu,g),e(yu,ej),m(a,vv,g),y($t,a,g),m(a,yv,g),m(a,yn,g),e(yn,tj),e(yn,En),e(En,sj),m(a,Ev,g),m(a,Eu,g),e(Eu,aj),m(a,wv,g),y(qt,a,g),m(a,bv,g),m(a,wu,g),e(wu,nj),m(a,Tv,g),m(a,_t,g),e(_t,gg),e(gg,wn),e(wn,bu),e(bu,rj),e(wn,oj),e(wn,mg),e(_t,lj),e(_t,te),e(te,bn),e(bn,Tn),e(Tn,$g),e($g,ij),e(Tn,uj),e(bn,cj),e(bn,Tu),e(Tu,fj),e(te,pj),e(te,jn),e(jn,ju),e(ju,qg),e(qg,hj),e(jn,dj),e(jn,ku),e(ku,gj),e(te,mj),e(te,kn),e(kn,Au),e(Au,$j),e(kn,qj),e(kn,vt),e(vt,_j),e(vt,_g),e(_g,vj),e(vt,yj),e(te,Ej),e(te,An),e(An,Du),e(Du,wj),e(An,bj),e(An,yt),e(yt,Tj),e(yt,vg),e(vg,jj),e(yt,kj),e(te,Aj),e(te,Dn),e(Dn,Ou),e(Ou,Dj),e(Dn,Oj),e(Dn,Et),e(Et,Pj),e(Et,yg),e(yg,Rj),e(Et,Sj),m(a,jv,g),m(a,Pu,g),e(Pu,Nj),m(a,kv,g),y(wt,a,g),m(a,Av,g),m(a,bt,g),e(bt,Eg),e(Eg,On),e(On,Ru),e(Ru,xj),e(On,Ij),e(On,wg),e(bt,Hj),e(bt,de),e(de,Pn),e(Pn,Su),e(Su,bg),e(bg,Bj),e(Pn,Cj),e(Pn,Nu),e(Nu,Gj),e(de,Lj),e(de,Rn),e(Rn,xu),e(xu,Tg),e(Tg,Mj),e(Rn,Uj),e(Rn,Iu),e(Iu,zj),e(de,Kj),e(de,Sn),e(Sn,Hu),e(Hu,jg),e(jg,Fj),e(Sn,Jj),e(Sn,Bu),e(Bu,Wj),e(de,Yj),e(de,Nn),e(Nn,Cu),e(Cu,kg),e(kg,Vj),e(Nn,Xj),e(Nn,Gu),e(Gu,Qj),m(a,Dv,g),m(a,Be,g),e(Be,Tt),e(Tt,Ag),y(xn,Ag,null),e(Be,Zj),e(Be,Dg),e(Dg,e4),m(a,Ov,g),m(a,jt,g),e(jt,t4),e(jt,Lu),e(Lu,s4),e(jt,a4),m(a,Pv,g),y(kt,a,g),m(a,Rv,g),m(a,In,g),e(In,n4),e(In,Hn),e(Hn,r4),m(a,Sv,g),m(a,Mu,g),e(Mu,o4),m(a,Nv,g),y(At,a,g),m(a,xv,g),m(a,Uu,g),e(Uu,l4),m(a,Iv,g),m(a,Dt,g),e(Dt,Og),e(Og,Bn),e(Bn,zu),e(zu,i4),e(Bn,u4),e(Bn,Pg),e(Dt,c4),e(Dt,G),e(G,Cn),e(Cn,Gn),e(Gn,Rg),e(Rg,f4),e(Gn,p4),e(Cn,h4),e(Cn,Ku),e(Ku,d4),e(G,g4),e(G,Ln),e(Ln,Fu),e(Fu,Sg),e(Sg,m4),e(Ln,$4),e(Ln,Ju),e(Ju,q4),e(G,_4),e(G,Mn),e(Mn,Wu),e(Wu,v4),e(Mn,y4),e(Mn,_e),e(_e,E4),e(_e,Ng),e(Ng,w4),e(_e,b4),e(_e,xg),e(xg,T4),e(_e,j4),e(G,k4),e(G,Un),e(Un,Yu),e(Yu,A4),e(Un,D4),e(Un,ve),e(ve,O4),e(ve,Ig),e(Ig,P4),e(ve,R4),e(ve,Hg),e(Hg,S4),e(ve,N4),e(G,x4),e(G,zn),e(zn,Vu),e(Vu,I4),e(zn,H4),e(zn,ye),e(ye,B4),e(ye,Bg),e(Bg,C4),e(ye,G4),e(ye,Cg),e(Cg,L4),e(ye,M4),e(G,U4),e(G,Kn),e(Kn,Xu),e(Xu,z4),e(Kn,K4),e(Kn,ie),e(ie,F4),e(ie,Gg),e(Gg,J4),e(ie,W4),e(ie,Lg),e(Lg,Y4),e(ie,V4),e(ie,Mg),e(Mg,X4),e(ie,Q4),e(G,Z4),e(G,Fn),e(Fn,Qu),e(Qu,e5),e(Fn,t5),e(Fn,ue),e(ue,s5),e(ue,Ug),e(Ug,a5),e(ue,n5),e(ue,zg),e(zg,r5),e(ue,o5),e(ue,Kg),e(Kg,l5),e(ue,i5),e(G,u5),e(G,Jn),e(Jn,Zu),e(Zu,c5),e(Jn,f5),e(Jn,Ot),e(Ot,p5),e(Ot,Fg),e(Fg,h5),e(Ot,d5),e(G,g5),e(G,Wn),e(Wn,ec),e(ec,m5),e(Wn,$5),e(Wn,Pt),e(Pt,q5),e(Pt,Jg),e(Jg,_5),e(Pt,v5),e(G,y5),e(G,Yn),e(Yn,tc),e(tc,Wg),e(Wg,E5),e(Yn,w5),e(Yn,sc),e(sc,b5),e(G,T5),e(G,Vn),e(Vn,ac),e(ac,j5),e(Vn,k5),e(Vn,Rt),e(Rt,A5),e(Rt,Yg),e(Yg,D5),e(Rt,O5),e(G,P5),e(G,Xn),e(Xn,nc),e(nc,R5),e(Xn,S5),e(Xn,St),e(St,N5),e(St,Vg),e(Vg,x5),e(St,I5),e(G,H5),e(G,Qn),e(Qn,rc),e(rc,B5),e(Qn,C5),e(Qn,Nt),e(Nt,G5),e(Nt,Xg),e(Xg,L5),e(Nt,M5),m(a,Hv,g),m(a,oc,g),e(oc,U5),m(a,Bv,g),m(a,xt,g),e(xt,Qg),e(Qg,Zn),e(Zn,lc),e(lc,z5),e(Zn,K5),e(Zn,Zg),e(xt,F5),e(xt,em),e(em,er),e(er,ic),e(ic,tm),e(tm,J5),e(er,W5),e(er,uc),e(uc,Y5),m(a,Cv,g),m(a,Ce,g),e(Ce,It),e(It,sm),y(tr,sm,null),e(Ce,V5),e(Ce,am),e(am,X5),m(a,Gv,g),m(a,cc,g),e(cc,Q5),m(a,Lv,g),y(Ht,a,g),m(a,Mv,g),m(a,Ge,g),e(Ge,Z5),e(Ge,sr),e(sr,ek),e(Ge,tk),e(Ge,ar),e(ar,sk),m(a,Uv,g),m(a,fc,g),e(fc,ak),m(a,zv,g),y(Bt,a,g),m(a,Kv,g),m(a,pc,g),e(pc,nk),m(a,Fv,g),m(a,hc,g),e(hc,rk),m(a,Jv,g),y(Ct,a,g),m(a,Wv,g),m(a,Gt,g),e(Gt,nm),e(nm,nr),e(nr,dc),e(dc,ok),e(nr,lk),e(nr,rm),e(Gt,ik),e(Gt,ge),e(ge,rr),e(rr,gc),e(gc,om),e(om,uk),e(rr,ck),e(rr,mc),e(mc,fk),e(ge,pk),e(ge,or),e(or,$c),e($c,lm),e(lm,hk),e(or,dk),e(or,qc),e(qc,gk),e(ge,mk),e(ge,lr),e(lr,_c),e(_c,im),e(im,$k),e(lr,qk),e(lr,Lt),e(Lt,_k),e(Lt,um),e(um,vk),e(Lt,yk),e(ge,Ek),e(ge,ir),e(ir,vc),e(vc,cm),e(cm,wk),e(ir,bk),e(ir,Mt),e(Mt,Tk),e(Mt,fm),e(fm,jk),e(Mt,kk),m(a,Yv,g),m(a,Le,g),e(Le,Ut),e(Ut,pm),y(ur,pm,null),e(Le,Ak),e(Le,hm),e(hm,Dk),m(a,Vv,g),m(a,yc,g),e(yc,Ok),m(a,Xv,g),y(zt,a,g),m(a,Qv,g),m(a,cr,g),e(cr,Pk),e(cr,fr),e(fr,Rk),m(a,Zv,g),m(a,Ec,g),e(Ec,Sk),m(a,e2,g),y(Kt,a,g),m(a,t2,g),m(a,wc,g),e(wc,Nk),m(a,s2,g),m(a,Ft,g),e(Ft,dm),e(dm,pr),e(pr,bc),e(bc,xk),e(pr,Ik),e(pr,gm),e(Ft,Hk),e(Ft,F),e(F,hr),e(hr,dr),e(dr,mm),e(mm,Bk),e(dr,Ck),e(hr,Gk),e(hr,$m),e(F,Lk),e(F,gr),e(gr,Tc),e(Tc,Mk),e(gr,Uk),e(gr,jc),e(jc,zk),e(F,Kk),e(F,mr),e(mr,kc),e(kc,Fk),e(mr,Jk),e(mr,Ac),e(Ac,Wk),e(F,Yk),e(F,$r),e($r,Dc),e(Dc,qm),e(qm,Vk),e($r,Xk),e($r,Oc),e(Oc,Qk),e(F,Zk),e(F,qr),e(qr,Pc),e(Pc,e6),e(qr,t6),e(qr,Jt),e(Jt,s6),e(Jt,_m),e(_m,a6),e(Jt,n6),e(F,r6),e(F,_r),e(_r,Rc),e(Rc,o6),e(_r,l6),e(_r,Wt),e(Wt,i6),e(Wt,vm),e(vm,u6),e(Wt,c6),e(F,f6),e(F,vr),e(vr,Sc),e(Sc,p6),e(vr,h6),e(vr,Yt),e(Yt,d6),e(Yt,ym),e(ym,g6),e(Yt,m6),m(a,a2,g),m(a,Nc,g),e(Nc,$6),m(a,n2,g),y(Vt,a,g),m(a,r2,g),m(a,Xt,g),e(Xt,Em),e(Em,yr),e(yr,xc),e(xc,q6),e(yr,_6),e(yr,wm),e(Xt,v6),e(Xt,me),e(me,Er),e(Er,Ic),e(Ic,bm),e(bm,y6),e(Er,E6),e(Er,Hc),e(Hc,w6),e(me,b6),e(me,wr),e(wr,Bc),e(Bc,Tm),e(Tm,T6),e(wr,j6),e(wr,Cc),e(Cc,k6),e(me,A6),e(me,br),e(br,Gc),e(Gc,jm),e(jm,D6),e(br,O6),e(br,Lc),e(Lc,P6),e(me,R6),e(me,Tr),e(Tr,Mc),e(Mc,km),e(km,S6),e(Tr,N6),e(Tr,Uc),e(Uc,x6),m(a,o2,g),m(a,Me,g),e(Me,Qt),e(Qt,Am),y(jr,Am,null),e(Me,I6),e(Me,Dm),e(Dm,H6),m(a,l2,g),m(a,zc,g),e(zc,B6),m(a,i2,g),y(Zt,a,g),m(a,u2,g),m(a,Ue,g),e(Ue,C6),e(Ue,kr),e(kr,G6),e(Ue,L6),e(Ue,Ar),e(Ar,M6),m(a,c2,g),m(a,Kc,g),e(Kc,U6),m(a,f2,g),y(es,a,g),m(a,p2,g),m(a,Fc,g),e(Fc,z6),m(a,h2,g),m(a,ts,g),e(ts,Om),e(Om,Dr),e(Dr,Jc),e(Jc,K6),e(Dr,F6),e(Dr,Pm),e(ts,J6),e(ts,J),e(J,Or),e(Or,Pr),e(Pr,Rm),e(Rm,W6),e(Pr,Y6),e(Or,V6),e(Or,Sm),e(J,X6),e(J,Rr),e(Rr,Wc),e(Wc,Q6),e(Rr,Z6),e(Rr,Yc),e(Yc,e7),e(J,t7),e(J,Sr),e(Sr,Vc),e(Vc,s7),e(Sr,a7),e(Sr,Xc),e(Xc,n7),e(J,r7),e(J,Nr),e(Nr,Qc),e(Qc,Nm),e(Nm,o7),e(Nr,l7),e(Nr,Zc),e(Zc,i7),e(J,u7),e(J,xr),e(xr,ef),e(ef,c7),e(xr,f7),e(xr,ss),e(ss,p7),e(ss,xm),e(xm,h7),e(ss,d7),e(J,g7),e(J,Ir),e(Ir,tf),e(tf,m7),e(Ir,$7),e(Ir,as),e(as,q7),e(as,Im),e(Im,_7),e(as,v7),e(J,y7),e(J,Hr),e(Hr,sf),e(sf,E7),e(Hr,w7),e(Hr,ns),e(ns,b7),e(ns,Hm),e(Hm,T7),e(ns,j7),m(a,d2,g),m(a,af,g),e(af,k7),m(a,g2,g),y(rs,a,g),m(a,m2,g),m(a,os,g),e(os,Bm),e(Bm,Br),e(Br,nf),e(nf,A7),e(Br,D7),e(Br,Cm),e(os,O7),e(os,Gm),e(Gm,Cr),e(Cr,rf),e(rf,Lm),e(Lm,P7),e(Cr,R7),e(Cr,of),e(of,S7),m(a,$2,g),m(a,ze,g),e(ze,ls),e(ls,Mm),y(Gr,Mm,null),e(ze,N7),e(ze,Um),e(Um,x7),m(a,q2,g),m(a,lf,g),e(lf,I7),m(a,_2,g),y(is,a,g),m(a,v2,g),m(a,Lr,g),e(Lr,H7),e(Lr,Mr),e(Mr,B7),m(a,y2,g),m(a,uf,g),e(uf,C7),m(a,E2,g),y(us,a,g),m(a,w2,g),m(a,cf,g),e(cf,G7),m(a,b2,g),m(a,cs,g),e(cs,zm),e(zm,Ur),e(Ur,ff),e(ff,L7),e(Ur,M7),e(Ur,Km),e(cs,U7),e(cs,se),e(se,zr),e(zr,Kr),e(Kr,Fm),e(Fm,z7),e(Kr,K7),e(zr,F7),e(zr,pf),e(pf,J7),e(se,W7),e(se,Fr),e(Fr,hf),e(hf,Jm),e(Jm,Y7),e(Fr,V7),e(Fr,df),e(df,X7),e(se,Q7),e(se,Jr),e(Jr,gf),e(gf,Z7),e(Jr,e9),e(Jr,fs),e(fs,t9),e(fs,Wm),e(Wm,s9),e(fs,a9),e(se,n9),e(se,Wr),e(Wr,mf),e(mf,r9),e(Wr,o9),e(Wr,ps),e(ps,l9),e(ps,Ym),e(Ym,i9),e(ps,u9),e(se,c9),e(se,Yr),e(Yr,$f),e($f,f9),e(Yr,p9),e(Yr,hs),e(hs,h9),e(hs,Vm),e(Vm,d9),e(hs,g9),m(a,T2,g),m(a,qf,g),e(qf,m9),m(a,j2,g),y(ds,a,g),m(a,k2,g),m(a,gs,g),e(gs,Xm),e(Xm,Vr),e(Vr,_f),e(_f,$9),e(Vr,q9),e(Vr,Qm),e(gs,_9),e(gs,Xr),e(Xr,Qr),e(Qr,vf),e(vf,Zm),e(Zm,v9),e(Qr,y9),e(Qr,yf),e(yf,E9),e(Xr,w9),e(Xr,Zr),e(Zr,Ef),e(Ef,e$),e(e$,b9),e(Zr,T9),e(Zr,wf),e(wf,j9),m(a,A2,g),m(a,Ke,g),e(Ke,ms),e(ms,t$),y(eo,t$,null),e(Ke,k9),e(Ke,s$),e(s$,A9),m(a,D2,g),m(a,bf,g),e(bf,D9),m(a,O2,g),y($s,a,g),m(a,P2,g),m(a,to,g),e(to,O9),e(to,so),e(so,P9),m(a,R2,g),m(a,Tf,g),e(Tf,R9),m(a,S2,g),y(qs,a,g),m(a,N2,g),m(a,jf,g),e(jf,S9),m(a,x2,g),m(a,_s,g),e(_s,a$),e(a$,ao),e(ao,kf),e(kf,N9),e(ao,x9),e(ao,n$),e(_s,I9),e(_s,I),e(I,no),e(no,ro),e(ro,r$),e(r$,H9),e(ro,B9),e(no,C9),e(no,Af),e(Af,G9),e(I,L9),e(I,oo),e(oo,Df),e(Df,o$),e(o$,M9),e(oo,U9),e(oo,Of),e(Of,z9),e(I,K9),e(I,lo),e(lo,Pf),e(Pf,F9),e(lo,J9),e(lo,Ee),e(Ee,W9),e(Ee,l$),e(l$,Y9),e(Ee,V9),e(Ee,i$),e(i$,X9),e(Ee,Q9),e(I,Z9),e(I,io),e(io,Rf),e(Rf,e8),e(io,t8),e(io,ce),e(ce,s8),e(ce,u$),e(u$,a8),e(ce,n8),e(ce,c$),e(c$,r8),e(ce,o8),e(ce,f$),e(f$,l8),e(ce,i8),e(I,u8),e(I,uo),e(uo,Sf),e(Sf,c8),e(uo,f8),e(uo,fe),e(fe,p8),e(fe,p$),e(p$,h8),e(fe,d8),e(fe,h$),e(h$,g8),e(fe,m8),e(fe,d$),e(d$,$8),e(fe,q8),e(I,_8),e(I,co),e(co,Nf),e(Nf,v8),e(co,y8),e(co,vs),e(vs,E8),e(vs,g$),e(g$,w8),e(vs,b8),e(I,T8),e(I,fo),e(fo,xf),e(xf,j8),e(fo,k8),e(fo,we),e(we,A8),e(we,m$),e(m$,D8),e(we,O8),e(we,$$),e($$,P8),e(we,R8),e(I,S8),e(I,po),e(po,If),e(If,N8),e(po,x8),e(po,be),e(be,I8),e(be,q$),e(q$,H8),e(be,B8),e(be,_$),e(_$,C8),e(be,G8),e(I,L8),e(I,ho),e(ho,Hf),e(Hf,M8),e(ho,U8),e(ho,Te),e(Te,z8),e(Te,v$),e(v$,K8),e(Te,F8),e(Te,y$),e(y$,J8),e(Te,W8),e(I,Y8),e(I,go),e(go,Bf),e(Bf,V8),e(go,X8),e(go,ys),e(ys,Q8),e(ys,E$),e(E$,Z8),e(ys,eA),e(I,tA),e(I,mo),e(mo,Cf),e(Cf,sA),e(mo,aA),e(mo,Es),e(Es,nA),e(Es,w$),e(w$,rA),e(Es,oA),e(I,lA),e(I,$o),e($o,Gf),e(Gf,b$),e(b$,iA),e($o,uA),e($o,Lf),e(Lf,cA),e(I,fA),e(I,qo),e(qo,Mf),e(Mf,pA),e(qo,hA),e(qo,ws),e(ws,dA),e(ws,T$),e(T$,gA),e(ws,mA),e(I,$A),e(I,_o),e(_o,Uf),e(Uf,qA),e(_o,_A),e(_o,bs),e(bs,vA),e(bs,j$),e(j$,yA),e(bs,EA),e(I,wA),e(I,vo),e(vo,zf),e(zf,bA),e(vo,TA),e(vo,Ts),e(Ts,jA),e(Ts,k$),e(k$,kA),e(Ts,AA),m(a,I2,g),m(a,Kf,g),e(Kf,DA),m(a,H2,g),y(js,a,g),m(a,B2,g),m(a,ks,g),e(ks,A$),e(A$,yo),e(yo,Ff),e(Ff,OA),e(yo,PA),e(yo,D$),e(ks,RA),e(ks,O$),e(O$,Eo),e(Eo,Jf),e(Jf,P$),e(P$,SA),e(Eo,NA),e(Eo,Wf),e(Wf,xA),m(a,C2,g),m(a,Fe,g),e(Fe,As),e(As,R$),y(wo,R$,null),e(Fe,IA),e(Fe,S$),e(S$,HA),m(a,G2,g),m(a,Ds,g),e(Ds,BA),e(Ds,Yf),e(Yf,CA),e(Ds,GA),m(a,L2,g),m(a,Je,g),e(Je,Os),e(Os,N$),y(bo,N$,null),e(Je,LA),e(Je,x$),e(x$,MA),m(a,M2,g),m(a,Vf,g),e(Vf,UA),m(a,U2,g),y(Ps,a,g),m(a,z2,g),m(a,We,g),e(We,zA),e(We,To),e(To,KA),e(We,FA),e(We,jo),e(jo,JA),m(a,K2,g),m(a,Xf,g),e(Xf,WA),m(a,F2,g),y(Rs,a,g),m(a,J2,g),m(a,Qf,g),e(Qf,YA),m(a,W2,g),m(a,Ss,g),e(Ss,I$),e(I$,ko),e(ko,Zf),e(Zf,VA),e(ko,XA),e(ko,H$),e(Ss,QA),e(Ss,W),e(W,Ao),e(Ao,Do),e(Do,B$),e(B$,ZA),e(Do,eD),e(Ao,tD),e(Ao,ep),e(ep,sD),e(W,aD),e(W,Oo),e(Oo,tp),e(tp,C$),e(C$,nD),e(Oo,rD),e(Oo,sp),e(sp,oD),e(W,lD),e(W,Po),e(Po,ap),e(ap,iD),e(Po,uD),e(Po,x),e(x,cD),e(x,G$),e(G$,fD),e(x,pD),e(x,hD),e(x,dD),e(x,L$),e(L$,gD),e(x,mD),e(x,$D),e(x,qD),e(x,M$),e(M$,_D),e(x,vD),e(x,yD),e(x,ED),e(x,U$),e(U$,wD),e(x,bD),e(x,z$),e(z$,TD),e(x,jD),e(x,kD),e(x,AD),e(x,K$),e(K$,DD),e(x,OD),e(x,F$),e(F$,PD),e(x,RD),e(x,SD),e(x,ND),e(x,J$),e(J$,xD),e(x,ID),e(x,W$),e(W$,HD),e(x,BD),e(W,CD),e(W,Ro),e(Ro,np),e(np,Y$),e(Y$,GD),e(Ro,LD),e(Ro,rp),e(rp,MD),e(W,UD),e(W,So),e(So,op),e(op,zD),e(So,KD),e(So,Ns),e(Ns,FD),e(Ns,V$),e(V$,JD),e(Ns,WD),e(W,YD),e(W,No),e(No,lp),e(lp,VD),e(No,XD),e(No,xs),e(xs,QD),e(xs,X$),e(X$,ZD),e(xs,eO),e(W,tO),e(W,xo),e(xo,ip),e(ip,sO),e(xo,aO),e(xo,Is),e(Is,nO),e(Is,Q$),e(Q$,rO),e(Is,oO),m(a,Y2,g),m(a,up,g),e(up,lO),m(a,V2,g),y(Hs,a,g),m(a,X2,g),m(a,Bs,g),e(Bs,Z$),e(Z$,Io),e(Io,cp),e(cp,iO),e(Io,uO),e(Io,eq),e(Bs,cO),e(Bs,ae),e(ae,Ho),e(Ho,fp),e(fp,tq),e(tq,fO),e(Ho,pO),e(Ho,pp),e(pp,hO),e(ae,dO),e(ae,Bo),e(Bo,hp),e(hp,sq),e(sq,gO),e(Bo,mO),e(Bo,dp),e(dp,$O),e(ae,qO),e(ae,Co),e(Co,gp),e(gp,aq),e(aq,_O),e(Co,vO),e(Co,mp),e(mp,yO),e(ae,EO),e(ae,Go),e(Go,$p),e($p,nq),e(nq,wO),e(Go,bO),e(Go,Cs),e(Cs,TO),e(Cs,rq),e(rq,jO),e(Cs,kO),e(ae,AO),e(ae,Lo),e(Lo,qp),e(qp,oq),e(oq,DO),e(Lo,OO),e(Lo,Gs),e(Gs,PO),e(Gs,lq),e(lq,RO),e(Gs,SO),m(a,Q2,g),m(a,Ye,g),e(Ye,Ls),e(Ls,iq),y(Mo,iq,null),e(Ye,NO),e(Ye,uq),e(uq,xO),m(a,Z2,g),m(a,Uo,g),e(Uo,IO),e(Uo,_p),e(_p,HO),m(a,ey,g),m(a,Ve,g),e(Ve,Ms),e(Ms,cq),y(zo,cq,null),e(Ve,BO),e(Ve,fq),e(fq,CO),m(a,ty,g),m(a,vp,g),e(vp,GO),m(a,sy,g),y(Us,a,g),m(a,ay,g),m(a,Ko,g),e(Ko,LO),e(Ko,Fo),e(Fo,MO),m(a,ny,g),m(a,yp,g),e(yp,UO),m(a,ry,g),y(zs,a,g),m(a,oy,g),m(a,Ep,g),e(Ep,zO),m(a,ly,g),m(a,Ks,g),e(Ks,pq),e(pq,Jo),e(Jo,wp),e(wp,KO),e(Jo,FO),e(Jo,hq),e(Ks,JO),e(Ks,ne),e(ne,Wo),e(Wo,Yo),e(Yo,dq),e(dq,WO),e(Yo,YO),e(Wo,VO),e(Wo,bp),e(bp,XO),e(ne,QO),e(ne,Vo),e(Vo,Tp),e(Tp,gq),e(gq,ZO),e(Vo,eP),e(Vo,jp),e(jp,tP),e(ne,sP),e(ne,Xo),e(Xo,kp),e(kp,aP),e(Xo,nP),e(Xo,Fs),e(Fs,rP),e(Fs,mq),e(mq,oP),e(Fs,lP),e(ne,iP),e(ne,Qo),e(Qo,Ap),e(Ap,uP),e(Qo,cP),e(Qo,Js),e(Js,fP),e(Js,$q),e($q,pP),e(Js,hP),e(ne,dP),e(ne,Zo),e(Zo,Dp),e(Dp,gP),e(Zo,mP),e(Zo,Ws),e(Ws,$P),e(Ws,qq),e(qq,qP),e(Ws,_P),m(a,iy,g),m(a,Op,g),e(Op,vP),m(a,uy,g),m(a,Ys,g),e(Ys,_q),e(_q,el),e(el,Pp),e(Pp,yP),e(el,EP),e(el,vq),e(Ys,wP),e(Ys,yq),e(yq,tl),e(tl,Rp),e(Rp,Eq),e(Eq,bP),e(tl,TP),e(tl,Sp),e(Sp,jP),m(a,cy,g),m(a,Xe,g),e(Xe,Vs),e(Vs,wq),y(sl,wq,null),e(Xe,kP),e(Xe,bq),e(bq,AP),m(a,fy,g),m(a,Np,g),e(Np,DP),m(a,py,g),y(Xs,a,g),m(a,hy,g),m(a,al,g),e(al,OP),e(al,nl),e(nl,PP),m(a,dy,g),m(a,xp,g),e(xp,RP),m(a,gy,g),y(Qs,a,g),m(a,my,g),m(a,Ip,g),e(Ip,SP),m(a,$y,g),m(a,Zs,g),e(Zs,Tq),e(Tq,rl),e(rl,Hp),e(Hp,NP),e(rl,xP),e(rl,jq),e(Zs,IP),e(Zs,z),e(z,ol),e(ol,ll),e(ll,kq),e(kq,HP),e(ll,BP),e(ol,CP),e(ol,Bp),e(Bp,GP),e(z,LP),e(z,il),e(il,ul),e(ul,Aq),e(Aq,MP),e(ul,UP),e(il,zP),e(il,Cp),e(Cp,KP),e(z,FP),e(z,cl),e(cl,Gp),e(Gp,JP),e(cl,WP),e(cl,je),e(je,YP),e(je,Dq),e(Dq,VP),e(je,XP),e(je,Oq),e(Oq,QP),e(je,ZP),e(z,eR),e(z,fl),e(fl,Lp),e(Lp,tR),e(fl,sR),e(fl,ea),e(ea,aR),e(ea,Pq),e(Pq,nR),e(ea,rR),e(z,oR),e(z,pl),e(pl,Mp),e(Mp,Rq),e(Rq,lR),e(pl,iR),e(pl,Up),e(Up,uR),e(z,cR),e(z,hl),e(hl,zp),e(zp,fR),e(hl,pR),e(hl,ta),e(ta,hR),e(ta,Sq),e(Sq,dR),e(ta,gR),e(z,mR),e(z,dl),e(dl,Kp),e(Kp,$R),e(dl,qR),e(dl,sa),e(sa,_R),e(sa,Nq),e(Nq,vR),e(sa,yR),e(z,ER),e(z,gl),e(gl,Fp),e(Fp,wR),e(gl,bR),e(gl,aa),e(aa,TR),e(aa,xq),e(xq,jR),e(aa,kR),m(a,qy,g),m(a,Jp,g),e(Jp,AR),m(a,_y,g),m(a,Wp,g),e(Wp,DR),m(a,vy,g),y(na,a,g),m(a,yy,g),m(a,ra,g),e(ra,Iq),e(Iq,ml),e(ml,Yp),e(Yp,OR),e(ml,PR),e(ml,Hq),e(ra,RR),e(ra,Qe),e(Qe,$l),e($l,Vp),e(Vp,Bq),e(Bq,SR),e($l,NR),e($l,Xp),e(Xp,xR),e(Qe,IR),e(Qe,ql),e(ql,Qp),e(Qp,Cq),e(Cq,HR),e(ql,BR),e(ql,Zp),e(Zp,CR),e(Qe,GR),e(Qe,_l),e(_l,eh),e(eh,Gq),e(Gq,LR),e(_l,MR),e(_l,oa),e(oa,UR),e(oa,Lq),e(Lq,zR),e(oa,KR),m(a,Ey,g),m(a,Ze,g),e(Ze,la),e(la,Mq),y(vl,Mq,null),e(Ze,FR),e(Ze,Uq),e(Uq,JR),m(a,wy,g),m(a,th,g),e(th,WR),m(a,by,g),y(ia,a,g),m(a,Ty,g),m(a,yl,g),e(yl,YR),e(yl,El),e(El,VR),m(a,jy,g),m(a,sh,g),e(sh,XR),m(a,ky,g),y(ua,a,g),m(a,Ay,g),m(a,ah,g),e(ah,QR),m(a,Dy,g),m(a,ca,g),e(ca,zq),e(zq,wl),e(wl,nh),e(nh,ZR),e(wl,eS),e(wl,Kq),e(ca,tS),e(ca,N),e(N,bl),e(bl,Tl),e(Tl,Fq),e(Fq,sS),e(Tl,aS),e(bl,nS),e(bl,Jq),e(N,rS),e(N,jl),e(jl,rh),e(rh,oS),e(jl,lS),e(jl,oh),e(oh,iS),e(N,uS),e(N,kl),e(kl,lh),e(lh,cS),e(kl,fS),e(kl,ih),e(ih,pS),e(N,hS),e(N,Al),e(Al,uh),e(uh,dS),e(Al,gS),e(Al,fa),e(fa,mS),e(fa,Wq),e(Wq,$S),e(fa,qS),e(N,_S),e(N,Dl),e(Dl,ch),e(ch,Yq),e(Yq,vS),e(Dl,yS),e(Dl,fh),e(fh,ES),e(N,wS),e(N,Ol),e(Ol,ph),e(ph,bS),e(Ol,TS),e(Ol,ke),e(ke,jS),e(ke,Vq),e(Vq,kS),e(ke,AS),e(ke,Xq),e(Xq,DS),e(ke,OS),e(N,PS),e(N,Pl),e(Pl,hh),e(hh,RS),e(Pl,SS),e(Pl,Ae),e(Ae,NS),e(Ae,Qq),e(Qq,xS),e(Ae,IS),e(Ae,Zq),e(Zq,HS),e(Ae,BS),e(N,CS),e(N,Rl),e(Rl,dh),e(dh,GS),e(Rl,LS),e(Rl,De),e(De,MS),e(De,e_),e(e_,US),e(De,zS),e(De,t_),e(t_,KS),e(De,FS),e(N,JS),e(N,Sl),e(Sl,gh),e(gh,WS),e(Sl,YS),e(Sl,pe),e(pe,VS),e(pe,s_),e(s_,XS),e(pe,QS),e(pe,a_),e(a_,ZS),e(pe,eN),e(pe,n_),e(n_,tN),e(pe,sN),e(N,aN),e(N,Nl),e(Nl,mh),e(mh,nN),e(Nl,rN),e(Nl,he),e(he,oN),e(he,r_),e(r_,lN),e(he,iN),e(he,o_),e(o_,uN),e(he,cN),e(he,l_),e(l_,fN),e(he,pN),e(N,hN),e(N,xl),e(xl,$h),e($h,dN),e(xl,gN),e(xl,pa),e(pa,mN),e(pa,i_),e(i_,$N),e(pa,qN),e(N,_N),e(N,Il),e(Il,qh),e(qh,vN),e(Il,yN),e(Il,ha),e(ha,EN),e(ha,u_),e(u_,wN),e(ha,bN),e(N,TN),e(N,Hl),e(Hl,_h),e(_h,c_),e(c_,jN),e(Hl,kN),e(Hl,vh),e(vh,AN),e(N,DN),e(N,Bl),e(Bl,yh),e(yh,ON),e(Bl,PN),e(Bl,da),e(da,RN),e(da,f_),e(f_,SN),e(da,NN),e(N,xN),e(N,Cl),e(Cl,Eh),e(Eh,IN),e(Cl,HN),e(Cl,ga),e(ga,BN),e(ga,p_),e(p_,CN),e(ga,GN),e(N,LN),e(N,Gl),e(Gl,wh),e(wh,MN),e(Gl,UN),e(Gl,ma),e(ma,zN),e(ma,h_),e(h_,KN),e(ma,FN),m(a,Oy,g),m(a,bh,g),e(bh,JN),m(a,Py,g),m(a,$a,g),e($a,d_),e(d_,Ll),e(Ll,Th),e(Th,WN),e(Ll,YN),e(Ll,g_),e($a,VN),e($a,$e),e($e,Ml),e(Ml,jh),e(jh,m_),e(m_,XN),e(Ml,QN),e(Ml,kh),e(kh,ZN),e($e,ex),e($e,Ul),e(Ul,Ah),e(Ah,$_),e($_,tx),e(Ul,sx),e(Ul,Dh),e(Dh,ax),e($e,nx),e($e,zl),e(zl,Oh),e(Oh,rx),e(zl,ox),e(zl,Ph),e(Ph,lx),e($e,ix),e($e,Kl),e(Kl,Rh),e(Rh,ux),e(Kl,cx),e(Kl,Sh),e(Sh,fx),m(a,Ry,g),m(a,et,g),e(et,qa),e(qa,q_),y(Fl,q_,null),e(et,px),e(et,__),e(__,hx),m(a,Sy,g),m(a,Nh,g),e(Nh,dx),m(a,Ny,g),y(_a,a,g),m(a,xy,g),m(a,tt,g),e(tt,gx),e(tt,Jl),e(Jl,mx),e(tt,$x),e(tt,Wl),e(Wl,qx),m(a,Iy,g),m(a,xh,g),e(xh,_x),m(a,Hy,g),m(a,va,g),e(va,v_),e(v_,Yl),e(Yl,Ih),e(Ih,vx),e(Yl,yx),e(Yl,y_),e(va,Ex),e(va,re),e(re,Vl),e(Vl,Xl),e(Xl,E_),e(E_,wx),e(Xl,bx),e(Vl,Tx),e(Vl,Hh),e(Hh,jx),e(re,kx),e(re,Ql),e(Ql,Bh),e(Bh,w_),e(w_,Ax),e(Ql,Dx),e(Ql,Ch),e(Ch,Ox),e(re,Px),e(re,Zl),e(Zl,Gh),e(Gh,Rx),e(Zl,Sx),e(Zl,ya),e(ya,Nx),e(ya,b_),e(b_,xx),e(ya,Ix),e(re,Hx),e(re,ei),e(ei,Lh),e(Lh,Bx),e(ei,Cx),e(ei,Ea),e(Ea,Gx),e(Ea,T_),e(T_,Lx),e(Ea,Mx),e(re,Ux),e(re,ti),e(ti,Mh),e(Mh,zx),e(ti,Kx),e(ti,wa),e(wa,Fx),e(wa,j_),e(j_,Jx),e(wa,Wx),m(a,By,g),m(a,Uh,g),e(Uh,Yx),m(a,Cy,g),m(a,ba,g),e(ba,k_),e(k_,si),e(si,zh),e(zh,Vx),e(si,Xx),e(si,A_),e(ba,Qx),e(ba,D_),e(D_,ai),e(ai,Kh),e(Kh,O_),e(O_,Zx),e(ai,eI),e(ai,Fh),e(Fh,tI),m(a,Gy,g),m(a,Jh,g),e(Jh,sI),m(a,Ly,g),m(a,st,g),e(st,Ta),e(Ta,P_),y(ni,P_,null),e(st,aI),e(st,R_),e(R_,nI),m(a,My,g),m(a,at,g),e(at,ja),e(ja,S_),y(ri,S_,null),e(at,rI),e(at,N_),e(N_,oI),m(a,Uy,g),m(a,Wh,g),e(Wh,lI),m(a,zy,g),y(ka,a,g),m(a,Ky,g),y(Aa,a,g),m(a,Fy,g),m(a,qe,g),e(qe,iI),e(qe,oi),e(oi,uI),e(qe,cI),e(qe,li),e(li,fI),e(qe,pI),e(qe,ii),e(ii,hI),m(a,Jy,g),m(a,Yh,g),e(Yh,dI),m(a,Wy,g),y(Da,a,g),m(a,Yy,g),m(a,Vh,g),e(Vh,gI),m(a,Vy,g),m(a,Oa,g),e(Oa,x_),e(x_,ui),e(ui,Xh),e(Xh,mI),e(ui,$I),e(ui,I_),e(Oa,qI),e(Oa,H_),e(H_,ci),e(ci,fi),e(fi,B_),e(B_,_I),e(fi,vI),e(ci,yI),e(ci,Qh),e(Qh,EI),m(a,Xy,g),m(a,Zh,g),e(Zh,wI),m(a,Qy,g),m(a,ed,g),e(ed,bI),m(a,Zy,g),y(Pa,a,g),m(a,eE,g),m(a,Ra,g),e(Ra,C_),e(C_,pi),e(pi,td),e(td,TI),e(pi,jI),e(pi,G_),e(Ra,kI),e(Ra,L_),e(L_,hi),e(hi,sd),e(sd,M_),e(M_,AI),e(hi,DI),e(hi,ad),e(ad,OI),m(a,tE,g),m(a,nt,g),e(nt,Sa),e(Sa,U_),y(di,U_,null),e(nt,PI),e(nt,z_),e(z_,RI),m(a,sE,g),m(a,nd,g),e(nd,SI),m(a,aE,g),y(Na,a,g),m(a,nE,g),m(a,rt,g),e(rt,NI),e(rt,gi),e(gi,xI),e(rt,II),e(rt,mi),e(mi,HI),m(a,rE,g),m(a,rd,g),e(rd,BI),m(a,oE,g),y(xa,a,g),m(a,lE,g),m(a,od,g),e(od,CI),m(a,iE,g),m(a,Ia,g),e(Ia,K_),e(K_,$i),e($i,ld),e(ld,GI),e($i,LI),e($i,F_),e(Ia,MI),e(Ia,J_),e(J_,qi),e(qi,_i),e(_i,W_),e(W_,UI),e(_i,zI),e(qi,KI),e(qi,id),e(id,FI),m(a,uE,g),m(a,ud,g),e(ud,JI),m(a,cE,g),y(Ha,a,g),m(a,fE,g),m(a,Ba,g),e(Ba,Y_),e(Y_,vi),e(vi,cd),e(cd,WI),e(vi,YI),e(vi,V_),e(Ba,VI),e(Ba,yi),e(yi,Ei),e(Ei,fd),e(fd,X_),e(X_,XI),e(Ei,QI),e(Ei,pd),e(pd,ZI),e(yi,eH),e(yi,wi),e(wi,hd),e(hd,Q_),e(Q_,tH),e(wi,sH),e(wi,dd),e(dd,aH),m(a,pE,g),m(a,ot,g),e(ot,Ca),e(Ca,Z_),y(bi,Z_,null),e(ot,nH),e(ot,e1),e(e1,rH),m(a,hE,g),m(a,lt,g),e(lt,Ga),e(Ga,t1),y(Ti,t1,null),e(lt,oH),e(lt,s1),e(s1,lH),m(a,dE,g),m(a,gd,g),e(gd,iH),m(a,gE,g),y(La,a,g),m(a,mE,g),m(a,ji,g),e(ji,uH),e(ji,ki),e(ki,cH),m(a,$E,g),m(a,md,g),e(md,fH),m(a,qE,g),y(Ma,a,g),m(a,_E,g),m(a,Ua,g),e(Ua,pH),e(Ua,Ai),e(Ai,hH),e(Ua,dH),m(a,vE,g),m(a,za,g),e(za,a1),e(a1,Di),e(Di,$d),e($d,gH),e(Di,mH),e(Di,n1),e(za,$H),e(za,r1),e(r1,Oi),e(Oi,Pi),e(Pi,o1),e(o1,qH),e(Pi,_H),e(Oi,vH),e(Oi,qd),e(qd,yH),m(a,yE,g),m(a,_d,g),e(_d,EH),m(a,EE,g),y(Ka,a,g),m(a,wE,g),m(a,Fa,g),e(Fa,l1),e(l1,Ri),e(Ri,vd),e(vd,wH),e(Ri,bH),e(Ri,i1),e(Fa,TH),e(Fa,Si),e(Si,Ni),e(Ni,yd),e(yd,u1),e(u1,jH),e(Ni,kH),e(Ni,Ed),e(Ed,AH),e(Si,DH),e(Si,xi),e(xi,wd),e(wd,c1),e(c1,OH),e(xi,PH),e(xi,bd),e(bd,RH),m(a,bE,g),m(a,it,g),e(it,Ja),e(Ja,f1),y(Ii,f1,null),e(it,SH),e(it,p1),e(p1,NH),m(a,TE,g),m(a,Td,g),e(Td,xH),m(a,jE,g),y(Wa,a,g),m(a,kE,g),m(a,Hi,g),e(Hi,IH),e(Hi,Bi),e(Bi,HH),m(a,AE,g),m(a,jd,g),e(jd,BH),m(a,DE,g),y(Ya,a,g),m(a,OE,g),m(a,Va,g),e(Va,CH),e(Va,Ci),e(Ci,GH),e(Va,LH),m(a,PE,g),m(a,Xa,g),e(Xa,h1),e(h1,Gi),e(Gi,kd),e(kd,MH),e(Gi,UH),e(Gi,d1),e(Xa,zH),e(Xa,g1),e(g1,Li),e(Li,Mi),e(Mi,m1),e(m1,KH),e(Mi,FH),e(Li,JH),e(Li,Ad),e(Ad,WH),m(a,RE,g),m(a,Dd,g),e(Dd,YH),m(a,SE,g),y(Qa,a,g),m(a,NE,g),m(a,Za,g),e(Za,$1),e($1,Ui),e(Ui,Od),e(Od,VH),e(Ui,XH),e(Ui,q1),e(Za,QH),e(Za,ut),e(ut,zi),e(zi,Pd),e(Pd,_1),e(_1,ZH),e(zi,eB),e(zi,Rd),e(Rd,tB),e(ut,sB),e(ut,Ki),e(Ki,Sd),e(Sd,v1),e(v1,aB),e(Ki,nB),e(Ki,Nd),e(Nd,rB),e(ut,oB),e(ut,Fi),e(Fi,xd),e(xd,y1),e(y1,lB),e(Fi,iB),e(Fi,Id),e(Id,uB),m(a,xE,g),m(a,ct,g),e(ct,en),e(en,E1),y(Ji,E1,null),e(ct,cB),e(ct,w1),e(w1,fB),m(a,IE,g),m(a,Hd,g),e(Hd,pB),m(a,HE,g),y(tn,a,g),m(a,BE,g),m(a,Wi,g),e(Wi,hB),e(Wi,Yi),e(Yi,dB),m(a,CE,g),m(a,Bd,g),e(Bd,gB),m(a,GE,g),y(sn,a,g),m(a,LE,g),m(a,an,g),e(an,mB),e(an,Vi),e(Vi,$B),e(an,qB),m(a,ME,g),m(a,nn,g),e(nn,b1),e(b1,Xi),e(Xi,Cd),e(Cd,_B),e(Xi,vB),e(Xi,T1),e(nn,yB),e(nn,j1),e(j1,Qi),e(Qi,Zi),e(Zi,k1),e(k1,EB),e(Zi,wB),e(Qi,bB),e(Qi,Gd),e(Gd,TB),m(a,UE,g),m(a,Ld,g),e(Ld,jB),m(a,zE,g),y(rn,a,g),m(a,KE,g),m(a,on,g),e(on,A1),e(A1,eu),e(eu,Md),e(Md,kB),e(eu,AB),e(eu,D1),e(on,DB),e(on,ft),e(ft,tu),e(tu,Ud),e(Ud,O1),e(O1,OB),e(tu,PB),e(tu,zd),e(zd,RB),e(ft,SB),e(ft,su),e(su,Kd),e(Kd,P1),e(P1,NB),e(su,xB),e(su,Fd),e(Fd,IB),e(ft,HB),e(ft,au),e(au,Jd),e(Jd,R1),e(R1,BB),e(au,CB),e(au,Wd),e(Wd,GB),FE=!0},p(a,[g]){const nu={};g&2&&(nu.$$scope={dirty:g,ctx:a}),$t.$set(nu);const S1={};g&2&&(S1.$$scope={dirty:g,ctx:a}),qt.$set(S1);const N1={};g&2&&(N1.$$scope={dirty:g,ctx:a}),wt.$set(N1);const x1={};g&2&&(x1.$$scope={dirty:g,ctx:a}),kt.$set(x1);const ru={};g&2&&(ru.$$scope={dirty:g,ctx:a}),At.$set(ru);const I1={};g&2&&(I1.$$scope={dirty:g,ctx:a}),Ht.$set(I1);const H1={};g&2&&(H1.$$scope={dirty:g,ctx:a}),Bt.$set(H1);const B1={};g&2&&(B1.$$scope={dirty:g,ctx:a}),Ct.$set(B1);const C1={};g&2&&(C1.$$scope={dirty:g,ctx:a}),zt.$set(C1);const G1={};g&2&&(G1.$$scope={dirty:g,ctx:a}),Kt.$set(G1);const ou={};g&2&&(ou.$$scope={dirty:g,ctx:a}),Vt.$set(ou);const L1={};g&2&&(L1.$$scope={dirty:g,ctx:a}),Zt.$set(L1);const M1={};g&2&&(M1.$$scope={dirty:g,ctx:a}),es.$set(M1);const U1={};g&2&&(U1.$$scope={dirty:g,ctx:a}),rs.$set(U1);const lu={};g&2&&(lu.$$scope={dirty:g,ctx:a}),is.$set(lu);const z1={};g&2&&(z1.$$scope={dirty:g,ctx:a}),us.$set(z1);const K1={};g&2&&(K1.$$scope={dirty:g,ctx:a}),ds.$set(K1);const F1={};g&2&&(F1.$$scope={dirty:g,ctx:a}),$s.$set(F1);const J1={};g&2&&(J1.$$scope={dirty:g,ctx:a}),qs.$set(J1);const Yd={};g&2&&(Yd.$$scope={dirty:g,ctx:a}),js.$set(Yd);const W1={};g&2&&(W1.$$scope={dirty:g,ctx:a}),Ps.$set(W1);const Y1={};g&2&&(Y1.$$scope={dirty:g,ctx:a}),Rs.$set(Y1);const V1={};g&2&&(V1.$$scope={dirty:g,ctx:a}),Hs.$set(V1);const iu={};g&2&&(iu.$$scope={dirty:g,ctx:a}),Us.$set(iu);const X1={};g&2&&(X1.$$scope={dirty:g,ctx:a}),zs.$set(X1);const uu={};g&2&&(uu.$$scope={dirty:g,ctx:a}),Xs.$set(uu);const Q1={};g&2&&(Q1.$$scope={dirty:g,ctx:a}),Qs.$set(Q1);const oe={};g&2&&(oe.$$scope={dirty:g,ctx:a}),na.$set(oe);const cu={};g&2&&(cu.$$scope={dirty:g,ctx:a}),ia.$set(cu);const Vd={};g&2&&(Vd.$$scope={dirty:g,ctx:a}),ua.$set(Vd);const Z1={};g&2&&(Z1.$$scope={dirty:g,ctx:a}),_a.$set(Z1);const ev={};g&2&&(ev.$$scope={dirty:g,ctx:a}),ka.$set(ev);const fu={};g&2&&(fu.$$scope={dirty:g,ctx:a}),Aa.$set(fu);const tv={};g&2&&(tv.$$scope={dirty:g,ctx:a}),Da.$set(tv);const sv={};g&2&&(sv.$$scope={dirty:g,ctx:a}),Pa.$set(sv);const av={};g&2&&(av.$$scope={dirty:g,ctx:a}),Na.$set(av);const pu={};g&2&&(pu.$$scope={dirty:g,ctx:a}),xa.$set(pu);const nv={};g&2&&(nv.$$scope={dirty:g,ctx:a}),Ha.$set(nv);const hu={};g&2&&(hu.$$scope={dirty:g,ctx:a}),La.$set(hu);const rv={};g&2&&(rv.$$scope={dirty:g,ctx:a}),Ma.$set(rv);const du={};g&2&&(du.$$scope={dirty:g,ctx:a}),Ka.$set(du);const ov={};g&2&&(ov.$$scope={dirty:g,ctx:a}),Wa.$set(ov);const gu={};g&2&&(gu.$$scope={dirty:g,ctx:a}),Ya.$set(gu);const lv={};g&2&&(lv.$$scope={dirty:g,ctx:a}),Qa.$set(lv);const mu={};g&2&&(mu.$$scope={dirty:g,ctx:a}),tn.$set(mu);const iv={};g&2&&(iv.$$scope={dirty:g,ctx:a}),sn.$set(iv);const $u={};g&2&&($u.$$scope={dirty:g,ctx:a}),rn.$set($u)},i(a){FE||(E(k.$$.fragment,a),E(ee.$$.fragment,a),E(_n.$$.fragment,a),E(vn.$$.fragment,a),E($t.$$.fragment,a),E(qt.$$.fragment,a),E(wt.$$.fragment,a),E(xn.$$.fragment,a),E(kt.$$.fragment,a),E(At.$$.fragment,a),E(tr.$$.fragment,a),E(Ht.$$.fragment,a),E(Bt.$$.fragment,a),E(Ct.$$.fragment,a),E(ur.$$.fragment,a),E(zt.$$.fragment,a),E(Kt.$$.fragment,a),E(Vt.$$.fragment,a),E(jr.$$.fragment,a),E(Zt.$$.fragment,a),E(es.$$.fragment,a),E(rs.$$.fragment,a),E(Gr.$$.fragment,a),E(is.$$.fragment,a),E(us.$$.fragment,a),E(ds.$$.fragment,a),E(eo.$$.fragment,a),E($s.$$.fragment,a),E(qs.$$.fragment,a),E(js.$$.fragment,a),E(wo.$$.fragment,a),E(bo.$$.fragment,a),E(Ps.$$.fragment,a),E(Rs.$$.fragment,a),E(Hs.$$.fragment,a),E(Mo.$$.fragment,a),E(zo.$$.fragment,a),E(Us.$$.fragment,a),E(zs.$$.fragment,a),E(sl.$$.fragment,a),E(Xs.$$.fragment,a),E(Qs.$$.fragment,a),E(na.$$.fragment,a),E(vl.$$.fragment,a),E(ia.$$.fragment,a),E(ua.$$.fragment,a),E(Fl.$$.fragment,a),E(_a.$$.fragment,a),E(ni.$$.fragment,a),E(ri.$$.fragment,a),E(ka.$$.fragment,a),E(Aa.$$.fragment,a),E(Da.$$.fragment,a),E(Pa.$$.fragment,a),E(di.$$.fragment,a),E(Na.$$.fragment,a),E(xa.$$.fragment,a),E(Ha.$$.fragment,a),E(bi.$$.fragment,a),E(Ti.$$.fragment,a),E(La.$$.fragment,a),E(Ma.$$.fragment,a),E(Ka.$$.fragment,a),E(Ii.$$.fragment,a),E(Wa.$$.fragment,a),E(Ya.$$.fragment,a),E(Qa.$$.fragment,a),E(Ji.$$.fragment,a),E(tn.$$.fragment,a),E(sn.$$.fragment,a),E(rn.$$.fragment,a),FE=!0)},o(a){w(k.$$.fragment,a),w(ee.$$.fragment,a),w(_n.$$.fragment,a),w(vn.$$.fragment,a),w($t.$$.fragment,a),w(qt.$$.fragment,a),w(wt.$$.fragment,a),w(xn.$$.fragment,a),w(kt.$$.fragment,a),w(At.$$.fragment,a),w(tr.$$.fragment,a),w(Ht.$$.fragment,a),w(Bt.$$.fragment,a),w(Ct.$$.fragment,a),w(ur.$$.fragment,a),w(zt.$$.fragment,a),w(Kt.$$.fragment,a),w(Vt.$$.fragment,a),w(jr.$$.fragment,a),w(Zt.$$.fragment,a),w(es.$$.fragment,a),w(rs.$$.fragment,a),w(Gr.$$.fragment,a),w(is.$$.fragment,a),w(us.$$.fragment,a),w(ds.$$.fragment,a),w(eo.$$.fragment,a),w($s.$$.fragment,a),w(qs.$$.fragment,a),w(js.$$.fragment,a),w(wo.$$.fragment,a),w(bo.$$.fragment,a),w(Ps.$$.fragment,a),w(Rs.$$.fragment,a),w(Hs.$$.fragment,a),w(Mo.$$.fragment,a),w(zo.$$.fragment,a),w(Us.$$.fragment,a),w(zs.$$.fragment,a),w(sl.$$.fragment,a),w(Xs.$$.fragment,a),w(Qs.$$.fragment,a),w(na.$$.fragment,a),w(vl.$$.fragment,a),w(ia.$$.fragment,a),w(ua.$$.fragment,a),w(Fl.$$.fragment,a),w(_a.$$.fragment,a),w(ni.$$.fragment,a),w(ri.$$.fragment,a),w(ka.$$.fragment,a),w(Aa.$$.fragment,a),w(Da.$$.fragment,a),w(Pa.$$.fragment,a),w(di.$$.fragment,a),w(Na.$$.fragment,a),w(xa.$$.fragment,a),w(Ha.$$.fragment,a),w(bi.$$.fragment,a),w(Ti.$$.fragment,a),w(La.$$.fragment,a),w(Ma.$$.fragment,a),w(Ka.$$.fragment,a),w(Ii.$$.fragment,a),w(Wa.$$.fragment,a),w(Ya.$$.fragment,a),w(Qa.$$.fragment,a),w(Ji.$$.fragment,a),w(tn.$$.fragment,a),w(sn.$$.fragment,a),w(rn.$$.fragment,a),FE=!1},d(a){t(n),a&&t(c),a&&t(s),b(k),a&&t(S),a&&t(D),b(ee),a&&t(qn),a&&t(xe),a&&t(dv),a&&t(vu),a&&t(gv),a&&t(ht),a&&t(mv),a&&t(dt),a&&t($v),a&&t(Ie),b(_n),a&&t(qv),a&&t(He),b(vn),a&&t(_v),a&&t(yu),a&&t(vv),b($t,a),a&&t(yv),a&&t(yn),a&&t(Ev),a&&t(Eu),a&&t(wv),b(qt,a),a&&t(bv),a&&t(wu),a&&t(Tv),a&&t(_t),a&&t(jv),a&&t(Pu),a&&t(kv),b(wt,a),a&&t(Av),a&&t(bt),a&&t(Dv),a&&t(Be),b(xn),a&&t(Ov),a&&t(jt),a&&t(Pv),b(kt,a),a&&t(Rv),a&&t(In),a&&t(Sv),a&&t(Mu),a&&t(Nv),b(At,a),a&&t(xv),a&&t(Uu),a&&t(Iv),a&&t(Dt),a&&t(Hv),a&&t(oc),a&&t(Bv),a&&t(xt),a&&t(Cv),a&&t(Ce),b(tr),a&&t(Gv),a&&t(cc),a&&t(Lv),b(Ht,a),a&&t(Mv),a&&t(Ge),a&&t(Uv),a&&t(fc),a&&t(zv),b(Bt,a),a&&t(Kv),a&&t(pc),a&&t(Fv),a&&t(hc),a&&t(Jv),b(Ct,a),a&&t(Wv),a&&t(Gt),a&&t(Yv),a&&t(Le),b(ur),a&&t(Vv),a&&t(yc),a&&t(Xv),b(zt,a),a&&t(Qv),a&&t(cr),a&&t(Zv),a&&t(Ec),a&&t(e2),b(Kt,a),a&&t(t2),a&&t(wc),a&&t(s2),a&&t(Ft),a&&t(a2),a&&t(Nc),a&&t(n2),b(Vt,a),a&&t(r2),a&&t(Xt),a&&t(o2),a&&t(Me),b(jr),a&&t(l2),a&&t(zc),a&&t(i2),b(Zt,a),a&&t(u2),a&&t(Ue),a&&t(c2),a&&t(Kc),a&&t(f2),b(es,a),a&&t(p2),a&&t(Fc),a&&t(h2),a&&t(ts),a&&t(d2),a&&t(af),a&&t(g2),b(rs,a),a&&t(m2),a&&t(os),a&&t($2),a&&t(ze),b(Gr),a&&t(q2),a&&t(lf),a&&t(_2),b(is,a),a&&t(v2),a&&t(Lr),a&&t(y2),a&&t(uf),a&&t(E2),b(us,a),a&&t(w2),a&&t(cf),a&&t(b2),a&&t(cs),a&&t(T2),a&&t(qf),a&&t(j2),b(ds,a),a&&t(k2),a&&t(gs),a&&t(A2),a&&t(Ke),b(eo),a&&t(D2),a&&t(bf),a&&t(O2),b($s,a),a&&t(P2),a&&t(to),a&&t(R2),a&&t(Tf),a&&t(S2),b(qs,a),a&&t(N2),a&&t(jf),a&&t(x2),a&&t(_s),a&&t(I2),a&&t(Kf),a&&t(H2),b(js,a),a&&t(B2),a&&t(ks),a&&t(C2),a&&t(Fe),b(wo),a&&t(G2),a&&t(Ds),a&&t(L2),a&&t(Je),b(bo),a&&t(M2),a&&t(Vf),a&&t(U2),b(Ps,a),a&&t(z2),a&&t(We),a&&t(K2),a&&t(Xf),a&&t(F2),b(Rs,a),a&&t(J2),a&&t(Qf),a&&t(W2),a&&t(Ss),a&&t(Y2),a&&t(up),a&&t(V2),b(Hs,a),a&&t(X2),a&&t(Bs),a&&t(Q2),a&&t(Ye),b(Mo),a&&t(Z2),a&&t(Uo),a&&t(ey),a&&t(Ve),b(zo),a&&t(ty),a&&t(vp),a&&t(sy),b(Us,a),a&&t(ay),a&&t(Ko),a&&t(ny),a&&t(yp),a&&t(ry),b(zs,a),a&&t(oy),a&&t(Ep),a&&t(ly),a&&t(Ks),a&&t(iy),a&&t(Op),a&&t(uy),a&&t(Ys),a&&t(cy),a&&t(Xe),b(sl),a&&t(fy),a&&t(Np),a&&t(py),b(Xs,a),a&&t(hy),a&&t(al),a&&t(dy),a&&t(xp),a&&t(gy),b(Qs,a),a&&t(my),a&&t(Ip),a&&t($y),a&&t(Zs),a&&t(qy),a&&t(Jp),a&&t(_y),a&&t(Wp),a&&t(vy),b(na,a),a&&t(yy),a&&t(ra),a&&t(Ey),a&&t(Ze),b(vl),a&&t(wy),a&&t(th),a&&t(by),b(ia,a),a&&t(Ty),a&&t(yl),a&&t(jy),a&&t(sh),a&&t(ky),b(ua,a),a&&t(Ay),a&&t(ah),a&&t(Dy),a&&t(ca),a&&t(Oy),a&&t(bh),a&&t(Py),a&&t($a),a&&t(Ry),a&&t(et),b(Fl),a&&t(Sy),a&&t(Nh),a&&t(Ny),b(_a,a),a&&t(xy),a&&t(tt),a&&t(Iy),a&&t(xh),a&&t(Hy),a&&t(va),a&&t(By),a&&t(Uh),a&&t(Cy),a&&t(ba),a&&t(Gy),a&&t(Jh),a&&t(Ly),a&&t(st),b(ni),a&&t(My),a&&t(at),b(ri),a&&t(Uy),a&&t(Wh),a&&t(zy),b(ka,a),a&&t(Ky),b(Aa,a),a&&t(Fy),a&&t(qe),a&&t(Jy),a&&t(Yh),a&&t(Wy),b(Da,a),a&&t(Yy),a&&t(Vh),a&&t(Vy),a&&t(Oa),a&&t(Xy),a&&t(Zh),a&&t(Qy),a&&t(ed),a&&t(Zy),b(Pa,a),a&&t(eE),a&&t(Ra),a&&t(tE),a&&t(nt),b(di),a&&t(sE),a&&t(nd),a&&t(aE),b(Na,a),a&&t(nE),a&&t(rt),a&&t(rE),a&&t(rd),a&&t(oE),b(xa,a),a&&t(lE),a&&t(od),a&&t(iE),a&&t(Ia),a&&t(uE),a&&t(ud),a&&t(cE),b(Ha,a),a&&t(fE),a&&t(Ba),a&&t(pE),a&&t(ot),b(bi),a&&t(hE),a&&t(lt),b(Ti),a&&t(dE),a&&t(gd),a&&t(gE),b(La,a),a&&t(mE),a&&t(ji),a&&t($E),a&&t(md),a&&t(qE),b(Ma,a),a&&t(_E),a&&t(Ua),a&&t(vE),a&&t(za),a&&t(yE),a&&t(_d),a&&t(EE),b(Ka,a),a&&t(wE),a&&t(Fa),a&&t(bE),a&&t(it),b(Ii),a&&t(TE),a&&t(Td),a&&t(jE),b(Wa,a),a&&t(kE),a&&t(Hi),a&&t(AE),a&&t(jd),a&&t(DE),b(Ya,a),a&&t(OE),a&&t(Va),a&&t(PE),a&&t(Xa),a&&t(RE),a&&t(Dd),a&&t(SE),b(Qa,a),a&&t(NE),a&&t(Za),a&&t(xE),a&&t(ct),b(Ji),a&&t(IE),a&&t(Hd),a&&t(HE),b(tn,a),a&&t(BE),a&&t(Wi),a&&t(CE),a&&t(Bd),a&&t(GE),b(sn,a),a&&t(LE),a&&t(an),a&&t(ME),a&&t(nn),a&&t(UE),a&&t(Ld),a&&t(zE),b(rn,a),a&&t(KE),a&&t(on)}}}const LQ={local:"detailed-parameters",sections:[{local:"which-task-is-used-by-this-model",title:"Which task is used by this model ?"},{local:"natural-language-processing",sections:[{local:"fill-mask-task",title:"Fill Mask task"},{local:"summarization-task",title:"Summarization task"},{local:"question-answering-task",title:"Question Answering task"},{local:"table-question-answering-task",title:"Table Question Answering task"},{local:"sentence-similarity-task",title:"Sentence Similarity task"},{local:"text-classification-task",title:"Text Classification task"},{local:"text-generation-task",title:"Text Generation task"},{local:"text2text-generation-task",title:"Text2Text Generation task"},{local:"token-classification-task",title:"Token Classification task"},{local:"named-entity-recognition-ner-task",title:"Named Entity Recognition (NER) task"},{local:"translation-task",title:"Translation task"},{local:"zeroshot-classification-task",title:"Zero-Shot Classification task"},{local:"conversational-task",title:"Conversational task"},{local:"feature-extraction-task",title:"Feature Extraction task"}],title:"Natural Language Processing"},{local:"audio",sections:[{local:"automatic-speech-recognition-task",title:"Automatic Speech Recognition task"},{local:"audio-classification-task",title:"Audio Classification task"}],title:"Audio"},{local:"computer-vision",sections:[{local:"image-classification-task",title:"Image Classification task"},{local:"object-detection-task",title:"Object Detection task"},{local:"image-segmentation-task",title:"Image Segmentation task"}],title:"Computer Vision"}],title:"Detailed parameters"};function MQ(q){return lV(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class JQ extends aV{constructor(n){super();nV(this,n,MQ,GQ,rV,{})}}export{JQ as default,LQ as metadata};
