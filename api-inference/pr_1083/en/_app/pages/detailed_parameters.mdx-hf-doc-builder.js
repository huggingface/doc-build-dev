import{S as ZY,i as eV,s as tV,e as r,k as p,w as _,t as i,M as sV,c as o,d as t,m as h,a as l,x as v,h as u,b as f,N as QY,G as e,g as m,y,q as E,o as w,B as b,v as aV,L as O}from"../chunks/vendor-hf-doc-builder.js";import{T as K}from"../chunks/Tip-hf-doc-builder.js";import{I as U}from"../chunks/IconCopyLink-hf-doc-builder.js";import{I as L,M as P,C as R}from"../chunks/InferenceApi-hf-doc-builder.js";function nV(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("bert-base-uncased"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"bert-base-uncased"),D.forEach(t),A=u(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/bert-base-uncased"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function rV(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/bert-base-uncased"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is [MASK]."})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is [MASK].&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function oV(q){let n,c;return n=new P({props:{$$slots:{default:[rV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function lV(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/bert-base-uncased",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is [MASK]."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;The answer to the universe is [MASK].&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:<span class="hljs-number">0.16963955760002136</span>,&quot;token&quot;:<span class="hljs-number">2053</span>,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:<span class="hljs-number">0.07344776391983032</span>,&quot;token&quot;:<span class="hljs-number">2498</span>,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:<span class="hljs-number">0.05803241208195686</span>,&quot;token&quot;:<span class="hljs-number">2748</span>,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:<span class="hljs-number">0.043957844376564026</span>,&quot;token&quot;:<span class="hljs-number">4242</span>,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:<span class="hljs-number">0.04015745222568512</span>,&quot;token&quot;:<span class="hljs-number">3722</span>,&quot;token_str&quot;:&quot;simple&quot;}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function iV(q){let n,c;return n=new P({props:{$$slots:{default:[lV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function uV(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is [MASK]."}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is [MASK].&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:0.16963955760002136,&quot;token&quot;:2053,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:0.07344776391983032,&quot;token&quot;:2498,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:0.05803241208195686,&quot;token&quot;:2748,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:0.043957844376564026,&quot;token&quot;:4242,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:0.04015745222568512,&quot;token&quot;:3722,&quot;token_str&quot;:&quot;simple&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function cV(q){let n,c;return n=new P({props:{$$slots:{default:[uV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function fV(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "sequence": "the answer to the universe is no.",
            "score": 0.1696,
            "token": 2053,
            "token_str": "no",
        },
        {
            "sequence": "the answer to the universe is nothing.",
            "score": 0.0734,
            "token": 2498,
            "token_str": "nothing",
        },
        {
            "sequence": "the answer to the universe is yes.",
            "score": 0.0580,
            "token": 2748,
            "token_str": "yes",
        },
        {
            "sequence": "the answer to the universe is unknown.",
            "score": 0.044,
            "token": 4242,
            "token_str": "unknown",
        },
        {
            "sequence": "the answer to the universe is simple.",
            "score": 0.0402,
            "token": 3722,
            "token_str": "simple",
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is no.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.1696</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2053</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;no&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is nothing.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0734</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2498</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;nothing&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is yes.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0580</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2748</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;yes&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is unknown.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.044</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">4242</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;unknown&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is simple.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0402</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">3722</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;simple&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function pV(q){let n,c;return n=new P({props:{$$slots:{default:[fV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function hV(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/bart-large-cnn"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/bart-large-cnn"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/bart-large-cnn"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function dV(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",
        "parameters": {"do_sample": False},
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "summary_text": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;do_sample&quot;</span>: <span class="hljs-literal">False</span>},
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;summary_text&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function gV(q){let n,c;return n=new P({props:{$$slots:{default:[dV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function mV(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-cnn",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres."}]`,highlighted:`import fetch from <span class="hljs-comment">&quot;node-fetch&quot;</span>;
async function query(data) {
    const response = await fetch(
        <span class="hljs-comment">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>,
        {
            headers: { <span class="hljs-type">Authorization</span>: \`<span class="hljs-type">Bearer</span> <span class="hljs-string">\${</span><span class="hljs-type">API_TOKEN</span>}\` },
            method: <span class="hljs-comment">&quot;POST&quot;</span>,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: <span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>}).then((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{<span class="hljs-comment">&quot;summary_text&quot;</span>:<span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres.&quot;</span>}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function $V(q){let n,c;return n=new P({props:{$$slots:{default:[mV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function qV(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d '{"inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.", "parameters": {"do_sample": false}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world."}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;, &quot;parameters&quot;: {&quot;do_sample&quot;: false}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;summary_text&quot;:&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function _V(q){let n,c;return n=new P({props:{$$slots:{default:[qV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function vV(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("deepset/roberta-base-squad2"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"deepset/roberta-base-squad2"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/deepset/roberta-base-squad2"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function yV(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "question": "What's my name?",
            "context": "My name is Clara and I live in Berkeley.",
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;question&quot;</span>: <span class="hljs-string">&quot;What&#x27;s my name?&quot;</span>,
            <span class="hljs-string">&quot;context&quot;</span>: <span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>,
        }
    }
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function EV(q){let n,c;return n=new P({props:{$$slots:{default:[yV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function wV(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{question:"What is my name?",context:"My name is Clara and I live in Berkeley."}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:{<span class="hljs-attr">question</span>:<span class="hljs-string">&quot;What is my name?&quot;</span>,<span class="hljs-attr">context</span>:<span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function bV(q){let n,c;return n=new P({props:{$$slots:{default:[wV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function TV(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d '{"inputs":{"question":"What is my name?","context":"My name is Clara and I live in Berkeley."}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;question&quot;:&quot;What is my name?&quot;,&quot;context&quot;:&quot;My name is Clara and I live in Berkeley.&quot;}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function jV(q){let n,c;return n=new P({props:{$$slots:{default:[TV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function kV(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data),
    {"score": 0.9327, "start": 11, "end": 16, "answer": "Clara"},
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9327</span>, <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;Clara&quot;</span>},
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function AV(q){let n,c;return n=new P({props:{$$slots:{default:[kV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function DV(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("google/tapas-base-finetuned-wtq"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"google/tapas-base-finetuned-wtq"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/google/tapas-base-finetuned-wtq"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function OV(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "query": "How many stars does the transformers repository have?",
            "table": {
                "Repository": ["Transformers", "Datasets", "Tokenizers"],
                "Stars": ["36542", "4512", "3934"],
                "Contributors": ["651", "77", "34"],
                "Programming language": [
                    "Python",
                    "Python",
                    "Rust, Python and NodeJS",
                ],
            },
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;query&quot;</span>: <span class="hljs-string">&quot;How many stars does the transformers repository have?&quot;</span>,
            <span class="hljs-string">&quot;table&quot;</span>: {
                <span class="hljs-string">&quot;Repository&quot;</span>: [<span class="hljs-string">&quot;Transformers&quot;</span>, <span class="hljs-string">&quot;Datasets&quot;</span>, <span class="hljs-string">&quot;Tokenizers&quot;</span>],
                <span class="hljs-string">&quot;Stars&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>, <span class="hljs-string">&quot;4512&quot;</span>, <span class="hljs-string">&quot;3934&quot;</span>],
                <span class="hljs-string">&quot;Contributors&quot;</span>: [<span class="hljs-string">&quot;651&quot;</span>, <span class="hljs-string">&quot;77&quot;</span>, <span class="hljs-string">&quot;34&quot;</span>],
                <span class="hljs-string">&quot;Programming language&quot;</span>: [
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Rust, Python and NodeJS&quot;</span>,
                ],
            },
        }
    }
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function PV(q){let n,c;return n=new P({props:{$$slots:{default:[OV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function RV(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{query:"How many stars does the transformers repository have?",table:{Repository:["Transformers","Datasets","Tokenizers"],Stars:["36542","4512","3934"],Contributors:["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:{query:&quot;How many stars does the transformers repository have?&quot;,<span class="hljs-keyword">table</span>:{Repository:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],Stars:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],Contributors:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function SV(q){let n,c;return n=new P({props:{$$slots:{default:[RV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function NV(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d '{"inputs":{"query":"How many stars does the transformers repository have?","table":{"Repository":["Transformers","Datasets","Tokenizers"],"Stars":["36542","4512","3934"],"Contributors":["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;query&quot;:&quot;How many stars does the transformers repository have?&quot;,&quot;table&quot;:{&quot;Repository&quot;:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],&quot;Stars&quot;:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],&quot;Contributors&quot;:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[0,1]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function xV(q){let n,c;return n=new P({props:{$$slots:{default:[NV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function IV(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    data,
    {
        "answer": "AVERAGE > 36542",
        "coordinates": [[0, 1]],
        "cells": ["36542"],
        "aggregator": "AVERAGE",
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;AVERAGE &gt; 36542&quot;</span>,
        <span class="hljs-string">&quot;coordinates&quot;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]],
        <span class="hljs-string">&quot;cells&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>],
        <span class="hljs-string">&quot;aggregator&quot;</span>: <span class="hljs-string">&quot;AVERAGE&quot;</span>,
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function HV(q){let n,c;return n=new P({props:{$$slots:{default:[IV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function BV(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("sentence-transformers/all-MiniLM-L6-v2"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"sentence-transformers/all-MiniLM-L6-v2"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function CV(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "source_sentence": "That is a happy person",
            "sentences": ["That is a happy dog", "That is a very happy person", "Today is a sunny day"],
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;source_sentence&quot;</span>: <span class="hljs-string">&quot;That is a happy person&quot;</span>,
            <span class="hljs-string">&quot;sentences&quot;</span>: [<span class="hljs-string">&quot;That is a happy dog&quot;</span>, <span class="hljs-string">&quot;That is a very happy person&quot;</span>, <span class="hljs-string">&quot;Today is a sunny day&quot;</span>],
        }
    }
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function GV(q){let n,c;return n=new P({props:{$$slots:{default:[CV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function LV(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{
    source_sentence: "That is a happy person",
    sentences: [
        "That is a happy dog",
        "That is a very happy person",
        "Today is a sunny day"
    ]
}}).then((response) => {
    console.log(JSON.stringify(response));
});
// [0.6945773363113403,0.9429150819778442,0.2568760812282562]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:{
    <span class="hljs-attr">source_sentence</span>: <span class="hljs-string">&quot;That is a happy person&quot;</span>,
    <span class="hljs-attr">sentences</span>: [
        <span class="hljs-string">&quot;That is a happy dog&quot;</span>,
        <span class="hljs-string">&quot;That is a very happy person&quot;</span>,
        <span class="hljs-string">&quot;Today is a sunny day&quot;</span>
    ]
}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [0.6945773363113403,0.9429150819778442,0.2568760812282562]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function MV(q){let n,c;return n=new P({props:{$$slots:{default:[LV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function UV(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2 \\
        -X POST \\
        -d '{"inputs":{"source_sentence": "That is a happy person", "sentences": ["That is a happy dog","That is a very happy person","Today is a sunny day"]}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [0.6945773363113403,0.9429150819778442,0.2568760812282562]`,highlighted:`curl https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;source_sentence&quot;: &quot;That is a happy person&quot;, &quot;sentences&quot;: [&quot;That is a happy dog&quot;,&quot;That is a very happy person&quot;,&quot;Today is a sunny day&quot;]}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [0.6945773363113403,0.9429150819778442,0.2568760812282562]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function zV(q){let n,c;return n=new P({props:{$$slots:{default:[UV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function KV(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    data,
    [0.6945773363113403, 0.9429150819778442, 0.2568760812282562],
)`,highlighted:`self.assertEqual(
    data,
    [<span class="hljs-number">0.6945773363113403</span>, <span class="hljs-number">0.9429150819778442</span>, <span class="hljs-number">0.2568760812282562</span>],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function FV(q){let n,c;return n=new P({props:{$$slots:{default:[KV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function JV(q){let n,c,s,d,$,k;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("distilbert-base-uncased-finetuned-sst-2-english"),this.h()},l(A){n=o(A,"P",{});var j=l(n);c=o(j,"STRONG",{});var T=l(c);s=u(T,"Recommended model"),T.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var S=l($);k=u(S,"distilbert-base-uncased-finetuned-sst-2-english"),S.forEach(t),j.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),f($,"rel","nofollow")},m(A,j){m(A,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k)},d(A){A&&t(n)}}}function WV(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "I like you. I love you"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;I like you. I love you&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function YV(q){let n,c;return n=new P({props:{$$slots:{default:[WV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function VV(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"I like you. I love you"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [[{"label":"POSITIVE","score":0.9998738765716553},{"label":"NEGATIVE","score":0.0001261125144083053}]]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;I like you. I love you&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [[{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553},{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053}]]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function XV(q){let n,c;return n=new P({props:{$$slots:{default:[VV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function QV(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d '{"inputs":"I like you. I love you"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [[{"label":"POSITIVE","score":0.9998738765716553},{"label":"NEGATIVE","score":0.0001261125144083053}]]`,highlighted:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;I like you. I love you&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [[{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553},{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053}]]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function ZV(q){let n,c;return n=new P({props:{$$slots:{default:[QV]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function eX(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data),
    [
        [
            {"label": "POSITIVE", "score": 0.9999},
            {"label": "NEGATIVE", "score": 0.0001},
        ]
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        [
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9999</span>},
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;NEGATIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0001</span>},
        ]
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function tX(q){let n,c;return n=new P({props:{$$slots:{default:[eX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function sX(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(": "),$=r("a"),k=i("gpt2"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,": "),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"gpt2"),D.forEach(t),A=u(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/gpt2"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function aX(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/gpt2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function nX(q){let n,c;return n=new P({props:{$$slots:{default:[aX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function rX(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/gpt2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;The answer to the universe is&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function oX(q){let n,c;return n=new P({props:{$$slots:{default:[rX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function lX(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function iX(q){let n,c;return n=new P({props:{$$slots:{default:[lX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function uX(q){let n,c;return n=new R({props:{code:`data == [
    {
        "generated_text": 'The answer to the universe is that we are the creation of the entire universe," says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.'
    }
]`,highlighted:`data == [
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&#x27;The answer to the universe is that we are the creation of the entire universe,&quot; says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.&#x27;</span>
    }
]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function cX(q){let n,c;return n=new P({props:{$$slots:{default:[uX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function fX(q){let n,c,s,d,$,k;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("dbmdz/bert-large-cased-finetuned-conll03-english"),this.h()},l(A){n=o(A,"P",{});var j=l(n);c=o(j,"STRONG",{});var T=l(c);s=u(T,"Recommended model"),T.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var S=l($);k=u(S,"dbmdz/bert-large-cased-finetuned-conll03-english"),S.forEach(t),j.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"),f($,"rel","nofollow")},m(A,j){m(A,n,j),e(n,c),e(c,s),e(n,d),e(n,$),e($,k)},d(A){A&&t(n)}}}function pX(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "My name is Sarah Jessica Parker but you can call me Jessica"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function hX(q){let n,c;return n=new P({props:{$$slots:{default:[pX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function dX(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"My name is Sarah Jessica Parker but you can call me Jessica"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9991337060928345</span>,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:<span class="hljs-number">11</span>,&quot;end&quot;:<span class="hljs-number">31</span>},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9979912042617798</span>,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:<span class="hljs-number">52</span>,&quot;end&quot;:<span class="hljs-number">59</span>}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function gX(q){let n,c;return n=new P({props:{$$slots:{default:[dX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function mX(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d '{"inputs":"My name is Sarah Jessica Parker but you can call me Jessica"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9991337060928345,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:11,&quot;end&quot;:31},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9979912042617798,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:52,&quot;end&quot;:59}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function $X(q){let n,c;return n=new P({props:{$$slots:{default:[mX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function qX(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "entity_group": "PER",
            "score": 0.9991,
            "word": "Sarah Jessica Parker",
            "start": 11,
            "end": 31,
        },
        {
            "entity_group": "PER",
            "score": 0.998,
            "word": "Jessica",
            "start": 52,
            "end": 59,
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9991</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Sarah Jessica Parker&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">31</span>,
        },
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.998</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Jessica&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">52</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">59</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function _X(q){let n,c;return n=new P({props:{$$slots:{default:[qX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function vX(q){let n,c,s,d,$,k,A,j,T,S,D,le,Ne;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("Helsinki-NLP/opus-mt-ru-en"),A=i(`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=r("strong"),T=i("Recommended model"),S=i(": "),D=r("a"),le=i("t5-base"),Ne=i("."),this.h()},l(ee){n=o(ee,"P",{});var V=l(n);c=o(V,"STRONG",{});var ft=l(c);s=u(ft,"Recommended model"),ft.forEach(t),d=u(V,`:
`),$=o(V,"A",{href:!0,rel:!0});var qu=l($);k=u(qu,"Helsinki-NLP/opus-mt-ru-en"),qu.forEach(t),A=u(V,`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=o(V,"STRONG",{});var $n=l(j);T=u($n,"Recommended model"),$n.forEach(t),S=u(V,": "),D=o(V,"A",{href:!0,rel:!0});var xe=l(D);le=u(xe,"t5-base"),xe.forEach(t),Ne=u(V,"."),V.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/Helsinki-NLP/opus-mt-ru-en"),f($,"rel","nofollow"),f(D,"href","https://huggingface.co/t5-base"),f(D,"rel","nofollow")},m(ee,V){m(ee,n,V),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A),e(n,j),e(j,T),e(n,S),e(n,D),e(D,le),e(n,Ne)},d(ee){ee&&t(n)}}}function yX(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435",
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "translation_text": "My name is Wolfgang and I live in Berlin.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>,
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;translation_text&quot;</span>: <span class="hljs-string">&quot;My name is Wolfgang and I live in Berlin.&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function EX(q){let n,c;return n=new P({props:{$$slots:{default:[yX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function wX(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function bX(q){let n,c;return n=new P({props:{$$slots:{default:[wX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function TX(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d '{"inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function jX(q){let n,c;return n=new P({props:{$$slots:{default:[TX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function kX(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/bart-large-mnli"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/bart-large-mnli"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/bart-large-mnli"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function AX(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-mnli"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "parameters": {"candidate_labels": ["refund", "legal", "faq"]},
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;candidate_labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]},
    }
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function DX(q){let n,c;return n=new P({props:{$$slots:{default:[AX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function OX(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-mnli",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", parameters: {candidate_labels: ["refund", "legal", "faq"]}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>, <span class="hljs-attr">parameters</span>: {<span class="hljs-attr">candidate_labels</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function PX(q){let n,c;return n=new P({props:{$$slots:{default:[OX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function RX(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d '{"inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;, &quot;parameters&quot;: {&quot;candidate_labels&quot;: [&quot;refund&quot;, &quot;legal&quot;, &quot;faq&quot;]}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function SX(q){let n,c;return n=new P({props:{$$slots:{default:[RX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function NX(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data),
    {
        "sequence": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "labels": ["refund", "faq", "legal"],
        "scores": [
            # 88% refund
            0.8778,
            0.1052,
            0.017,
        ],
    },
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {
        <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>],
        <span class="hljs-string">&quot;scores&quot;</span>: [
            <span class="hljs-comment"># 88% refund</span>
            <span class="hljs-number">0.8778</span>,
            <span class="hljs-number">0.1052</span>,
            <span class="hljs-number">0.017</span>,
        ],
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function xX(q){let n,c;return n=new P({props:{$$slots:{default:[NX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function IX(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("microsoft/DialoGPT-large"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"microsoft/DialoGPT-large"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/microsoft/DialoGPT-large"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function HX(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "past_user_inputs": ["Which movie is the best ?"],
            "generated_responses": ["It's Die Hard for sure."],
            "text": "Can you explain why ?",
        },
    }
)
# Response
self.assertEqual(
    data,
    {
        "generated_text": "It's the best movie ever.",
        "conversation": {
            "past_user_inputs": [
                "Which movie is the best ?",
                "Can you explain why ?",
            ],
            "generated_responses": [
                "It's Die Hard for sure.",
                "It's the best movie ever.",
            ],
        },
        "warnings": ["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."],
    },
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [<span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>],
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
        },
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
        <span class="hljs-string">&quot;conversation&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [
                <span class="hljs-string">&quot;Which movie is the best ?&quot;</span>,
                <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
            ],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [
                <span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>,
                <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
            ],
        },
        <span class="hljs-string">&quot;warnings&quot;</span>: [<span class="hljs-string">&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;</span>],
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function BX(q){let n,c;return n=new P({props:{$$slots:{default:[HX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function CX(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: {past_user_inputs: ["Which movie is the best ?"], generated_responses: ["It is Die Hard for sure."], text:"Can you explain why ?"}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs: {past_user_inputs: [&quot;Which movie is the best ?&quot;], generated_responses: [&quot;It is Die Hard for sure.&quot;], <span class="hljs-type">text</span>:&quot;Can you explain why ?&quot;}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;]}`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function GX(q){let n,c;return n=new P({props:{$$slots:{default:[CX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function LX(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\
        -X POST \\
        -d '{"inputs": {"past_user_inputs": ["Which movie is the best ?"], "generated_responses": ["It is Die Hard for sure."], "text":"Can you explain why ?"}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:'curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\\n        -X POST \\\n        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: {&quot;past_user_inputs&quot;: [&quot;Which movie is the best ?&quot;], &quot;generated_responses&quot;: [&quot;It is Die Hard for sure.&quot;], &quot;text&quot;:&quot;Can you explain why ?&quot;}}&#x27;</span> \\\n        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">${HF_API_TOKEN}</span>&quot;</span>\n<span class="hljs-comment"># {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.&quot;]}</span>'}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function MX(q){let n,c;return n=new P({props:{$$slots:{default:[LX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function UX(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("Sentence-transformers"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"Sentence-transformers"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function zX(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(": "),$=r("a"),k=i(`Check your
langage`),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,": "),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,`Check your
langage`),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function KX(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("English"),d=i(`:
`),$=r("a"),k=i("facebook/wav2vec2-large-960h-lv60-self"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"English"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/wav2vec2-large-960h-lv60-self"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function FX(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("sample1.flac")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function JX(q){let n,c;return n=new P({props:{$$slots:{default:[FX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function WX(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("sample1.flac").then((response) => {
    console.log(JSON.stringify(response));
});
// {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;fs&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>) {
    <span class="hljs-keyword">const</span> data = fs.<span class="hljs-title function_">readFileSync</span>(filename);
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: data,
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>(<span class="hljs-string">&quot;sample1.flac&quot;</span>).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function YX(q){let n,c;return n=new P({props:{$$slots:{default:[WX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function VX(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
        -X POST \\
        --data-binary '@sample1.flac' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function XX(q){let n,c;return n=new P({props:{$$slots:{default:[VX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function QX(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    data,
    {
        "text": "GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;</span>
    },
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function ZX(q){let n,c;return n=new P({props:{$$slots:{default:[QX]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function eQ(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("superb/hubert-large-superb-er"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"superb/hubert-large-superb-er"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/superb/hubert-large-superb-er"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function tQ(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("sample1.flac")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function sQ(q){let n,c;return n=new P({props:{$$slots:{default:[tQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function aQ(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("sample1.flac").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;sample1.flac&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.5927661657333374</span>,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:<span class="hljs-number">0.2002529799938202</span>,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:<span class="hljs-number">0.12795612215995789</span>,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:<span class="hljs-number">0.07902472466230392</span>,&quot;label&quot;:&quot;sad&quot;}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function nQ(q){let n,c;return n=new P({props:{$$slots:{default:[aQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function rQ(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
        -X POST \\
        --data-binary '@sample1.flac' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.5927661657333374,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:0.2002529799938202,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:0.12795612215995789,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:0.07902472466230392,&quot;label&quot;:&quot;sad&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function oQ(q){let n,c;return n=new P({props:{$$slots:{default:[rQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function lQ(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {"score": 0.5928, "label": "neu"},
        {"score": 0.2003, "label": "hap"},
        {"score": 0.128, "label": "ang"},
        {"score": 0.079, "label": "sad"},
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.5928</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;neu&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.2003</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;hap&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.128</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;ang&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.079</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;sad&quot;</span>},
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function iQ(q){let n,c;return n=new P({props:{$$slots:{default:[lQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function uQ(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("google/vit-base-patch16-224"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"google/vit-base-patch16-224"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/google/vit-base-patch16-224"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function cQ(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/vit-base-patch16-224"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/vit-base-patch16-224&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function fQ(q){let n,c;return n=new P({props:{$$slots:{default:[cQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function pQ(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/vit-base-patch16-224",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.9374412894248962,"label":"Egyptian cat"},{"score":0.03844260051846504,"label":"tabby, tabby cat"},{"score":0.014411412179470062,"label":"tiger cat"},{"score":0.003274323185905814,"label":"lynx, catamount"},{"score":0.0006795919616706669,"label":"Siamese cat, Siamese"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/vit-base-patch16-224&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.9374412894248962</span>,&quot;label&quot;:&quot;Egyptian cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.03844260051846504</span>,&quot;label&quot;:&quot;tabby, tabby cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.014411412179470062</span>,&quot;label&quot;:&quot;tiger cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.003274323185905814</span>,&quot;label&quot;:&quot;lynx, catamount&quot;},{&quot;score&quot;:<span class="hljs-number">0.0006795919616706669</span>,&quot;label&quot;:&quot;Siamese cat, Siamese&quot;}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function hQ(q){let n,c;return n=new P({props:{$$slots:{default:[pQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function dQ(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/google/vit-base-patch16-224 \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.9374412894248962,"label":"Egyptian cat"},{"score":0.03844260051846504,"label":"tabby, tabby cat"},{"score":0.014411412179470062,"label":"tiger cat"},{"score":0.003274323185905814,"label":"lynx, catamount"},{"score":0.0006795919616706669,"label":"Siamese cat, Siamese"}]`,highlighted:`curl https://api-inference.huggingface.co/models/google/vit-base-patch16-224 \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.9374412894248962,&quot;label&quot;:&quot;Egyptian cat&quot;},{&quot;score&quot;:0.03844260051846504,&quot;label&quot;:&quot;tabby, tabby cat&quot;},{&quot;score&quot;:0.014411412179470062,&quot;label&quot;:&quot;tiger cat&quot;},{&quot;score&quot;:0.003274323185905814,&quot;label&quot;:&quot;lynx, catamount&quot;},{&quot;score&quot;:0.0006795919616706669,&quot;label&quot;:&quot;Siamese cat, Siamese&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function gQ(q){let n,c;return n=new P({props:{$$slots:{default:[dQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function mQ(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {"score": 0.9374, "label": "Egyptian cat"},
        {"score": 0.0384, "label": "tabby, tabby cat"},
        {"score": 0.0144, "label": "tiger cat"},
        {"score": 0.0033, "label": "lynx, catamount"},
        {"score": 0.0007, "label": "Siamese cat, Siamese"},
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9374</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;Egyptian cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0384</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;tabby, tabby cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0144</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;tiger cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0033</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;lynx, catamount&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0007</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;Siamese cat, Siamese&quot;</span>},
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function $Q(q){let n,c;return n=new P({props:{$$slots:{default:[mQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function qQ(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/detr-resnet-50"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/detr-resnet-50"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/detr-resnet-50"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function _Q(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function vQ(q){let n,c;return n=new P({props:{$$slots:{default:[_Q]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function yQ(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/detr-resnet-50",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.9982201457023621</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">40</span>,&quot;ymin&quot;:<span class="hljs-number">70</span>,&quot;xmax&quot;:<span class="hljs-number">175</span>,&quot;ymax&quot;:<span class="hljs-number">117</span>}},{&quot;score&quot;:<span class="hljs-number">0.9960021376609802</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">333</span>,&quot;ymin&quot;:<span class="hljs-number">72</span>,&quot;xmax&quot;:<span class="hljs-number">368</span>,&quot;ymax&quot;:<span class="hljs-number">187</span>}},{&quot;score&quot;:<span class="hljs-number">0.9954745173454285</span>,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">0</span>,&quot;ymin&quot;:<span class="hljs-number">1</span>,&quot;xmax&quot;:<span class="hljs-number">639</span>,&quot;ymax&quot;:<span class="hljs-number">473</span>}},{&quot;score&quot;:<span class="hljs-number">0.9988006353378296</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">13</span>,&quot;ymin&quot;:<span class="hljs-number">52</span>,&quot;xmax&quot;:<span class="hljs-number">314</span>,&quot;ymax&quot;:<span class="hljs-number">470</span>}},{&quot;score&quot;:<span class="hljs-number">0.9986783862113953</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">345</span>,&quot;ymin&quot;:<span class="hljs-number">23</span>,&quot;xmax&quot;:<span class="hljs-number">640</span>,&quot;ymax&quot;:<span class="hljs-number">368</span>}}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function EQ(q){let n,c;return n=new P({props:{$$slots:{default:[yQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function wQ(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.9982201457023621,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:40,&quot;ymin&quot;:70,&quot;xmax&quot;:175,&quot;ymax&quot;:117}},{&quot;score&quot;:0.9960021376609802,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:333,&quot;ymin&quot;:72,&quot;xmax&quot;:368,&quot;ymax&quot;:187}},{&quot;score&quot;:0.9954745173454285,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:0,&quot;ymin&quot;:1,&quot;xmax&quot;:639,&quot;ymax&quot;:473}},{&quot;score&quot;:0.9988006353378296,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:13,&quot;ymin&quot;:52,&quot;xmax&quot;:314,&quot;ymax&quot;:470}},{&quot;score&quot;:0.9986783862113953,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:345,&quot;ymin&quot;:23,&quot;xmax&quot;:640,&quot;ymax&quot;:368}}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function bQ(q){let n,c;return n=new P({props:{$$slots:{default:[wQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function TQ(q){let n,c;return n=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {
            "score": 0.9982,
            "label": "remote",
            "box": {"xmin": 40, "ymin": 70, "xmax": 175, "ymax": 117},
        },
        {
            "score": 0.9960,
            "label": "remote",
            "box": {"xmin": 333, "ymin": 72, "xmax": 368, "ymax": 187},
        },
        {
            "score": 0.9955,
            "label": "couch",
            "box": {"xmin": 0, "ymin": 1, "xmax": 639, "ymax": 473},
        },
        {
            "score": 0.9988,
            "label": "cat",
            "box": {"xmin": 13, "ymin": 52, "xmax": 314, "ymax": 470},
        },
        {
            "score": 0.9987,
            "label": "cat",
            "box": {"xmin": 345, "ymin": 23, "xmax": 640, "ymax": 368},
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9982</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">40</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">70</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">175</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">117</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9960</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">333</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">72</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">368</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">187</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9955</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;couch&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">639</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">473</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9988</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">52</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">314</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">470</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9987</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">345</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">23</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">640</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">368</span>},
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function jQ(q){let n,c;return n=new P({props:{$$slots:{default:[TQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function kQ(q){let n,c,s,d,$,k,A;return{c(){n=r("p"),c=r("strong"),s=i("Recommended model"),d=i(`:
`),$=r("a"),k=i("facebook/detr-resnet-50-panoptic"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/detr-resnet-50-panoptic"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/detr-resnet-50-panoptic"),f($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,c),e(c,s),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&t(n)}}}function AQ(q){let n,c;return n=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function DQ(q){let n,c;return n=new P({props:{$$slots:{default:[AQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function OQ(q){let n,c;return n=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score": 0.9094282388687134, "label": "blanket", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9940965175628662, "label": "cat", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9986692667007446, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994757771492004, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9722068309783936, "label": "couch", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994235038757324, "label": "cat", "mask": "BASE64ENCODED_MASK"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;: <span class="hljs-number">0.9094282388687134</span>, &quot;label&quot;: &quot;blanket&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9940965175628662</span>, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9986692667007446</span>, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9994757771492004</span>, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9722068309783936</span>, &quot;label&quot;: &quot;couch&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9994235038757324</span>, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}]`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function PQ(q){let n,c;return n=new P({props:{$$slots:{default:[OQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function RQ(q){let n,c;return n=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score": 0.9094282388687134, "label": "blanket", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9940965175628662, "label": "cat", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9986692667007446, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994757771492004, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9722068309783936, "label": "couch", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994235038757324, "label": "cat", "mask": "BASE64ENCODED_MASK"}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;: 0.9094282388687134, &quot;label&quot;: &quot;blanket&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9940965175628662, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9986692667007446, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9994757771492004, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9722068309783936, &quot;label&quot;: &quot;couch&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9994235038757324, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function SQ(q){let n,c;return n=new P({props:{$$slots:{default:[RQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function NQ(q){let n,c;return n=new R({props:{code:`import base64
from io import BytesIO
from PIL import Image
with Image.open("cats.jpg") as img:
    masks = [d["mask"] for d in data]
    self.assertEqual(img.size, (640, 480))
    mask_imgs = [Image.open(BytesIO(base64.b64decode(mask))) for mask in masks]
    for mask_img in mask_imgs:
        self.assertEqual(mask_img.size, img.size)
        self.assertEqual(mask_img.mode, "L")  # L (8-bit pixels, black and white)
    first_mask_img = mask_imgs[0]
    min_pxl_val, max_pxl_val = first_mask_img.getextrema()
    self.assertGreaterEqual(min_pxl_val, 0)
    self.assertLessEqual(max_pxl_val, 255)`,highlighted:`<span class="hljs-keyword">import</span> base64
<span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">with</span> Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;cats.jpg&quot;</span>) <span class="hljs-keyword">as</span> img:
    masks = [d[<span class="hljs-string">&quot;mask&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> data]
    self.assertEqual(img.size, (<span class="hljs-number">640</span>, <span class="hljs-number">480</span>))
    mask_imgs = [Image.<span class="hljs-built_in">open</span>(BytesIO(base64.b64decode(mask))) <span class="hljs-keyword">for</span> mask <span class="hljs-keyword">in</span> masks]
    <span class="hljs-keyword">for</span> mask_img <span class="hljs-keyword">in</span> mask_imgs:
        self.assertEqual(mask_img.size, img.size)
        self.assertEqual(mask_img.mode, <span class="hljs-string">&quot;L&quot;</span>)  <span class="hljs-comment"># L (8-bit pixels, black and white)</span>
    first_mask_img = mask_imgs[<span class="hljs-number">0</span>]
    min_pxl_val, max_pxl_val = first_mask_img.getextrema()
    self.assertGreaterEqual(min_pxl_val, <span class="hljs-number">0</span>)
    self.assertLessEqual(max_pxl_val, <span class="hljs-number">255</span>)`}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p:O,i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function xQ(q){let n,c;return n=new P({props:{$$slots:{default:[NQ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,d){y(n,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),n.$set($)},i(s){c||(E(n.$$.fragment,s),c=!0)},o(s){w(n.$$.fragment,s),c=!1},d(s){b(n,s)}}}function IQ(q){let n,c,s,d,$,k,A,j,T,S,D,le,Ne,ee,V,ft,qu,$n,xe,FT,p2,_u,JT,h2,pt,fC,d2,ht,pC,g2,Ie,dt,cg,qn,WT,fg,YT,m2,He,gt,pg,_n,VT,hg,XT,$2,vu,QT,q2,mt,_2,vn,ZT,yn,ej,v2,yu,tj,y2,$t,E2,Eu,sj,w2,qt,dg,En,wu,aj,nj,gg,rj,te,wn,bn,mg,oj,lj,ij,bu,uj,cj,Tn,Tu,$g,fj,pj,ju,hj,dj,jn,ku,gj,mj,_t,$j,qg,qj,_j,vj,kn,Au,yj,Ej,vt,wj,_g,bj,Tj,jj,An,Du,kj,Aj,yt,Dj,vg,Oj,Pj,b2,Ou,Rj,T2,Et,j2,wt,yg,Dn,Pu,Sj,Nj,Eg,xj,de,On,Ru,wg,Ij,Hj,Su,Bj,Cj,Pn,Nu,bg,Gj,Lj,xu,Mj,Uj,Rn,Iu,Tg,zj,Kj,Hu,Fj,Jj,Sn,Bu,jg,Wj,Yj,Cu,Vj,k2,Be,bt,kg,Nn,Xj,Ag,Qj,A2,Tt,Zj,Gu,e4,t4,D2,jt,O2,xn,s4,In,a4,P2,Lu,n4,R2,kt,S2,Mu,r4,N2,At,Dg,Hn,Uu,o4,l4,Og,i4,G,Bn,Cn,Pg,u4,c4,f4,zu,p4,h4,Gn,Ku,Rg,d4,g4,Fu,m4,$4,Ln,Ju,q4,_4,_e,v4,Sg,y4,E4,Ng,w4,b4,T4,Mn,Wu,j4,k4,ve,A4,xg,D4,O4,Ig,P4,R4,S4,Un,Yu,N4,x4,ye,I4,Hg,H4,B4,Bg,C4,G4,L4,zn,Vu,M4,U4,ie,z4,Cg,K4,F4,Gg,J4,W4,Lg,Y4,V4,X4,Kn,Xu,Q4,Z4,ue,e5,Mg,t5,s5,Ug,a5,n5,zg,r5,o5,l5,Fn,Qu,i5,u5,Dt,c5,Kg,f5,p5,h5,Jn,Zu,d5,g5,Ot,m5,Fg,$5,q5,_5,Wn,ec,Jg,v5,y5,tc,E5,w5,Yn,sc,b5,T5,Pt,j5,Wg,k5,A5,D5,Vn,ac,O5,P5,Rt,R5,Yg,S5,N5,x5,Xn,nc,I5,H5,St,B5,Vg,C5,G5,x2,rc,L5,I2,Nt,Xg,Qn,oc,M5,U5,Qg,z5,Zg,Zn,lc,em,K5,F5,ic,J5,H2,Ce,xt,tm,er,W5,sm,Y5,B2,uc,V5,C2,It,G2,Ge,X5,tr,Q5,Z5,sr,ek,L2,cc,tk,M2,Ht,U2,fc,sk,z2,pc,ak,K2,Bt,F2,Ct,am,ar,hc,nk,rk,nm,ok,ge,nr,dc,rm,lk,ik,gc,uk,ck,rr,mc,om,fk,pk,$c,hk,dk,or,qc,lm,gk,mk,Gt,$k,im,qk,_k,vk,lr,_c,um,yk,Ek,Lt,wk,cm,bk,Tk,J2,Le,Mt,fm,ir,jk,pm,kk,W2,vc,Ak,Y2,Ut,V2,ur,Dk,cr,Ok,X2,yc,Pk,Q2,zt,Z2,Ec,Rk,ev,Kt,hm,fr,wc,Sk,Nk,dm,xk,F,pr,hr,gm,Ik,Hk,Bk,mm,Ck,dr,bc,Gk,Lk,Tc,Mk,Uk,gr,jc,zk,Kk,kc,Fk,Jk,mr,Ac,$m,Wk,Yk,Dc,Vk,Xk,$r,Oc,Qk,Zk,Ft,e6,qm,t6,s6,a6,qr,Pc,n6,r6,Jt,o6,_m,l6,i6,u6,_r,Rc,c6,f6,Wt,p6,vm,h6,d6,tv,Sc,g6,sv,Yt,av,Vt,ym,vr,Nc,m6,$6,Em,q6,me,yr,xc,wm,_6,v6,Ic,y6,E6,Er,Hc,bm,w6,b6,Bc,T6,j6,wr,Cc,Tm,k6,A6,Gc,D6,O6,br,Lc,jm,P6,R6,Mc,S6,nv,Me,Xt,km,Tr,N6,Am,x6,rv,Uc,I6,ov,Qt,lv,jr,H6,kr,B6,iv,zc,C6,uv,Zt,cv,Kc,G6,fv,es,Dm,Ar,Fc,L6,M6,Om,U6,J,Dr,Or,Pm,z6,K6,F6,Rm,J6,Pr,Jc,W6,Y6,Wc,V6,X6,Rr,Yc,Q6,Z6,Vc,e7,t7,Sr,Xc,Sm,s7,a7,Qc,n7,r7,Nr,Zc,o7,l7,ts,i7,Nm,u7,c7,f7,xr,ef,p7,h7,ss,d7,xm,g7,m7,$7,Ir,tf,q7,_7,as,v7,Im,y7,E7,pv,sf,w7,hv,ns,dv,rs,Hm,Hr,af,b7,T7,Bm,j7,Cm,Br,nf,Gm,k7,A7,rf,D7,gv,Ue,os,Lm,Cr,O7,Mm,P7,mv,of,R7,$v,ls,qv,Gr,S7,Lr,N7,_v,lf,x7,vv,is,yv,uf,I7,Ev,us,Um,Mr,cf,H7,B7,zm,C7,se,Ur,zr,Km,G7,L7,M7,ff,U7,z7,Kr,pf,Fm,K7,F7,hf,J7,W7,Fr,df,Y7,V7,cs,X7,Jm,Q7,Z7,e9,Jr,gf,t9,s9,fs,a9,Wm,n9,r9,o9,Wr,mf,l9,i9,ps,u9,Ym,c9,f9,wv,$f,p9,bv,hs,Tv,ds,Vm,Yr,qf,h9,d9,Xm,g9,Vr,Xr,_f,Qm,m9,$9,vf,q9,_9,Qr,yf,Zm,v9,y9,Ef,E9,jv,ze,gs,e$,Zr,w9,t$,b9,kv,wf,T9,Av,ms,Dv,eo,j9,to,k9,Ov,bf,A9,Pv,$s,Rv,Tf,D9,Sv,qs,s$,so,jf,O9,P9,a$,R9,I,ao,no,n$,S9,N9,x9,kf,I9,H9,ro,Af,r$,B9,C9,Df,G9,L9,oo,Of,M9,U9,Ee,z9,o$,K9,F9,l$,J9,W9,Y9,lo,Pf,V9,X9,ce,Q9,i$,Z9,e8,u$,t8,s8,c$,a8,n8,r8,io,Rf,o8,l8,fe,i8,f$,u8,c8,p$,f8,p8,h$,h8,d8,g8,uo,Sf,m8,$8,_s,q8,d$,_8,v8,y8,co,Nf,E8,w8,we,b8,g$,T8,j8,m$,k8,A8,D8,fo,xf,O8,P8,be,R8,$$,S8,N8,q$,x8,I8,H8,po,If,B8,C8,Te,G8,_$,L8,M8,v$,U8,z8,K8,ho,Hf,F8,J8,vs,W8,y$,Y8,V8,X8,go,Bf,Q8,Z8,ys,eA,E$,tA,sA,aA,mo,Cf,w$,nA,rA,Gf,oA,lA,$o,Lf,iA,uA,Es,cA,b$,fA,pA,hA,qo,Mf,dA,gA,ws,mA,T$,$A,qA,_A,_o,Uf,vA,yA,bs,EA,j$,wA,bA,Nv,zf,TA,xv,Ts,Iv,js,k$,vo,Kf,jA,kA,A$,AA,D$,yo,Ff,O$,DA,OA,Jf,PA,Hv,Ke,ks,P$,Eo,RA,R$,SA,Bv,As,NA,Wf,xA,IA,Cv,Fe,Ds,S$,wo,HA,N$,BA,Gv,Yf,CA,Lv,Os,Mv,Je,GA,bo,LA,MA,To,UA,Uv,Vf,zA,zv,Ps,Kv,Xf,KA,Fv,Rs,x$,jo,Qf,FA,JA,I$,WA,W,ko,Ao,H$,YA,VA,XA,Zf,QA,ZA,Do,ep,B$,eD,tD,tp,sD,aD,Oo,sp,nD,rD,x,oD,C$,lD,iD,uD,cD,G$,fD,pD,hD,dD,L$,gD,mD,$D,qD,M$,_D,vD,U$,yD,ED,wD,bD,z$,TD,jD,K$,kD,AD,DD,OD,F$,PD,RD,J$,SD,ND,xD,Po,ap,W$,ID,HD,np,BD,CD,Ro,rp,GD,LD,Ss,MD,Y$,UD,zD,KD,So,op,FD,JD,Ns,WD,V$,YD,VD,XD,No,lp,QD,ZD,xs,eO,X$,tO,sO,Jv,ip,aO,Wv,Is,Yv,Hs,Q$,xo,up,nO,rO,Z$,oO,ae,Io,cp,eq,lO,iO,fp,uO,cO,Ho,pp,tq,fO,pO,hp,hO,dO,Bo,dp,sq,gO,mO,gp,$O,qO,Co,mp,aq,_O,vO,Bs,yO,nq,EO,wO,bO,Go,$p,rq,TO,jO,Cs,kO,oq,AO,DO,Vv,We,Gs,lq,Lo,OO,iq,PO,Xv,Mo,RO,qp,SO,Qv,Ye,Ls,uq,Uo,NO,cq,xO,Zv,_p,IO,ey,Ms,ty,zo,HO,Ko,BO,sy,vp,CO,ay,Us,ny,yp,GO,ry,zs,fq,Fo,Ep,LO,MO,pq,UO,ne,Jo,Wo,hq,zO,KO,FO,wp,JO,WO,Yo,bp,dq,YO,VO,Tp,XO,QO,Vo,jp,ZO,eP,Ks,tP,gq,sP,aP,nP,Xo,kp,rP,oP,Fs,lP,mq,iP,uP,cP,Qo,Ap,fP,pP,Js,hP,$q,dP,gP,oy,Dp,mP,ly,Ws,qq,Zo,Op,$P,qP,_q,_P,vq,el,Pp,yq,vP,yP,Rp,EP,iy,Ve,Ys,Eq,tl,wP,wq,bP,uy,Sp,TP,cy,Vs,fy,sl,jP,al,kP,py,Np,AP,hy,Xs,dy,xp,DP,gy,Qs,bq,nl,Ip,OP,PP,Tq,RP,z,rl,ol,jq,SP,NP,xP,Hp,IP,HP,ll,il,kq,BP,CP,GP,Bp,LP,MP,ul,Cp,UP,zP,je,KP,Aq,FP,JP,Dq,WP,YP,VP,cl,Gp,XP,QP,Zs,ZP,Oq,eR,tR,sR,fl,Lp,Pq,aR,nR,Mp,rR,oR,pl,Up,lR,iR,ea,uR,Rq,cR,fR,pR,hl,zp,hR,dR,ta,gR,Sq,mR,$R,qR,dl,Kp,_R,vR,sa,yR,Nq,ER,wR,my,Fp,bR,$y,Jp,TR,qy,aa,_y,na,xq,gl,Wp,jR,kR,Iq,AR,Xe,ml,Yp,Hq,DR,OR,Vp,PR,RR,$l,Xp,Bq,SR,NR,Qp,xR,IR,ql,Zp,Cq,HR,BR,ra,CR,Gq,GR,LR,vy,Qe,oa,Lq,_l,MR,Mq,UR,yy,eh,zR,Ey,la,wy,vl,KR,yl,FR,by,th,JR,Ty,ia,jy,sh,WR,ky,ua,Uq,El,ah,YR,VR,zq,XR,N,wl,bl,Kq,QR,ZR,eS,Fq,tS,Tl,nh,sS,aS,rh,nS,rS,jl,oh,oS,lS,lh,iS,uS,kl,ih,cS,fS,ca,pS,Jq,hS,dS,gS,Al,uh,Wq,mS,$S,ch,qS,_S,Dl,fh,vS,yS,ke,ES,Yq,wS,bS,Vq,TS,jS,kS,Ol,ph,AS,DS,Ae,OS,Xq,PS,RS,Qq,SS,NS,xS,Pl,hh,IS,HS,De,BS,Zq,CS,GS,e_,LS,MS,US,Rl,dh,zS,KS,pe,FS,t_,JS,WS,s_,YS,VS,a_,XS,QS,ZS,Sl,gh,eN,tN,he,sN,n_,aN,nN,r_,rN,oN,o_,lN,iN,uN,Nl,mh,cN,fN,fa,pN,l_,hN,dN,gN,xl,$h,mN,$N,pa,qN,i_,_N,vN,yN,Il,qh,u_,EN,wN,_h,bN,TN,Hl,vh,jN,kN,ha,AN,c_,DN,ON,PN,Bl,yh,RN,SN,da,NN,f_,xN,IN,HN,Cl,Eh,BN,CN,ga,GN,p_,LN,MN,Ay,wh,UN,Dy,ma,h_,Gl,bh,zN,KN,d_,FN,$e,Ll,Th,g_,JN,WN,jh,YN,VN,Ml,kh,m_,XN,QN,Ah,ZN,ex,Ul,Dh,tx,sx,Oh,ax,nx,zl,Ph,rx,ox,Rh,lx,Oy,Ze,$a,$_,Kl,ix,q_,ux,Py,Sh,cx,Ry,qa,Sy,et,fx,Fl,px,hx,Jl,dx,Ny,Nh,gx,xy,_a,__,Wl,xh,mx,$x,v_,qx,re,Yl,Vl,y_,_x,vx,yx,Ih,Ex,wx,Xl,Hh,E_,bx,Tx,Bh,jx,kx,Ql,Ch,Ax,Dx,va,Ox,w_,Px,Rx,Sx,Zl,Gh,Nx,xx,ya,Ix,b_,Hx,Bx,Cx,ei,Lh,Gx,Lx,Ea,Mx,T_,Ux,zx,Iy,Mh,Kx,Hy,wa,j_,ti,Uh,Fx,Jx,k_,Wx,A_,si,zh,D_,Yx,Vx,Kh,Xx,By,Fh,Qx,Cy,tt,ba,O_,ai,Zx,P_,eI,Gy,st,Ta,R_,ni,tI,S_,sI,Ly,Jh,aI,My,ja,Uy,ka,zy,qe,nI,ri,rI,oI,oi,lI,iI,li,uI,Ky,Wh,cI,Fy,Aa,Jy,Yh,fI,Wy,Da,N_,ii,Vh,pI,hI,x_,dI,I_,ui,ci,H_,gI,mI,$I,Xh,qI,Yy,Qh,_I,Vy,Zh,vI,Xy,Oa,Qy,Pa,B_,fi,ed,yI,EI,C_,wI,G_,pi,td,L_,bI,TI,sd,jI,Zy,at,Ra,M_,hi,kI,U_,AI,eE,ad,DI,tE,Sa,sE,nt,OI,di,PI,RI,gi,SI,aE,nd,NI,nE,Na,rE,rd,xI,oE,xa,z_,mi,od,II,HI,K_,BI,F_,$i,qi,J_,CI,GI,LI,ld,MI,lE,id,UI,iE,Ia,uE,Ha,W_,_i,ud,zI,KI,Y_,FI,vi,yi,cd,V_,JI,WI,fd,YI,VI,Ei,pd,X_,XI,QI,hd,ZI,cE,rt,Ba,Q_,wi,eH,Z_,tH,fE,ot,Ca,e1,bi,sH,t1,aH,pE,dd,nH,hE,Ga,dE,Ti,rH,ji,oH,gE,gd,lH,mE,La,$E,Ma,iH,ki,uH,cH,qE,Ua,s1,Ai,md,fH,pH,a1,hH,n1,Di,Oi,r1,dH,gH,mH,$d,$H,_E,qd,qH,vE,za,yE,Ka,o1,Pi,_d,_H,vH,l1,yH,Ri,Si,vd,i1,EH,wH,yd,bH,TH,Ni,Ed,u1,jH,kH,wd,AH,EE,lt,Fa,c1,xi,DH,f1,OH,wE,bd,PH,bE,Ja,TE,Ii,RH,Hi,SH,jE,Td,NH,kE,Wa,AE,Ya,xH,Bi,IH,HH,DE,Va,p1,Ci,jd,BH,CH,h1,GH,d1,Gi,Li,g1,LH,MH,UH,kd,zH,OE,Ad,KH,PE,Xa,RE,Qa,m1,Mi,Dd,FH,JH,$1,WH,it,Ui,Od,q1,YH,VH,Pd,XH,QH,zi,Rd,_1,ZH,eB,Sd,tB,sB,Ki,Nd,v1,aB,nB,xd,rB,SE,ut,Za,y1,Fi,oB,E1,lB,NE,Id,iB,xE,en,IE,Ji,uB,Wi,cB,HE,Hd,fB,BE,tn,CE,sn,pB,Yi,hB,dB,GE,an,w1,Vi,Bd,gB,mB,b1,$B,T1,Xi,Qi,j1,qB,_B,vB,Cd,yB,LE,Gd,EB,ME,nn,UE,rn,k1,Zi,Ld,wB,bB,A1,TB,ct,eu,Md,D1,jB,kB,Ud,AB,DB,tu,zd,O1,OB,PB,Kd,RB,SB,su,Fd,P1,NB,xB,Jd,IB,zE;return k=new U({}),ee=new U({}),qn=new U({}),_n=new U({}),mt=new K({props:{$$slots:{default:[nV]},$$scope:{ctx:q}}}),$t=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[cV],js:[iV],python:[oV]},$$scope:{ctx:q}}}),Et=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[pV]},$$scope:{ctx:q}}}),Nn=new U({}),jt=new K({props:{$$slots:{default:[hV]},$$scope:{ctx:q}}}),kt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[_V],js:[$V],python:[gV]},$$scope:{ctx:q}}}),er=new U({}),It=new K({props:{$$slots:{default:[vV]},$$scope:{ctx:q}}}),Ht=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[jV],js:[bV],python:[EV]},$$scope:{ctx:q}}}),Bt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[AV]},$$scope:{ctx:q}}}),ir=new U({}),Ut=new K({props:{$$slots:{default:[DV]},$$scope:{ctx:q}}}),zt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[xV],js:[SV],python:[PV]},$$scope:{ctx:q}}}),Yt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[HV]},$$scope:{ctx:q}}}),Tr=new U({}),Qt=new K({props:{$$slots:{default:[BV]},$$scope:{ctx:q}}}),Zt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[zV],js:[MV],python:[GV]},$$scope:{ctx:q}}}),ns=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[FV]},$$scope:{ctx:q}}}),Cr=new U({}),ls=new K({props:{$$slots:{default:[JV]},$$scope:{ctx:q}}}),is=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[ZV],js:[XV],python:[YV]},$$scope:{ctx:q}}}),hs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[tX]},$$scope:{ctx:q}}}),Zr=new U({}),ms=new K({props:{$$slots:{default:[sX]},$$scope:{ctx:q}}}),$s=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[iX],js:[oX],python:[nX]},$$scope:{ctx:q}}}),Ts=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[cX]},$$scope:{ctx:q}}}),Eo=new U({}),wo=new U({}),Os=new K({props:{$$slots:{default:[fX]},$$scope:{ctx:q}}}),Ps=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[$X],js:[gX],python:[hX]},$$scope:{ctx:q}}}),Is=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[_X]},$$scope:{ctx:q}}}),Lo=new U({}),Uo=new U({}),Ms=new K({props:{$$slots:{default:[vX]},$$scope:{ctx:q}}}),Us=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[jX],js:[bX],python:[EX]},$$scope:{ctx:q}}}),tl=new U({}),Vs=new K({props:{$$slots:{default:[kX]},$$scope:{ctx:q}}}),Xs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[SX],js:[PX],python:[DX]},$$scope:{ctx:q}}}),aa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[xX]},$$scope:{ctx:q}}}),_l=new U({}),la=new K({props:{$$slots:{default:[IX]},$$scope:{ctx:q}}}),ia=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[MX],js:[GX],python:[BX]},$$scope:{ctx:q}}}),Kl=new U({}),qa=new K({props:{$$slots:{default:[UX]},$$scope:{ctx:q}}}),ai=new U({}),ni=new U({}),ja=new K({props:{$$slots:{default:[zX]},$$scope:{ctx:q}}}),ka=new K({props:{$$slots:{default:[KX]},$$scope:{ctx:q}}}),Aa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[XX],js:[YX],python:[JX]},$$scope:{ctx:q}}}),Oa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[ZX]},$$scope:{ctx:q}}}),hi=new U({}),Sa=new K({props:{$$slots:{default:[eQ]},$$scope:{ctx:q}}}),Na=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[oQ],js:[nQ],python:[sQ]},$$scope:{ctx:q}}}),Ia=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[iQ]},$$scope:{ctx:q}}}),wi=new U({}),bi=new U({}),Ga=new K({props:{$$slots:{default:[uQ]},$$scope:{ctx:q}}}),La=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[gQ],js:[hQ],python:[fQ]},$$scope:{ctx:q}}}),za=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[$Q]},$$scope:{ctx:q}}}),xi=new U({}),Ja=new K({props:{$$slots:{default:[qQ]},$$scope:{ctx:q}}}),Wa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[bQ],js:[EQ],python:[vQ]},$$scope:{ctx:q}}}),Xa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[jQ]},$$scope:{ctx:q}}}),Fi=new U({}),en=new K({props:{$$slots:{default:[kQ]},$$scope:{ctx:q}}}),tn=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[SQ],js:[PQ],python:[DQ]},$$scope:{ctx:q}}}),nn=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[xQ]},$$scope:{ctx:q}}}),{c(){n=r("meta"),c=p(),s=r("h1"),d=r("a"),$=r("span"),_(k.$$.fragment),A=p(),j=r("span"),T=i("Detailed parameters"),S=p(),D=r("h2"),le=r("a"),Ne=r("span"),_(ee.$$.fragment),V=p(),ft=r("span"),qu=i("Which task is used by this model ?"),$n=p(),xe=r("p"),FT=i(`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),p2=p(),_u=r("p"),JT=i("The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),h2=p(),pt=r("img"),d2=p(),ht=r("img"),g2=p(),Ie=r("h2"),dt=r("a"),cg=r("span"),_(qn.$$.fragment),WT=p(),fg=r("span"),YT=i("Natural Language Processing"),m2=p(),He=r("h3"),gt=r("a"),pg=r("span"),_(_n.$$.fragment),VT=p(),hg=r("span"),XT=i("Fill Mask task"),$2=p(),vu=r("p"),QT=i(`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),q2=p(),_(mt.$$.fragment),_2=p(),vn=r("p"),ZT=i("Available with: "),yn=r("a"),ej=i("\u{1F917} Transformers"),v2=p(),yu=r("p"),tj=i("Example:"),y2=p(),_($t.$$.fragment),E2=p(),Eu=r("p"),sj=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),w2=p(),qt=r("table"),dg=r("thead"),En=r("tr"),wu=r("th"),aj=i("All parameters"),nj=p(),gg=r("th"),rj=p(),te=r("tbody"),wn=r("tr"),bn=r("td"),mg=r("strong"),oj=i("inputs"),lj=i(" (required):"),ij=p(),bu=r("td"),uj=i("a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),cj=p(),Tn=r("tr"),Tu=r("td"),$g=r("strong"),fj=i("options"),pj=p(),ju=r("td"),hj=i("a dict containing the following keys:"),dj=p(),jn=r("tr"),ku=r("td"),gj=i("use_gpu"),mj=p(),_t=r("td"),$j=i("(Default: "),qg=r("code"),qj=i("false"),_j=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),vj=p(),kn=r("tr"),Au=r("td"),yj=i("use_cache"),Ej=p(),vt=r("td"),wj=i("(Default: "),_g=r("code"),bj=i("true"),Tj=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),jj=p(),An=r("tr"),Du=r("td"),kj=i("wait_for_model"),Aj=p(),yt=r("td"),Dj=i("(Default: "),vg=r("code"),Oj=i("false"),Pj=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),b2=p(),Ou=r("p"),Rj=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),T2=p(),_(Et.$$.fragment),j2=p(),wt=r("table"),yg=r("thead"),Dn=r("tr"),Pu=r("th"),Sj=i("Returned values"),Nj=p(),Eg=r("th"),xj=p(),de=r("tbody"),On=r("tr"),Ru=r("td"),wg=r("strong"),Ij=i("sequence"),Hj=p(),Su=r("td"),Bj=i("The actual sequence of tokens that ran against the model (may contain special tokens)"),Cj=p(),Pn=r("tr"),Nu=r("td"),bg=r("strong"),Gj=i("score"),Lj=p(),xu=r("td"),Mj=i("The probability for this token."),Uj=p(),Rn=r("tr"),Iu=r("td"),Tg=r("strong"),zj=i("token"),Kj=p(),Hu=r("td"),Fj=i("The id of the token"),Jj=p(),Sn=r("tr"),Bu=r("td"),jg=r("strong"),Wj=i("token_str"),Yj=p(),Cu=r("td"),Vj=i("The string representation of the token"),k2=p(),Be=r("h3"),bt=r("a"),kg=r("span"),_(Nn.$$.fragment),Xj=p(),Ag=r("span"),Qj=i("Summarization task"),A2=p(),Tt=r("p"),Zj=i(`This task is well known to summarize longer text into shorter text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss your summarization needs,
please get in touch with us: <`),Gu=r("a"),e4=i("api-enterprise@huggingface.co"),t4=i(">"),D2=p(),_(jt.$$.fragment),O2=p(),xn=r("p"),s4=i("Available with: "),In=r("a"),a4=i("\u{1F917} Transformers"),P2=p(),Lu=r("p"),n4=i("Example:"),R2=p(),_(kt.$$.fragment),S2=p(),Mu=r("p"),r4=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),N2=p(),At=r("table"),Dg=r("thead"),Hn=r("tr"),Uu=r("th"),o4=i("All parameters"),l4=p(),Og=r("th"),i4=p(),G=r("tbody"),Bn=r("tr"),Cn=r("td"),Pg=r("strong"),u4=i("inputs"),c4=i(" (required)"),f4=p(),zu=r("td"),p4=i("a string to be summarized"),h4=p(),Gn=r("tr"),Ku=r("td"),Rg=r("strong"),d4=i("parameters"),g4=p(),Fu=r("td"),m4=i("a dict containing the following keys:"),$4=p(),Ln=r("tr"),Ju=r("td"),q4=i("min_length"),_4=p(),_e=r("td"),v4=i("(Default: "),Sg=r("code"),y4=i("None"),E4=i("). Integer to define the minimum length "),Ng=r("strong"),w4=i("in tokens"),b4=i(" of the output summary."),T4=p(),Mn=r("tr"),Wu=r("td"),j4=i("max_length"),k4=p(),ve=r("td"),A4=i("(Default: "),xg=r("code"),D4=i("None"),O4=i("). Integer to define the maximum length "),Ig=r("strong"),P4=i("in tokens"),R4=i(" of the output summary."),S4=p(),Un=r("tr"),Yu=r("td"),N4=i("top_k"),x4=p(),ye=r("td"),I4=i("(Default: "),Hg=r("code"),H4=i("None"),B4=i("). Integer to define the top tokens considered within the "),Bg=r("code"),C4=i("sample"),G4=i(" operation to create new text."),L4=p(),zn=r("tr"),Vu=r("td"),M4=i("top_p"),U4=p(),ie=r("td"),z4=i("(Default: "),Cg=r("code"),K4=i("None"),F4=i("). Float to define the tokens that are within the "),Gg=r("code"),J4=i("sample"),W4=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Lg=r("code"),Y4=i("top_p"),V4=i("."),X4=p(),Kn=r("tr"),Xu=r("td"),Q4=i("temperature"),Z4=p(),ue=r("td"),e5=i("(Default: "),Mg=r("code"),t5=i("1.0"),s5=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),Ug=r("code"),a5=i("0"),n5=i(" means always take the highest score, "),zg=r("code"),r5=i("100.0"),o5=i(" is getting closer to uniform probability."),l5=p(),Fn=r("tr"),Qu=r("td"),i5=i("repetition_penalty"),u5=p(),Dt=r("td"),c5=i("(Default: "),Kg=r("code"),f5=i("None"),p5=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),h5=p(),Jn=r("tr"),Zu=r("td"),d5=i("max_time"),g5=p(),Ot=r("td"),m5=i("(Default: "),Fg=r("code"),$5=i("None"),q5=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),_5=p(),Wn=r("tr"),ec=r("td"),Jg=r("strong"),v5=i("options"),y5=p(),tc=r("td"),E5=i("a dict containing the following keys:"),w5=p(),Yn=r("tr"),sc=r("td"),b5=i("use_gpu"),T5=p(),Pt=r("td"),j5=i("(Default: "),Wg=r("code"),k5=i("false"),A5=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),D5=p(),Vn=r("tr"),ac=r("td"),O5=i("use_cache"),P5=p(),Rt=r("td"),R5=i("(Default: "),Yg=r("code"),S5=i("true"),N5=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),x5=p(),Xn=r("tr"),nc=r("td"),I5=i("wait_for_model"),H5=p(),St=r("td"),B5=i("(Default: "),Vg=r("code"),C5=i("false"),G5=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),x2=p(),rc=r("p"),L5=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),I2=p(),Nt=r("table"),Xg=r("thead"),Qn=r("tr"),oc=r("th"),M5=i("Returned values"),U5=p(),Qg=r("th"),z5=p(),Zg=r("tbody"),Zn=r("tr"),lc=r("td"),em=r("strong"),K5=i("summarization_text"),F5=p(),ic=r("td"),J5=i("The string after translation"),H2=p(),Ce=r("h3"),xt=r("a"),tm=r("span"),_(er.$$.fragment),W5=p(),sm=r("span"),Y5=i("Question Answering task"),B2=p(),uc=r("p"),V5=i("Want to have a nice know-it-all bot that can answer any question?"),C2=p(),_(It.$$.fragment),G2=p(),Ge=r("p"),X5=i("Available with: "),tr=r("a"),Q5=i("\u{1F917}Transformers"),Z5=i(` and
`),sr=r("a"),ek=i("AllenNLP"),L2=p(),cc=r("p"),tk=i("Example:"),M2=p(),_(Ht.$$.fragment),U2=p(),fc=r("p"),sk=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),z2=p(),pc=r("p"),ak=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),K2=p(),_(Bt.$$.fragment),F2=p(),Ct=r("table"),am=r("thead"),ar=r("tr"),hc=r("th"),nk=i("Returned values"),rk=p(),nm=r("th"),ok=p(),ge=r("tbody"),nr=r("tr"),dc=r("td"),rm=r("strong"),lk=i("answer"),ik=p(),gc=r("td"),uk=i("A string that\u2019s the answer within the text."),ck=p(),rr=r("tr"),mc=r("td"),om=r("strong"),fk=i("score"),pk=p(),$c=r("td"),hk=i("A float that represents how likely that the answer is correct"),dk=p(),or=r("tr"),qc=r("td"),lm=r("strong"),gk=i("start"),mk=p(),Gt=r("td"),$k=i("The index (string wise) of the start of the answer within "),im=r("code"),qk=i("context"),_k=i("."),vk=p(),lr=r("tr"),_c=r("td"),um=r("strong"),yk=i("stop"),Ek=p(),Lt=r("td"),wk=i("The index (string wise) of the stop of the answer within "),cm=r("code"),bk=i("context"),Tk=i("."),J2=p(),Le=r("h3"),Mt=r("a"),fm=r("span"),_(ir.$$.fragment),jk=p(),pm=r("span"),kk=i("Table Question Answering task"),W2=p(),vc=r("p"),Ak=i(`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),Y2=p(),_(Ut.$$.fragment),V2=p(),ur=r("p"),Dk=i("Available with: "),cr=r("a"),Ok=i("\u{1F917} Transformers"),X2=p(),yc=r("p"),Pk=i("Example:"),Q2=p(),_(zt.$$.fragment),Z2=p(),Ec=r("p"),Rk=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),ev=p(),Kt=r("table"),hm=r("thead"),fr=r("tr"),wc=r("th"),Sk=i("All parameters"),Nk=p(),dm=r("th"),xk=p(),F=r("tbody"),pr=r("tr"),hr=r("td"),gm=r("strong"),Ik=i("inputs"),Hk=i(" (required)"),Bk=p(),mm=r("td"),Ck=p(),dr=r("tr"),bc=r("td"),Gk=i("query (required)"),Lk=p(),Tc=r("td"),Mk=i("The query in plain text that you want to ask the table"),Uk=p(),gr=r("tr"),jc=r("td"),zk=i("table (required)"),Kk=p(),kc=r("td"),Fk=i("A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),Jk=p(),mr=r("tr"),Ac=r("td"),$m=r("strong"),Wk=i("options"),Yk=p(),Dc=r("td"),Vk=i("a dict containing the following keys:"),Xk=p(),$r=r("tr"),Oc=r("td"),Qk=i("use_gpu"),Zk=p(),Ft=r("td"),e6=i("(Default: "),qm=r("code"),t6=i("false"),s6=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),a6=p(),qr=r("tr"),Pc=r("td"),n6=i("use_cache"),r6=p(),Jt=r("td"),o6=i("(Default: "),_m=r("code"),l6=i("true"),i6=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),u6=p(),_r=r("tr"),Rc=r("td"),c6=i("wait_for_model"),f6=p(),Wt=r("td"),p6=i("(Default: "),vm=r("code"),h6=i("false"),d6=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),tv=p(),Sc=r("p"),g6=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),sv=p(),_(Yt.$$.fragment),av=p(),Vt=r("table"),ym=r("thead"),vr=r("tr"),Nc=r("th"),m6=i("Returned values"),$6=p(),Em=r("th"),q6=p(),me=r("tbody"),yr=r("tr"),xc=r("td"),wm=r("strong"),_6=i("answer"),v6=p(),Ic=r("td"),y6=i("The plaintext answer"),E6=p(),Er=r("tr"),Hc=r("td"),bm=r("strong"),w6=i("coordinates"),b6=p(),Bc=r("td"),T6=i("a list of coordinates of the cells referenced in the answer"),j6=p(),wr=r("tr"),Cc=r("td"),Tm=r("strong"),k6=i("cells"),A6=p(),Gc=r("td"),D6=i("a list of coordinates of the cells contents"),O6=p(),br=r("tr"),Lc=r("td"),jm=r("strong"),P6=i("aggregator"),R6=p(),Mc=r("td"),S6=i("The aggregator used to get the answer"),nv=p(),Me=r("h3"),Xt=r("a"),km=r("span"),_(Tr.$$.fragment),N6=p(),Am=r("span"),x6=i("Sentence Similarity task"),rv=p(),Uc=r("p"),I6=i("Calculate the semantic similarity between one text and a list of other sentences by comparing their embeddings."),ov=p(),_(Qt.$$.fragment),lv=p(),jr=r("p"),H6=i("Available with: "),kr=r("a"),B6=i("Sentence Transformers"),iv=p(),zc=r("p"),C6=i("Example:"),uv=p(),_(Zt.$$.fragment),cv=p(),Kc=r("p"),G6=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),fv=p(),es=r("table"),Dm=r("thead"),Ar=r("tr"),Fc=r("th"),L6=i("All parameters"),M6=p(),Om=r("th"),U6=p(),J=r("tbody"),Dr=r("tr"),Or=r("td"),Pm=r("strong"),z6=i("inputs"),K6=i(" (required)"),F6=p(),Rm=r("td"),J6=p(),Pr=r("tr"),Jc=r("td"),W6=i("source_sentence (required)"),Y6=p(),Wc=r("td"),V6=i("The string that you wish to compare the other strings with. This can be a phrase, sentence, or longer passage, depending on the model being used."),X6=p(),Rr=r("tr"),Yc=r("td"),Q6=i("sentences (required)"),Z6=p(),Vc=r("td"),e7=i("A list of strings which will be compared against the source_sentence."),t7=p(),Sr=r("tr"),Xc=r("td"),Sm=r("strong"),s7=i("options"),a7=p(),Qc=r("td"),n7=i("a dict containing the following keys:"),r7=p(),Nr=r("tr"),Zc=r("td"),o7=i("use_gpu"),l7=p(),ts=r("td"),i7=i("(Default: "),Nm=r("code"),u7=i("false"),c7=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),f7=p(),xr=r("tr"),ef=r("td"),p7=i("use_cache"),h7=p(),ss=r("td"),d7=i("(Default: "),xm=r("code"),g7=i("true"),m7=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),$7=p(),Ir=r("tr"),tf=r("td"),q7=i("wait_for_model"),_7=p(),as=r("td"),v7=i("(Default: "),Im=r("code"),y7=i("false"),E7=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),pv=p(),sf=r("p"),w7=i("The return value is a list of similarity scores, given as floats."),hv=p(),_(ns.$$.fragment),dv=p(),rs=r("table"),Hm=r("thead"),Hr=r("tr"),af=r("th"),b7=i("Returned values"),T7=p(),Bm=r("th"),j7=p(),Cm=r("tbody"),Br=r("tr"),nf=r("td"),Gm=r("strong"),k7=i("Scores"),A7=p(),rf=r("td"),D7=i("The associated similarity score for each of the given strings"),gv=p(),Ue=r("h3"),os=r("a"),Lm=r("span"),_(Cr.$$.fragment),O7=p(),Mm=r("span"),P7=i("Text Classification task"),mv=p(),of=r("p"),R7=i(`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),$v=p(),_(ls.$$.fragment),qv=p(),Gr=r("p"),S7=i("Available with: "),Lr=r("a"),N7=i("\u{1F917} Transformers"),_v=p(),lf=r("p"),x7=i("Example:"),vv=p(),_(is.$$.fragment),yv=p(),uf=r("p"),I7=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Ev=p(),us=r("table"),Um=r("thead"),Mr=r("tr"),cf=r("th"),H7=i("All parameters"),B7=p(),zm=r("th"),C7=p(),se=r("tbody"),Ur=r("tr"),zr=r("td"),Km=r("strong"),G7=i("inputs"),L7=i(" (required)"),M7=p(),ff=r("td"),U7=i("a string to be classified"),z7=p(),Kr=r("tr"),pf=r("td"),Fm=r("strong"),K7=i("options"),F7=p(),hf=r("td"),J7=i("a dict containing the following keys:"),W7=p(),Fr=r("tr"),df=r("td"),Y7=i("use_gpu"),V7=p(),cs=r("td"),X7=i("(Default: "),Jm=r("code"),Q7=i("false"),Z7=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),e9=p(),Jr=r("tr"),gf=r("td"),t9=i("use_cache"),s9=p(),fs=r("td"),a9=i("(Default: "),Wm=r("code"),n9=i("true"),r9=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),o9=p(),Wr=r("tr"),mf=r("td"),l9=i("wait_for_model"),i9=p(),ps=r("td"),u9=i("(Default: "),Ym=r("code"),c9=i("false"),f9=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),wv=p(),$f=r("p"),p9=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),bv=p(),_(hs.$$.fragment),Tv=p(),ds=r("table"),Vm=r("thead"),Yr=r("tr"),qf=r("th"),h9=i("Returned values"),d9=p(),Xm=r("th"),g9=p(),Vr=r("tbody"),Xr=r("tr"),_f=r("td"),Qm=r("strong"),m9=i("label"),$9=p(),vf=r("td"),q9=i("The label for the class (model specific)"),_9=p(),Qr=r("tr"),yf=r("td"),Zm=r("strong"),v9=i("score"),y9=p(),Ef=r("td"),E9=i("A floats that represents how likely is that the text belongs the this class."),jv=p(),ze=r("h3"),gs=r("a"),e$=r("span"),_(Zr.$$.fragment),w9=p(),t$=r("span"),b9=i("Text Generation task"),kv=p(),wf=r("p"),T9=i("Use to continue text from a prompt. This is a very generic task."),Av=p(),_(ms.$$.fragment),Dv=p(),eo=r("p"),j9=i("Available with: "),to=r("a"),k9=i("\u{1F917} Transformers"),Ov=p(),bf=r("p"),A9=i("Example:"),Pv=p(),_($s.$$.fragment),Rv=p(),Tf=r("p"),D9=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Sv=p(),qs=r("table"),s$=r("thead"),so=r("tr"),jf=r("th"),O9=i("All parameters"),P9=p(),a$=r("th"),R9=p(),I=r("tbody"),ao=r("tr"),no=r("td"),n$=r("strong"),S9=i("inputs"),N9=i(" (required):"),x9=p(),kf=r("td"),I9=i("a string to be generated from"),H9=p(),ro=r("tr"),Af=r("td"),r$=r("strong"),B9=i("parameters"),C9=p(),Df=r("td"),G9=i("dict containing the following keys:"),L9=p(),oo=r("tr"),Of=r("td"),M9=i("top_k"),U9=p(),Ee=r("td"),z9=i("(Default: "),o$=r("code"),K9=i("None"),F9=i("). Integer to define the top tokens considered within the "),l$=r("code"),J9=i("sample"),W9=i(" operation to create new text."),Y9=p(),lo=r("tr"),Pf=r("td"),V9=i("top_p"),X9=p(),ce=r("td"),Q9=i("(Default: "),i$=r("code"),Z9=i("None"),e8=i("). Float to define the tokens that are within the "),u$=r("code"),t8=i("sample"),s8=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),c$=r("code"),a8=i("top_p"),n8=i("."),r8=p(),io=r("tr"),Rf=r("td"),o8=i("temperature"),l8=p(),fe=r("td"),i8=i("(Default: "),f$=r("code"),u8=i("1.0"),c8=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),p$=r("code"),f8=i("0"),p8=i(" means always take the highest score, "),h$=r("code"),h8=i("100.0"),d8=i(" is getting closer to uniform probability."),g8=p(),uo=r("tr"),Sf=r("td"),m8=i("repetition_penalty"),$8=p(),_s=r("td"),q8=i("(Default: "),d$=r("code"),_8=i("None"),v8=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),y8=p(),co=r("tr"),Nf=r("td"),E8=i("max_new_tokens"),w8=p(),we=r("td"),b8=i("(Default: "),g$=r("code"),T8=i("None"),j8=i("). Int (0-250). The amount of new tokens to be generated, this does "),m$=r("strong"),k8=i("not"),A8=i(" include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),D8=p(),fo=r("tr"),xf=r("td"),O8=i("max_time"),P8=p(),be=r("td"),R8=i("(Default: "),$$=r("code"),S8=i("None"),N8=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),q$=r("code"),x8=i("max_new_tokens"),I8=i(" for best results."),H8=p(),po=r("tr"),If=r("td"),B8=i("return_full_text"),C8=p(),Te=r("td"),G8=i("(Default: "),_$=r("code"),L8=i("True"),M8=i("). Bool. If set to False, the return results will "),v$=r("strong"),U8=i("not"),z8=i(" contain the original query making it easier for prompting."),K8=p(),ho=r("tr"),Hf=r("td"),F8=i("num_return_sequences"),J8=p(),vs=r("td"),W8=i("(Default: "),y$=r("code"),Y8=i("1"),V8=i("). Integer. The number of proposition you want to be returned."),X8=p(),go=r("tr"),Bf=r("td"),Q8=i("do_sample"),Z8=p(),ys=r("td"),eA=i("(Optional: "),E$=r("code"),tA=i("True"),sA=i("). Bool. Whether or not to use sampling, use greedy decoding otherwise."),aA=p(),mo=r("tr"),Cf=r("td"),w$=r("strong"),nA=i("options"),rA=p(),Gf=r("td"),oA=i("a dict containing the following keys:"),lA=p(),$o=r("tr"),Lf=r("td"),iA=i("use_gpu"),uA=p(),Es=r("td"),cA=i("(Default: "),b$=r("code"),fA=i("false"),pA=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),hA=p(),qo=r("tr"),Mf=r("td"),dA=i("use_cache"),gA=p(),ws=r("td"),mA=i("(Default: "),T$=r("code"),$A=i("true"),qA=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),_A=p(),_o=r("tr"),Uf=r("td"),vA=i("wait_for_model"),yA=p(),bs=r("td"),EA=i("(Default: "),j$=r("code"),wA=i("false"),bA=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Nv=p(),zf=r("p"),TA=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),xv=p(),_(Ts.$$.fragment),Iv=p(),js=r("table"),k$=r("thead"),vo=r("tr"),Kf=r("th"),jA=i("Returned values"),kA=p(),A$=r("th"),AA=p(),D$=r("tbody"),yo=r("tr"),Ff=r("td"),O$=r("strong"),DA=i("generated_text"),OA=p(),Jf=r("td"),PA=i("The continuated string"),Hv=p(),Ke=r("h3"),ks=r("a"),P$=r("span"),_(Eo.$$.fragment),RA=p(),R$=r("span"),SA=i("Text2Text Generation task"),Bv=p(),As=r("p"),NA=i("Essentially "),Wf=r("a"),xA=i("Text-generation task"),IA=i(`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),Cv=p(),Fe=r("h3"),Ds=r("a"),S$=r("span"),_(wo.$$.fragment),HA=p(),N$=r("span"),BA=i("Token Classification task"),Gv=p(),Yf=r("p"),CA=i(`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),Lv=p(),_(Os.$$.fragment),Mv=p(),Je=r("p"),GA=i("Available with: "),bo=r("a"),LA=i("\u{1F917} Transformers"),MA=i(`,
`),To=r("a"),UA=i("Flair"),Uv=p(),Vf=r("p"),zA=i("Example:"),zv=p(),_(Ps.$$.fragment),Kv=p(),Xf=r("p"),KA=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Fv=p(),Rs=r("table"),x$=r("thead"),jo=r("tr"),Qf=r("th"),FA=i("All parameters"),JA=p(),I$=r("th"),WA=p(),W=r("tbody"),ko=r("tr"),Ao=r("td"),H$=r("strong"),YA=i("inputs"),VA=i(" (required)"),XA=p(),Zf=r("td"),QA=i("a string to be classified"),ZA=p(),Do=r("tr"),ep=r("td"),B$=r("strong"),eD=i("parameters"),tD=p(),tp=r("td"),sD=i("a dict containing the following key:"),aD=p(),Oo=r("tr"),sp=r("td"),nD=i("aggregation_strategy"),rD=p(),x=r("td"),oD=i("(Default: "),C$=r("code"),lD=i("simple"),iD=i("). There are several aggregation strategies: "),uD=r("br"),cD=p(),G$=r("code"),fD=i("none"),pD=i(": Every token gets classified without further aggregation. "),hD=r("br"),dD=p(),L$=r("code"),gD=i("simple"),mD=i(": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),$D=r("br"),qD=p(),M$=r("code"),_D=i("first"),vD=i(": Same as the "),U$=r("code"),yD=i("simple"),ED=i(" strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),wD=r("br"),bD=p(),z$=r("code"),TD=i("average"),jD=i(": Same as the "),K$=r("code"),kD=i("simple"),AD=i(" strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),DD=r("br"),OD=p(),F$=r("code"),PD=i("max"),RD=i(": Same as the "),J$=r("code"),SD=i("simple"),ND=i(" strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),xD=p(),Po=r("tr"),ap=r("td"),W$=r("strong"),ID=i("options"),HD=p(),np=r("td"),BD=i("a dict containing the following keys:"),CD=p(),Ro=r("tr"),rp=r("td"),GD=i("use_gpu"),LD=p(),Ss=r("td"),MD=i("(Default: "),Y$=r("code"),UD=i("false"),zD=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),KD=p(),So=r("tr"),op=r("td"),FD=i("use_cache"),JD=p(),Ns=r("td"),WD=i("(Default: "),V$=r("code"),YD=i("true"),VD=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),XD=p(),No=r("tr"),lp=r("td"),QD=i("wait_for_model"),ZD=p(),xs=r("td"),eO=i("(Default: "),X$=r("code"),tO=i("false"),sO=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Jv=p(),ip=r("p"),aO=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Wv=p(),_(Is.$$.fragment),Yv=p(),Hs=r("table"),Q$=r("thead"),xo=r("tr"),up=r("th"),nO=i("Returned values"),rO=p(),Z$=r("th"),oO=p(),ae=r("tbody"),Io=r("tr"),cp=r("td"),eq=r("strong"),lO=i("entity_group"),iO=p(),fp=r("td"),uO=i("The type for the entity being recognized (model specific)."),cO=p(),Ho=r("tr"),pp=r("td"),tq=r("strong"),fO=i("score"),pO=p(),hp=r("td"),hO=i("How likely the entity was recognized."),dO=p(),Bo=r("tr"),dp=r("td"),sq=r("strong"),gO=i("word"),mO=p(),gp=r("td"),$O=i("The string that was captured"),qO=p(),Co=r("tr"),mp=r("td"),aq=r("strong"),_O=i("start"),vO=p(),Bs=r("td"),yO=i("The offset stringwise where the answer is located. Useful to disambiguate if "),nq=r("code"),EO=i("word"),wO=i(" occurs multiple times."),bO=p(),Go=r("tr"),$p=r("td"),rq=r("strong"),TO=i("end"),jO=p(),Cs=r("td"),kO=i("The offset stringwise where the answer is located. Useful to disambiguate if "),oq=r("code"),AO=i("word"),DO=i(" occurs multiple times."),Vv=p(),We=r("h3"),Gs=r("a"),lq=r("span"),_(Lo.$$.fragment),OO=p(),iq=r("span"),PO=i("Named Entity Recognition (NER) task"),Xv=p(),Mo=r("p"),RO=i("See "),qp=r("a"),SO=i("Token-classification task"),Qv=p(),Ye=r("h3"),Ls=r("a"),uq=r("span"),_(Uo.$$.fragment),NO=p(),cq=r("span"),xO=i("Translation task"),Zv=p(),_p=r("p"),IO=i("This task is well known to translate text from one language to another"),ey=p(),_(Ms.$$.fragment),ty=p(),zo=r("p"),HO=i("Available with: "),Ko=r("a"),BO=i("\u{1F917} Transformers"),sy=p(),vp=r("p"),CO=i("Example:"),ay=p(),_(Us.$$.fragment),ny=p(),yp=r("p"),GO=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),ry=p(),zs=r("table"),fq=r("thead"),Fo=r("tr"),Ep=r("th"),LO=i("All parameters"),MO=p(),pq=r("th"),UO=p(),ne=r("tbody"),Jo=r("tr"),Wo=r("td"),hq=r("strong"),zO=i("inputs"),KO=i(" (required)"),FO=p(),wp=r("td"),JO=i("a string to be translated in the original languages"),WO=p(),Yo=r("tr"),bp=r("td"),dq=r("strong"),YO=i("options"),VO=p(),Tp=r("td"),XO=i("a dict containing the following keys:"),QO=p(),Vo=r("tr"),jp=r("td"),ZO=i("use_gpu"),eP=p(),Ks=r("td"),tP=i("(Default: "),gq=r("code"),sP=i("false"),aP=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),nP=p(),Xo=r("tr"),kp=r("td"),rP=i("use_cache"),oP=p(),Fs=r("td"),lP=i("(Default: "),mq=r("code"),iP=i("true"),uP=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),cP=p(),Qo=r("tr"),Ap=r("td"),fP=i("wait_for_model"),pP=p(),Js=r("td"),hP=i("(Default: "),$q=r("code"),dP=i("false"),gP=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),oy=p(),Dp=r("p"),mP=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),ly=p(),Ws=r("table"),qq=r("thead"),Zo=r("tr"),Op=r("th"),$P=i("Returned values"),qP=p(),_q=r("th"),_P=p(),vq=r("tbody"),el=r("tr"),Pp=r("td"),yq=r("strong"),vP=i("translation_text"),yP=p(),Rp=r("td"),EP=i("The string after translation"),iy=p(),Ve=r("h3"),Ys=r("a"),Eq=r("span"),_(tl.$$.fragment),wP=p(),wq=r("span"),bP=i("Zero-Shot Classification task"),uy=p(),Sp=r("p"),TP=i(`This task is super useful to try out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence, and you get a result.`),cy=p(),_(Vs.$$.fragment),fy=p(),sl=r("p"),jP=i("Available with: "),al=r("a"),kP=i("\u{1F917} Transformers"),py=p(),Np=r("p"),AP=i("Request:"),hy=p(),_(Xs.$$.fragment),dy=p(),xp=r("p"),DP=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),gy=p(),Qs=r("table"),bq=r("thead"),nl=r("tr"),Ip=r("th"),OP=i("All parameters"),PP=p(),Tq=r("th"),RP=p(),z=r("tbody"),rl=r("tr"),ol=r("td"),jq=r("strong"),SP=i("inputs"),NP=i(" (required)"),xP=p(),Hp=r("td"),IP=i("a string or list of strings"),HP=p(),ll=r("tr"),il=r("td"),kq=r("strong"),BP=i("parameters"),CP=i(" (required)"),GP=p(),Bp=r("td"),LP=i("a dict containing the following keys:"),MP=p(),ul=r("tr"),Cp=r("td"),UP=i("candidate_labels (required)"),zP=p(),je=r("td"),KP=i("a list of strings that are potential classes for "),Aq=r("code"),FP=i("inputs"),JP=i(". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),Dq=r("code"),WP=i("multi_label=True"),YP=i(" and do the scaling on your end. )"),VP=p(),cl=r("tr"),Gp=r("td"),XP=i("multi_label"),QP=p(),Zs=r("td"),ZP=i("(Default: "),Oq=r("code"),eR=i("false"),tR=i(") Boolean that is set to True if classes can overlap"),sR=p(),fl=r("tr"),Lp=r("td"),Pq=r("strong"),aR=i("options"),nR=p(),Mp=r("td"),rR=i("a dict containing the following keys:"),oR=p(),pl=r("tr"),Up=r("td"),lR=i("use_gpu"),iR=p(),ea=r("td"),uR=i("(Default: "),Rq=r("code"),cR=i("false"),fR=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),pR=p(),hl=r("tr"),zp=r("td"),hR=i("use_cache"),dR=p(),ta=r("td"),gR=i("(Default: "),Sq=r("code"),mR=i("true"),$R=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),qR=p(),dl=r("tr"),Kp=r("td"),_R=i("wait_for_model"),vR=p(),sa=r("td"),yR=i("(Default: "),Nq=r("code"),ER=i("false"),wR=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),my=p(),Fp=r("p"),bR=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),$y=p(),Jp=r("p"),TR=i("Response:"),qy=p(),_(aa.$$.fragment),_y=p(),na=r("table"),xq=r("thead"),gl=r("tr"),Wp=r("th"),jR=i("Returned values"),kR=p(),Iq=r("th"),AR=p(),Xe=r("tbody"),ml=r("tr"),Yp=r("td"),Hq=r("strong"),DR=i("sequence"),OR=p(),Vp=r("td"),PR=i("The string sent as an input"),RR=p(),$l=r("tr"),Xp=r("td"),Bq=r("strong"),SR=i("labels"),NR=p(),Qp=r("td"),xR=i("The list of strings for labels that you sent (in order)"),IR=p(),ql=r("tr"),Zp=r("td"),Cq=r("strong"),HR=i("scores"),BR=p(),ra=r("td"),CR=i("a list of floats that correspond the the probability of label, in the same order as "),Gq=r("code"),GR=i("labels"),LR=i("."),vy=p(),Qe=r("h3"),oa=r("a"),Lq=r("span"),_(_l.$$.fragment),MR=p(),Mq=r("span"),UR=i("Conversational task"),yy=p(),eh=r("p"),zR=i(`This task corresponds to any chatbot like structure. Models tend to have
shorter max_length, so please check with caution when using a given
model if you need long range dependency or not.`),Ey=p(),_(la.$$.fragment),wy=p(),vl=r("p"),KR=i("Available with: "),yl=r("a"),FR=i("\u{1F917} Transformers"),by=p(),th=r("p"),JR=i("Example:"),Ty=p(),_(ia.$$.fragment),jy=p(),sh=r("p"),WR=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),ky=p(),ua=r("table"),Uq=r("thead"),El=r("tr"),ah=r("th"),YR=i("All parameters"),VR=p(),zq=r("th"),XR=p(),N=r("tbody"),wl=r("tr"),bl=r("td"),Kq=r("strong"),QR=i("inputs"),ZR=i(" (required)"),eS=p(),Fq=r("td"),tS=p(),Tl=r("tr"),nh=r("td"),sS=i("text (required)"),aS=p(),rh=r("td"),nS=i("The last input from the user in the conversation."),rS=p(),jl=r("tr"),oh=r("td"),oS=i("generated_responses"),lS=p(),lh=r("td"),iS=i("A list of strings corresponding to the earlier replies from the model."),uS=p(),kl=r("tr"),ih=r("td"),cS=i("past_user_inputs"),fS=p(),ca=r("td"),pS=i("A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),Jq=r("code"),hS=i("generated_responses"),dS=i("."),gS=p(),Al=r("tr"),uh=r("td"),Wq=r("strong"),mS=i("parameters"),$S=p(),ch=r("td"),qS=i("a dict containing the following keys:"),_S=p(),Dl=r("tr"),fh=r("td"),vS=i("min_length"),yS=p(),ke=r("td"),ES=i("(Default: "),Yq=r("code"),wS=i("None"),bS=i("). Integer to define the minimum length "),Vq=r("strong"),TS=i("in tokens"),jS=i(" of the output summary."),kS=p(),Ol=r("tr"),ph=r("td"),AS=i("max_length"),DS=p(),Ae=r("td"),OS=i("(Default: "),Xq=r("code"),PS=i("None"),RS=i("). Integer to define the maximum length "),Qq=r("strong"),SS=i("in tokens"),NS=i(" of the output summary."),xS=p(),Pl=r("tr"),hh=r("td"),IS=i("top_k"),HS=p(),De=r("td"),BS=i("(Default: "),Zq=r("code"),CS=i("None"),GS=i("). Integer to define the top tokens considered within the "),e_=r("code"),LS=i("sample"),MS=i(" operation to create new text."),US=p(),Rl=r("tr"),dh=r("td"),zS=i("top_p"),KS=p(),pe=r("td"),FS=i("(Default: "),t_=r("code"),JS=i("None"),WS=i("). Float to define the tokens that are within the "),s_=r("code"),YS=i("sample"),VS=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),a_=r("code"),XS=i("top_p"),QS=i("."),ZS=p(),Sl=r("tr"),gh=r("td"),eN=i("temperature"),tN=p(),he=r("td"),sN=i("(Default: "),n_=r("code"),aN=i("1.0"),nN=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),r_=r("code"),rN=i("0"),oN=i(" means always take the highest score, "),o_=r("code"),lN=i("100.0"),iN=i(" is getting closer to uniform probability."),uN=p(),Nl=r("tr"),mh=r("td"),cN=i("repetition_penalty"),fN=p(),fa=r("td"),pN=i("(Default: "),l_=r("code"),hN=i("None"),dN=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),gN=p(),xl=r("tr"),$h=r("td"),mN=i("max_time"),$N=p(),pa=r("td"),qN=i("(Default: "),i_=r("code"),_N=i("None"),vN=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),yN=p(),Il=r("tr"),qh=r("td"),u_=r("strong"),EN=i("options"),wN=p(),_h=r("td"),bN=i("a dict containing the following keys:"),TN=p(),Hl=r("tr"),vh=r("td"),jN=i("use_gpu"),kN=p(),ha=r("td"),AN=i("(Default: "),c_=r("code"),DN=i("false"),ON=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),PN=p(),Bl=r("tr"),yh=r("td"),RN=i("use_cache"),SN=p(),da=r("td"),NN=i("(Default: "),f_=r("code"),xN=i("true"),IN=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),HN=p(),Cl=r("tr"),Eh=r("td"),BN=i("wait_for_model"),CN=p(),ga=r("td"),GN=i("(Default: "),p_=r("code"),LN=i("false"),MN=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Ay=p(),wh=r("p"),UN=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Dy=p(),ma=r("table"),h_=r("thead"),Gl=r("tr"),bh=r("th"),zN=i("Returned values"),KN=p(),d_=r("th"),FN=p(),$e=r("tbody"),Ll=r("tr"),Th=r("td"),g_=r("strong"),JN=i("generated_text"),WN=p(),jh=r("td"),YN=i("The answer of the bot"),VN=p(),Ml=r("tr"),kh=r("td"),m_=r("strong"),XN=i("conversation"),QN=p(),Ah=r("td"),ZN=i("A facility dictionnary to send back for the next input (with the new user input addition)."),ex=p(),Ul=r("tr"),Dh=r("td"),tx=i("past_user_inputs"),sx=p(),Oh=r("td"),ax=i("List of strings. The last inputs from the user in the conversation, <em>after the model has run."),nx=p(),zl=r("tr"),Ph=r("td"),rx=i("generated_responses"),ox=p(),Rh=r("td"),lx=i("List of strings. The last outputs from the model in the conversation, <em>after the model has run."),Oy=p(),Ze=r("h3"),$a=r("a"),$_=r("span"),_(Kl.$$.fragment),ix=p(),q_=r("span"),ux=i("Feature Extraction task"),Py=p(),Sh=r("p"),cx=i(`This task reads some text and outputs raw float values, that are usually
consumed as part of a semantic database/semantic search.`),Ry=p(),_(qa.$$.fragment),Sy=p(),et=r("p"),fx=i("Available with: "),Fl=r("a"),px=i("\u{1F917} Transformers"),hx=p(),Jl=r("a"),dx=i("Sentence-transformers"),Ny=p(),Nh=r("p"),gx=i("Request:"),xy=p(),_a=r("table"),__=r("thead"),Wl=r("tr"),xh=r("th"),mx=i("All parameters"),$x=p(),v_=r("th"),qx=p(),re=r("tbody"),Yl=r("tr"),Vl=r("td"),y_=r("strong"),_x=i("inputs"),vx=i(" (required):"),yx=p(),Ih=r("td"),Ex=i("a string or a list of strings to get the features from."),wx=p(),Xl=r("tr"),Hh=r("td"),E_=r("strong"),bx=i("options"),Tx=p(),Bh=r("td"),jx=i("a dict containing the following keys:"),kx=p(),Ql=r("tr"),Ch=r("td"),Ax=i("use_gpu"),Dx=p(),va=r("td"),Ox=i("(Default: "),w_=r("code"),Px=i("false"),Rx=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Sx=p(),Zl=r("tr"),Gh=r("td"),Nx=i("use_cache"),xx=p(),ya=r("td"),Ix=i("(Default: "),b_=r("code"),Hx=i("true"),Bx=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Cx=p(),ei=r("tr"),Lh=r("td"),Gx=i("wait_for_model"),Lx=p(),Ea=r("td"),Mx=i("(Default: "),T_=r("code"),Ux=i("false"),zx=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Iy=p(),Mh=r("p"),Kx=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Hy=p(),wa=r("table"),j_=r("thead"),ti=r("tr"),Uh=r("th"),Fx=i("Returned values"),Jx=p(),k_=r("th"),Wx=p(),A_=r("tbody"),si=r("tr"),zh=r("td"),D_=r("strong"),Yx=i("A list of float (or list of list of floats)"),Vx=p(),Kh=r("td"),Xx=i("The numbers that are the representation features of the input."),By=p(),Fh=r("small"),Qx=i(`Returned values are a list of floats, or a list of list of floats (depending
  on if you sent a string or a list of string, and if the automatic reduction,
  usually mean_pooling for instance was applied for you or not. This should be
  explained on the model's README.`),Cy=p(),tt=r("h2"),ba=r("a"),O_=r("span"),_(ai.$$.fragment),Zx=p(),P_=r("span"),eI=i("Audio"),Gy=p(),st=r("h3"),Ta=r("a"),R_=r("span"),_(ni.$$.fragment),tI=p(),S_=r("span"),sI=i("Automatic Speech Recognition task"),Ly=p(),Jh=r("p"),aI=i(`This task reads some audio input and outputs the said words within the
audio files.`),My=p(),_(ja.$$.fragment),Uy=p(),_(ka.$$.fragment),zy=p(),qe=r("p"),nI=i("Available with: "),ri=r("a"),rI=i("\u{1F917} Transformers"),oI=p(),oi=r("a"),lI=i("ESPnet"),iI=i(` and
`),li=r("a"),uI=i("SpeechBrain"),Ky=p(),Wh=r("p"),cI=i("Request:"),Fy=p(),_(Aa.$$.fragment),Jy=p(),Yh=r("p"),fI=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),Wy=p(),Da=r("table"),N_=r("thead"),ii=r("tr"),Vh=r("th"),pI=i("All parameters"),hI=p(),x_=r("th"),dI=p(),I_=r("tbody"),ui=r("tr"),ci=r("td"),H_=r("strong"),gI=i("no parameter"),mI=i(" (required)"),$I=p(),Xh=r("td"),qI=i("a binary representation of the audio file. No other parameters are currently allowed."),Yy=p(),Qh=r("p"),_I=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Vy=p(),Zh=r("p"),vI=i("Response:"),Xy=p(),_(Oa.$$.fragment),Qy=p(),Pa=r("table"),B_=r("thead"),fi=r("tr"),ed=r("th"),yI=i("Returned values"),EI=p(),C_=r("th"),wI=p(),G_=r("tbody"),pi=r("tr"),td=r("td"),L_=r("strong"),bI=i("text"),TI=p(),sd=r("td"),jI=i("The string that was recognized within the audio file."),Zy=p(),at=r("h3"),Ra=r("a"),M_=r("span"),_(hi.$$.fragment),kI=p(),U_=r("span"),AI=i("Audio Classification task"),eE=p(),ad=r("p"),DI=i("This task reads some audio input and outputs the likelihood of classes."),tE=p(),_(Sa.$$.fragment),sE=p(),nt=r("p"),OI=i("Available with: "),di=r("a"),PI=i("\u{1F917} Transformers"),RI=p(),gi=r("a"),SI=i("SpeechBrain"),aE=p(),nd=r("p"),NI=i("Request:"),nE=p(),_(Na.$$.fragment),rE=p(),rd=r("p"),xI=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),oE=p(),xa=r("table"),z_=r("thead"),mi=r("tr"),od=r("th"),II=i("All parameters"),HI=p(),K_=r("th"),BI=p(),F_=r("tbody"),$i=r("tr"),qi=r("td"),J_=r("strong"),CI=i("no parameter"),GI=i(" (required)"),LI=p(),ld=r("td"),MI=i("a binary representation of the audio file. No other parameters are currently allowed."),lE=p(),id=r("p"),UI=i("Return value is a dict"),iE=p(),_(Ia.$$.fragment),uE=p(),Ha=r("table"),W_=r("thead"),_i=r("tr"),ud=r("th"),zI=i("Returned values"),KI=p(),Y_=r("th"),FI=p(),vi=r("tbody"),yi=r("tr"),cd=r("td"),V_=r("strong"),JI=i("label"),WI=p(),fd=r("td"),YI=i("The label for the class (model specific)"),VI=p(),Ei=r("tr"),pd=r("td"),X_=r("strong"),XI=i("score"),QI=p(),hd=r("td"),ZI=i("A float that represents how likely it is that the audio file belongs to this class."),cE=p(),rt=r("h2"),Ba=r("a"),Q_=r("span"),_(wi.$$.fragment),eH=p(),Z_=r("span"),tH=i("Computer Vision"),fE=p(),ot=r("h3"),Ca=r("a"),e1=r("span"),_(bi.$$.fragment),sH=p(),t1=r("span"),aH=i("Image Classification task"),pE=p(),dd=r("p"),nH=i("This task reads some image input and outputs the likelihood of classes."),hE=p(),_(Ga.$$.fragment),dE=p(),Ti=r("p"),rH=i("Available with: "),ji=r("a"),oH=i("\u{1F917} Transformers"),gE=p(),gd=r("p"),lH=i("Request:"),mE=p(),_(La.$$.fragment),$E=p(),Ma=r("p"),iH=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),ki=r("a"),uH=i(`Pillow
supports`),cH=i("."),qE=p(),Ua=r("table"),s1=r("thead"),Ai=r("tr"),md=r("th"),fH=i("All parameters"),pH=p(),a1=r("th"),hH=p(),n1=r("tbody"),Di=r("tr"),Oi=r("td"),r1=r("strong"),dH=i("no parameter"),gH=i(" (required)"),mH=p(),$d=r("td"),$H=i("a binary representation of the image file. No other parameters are currently allowed."),_E=p(),qd=r("p"),qH=i("Return value is a dict"),vE=p(),_(za.$$.fragment),yE=p(),Ka=r("table"),o1=r("thead"),Pi=r("tr"),_d=r("th"),_H=i("Returned values"),vH=p(),l1=r("th"),yH=p(),Ri=r("tbody"),Si=r("tr"),vd=r("td"),i1=r("strong"),EH=i("label"),wH=p(),yd=r("td"),bH=i("The label for the class (model specific)"),TH=p(),Ni=r("tr"),Ed=r("td"),u1=r("strong"),jH=i("score"),kH=p(),wd=r("td"),AH=i("A float that represents how likely it is that the image file belongs to this class."),EE=p(),lt=r("h3"),Fa=r("a"),c1=r("span"),_(xi.$$.fragment),DH=p(),f1=r("span"),OH=i("Object Detection task"),wE=p(),bd=r("p"),PH=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),bE=p(),_(Ja.$$.fragment),TE=p(),Ii=r("p"),RH=i("Available with: "),Hi=r("a"),SH=i("\u{1F917} Transformers"),jE=p(),Td=r("p"),NH=i("Request:"),kE=p(),_(Wa.$$.fragment),AE=p(),Ya=r("p"),xH=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Bi=r("a"),IH=i(`Pillow
supports`),HH=i("."),DE=p(),Va=r("table"),p1=r("thead"),Ci=r("tr"),jd=r("th"),BH=i("All parameters"),CH=p(),h1=r("th"),GH=p(),d1=r("tbody"),Gi=r("tr"),Li=r("td"),g1=r("strong"),LH=i("no parameter"),MH=i(" (required)"),UH=p(),kd=r("td"),zH=i("a binary representation of the image file. No other parameters are currently allowed."),OE=p(),Ad=r("p"),KH=i("Return value is a dict"),PE=p(),_(Xa.$$.fragment),RE=p(),Qa=r("table"),m1=r("thead"),Mi=r("tr"),Dd=r("th"),FH=i("Returned values"),JH=p(),$1=r("th"),WH=p(),it=r("tbody"),Ui=r("tr"),Od=r("td"),q1=r("strong"),YH=i("label"),VH=p(),Pd=r("td"),XH=i("The label for the class (model specific) of a detected object."),QH=p(),zi=r("tr"),Rd=r("td"),_1=r("strong"),ZH=i("score"),eB=p(),Sd=r("td"),tB=i("A float that represents how likely it is that the detected object belongs to the given class."),sB=p(),Ki=r("tr"),Nd=r("td"),v1=r("strong"),aB=i("box"),nB=p(),xd=r("td"),rB=i("A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),SE=p(),ut=r("h3"),Za=r("a"),y1=r("span"),_(Fi.$$.fragment),oB=p(),E1=r("span"),lB=i("Image Segmentation task"),NE=p(),Id=r("p"),iB=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),xE=p(),_(en.$$.fragment),IE=p(),Ji=r("p"),uB=i("Available with: "),Wi=r("a"),cB=i("\u{1F917} Transformers"),HE=p(),Hd=r("p"),fB=i("Request:"),BE=p(),_(tn.$$.fragment),CE=p(),sn=r("p"),pB=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Yi=r("a"),hB=i(`Pillow
supports`),dB=i("."),GE=p(),an=r("table"),w1=r("thead"),Vi=r("tr"),Bd=r("th"),gB=i("All parameters"),mB=p(),b1=r("th"),$B=p(),T1=r("tbody"),Xi=r("tr"),Qi=r("td"),j1=r("strong"),qB=i("no parameter"),_B=i(" (required)"),vB=p(),Cd=r("td"),yB=i("a binary representation of the image file. No other parameters are currently allowed."),LE=p(),Gd=r("p"),EB=i("Return value is a dict"),ME=p(),_(nn.$$.fragment),UE=p(),rn=r("table"),k1=r("thead"),Zi=r("tr"),Ld=r("th"),wB=i("Returned values"),bB=p(),A1=r("th"),TB=p(),ct=r("tbody"),eu=r("tr"),Md=r("td"),D1=r("strong"),jB=i("label"),kB=p(),Ud=r("td"),AB=i("The label for the class (model specific) of a segment."),DB=p(),tu=r("tr"),zd=r("td"),O1=r("strong"),OB=i("score"),PB=p(),Kd=r("td"),RB=i("A float that represents how likely it is that the segment belongs to the given class."),SB=p(),su=r("tr"),Fd=r("td"),P1=r("strong"),NB=i("mask"),xB=p(),Jd=r("td"),IB=i("A str (base64 str of a single channel black-and-white img) representing the mask of a segment."),this.h()},l(a){const g=sV('[data-svelte="svelte-1phssyn"]',document.head);n=o(g,"META",{name:!0,content:!0}),g.forEach(t),c=h(a),s=o(a,"H1",{class:!0});var au=l(s);d=o(au,"A",{id:!0,class:!0,href:!0});var R1=l(d);$=o(R1,"SPAN",{});var S1=l($);v(k.$$.fragment,S1),S1.forEach(t),R1.forEach(t),A=h(au),j=o(au,"SPAN",{});var N1=l(j);T=u(N1,"Detailed parameters"),N1.forEach(t),au.forEach(t),S=h(a),D=o(a,"H2",{class:!0});var nu=l(D);le=o(nu,"A",{id:!0,class:!0,href:!0});var x1=l(le);Ne=o(x1,"SPAN",{});var I1=l(Ne);v(ee.$$.fragment,I1),I1.forEach(t),x1.forEach(t),V=h(nu),ft=o(nu,"SPAN",{});var H1=l(ft);qu=u(H1,"Which task is used by this model ?"),H1.forEach(t),nu.forEach(t),$n=h(a),xe=o(a,"P",{});var B1=l(xe);FT=u(B1,`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),B1.forEach(t),p2=h(a),_u=o(a,"P",{});var C1=l(_u);JT=u(C1,"The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),C1.forEach(t),h2=h(a),pt=o(a,"IMG",{class:!0,src:!0,width:!0}),d2=h(a),ht=o(a,"IMG",{class:!0,src:!0,width:!0}),g2=h(a),Ie=o(a,"H2",{class:!0});var ru=l(Ie);dt=o(ru,"A",{id:!0,class:!0,href:!0});var G1=l(dt);cg=o(G1,"SPAN",{});var L1=l(cg);v(qn.$$.fragment,L1),L1.forEach(t),G1.forEach(t),WT=h(ru),fg=o(ru,"SPAN",{});var M1=l(fg);YT=u(M1,"Natural Language Processing"),M1.forEach(t),ru.forEach(t),m2=h(a),He=o(a,"H3",{class:!0});var ou=l(He);gt=o(ou,"A",{id:!0,class:!0,href:!0});var U1=l(gt);pg=o(U1,"SPAN",{});var z1=l(pg);v(_n.$$.fragment,z1),z1.forEach(t),U1.forEach(t),VT=h(ou),hg=o(ou,"SPAN",{});var K1=l(hg);XT=u(K1,"Fill Mask task"),K1.forEach(t),ou.forEach(t),$2=h(a),vu=o(a,"P",{});var F1=l(vu);QT=u(F1,`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),F1.forEach(t),q2=h(a),v(mt.$$.fragment,a),_2=h(a),vn=o(a,"P",{});var Wd=l(vn);ZT=u(Wd,"Available with: "),yn=o(Wd,"A",{href:!0,rel:!0});var J1=l(yn);ej=u(J1,"\u{1F917} Transformers"),J1.forEach(t),Wd.forEach(t),v2=h(a),yu=o(a,"P",{});var W1=l(yu);tj=u(W1,"Example:"),W1.forEach(t),y2=h(a),v($t.$$.fragment,a),E2=h(a),Eu=o(a,"P",{});var Y1=l(Eu);sj=u(Y1,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Y1.forEach(t),w2=h(a),qt=o(a,"TABLE",{});var lu=l(qt);dg=o(lu,"THEAD",{});var V1=l(dg);En=o(V1,"TR",{});var iu=l(En);wu=o(iu,"TH",{align:!0});var X1=l(wu);aj=u(X1,"All parameters"),X1.forEach(t),nj=h(iu),gg=o(iu,"TH",{align:!0}),l(gg).forEach(t),iu.forEach(t),V1.forEach(t),rj=h(lu),te=o(lu,"TBODY",{});var oe=l(te);wn=o(oe,"TR",{});var uu=l(wn);bn=o(uu,"TD",{align:!0});var Yd=l(bn);mg=o(Yd,"STRONG",{});var Q1=l(mg);oj=u(Q1,"inputs"),Q1.forEach(t),lj=u(Yd," (required):"),Yd.forEach(t),ij=h(uu),bu=o(uu,"TD",{align:!0});var Z1=l(bu);uj=u(Z1,"a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),Z1.forEach(t),uu.forEach(t),cj=h(oe),Tn=o(oe,"TR",{});var cu=l(Tn);Tu=o(cu,"TD",{align:!0});var e2=l(Tu);$g=o(e2,"STRONG",{});var t2=l($g);fj=u(t2,"options"),t2.forEach(t),e2.forEach(t),pj=h(cu),ju=o(cu,"TD",{align:!0});var s2=l(ju);hj=u(s2,"a dict containing the following keys:"),s2.forEach(t),cu.forEach(t),dj=h(oe),jn=o(oe,"TR",{});var fu=l(jn);ku=o(fu,"TD",{align:!0});var a2=l(ku);gj=u(a2,"use_gpu"),a2.forEach(t),mj=h(fu),_t=o(fu,"TD",{align:!0});var pu=l(_t);$j=u(pu,"(Default: "),qg=o(pu,"CODE",{});var n2=l(qg);qj=u(n2,"false"),n2.forEach(t),_j=u(pu,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),pu.forEach(t),fu.forEach(t),vj=h(oe),kn=o(oe,"TR",{});var hu=l(kn);Au=o(hu,"TD",{align:!0});var r2=l(Au);yj=u(r2,"use_cache"),r2.forEach(t),Ej=h(hu),vt=o(hu,"TD",{align:!0});var du=l(vt);wj=u(du,"(Default: "),_g=o(du,"CODE",{});var o2=l(_g);bj=u(o2,"true"),o2.forEach(t),Tj=u(du,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),du.forEach(t),hu.forEach(t),jj=h(oe),An=o(oe,"TR",{});var gu=l(An);Du=o(gu,"TD",{align:!0});var l2=l(Du);kj=u(l2,"wait_for_model"),l2.forEach(t),Aj=h(gu),yt=o(gu,"TD",{align:!0});var mu=l(yt);Dj=u(mu,"(Default: "),vg=o(mu,"CODE",{});var hC=l(vg);Oj=u(hC,"false"),hC.forEach(t),Pj=u(mu,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),mu.forEach(t),gu.forEach(t),oe.forEach(t),lu.forEach(t),b2=h(a),Ou=o(a,"P",{});var dC=l(Ou);Rj=u(dC,"Return value is either a dict or a list of dicts if you sent a list of inputs"),dC.forEach(t),T2=h(a),v(Et.$$.fragment,a),j2=h(a),wt=o(a,"TABLE",{});var KE=l(wt);yg=o(KE,"THEAD",{});var gC=l(yg);Dn=o(gC,"TR",{});var FE=l(Dn);Pu=o(FE,"TH",{align:!0});var mC=l(Pu);Sj=u(mC,"Returned values"),mC.forEach(t),Nj=h(FE),Eg=o(FE,"TH",{align:!0}),l(Eg).forEach(t),FE.forEach(t),gC.forEach(t),xj=h(KE),de=o(KE,"TBODY",{});var on=l(de);On=o(on,"TR",{});var JE=l(On);Ru=o(JE,"TD",{align:!0});var $C=l(Ru);wg=o($C,"STRONG",{});var qC=l(wg);Ij=u(qC,"sequence"),qC.forEach(t),$C.forEach(t),Hj=h(JE),Su=o(JE,"TD",{align:!0});var _C=l(Su);Bj=u(_C,"The actual sequence of tokens that ran against the model (may contain special tokens)"),_C.forEach(t),JE.forEach(t),Cj=h(on),Pn=o(on,"TR",{});var WE=l(Pn);Nu=o(WE,"TD",{align:!0});var vC=l(Nu);bg=o(vC,"STRONG",{});var yC=l(bg);Gj=u(yC,"score"),yC.forEach(t),vC.forEach(t),Lj=h(WE),xu=o(WE,"TD",{align:!0});var EC=l(xu);Mj=u(EC,"The probability for this token."),EC.forEach(t),WE.forEach(t),Uj=h(on),Rn=o(on,"TR",{});var YE=l(Rn);Iu=o(YE,"TD",{align:!0});var wC=l(Iu);Tg=o(wC,"STRONG",{});var bC=l(Tg);zj=u(bC,"token"),bC.forEach(t),wC.forEach(t),Kj=h(YE),Hu=o(YE,"TD",{align:!0});var TC=l(Hu);Fj=u(TC,"The id of the token"),TC.forEach(t),YE.forEach(t),Jj=h(on),Sn=o(on,"TR",{});var VE=l(Sn);Bu=o(VE,"TD",{align:!0});var jC=l(Bu);jg=o(jC,"STRONG",{});var kC=l(jg);Wj=u(kC,"token_str"),kC.forEach(t),jC.forEach(t),Yj=h(VE),Cu=o(VE,"TD",{align:!0});var AC=l(Cu);Vj=u(AC,"The string representation of the token"),AC.forEach(t),VE.forEach(t),on.forEach(t),KE.forEach(t),k2=h(a),Be=o(a,"H3",{class:!0});var XE=l(Be);bt=o(XE,"A",{id:!0,class:!0,href:!0});var DC=l(bt);kg=o(DC,"SPAN",{});var OC=l(kg);v(Nn.$$.fragment,OC),OC.forEach(t),DC.forEach(t),Xj=h(XE),Ag=o(XE,"SPAN",{});var PC=l(Ag);Qj=u(PC,"Summarization task"),PC.forEach(t),XE.forEach(t),A2=h(a),Tt=o(a,"P",{});var QE=l(Tt);Zj=u(QE,`This task is well known to summarize longer text into shorter text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss your summarization needs,
please get in touch with us: <`),Gu=o(QE,"A",{href:!0});var RC=l(Gu);e4=u(RC,"api-enterprise@huggingface.co"),RC.forEach(t),t4=u(QE,">"),QE.forEach(t),D2=h(a),v(jt.$$.fragment,a),O2=h(a),xn=o(a,"P",{});var HB=l(xn);s4=u(HB,"Available with: "),In=o(HB,"A",{href:!0,rel:!0});var SC=l(In);a4=u(SC,"\u{1F917} Transformers"),SC.forEach(t),HB.forEach(t),P2=h(a),Lu=o(a,"P",{});var NC=l(Lu);n4=u(NC,"Example:"),NC.forEach(t),R2=h(a),v(kt.$$.fragment,a),S2=h(a),Mu=o(a,"P",{});var xC=l(Mu);r4=u(xC,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),xC.forEach(t),N2=h(a),At=o(a,"TABLE",{});var ZE=l(At);Dg=o(ZE,"THEAD",{});var IC=l(Dg);Hn=o(IC,"TR",{});var ew=l(Hn);Uu=o(ew,"TH",{align:!0});var HC=l(Uu);o4=u(HC,"All parameters"),HC.forEach(t),l4=h(ew),Og=o(ew,"TH",{align:!0}),l(Og).forEach(t),ew.forEach(t),IC.forEach(t),i4=h(ZE),G=o(ZE,"TBODY",{});var M=l(G);Bn=o(M,"TR",{});var tw=l(Bn);Cn=o(tw,"TD",{align:!0});var BB=l(Cn);Pg=o(BB,"STRONG",{});var BC=l(Pg);u4=u(BC,"inputs"),BC.forEach(t),c4=u(BB," (required)"),BB.forEach(t),f4=h(tw),zu=o(tw,"TD",{align:!0});var CC=l(zu);p4=u(CC,"a string to be summarized"),CC.forEach(t),tw.forEach(t),h4=h(M),Gn=o(M,"TR",{});var sw=l(Gn);Ku=o(sw,"TD",{align:!0});var GC=l(Ku);Rg=o(GC,"STRONG",{});var LC=l(Rg);d4=u(LC,"parameters"),LC.forEach(t),GC.forEach(t),g4=h(sw),Fu=o(sw,"TD",{align:!0});var MC=l(Fu);m4=u(MC,"a dict containing the following keys:"),MC.forEach(t),sw.forEach(t),$4=h(M),Ln=o(M,"TR",{});var aw=l(Ln);Ju=o(aw,"TD",{align:!0});var UC=l(Ju);q4=u(UC,"min_length"),UC.forEach(t),_4=h(aw),_e=o(aw,"TD",{align:!0});var Vd=l(_e);v4=u(Vd,"(Default: "),Sg=o(Vd,"CODE",{});var zC=l(Sg);y4=u(zC,"None"),zC.forEach(t),E4=u(Vd,"). Integer to define the minimum length "),Ng=o(Vd,"STRONG",{});var KC=l(Ng);w4=u(KC,"in tokens"),KC.forEach(t),b4=u(Vd," of the output summary."),Vd.forEach(t),aw.forEach(t),T4=h(M),Mn=o(M,"TR",{});var nw=l(Mn);Wu=o(nw,"TD",{align:!0});var FC=l(Wu);j4=u(FC,"max_length"),FC.forEach(t),k4=h(nw),ve=o(nw,"TD",{align:!0});var Xd=l(ve);A4=u(Xd,"(Default: "),xg=o(Xd,"CODE",{});var JC=l(xg);D4=u(JC,"None"),JC.forEach(t),O4=u(Xd,"). Integer to define the maximum length "),Ig=o(Xd,"STRONG",{});var WC=l(Ig);P4=u(WC,"in tokens"),WC.forEach(t),R4=u(Xd," of the output summary."),Xd.forEach(t),nw.forEach(t),S4=h(M),Un=o(M,"TR",{});var rw=l(Un);Yu=o(rw,"TD",{align:!0});var YC=l(Yu);N4=u(YC,"top_k"),YC.forEach(t),x4=h(rw),ye=o(rw,"TD",{align:!0});var Qd=l(ye);I4=u(Qd,"(Default: "),Hg=o(Qd,"CODE",{});var VC=l(Hg);H4=u(VC,"None"),VC.forEach(t),B4=u(Qd,"). Integer to define the top tokens considered within the "),Bg=o(Qd,"CODE",{});var XC=l(Bg);C4=u(XC,"sample"),XC.forEach(t),G4=u(Qd," operation to create new text."),Qd.forEach(t),rw.forEach(t),L4=h(M),zn=o(M,"TR",{});var ow=l(zn);Vu=o(ow,"TD",{align:!0});var QC=l(Vu);M4=u(QC,"top_p"),QC.forEach(t),U4=h(ow),ie=o(ow,"TD",{align:!0});var ln=l(ie);z4=u(ln,"(Default: "),Cg=o(ln,"CODE",{});var ZC=l(Cg);K4=u(ZC,"None"),ZC.forEach(t),F4=u(ln,"). Float to define the tokens that are within the "),Gg=o(ln,"CODE",{});var eG=l(Gg);J4=u(eG,"sample"),eG.forEach(t),W4=u(ln," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Lg=o(ln,"CODE",{});var tG=l(Lg);Y4=u(tG,"top_p"),tG.forEach(t),V4=u(ln,"."),ln.forEach(t),ow.forEach(t),X4=h(M),Kn=o(M,"TR",{});var lw=l(Kn);Xu=o(lw,"TD",{align:!0});var sG=l(Xu);Q4=u(sG,"temperature"),sG.forEach(t),Z4=h(lw),ue=o(lw,"TD",{align:!0});var un=l(ue);e5=u(un,"(Default: "),Mg=o(un,"CODE",{});var aG=l(Mg);t5=u(aG,"1.0"),aG.forEach(t),s5=u(un,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),Ug=o(un,"CODE",{});var nG=l(Ug);a5=u(nG,"0"),nG.forEach(t),n5=u(un," means always take the highest score, "),zg=o(un,"CODE",{});var rG=l(zg);r5=u(rG,"100.0"),rG.forEach(t),o5=u(un," is getting closer to uniform probability."),un.forEach(t),lw.forEach(t),l5=h(M),Fn=o(M,"TR",{});var iw=l(Fn);Qu=o(iw,"TD",{align:!0});var oG=l(Qu);i5=u(oG,"repetition_penalty"),oG.forEach(t),u5=h(iw),Dt=o(iw,"TD",{align:!0});var uw=l(Dt);c5=u(uw,"(Default: "),Kg=o(uw,"CODE",{});var lG=l(Kg);f5=u(lG,"None"),lG.forEach(t),p5=u(uw,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),uw.forEach(t),iw.forEach(t),h5=h(M),Jn=o(M,"TR",{});var cw=l(Jn);Zu=o(cw,"TD",{align:!0});var iG=l(Zu);d5=u(iG,"max_time"),iG.forEach(t),g5=h(cw),Ot=o(cw,"TD",{align:!0});var fw=l(Ot);m5=u(fw,"(Default: "),Fg=o(fw,"CODE",{});var uG=l(Fg);$5=u(uG,"None"),uG.forEach(t),q5=u(fw,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),fw.forEach(t),cw.forEach(t),_5=h(M),Wn=o(M,"TR",{});var pw=l(Wn);ec=o(pw,"TD",{align:!0});var cG=l(ec);Jg=o(cG,"STRONG",{});var fG=l(Jg);v5=u(fG,"options"),fG.forEach(t),cG.forEach(t),y5=h(pw),tc=o(pw,"TD",{align:!0});var pG=l(tc);E5=u(pG,"a dict containing the following keys:"),pG.forEach(t),pw.forEach(t),w5=h(M),Yn=o(M,"TR",{});var hw=l(Yn);sc=o(hw,"TD",{align:!0});var hG=l(sc);b5=u(hG,"use_gpu"),hG.forEach(t),T5=h(hw),Pt=o(hw,"TD",{align:!0});var dw=l(Pt);j5=u(dw,"(Default: "),Wg=o(dw,"CODE",{});var dG=l(Wg);k5=u(dG,"false"),dG.forEach(t),A5=u(dw,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),dw.forEach(t),hw.forEach(t),D5=h(M),Vn=o(M,"TR",{});var gw=l(Vn);ac=o(gw,"TD",{align:!0});var gG=l(ac);O5=u(gG,"use_cache"),gG.forEach(t),P5=h(gw),Rt=o(gw,"TD",{align:!0});var mw=l(Rt);R5=u(mw,"(Default: "),Yg=o(mw,"CODE",{});var mG=l(Yg);S5=u(mG,"true"),mG.forEach(t),N5=u(mw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),mw.forEach(t),gw.forEach(t),x5=h(M),Xn=o(M,"TR",{});var $w=l(Xn);nc=o($w,"TD",{align:!0});var $G=l(nc);I5=u($G,"wait_for_model"),$G.forEach(t),H5=h($w),St=o($w,"TD",{align:!0});var qw=l(St);B5=u(qw,"(Default: "),Vg=o(qw,"CODE",{});var qG=l(Vg);C5=u(qG,"false"),qG.forEach(t),G5=u(qw,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),qw.forEach(t),$w.forEach(t),M.forEach(t),ZE.forEach(t),x2=h(a),rc=o(a,"P",{});var _G=l(rc);L5=u(_G,"Return value is either a dict or a list of dicts if you sent a list of inputs"),_G.forEach(t),I2=h(a),Nt=o(a,"TABLE",{});var _w=l(Nt);Xg=o(_w,"THEAD",{});var vG=l(Xg);Qn=o(vG,"TR",{});var vw=l(Qn);oc=o(vw,"TH",{align:!0});var yG=l(oc);M5=u(yG,"Returned values"),yG.forEach(t),U5=h(vw),Qg=o(vw,"TH",{align:!0}),l(Qg).forEach(t),vw.forEach(t),vG.forEach(t),z5=h(_w),Zg=o(_w,"TBODY",{});var EG=l(Zg);Zn=o(EG,"TR",{});var yw=l(Zn);lc=o(yw,"TD",{align:!0});var wG=l(lc);em=o(wG,"STRONG",{});var bG=l(em);K5=u(bG,"summarization_text"),bG.forEach(t),wG.forEach(t),F5=h(yw),ic=o(yw,"TD",{align:!0});var TG=l(ic);J5=u(TG,"The string after translation"),TG.forEach(t),yw.forEach(t),EG.forEach(t),_w.forEach(t),H2=h(a),Ce=o(a,"H3",{class:!0});var Ew=l(Ce);xt=o(Ew,"A",{id:!0,class:!0,href:!0});var jG=l(xt);tm=o(jG,"SPAN",{});var kG=l(tm);v(er.$$.fragment,kG),kG.forEach(t),jG.forEach(t),W5=h(Ew),sm=o(Ew,"SPAN",{});var AG=l(sm);Y5=u(AG,"Question Answering task"),AG.forEach(t),Ew.forEach(t),B2=h(a),uc=o(a,"P",{});var DG=l(uc);V5=u(DG,"Want to have a nice know-it-all bot that can answer any question?"),DG.forEach(t),C2=h(a),v(It.$$.fragment,a),G2=h(a),Ge=o(a,"P",{});var i2=l(Ge);X5=u(i2,"Available with: "),tr=o(i2,"A",{href:!0,rel:!0});var OG=l(tr);Q5=u(OG,"\u{1F917}Transformers"),OG.forEach(t),Z5=u(i2,` and
`),sr=o(i2,"A",{href:!0,rel:!0});var PG=l(sr);ek=u(PG,"AllenNLP"),PG.forEach(t),i2.forEach(t),L2=h(a),cc=o(a,"P",{});var RG=l(cc);tk=u(RG,"Example:"),RG.forEach(t),M2=h(a),v(Ht.$$.fragment,a),U2=h(a),fc=o(a,"P",{});var SG=l(fc);sk=u(SG,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),SG.forEach(t),z2=h(a),pc=o(a,"P",{});var NG=l(pc);ak=u(NG,"Return value is either a dict or a list of dicts if you sent a list of inputs"),NG.forEach(t),K2=h(a),v(Bt.$$.fragment,a),F2=h(a),Ct=o(a,"TABLE",{});var ww=l(Ct);am=o(ww,"THEAD",{});var xG=l(am);ar=o(xG,"TR",{});var bw=l(ar);hc=o(bw,"TH",{align:!0});var IG=l(hc);nk=u(IG,"Returned values"),IG.forEach(t),rk=h(bw),nm=o(bw,"TH",{align:!0}),l(nm).forEach(t),bw.forEach(t),xG.forEach(t),ok=h(ww),ge=o(ww,"TBODY",{});var cn=l(ge);nr=o(cn,"TR",{});var Tw=l(nr);dc=o(Tw,"TD",{align:!0});var HG=l(dc);rm=o(HG,"STRONG",{});var BG=l(rm);lk=u(BG,"answer"),BG.forEach(t),HG.forEach(t),ik=h(Tw),gc=o(Tw,"TD",{align:!0});var CG=l(gc);uk=u(CG,"A string that\u2019s the answer within the text."),CG.forEach(t),Tw.forEach(t),ck=h(cn),rr=o(cn,"TR",{});var jw=l(rr);mc=o(jw,"TD",{align:!0});var GG=l(mc);om=o(GG,"STRONG",{});var LG=l(om);fk=u(LG,"score"),LG.forEach(t),GG.forEach(t),pk=h(jw),$c=o(jw,"TD",{align:!0});var MG=l($c);hk=u(MG,"A float that represents how likely that the answer is correct"),MG.forEach(t),jw.forEach(t),dk=h(cn),or=o(cn,"TR",{});var kw=l(or);qc=o(kw,"TD",{align:!0});var UG=l(qc);lm=o(UG,"STRONG",{});var zG=l(lm);gk=u(zG,"start"),zG.forEach(t),UG.forEach(t),mk=h(kw),Gt=o(kw,"TD",{align:!0});var Aw=l(Gt);$k=u(Aw,"The index (string wise) of the start of the answer within "),im=o(Aw,"CODE",{});var KG=l(im);qk=u(KG,"context"),KG.forEach(t),_k=u(Aw,"."),Aw.forEach(t),kw.forEach(t),vk=h(cn),lr=o(cn,"TR",{});var Dw=l(lr);_c=o(Dw,"TD",{align:!0});var FG=l(_c);um=o(FG,"STRONG",{});var JG=l(um);yk=u(JG,"stop"),JG.forEach(t),FG.forEach(t),Ek=h(Dw),Lt=o(Dw,"TD",{align:!0});var Ow=l(Lt);wk=u(Ow,"The index (string wise) of the stop of the answer within "),cm=o(Ow,"CODE",{});var WG=l(cm);bk=u(WG,"context"),WG.forEach(t),Tk=u(Ow,"."),Ow.forEach(t),Dw.forEach(t),cn.forEach(t),ww.forEach(t),J2=h(a),Le=o(a,"H3",{class:!0});var Pw=l(Le);Mt=o(Pw,"A",{id:!0,class:!0,href:!0});var YG=l(Mt);fm=o(YG,"SPAN",{});var VG=l(fm);v(ir.$$.fragment,VG),VG.forEach(t),YG.forEach(t),jk=h(Pw),pm=o(Pw,"SPAN",{});var XG=l(pm);kk=u(XG,"Table Question Answering task"),XG.forEach(t),Pw.forEach(t),W2=h(a),vc=o(a,"P",{});var QG=l(vc);Ak=u(QG,`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),QG.forEach(t),Y2=h(a),v(Ut.$$.fragment,a),V2=h(a),ur=o(a,"P",{});var CB=l(ur);Dk=u(CB,"Available with: "),cr=o(CB,"A",{href:!0,rel:!0});var ZG=l(cr);Ok=u(ZG,"\u{1F917} Transformers"),ZG.forEach(t),CB.forEach(t),X2=h(a),yc=o(a,"P",{});var eL=l(yc);Pk=u(eL,"Example:"),eL.forEach(t),Q2=h(a),v(zt.$$.fragment,a),Z2=h(a),Ec=o(a,"P",{});var tL=l(Ec);Rk=u(tL,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),tL.forEach(t),ev=h(a),Kt=o(a,"TABLE",{});var Rw=l(Kt);hm=o(Rw,"THEAD",{});var sL=l(hm);fr=o(sL,"TR",{});var Sw=l(fr);wc=o(Sw,"TH",{align:!0});var aL=l(wc);Sk=u(aL,"All parameters"),aL.forEach(t),Nk=h(Sw),dm=o(Sw,"TH",{align:!0}),l(dm).forEach(t),Sw.forEach(t),sL.forEach(t),xk=h(Rw),F=o(Rw,"TBODY",{});var X=l(F);pr=o(X,"TR",{});var Nw=l(pr);hr=o(Nw,"TD",{align:!0});var GB=l(hr);gm=o(GB,"STRONG",{});var nL=l(gm);Ik=u(nL,"inputs"),nL.forEach(t),Hk=u(GB," (required)"),GB.forEach(t),Bk=h(Nw),mm=o(Nw,"TD",{align:!0}),l(mm).forEach(t),Nw.forEach(t),Ck=h(X),dr=o(X,"TR",{});var xw=l(dr);bc=o(xw,"TD",{align:!0});var rL=l(bc);Gk=u(rL,"query (required)"),rL.forEach(t),Lk=h(xw),Tc=o(xw,"TD",{align:!0});var oL=l(Tc);Mk=u(oL,"The query in plain text that you want to ask the table"),oL.forEach(t),xw.forEach(t),Uk=h(X),gr=o(X,"TR",{});var Iw=l(gr);jc=o(Iw,"TD",{align:!0});var lL=l(jc);zk=u(lL,"table (required)"),lL.forEach(t),Kk=h(Iw),kc=o(Iw,"TD",{align:!0});var iL=l(kc);Fk=u(iL,"A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),iL.forEach(t),Iw.forEach(t),Jk=h(X),mr=o(X,"TR",{});var Hw=l(mr);Ac=o(Hw,"TD",{align:!0});var uL=l(Ac);$m=o(uL,"STRONG",{});var cL=l($m);Wk=u(cL,"options"),cL.forEach(t),uL.forEach(t),Yk=h(Hw),Dc=o(Hw,"TD",{align:!0});var fL=l(Dc);Vk=u(fL,"a dict containing the following keys:"),fL.forEach(t),Hw.forEach(t),Xk=h(X),$r=o(X,"TR",{});var Bw=l($r);Oc=o(Bw,"TD",{align:!0});var pL=l(Oc);Qk=u(pL,"use_gpu"),pL.forEach(t),Zk=h(Bw),Ft=o(Bw,"TD",{align:!0});var Cw=l(Ft);e6=u(Cw,"(Default: "),qm=o(Cw,"CODE",{});var hL=l(qm);t6=u(hL,"false"),hL.forEach(t),s6=u(Cw,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Cw.forEach(t),Bw.forEach(t),a6=h(X),qr=o(X,"TR",{});var Gw=l(qr);Pc=o(Gw,"TD",{align:!0});var dL=l(Pc);n6=u(dL,"use_cache"),dL.forEach(t),r6=h(Gw),Jt=o(Gw,"TD",{align:!0});var Lw=l(Jt);o6=u(Lw,"(Default: "),_m=o(Lw,"CODE",{});var gL=l(_m);l6=u(gL,"true"),gL.forEach(t),i6=u(Lw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Lw.forEach(t),Gw.forEach(t),u6=h(X),_r=o(X,"TR",{});var Mw=l(_r);Rc=o(Mw,"TD",{align:!0});var mL=l(Rc);c6=u(mL,"wait_for_model"),mL.forEach(t),f6=h(Mw),Wt=o(Mw,"TD",{align:!0});var Uw=l(Wt);p6=u(Uw,"(Default: "),vm=o(Uw,"CODE",{});var $L=l(vm);h6=u($L,"false"),$L.forEach(t),d6=u(Uw,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Uw.forEach(t),Mw.forEach(t),X.forEach(t),Rw.forEach(t),tv=h(a),Sc=o(a,"P",{});var qL=l(Sc);g6=u(qL,"Return value is either a dict or a list of dicts if you sent a list of inputs"),qL.forEach(t),sv=h(a),v(Yt.$$.fragment,a),av=h(a),Vt=o(a,"TABLE",{});var zw=l(Vt);ym=o(zw,"THEAD",{});var _L=l(ym);vr=o(_L,"TR",{});var Kw=l(vr);Nc=o(Kw,"TH",{align:!0});var vL=l(Nc);m6=u(vL,"Returned values"),vL.forEach(t),$6=h(Kw),Em=o(Kw,"TH",{align:!0}),l(Em).forEach(t),Kw.forEach(t),_L.forEach(t),q6=h(zw),me=o(zw,"TBODY",{});var fn=l(me);yr=o(fn,"TR",{});var Fw=l(yr);xc=o(Fw,"TD",{align:!0});var yL=l(xc);wm=o(yL,"STRONG",{});var EL=l(wm);_6=u(EL,"answer"),EL.forEach(t),yL.forEach(t),v6=h(Fw),Ic=o(Fw,"TD",{align:!0});var wL=l(Ic);y6=u(wL,"The plaintext answer"),wL.forEach(t),Fw.forEach(t),E6=h(fn),Er=o(fn,"TR",{});var Jw=l(Er);Hc=o(Jw,"TD",{align:!0});var bL=l(Hc);bm=o(bL,"STRONG",{});var TL=l(bm);w6=u(TL,"coordinates"),TL.forEach(t),bL.forEach(t),b6=h(Jw),Bc=o(Jw,"TD",{align:!0});var jL=l(Bc);T6=u(jL,"a list of coordinates of the cells referenced in the answer"),jL.forEach(t),Jw.forEach(t),j6=h(fn),wr=o(fn,"TR",{});var Ww=l(wr);Cc=o(Ww,"TD",{align:!0});var kL=l(Cc);Tm=o(kL,"STRONG",{});var AL=l(Tm);k6=u(AL,"cells"),AL.forEach(t),kL.forEach(t),A6=h(Ww),Gc=o(Ww,"TD",{align:!0});var DL=l(Gc);D6=u(DL,"a list of coordinates of the cells contents"),DL.forEach(t),Ww.forEach(t),O6=h(fn),br=o(fn,"TR",{});var Yw=l(br);Lc=o(Yw,"TD",{align:!0});var OL=l(Lc);jm=o(OL,"STRONG",{});var PL=l(jm);P6=u(PL,"aggregator"),PL.forEach(t),OL.forEach(t),R6=h(Yw),Mc=o(Yw,"TD",{align:!0});var RL=l(Mc);S6=u(RL,"The aggregator used to get the answer"),RL.forEach(t),Yw.forEach(t),fn.forEach(t),zw.forEach(t),nv=h(a),Me=o(a,"H3",{class:!0});var Vw=l(Me);Xt=o(Vw,"A",{id:!0,class:!0,href:!0});var SL=l(Xt);km=o(SL,"SPAN",{});var NL=l(km);v(Tr.$$.fragment,NL),NL.forEach(t),SL.forEach(t),N6=h(Vw),Am=o(Vw,"SPAN",{});var xL=l(Am);x6=u(xL,"Sentence Similarity task"),xL.forEach(t),Vw.forEach(t),rv=h(a),Uc=o(a,"P",{});var IL=l(Uc);I6=u(IL,"Calculate the semantic similarity between one text and a list of other sentences by comparing their embeddings."),IL.forEach(t),ov=h(a),v(Qt.$$.fragment,a),lv=h(a),jr=o(a,"P",{});var LB=l(jr);H6=u(LB,"Available with: "),kr=o(LB,"A",{href:!0,rel:!0});var HL=l(kr);B6=u(HL,"Sentence Transformers"),HL.forEach(t),LB.forEach(t),iv=h(a),zc=o(a,"P",{});var BL=l(zc);C6=u(BL,"Example:"),BL.forEach(t),uv=h(a),v(Zt.$$.fragment,a),cv=h(a),Kc=o(a,"P",{});var CL=l(Kc);G6=u(CL,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),CL.forEach(t),fv=h(a),es=o(a,"TABLE",{});var Xw=l(es);Dm=o(Xw,"THEAD",{});var GL=l(Dm);Ar=o(GL,"TR",{});var Qw=l(Ar);Fc=o(Qw,"TH",{align:!0});var LL=l(Fc);L6=u(LL,"All parameters"),LL.forEach(t),M6=h(Qw),Om=o(Qw,"TH",{align:!0}),l(Om).forEach(t),Qw.forEach(t),GL.forEach(t),U6=h(Xw),J=o(Xw,"TBODY",{});var Q=l(J);Dr=o(Q,"TR",{});var Zw=l(Dr);Or=o(Zw,"TD",{align:!0});var MB=l(Or);Pm=o(MB,"STRONG",{});var ML=l(Pm);z6=u(ML,"inputs"),ML.forEach(t),K6=u(MB," (required)"),MB.forEach(t),F6=h(Zw),Rm=o(Zw,"TD",{align:!0}),l(Rm).forEach(t),Zw.forEach(t),J6=h(Q),Pr=o(Q,"TR",{});var e0=l(Pr);Jc=o(e0,"TD",{align:!0});var UL=l(Jc);W6=u(UL,"source_sentence (required)"),UL.forEach(t),Y6=h(e0),Wc=o(e0,"TD",{align:!0});var zL=l(Wc);V6=u(zL,"The string that you wish to compare the other strings with. This can be a phrase, sentence, or longer passage, depending on the model being used."),zL.forEach(t),e0.forEach(t),X6=h(Q),Rr=o(Q,"TR",{});var t0=l(Rr);Yc=o(t0,"TD",{align:!0});var KL=l(Yc);Q6=u(KL,"sentences (required)"),KL.forEach(t),Z6=h(t0),Vc=o(t0,"TD",{align:!0});var FL=l(Vc);e7=u(FL,"A list of strings which will be compared against the source_sentence."),FL.forEach(t),t0.forEach(t),t7=h(Q),Sr=o(Q,"TR",{});var s0=l(Sr);Xc=o(s0,"TD",{align:!0});var JL=l(Xc);Sm=o(JL,"STRONG",{});var WL=l(Sm);s7=u(WL,"options"),WL.forEach(t),JL.forEach(t),a7=h(s0),Qc=o(s0,"TD",{align:!0});var YL=l(Qc);n7=u(YL,"a dict containing the following keys:"),YL.forEach(t),s0.forEach(t),r7=h(Q),Nr=o(Q,"TR",{});var a0=l(Nr);Zc=o(a0,"TD",{align:!0});var VL=l(Zc);o7=u(VL,"use_gpu"),VL.forEach(t),l7=h(a0),ts=o(a0,"TD",{align:!0});var n0=l(ts);i7=u(n0,"(Default: "),Nm=o(n0,"CODE",{});var XL=l(Nm);u7=u(XL,"false"),XL.forEach(t),c7=u(n0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),n0.forEach(t),a0.forEach(t),f7=h(Q),xr=o(Q,"TR",{});var r0=l(xr);ef=o(r0,"TD",{align:!0});var QL=l(ef);p7=u(QL,"use_cache"),QL.forEach(t),h7=h(r0),ss=o(r0,"TD",{align:!0});var o0=l(ss);d7=u(o0,"(Default: "),xm=o(o0,"CODE",{});var ZL=l(xm);g7=u(ZL,"true"),ZL.forEach(t),m7=u(o0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),o0.forEach(t),r0.forEach(t),$7=h(Q),Ir=o(Q,"TR",{});var l0=l(Ir);tf=o(l0,"TD",{align:!0});var eM=l(tf);q7=u(eM,"wait_for_model"),eM.forEach(t),_7=h(l0),as=o(l0,"TD",{align:!0});var i0=l(as);v7=u(i0,"(Default: "),Im=o(i0,"CODE",{});var tM=l(Im);y7=u(tM,"false"),tM.forEach(t),E7=u(i0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),i0.forEach(t),l0.forEach(t),Q.forEach(t),Xw.forEach(t),pv=h(a),sf=o(a,"P",{});var sM=l(sf);w7=u(sM,"The return value is a list of similarity scores, given as floats."),sM.forEach(t),hv=h(a),v(ns.$$.fragment,a),dv=h(a),rs=o(a,"TABLE",{});var u0=l(rs);Hm=o(u0,"THEAD",{});var aM=l(Hm);Hr=o(aM,"TR",{});var c0=l(Hr);af=o(c0,"TH",{align:!0});var nM=l(af);b7=u(nM,"Returned values"),nM.forEach(t),T7=h(c0),Bm=o(c0,"TH",{align:!0}),l(Bm).forEach(t),c0.forEach(t),aM.forEach(t),j7=h(u0),Cm=o(u0,"TBODY",{});var rM=l(Cm);Br=o(rM,"TR",{});var f0=l(Br);nf=o(f0,"TD",{align:!0});var oM=l(nf);Gm=o(oM,"STRONG",{});var lM=l(Gm);k7=u(lM,"Scores"),lM.forEach(t),oM.forEach(t),A7=h(f0),rf=o(f0,"TD",{align:!0});var iM=l(rf);D7=u(iM,"The associated similarity score for each of the given strings"),iM.forEach(t),f0.forEach(t),rM.forEach(t),u0.forEach(t),gv=h(a),Ue=o(a,"H3",{class:!0});var p0=l(Ue);os=o(p0,"A",{id:!0,class:!0,href:!0});var uM=l(os);Lm=o(uM,"SPAN",{});var cM=l(Lm);v(Cr.$$.fragment,cM),cM.forEach(t),uM.forEach(t),O7=h(p0),Mm=o(p0,"SPAN",{});var fM=l(Mm);P7=u(fM,"Text Classification task"),fM.forEach(t),p0.forEach(t),mv=h(a),of=o(a,"P",{});var pM=l(of);R7=u(pM,`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),pM.forEach(t),$v=h(a),v(ls.$$.fragment,a),qv=h(a),Gr=o(a,"P",{});var UB=l(Gr);S7=u(UB,"Available with: "),Lr=o(UB,"A",{href:!0,rel:!0});var hM=l(Lr);N7=u(hM,"\u{1F917} Transformers"),hM.forEach(t),UB.forEach(t),_v=h(a),lf=o(a,"P",{});var dM=l(lf);x7=u(dM,"Example:"),dM.forEach(t),vv=h(a),v(is.$$.fragment,a),yv=h(a),uf=o(a,"P",{});var gM=l(uf);I7=u(gM,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),gM.forEach(t),Ev=h(a),us=o(a,"TABLE",{});var h0=l(us);Um=o(h0,"THEAD",{});var mM=l(Um);Mr=o(mM,"TR",{});var d0=l(Mr);cf=o(d0,"TH",{align:!0});var $M=l(cf);H7=u($M,"All parameters"),$M.forEach(t),B7=h(d0),zm=o(d0,"TH",{align:!0}),l(zm).forEach(t),d0.forEach(t),mM.forEach(t),C7=h(h0),se=o(h0,"TBODY",{});var Oe=l(se);Ur=o(Oe,"TR",{});var g0=l(Ur);zr=o(g0,"TD",{align:!0});var zB=l(zr);Km=o(zB,"STRONG",{});var qM=l(Km);G7=u(qM,"inputs"),qM.forEach(t),L7=u(zB," (required)"),zB.forEach(t),M7=h(g0),ff=o(g0,"TD",{align:!0});var _M=l(ff);U7=u(_M,"a string to be classified"),_M.forEach(t),g0.forEach(t),z7=h(Oe),Kr=o(Oe,"TR",{});var m0=l(Kr);pf=o(m0,"TD",{align:!0});var vM=l(pf);Fm=o(vM,"STRONG",{});var yM=l(Fm);K7=u(yM,"options"),yM.forEach(t),vM.forEach(t),F7=h(m0),hf=o(m0,"TD",{align:!0});var EM=l(hf);J7=u(EM,"a dict containing the following keys:"),EM.forEach(t),m0.forEach(t),W7=h(Oe),Fr=o(Oe,"TR",{});var $0=l(Fr);df=o($0,"TD",{align:!0});var wM=l(df);Y7=u(wM,"use_gpu"),wM.forEach(t),V7=h($0),cs=o($0,"TD",{align:!0});var q0=l(cs);X7=u(q0,"(Default: "),Jm=o(q0,"CODE",{});var bM=l(Jm);Q7=u(bM,"false"),bM.forEach(t),Z7=u(q0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),q0.forEach(t),$0.forEach(t),e9=h(Oe),Jr=o(Oe,"TR",{});var _0=l(Jr);gf=o(_0,"TD",{align:!0});var TM=l(gf);t9=u(TM,"use_cache"),TM.forEach(t),s9=h(_0),fs=o(_0,"TD",{align:!0});var v0=l(fs);a9=u(v0,"(Default: "),Wm=o(v0,"CODE",{});var jM=l(Wm);n9=u(jM,"true"),jM.forEach(t),r9=u(v0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),v0.forEach(t),_0.forEach(t),o9=h(Oe),Wr=o(Oe,"TR",{});var y0=l(Wr);mf=o(y0,"TD",{align:!0});var kM=l(mf);l9=u(kM,"wait_for_model"),kM.forEach(t),i9=h(y0),ps=o(y0,"TD",{align:!0});var E0=l(ps);u9=u(E0,"(Default: "),Ym=o(E0,"CODE",{});var AM=l(Ym);c9=u(AM,"false"),AM.forEach(t),f9=u(E0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),E0.forEach(t),y0.forEach(t),Oe.forEach(t),h0.forEach(t),wv=h(a),$f=o(a,"P",{});var DM=l($f);p9=u(DM,"Return value is either a dict or a list of dicts if you sent a list of inputs"),DM.forEach(t),bv=h(a),v(hs.$$.fragment,a),Tv=h(a),ds=o(a,"TABLE",{});var w0=l(ds);Vm=o(w0,"THEAD",{});var OM=l(Vm);Yr=o(OM,"TR",{});var b0=l(Yr);qf=o(b0,"TH",{align:!0});var PM=l(qf);h9=u(PM,"Returned values"),PM.forEach(t),d9=h(b0),Xm=o(b0,"TH",{align:!0}),l(Xm).forEach(t),b0.forEach(t),OM.forEach(t),g9=h(w0),Vr=o(w0,"TBODY",{});var T0=l(Vr);Xr=o(T0,"TR",{});var j0=l(Xr);_f=o(j0,"TD",{align:!0});var RM=l(_f);Qm=o(RM,"STRONG",{});var SM=l(Qm);m9=u(SM,"label"),SM.forEach(t),RM.forEach(t),$9=h(j0),vf=o(j0,"TD",{align:!0});var NM=l(vf);q9=u(NM,"The label for the class (model specific)"),NM.forEach(t),j0.forEach(t),_9=h(T0),Qr=o(T0,"TR",{});var k0=l(Qr);yf=o(k0,"TD",{align:!0});var xM=l(yf);Zm=o(xM,"STRONG",{});var IM=l(Zm);v9=u(IM,"score"),IM.forEach(t),xM.forEach(t),y9=h(k0),Ef=o(k0,"TD",{align:!0});var HM=l(Ef);E9=u(HM,"A floats that represents how likely is that the text belongs the this class."),HM.forEach(t),k0.forEach(t),T0.forEach(t),w0.forEach(t),jv=h(a),ze=o(a,"H3",{class:!0});var A0=l(ze);gs=o(A0,"A",{id:!0,class:!0,href:!0});var BM=l(gs);e$=o(BM,"SPAN",{});var CM=l(e$);v(Zr.$$.fragment,CM),CM.forEach(t),BM.forEach(t),w9=h(A0),t$=o(A0,"SPAN",{});var GM=l(t$);b9=u(GM,"Text Generation task"),GM.forEach(t),A0.forEach(t),kv=h(a),wf=o(a,"P",{});var LM=l(wf);T9=u(LM,"Use to continue text from a prompt. This is a very generic task."),LM.forEach(t),Av=h(a),v(ms.$$.fragment,a),Dv=h(a),eo=o(a,"P",{});var KB=l(eo);j9=u(KB,"Available with: "),to=o(KB,"A",{href:!0,rel:!0});var MM=l(to);k9=u(MM,"\u{1F917} Transformers"),MM.forEach(t),KB.forEach(t),Ov=h(a),bf=o(a,"P",{});var UM=l(bf);A9=u(UM,"Example:"),UM.forEach(t),Pv=h(a),v($s.$$.fragment,a),Rv=h(a),Tf=o(a,"P",{});var zM=l(Tf);D9=u(zM,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),zM.forEach(t),Sv=h(a),qs=o(a,"TABLE",{});var D0=l(qs);s$=o(D0,"THEAD",{});var KM=l(s$);so=o(KM,"TR",{});var O0=l(so);jf=o(O0,"TH",{align:!0});var FM=l(jf);O9=u(FM,"All parameters"),FM.forEach(t),P9=h(O0),a$=o(O0,"TH",{align:!0}),l(a$).forEach(t),O0.forEach(t),KM.forEach(t),R9=h(D0),I=o(D0,"TBODY",{});var B=l(I);ao=o(B,"TR",{});var P0=l(ao);no=o(P0,"TD",{align:!0});var FB=l(no);n$=o(FB,"STRONG",{});var JM=l(n$);S9=u(JM,"inputs"),JM.forEach(t),N9=u(FB," (required):"),FB.forEach(t),x9=h(P0),kf=o(P0,"TD",{align:!0});var WM=l(kf);I9=u(WM,"a string to be generated from"),WM.forEach(t),P0.forEach(t),H9=h(B),ro=o(B,"TR",{});var R0=l(ro);Af=o(R0,"TD",{align:!0});var YM=l(Af);r$=o(YM,"STRONG",{});var VM=l(r$);B9=u(VM,"parameters"),VM.forEach(t),YM.forEach(t),C9=h(R0),Df=o(R0,"TD",{align:!0});var XM=l(Df);G9=u(XM,"dict containing the following keys:"),XM.forEach(t),R0.forEach(t),L9=h(B),oo=o(B,"TR",{});var S0=l(oo);Of=o(S0,"TD",{align:!0});var QM=l(Of);M9=u(QM,"top_k"),QM.forEach(t),U9=h(S0),Ee=o(S0,"TD",{align:!0});var Zd=l(Ee);z9=u(Zd,"(Default: "),o$=o(Zd,"CODE",{});var ZM=l(o$);K9=u(ZM,"None"),ZM.forEach(t),F9=u(Zd,"). Integer to define the top tokens considered within the "),l$=o(Zd,"CODE",{});var eU=l(l$);J9=u(eU,"sample"),eU.forEach(t),W9=u(Zd," operation to create new text."),Zd.forEach(t),S0.forEach(t),Y9=h(B),lo=o(B,"TR",{});var N0=l(lo);Pf=o(N0,"TD",{align:!0});var tU=l(Pf);V9=u(tU,"top_p"),tU.forEach(t),X9=h(N0),ce=o(N0,"TD",{align:!0});var pn=l(ce);Q9=u(pn,"(Default: "),i$=o(pn,"CODE",{});var sU=l(i$);Z9=u(sU,"None"),sU.forEach(t),e8=u(pn,"). Float to define the tokens that are within the "),u$=o(pn,"CODE",{});var aU=l(u$);t8=u(aU,"sample"),aU.forEach(t),s8=u(pn," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),c$=o(pn,"CODE",{});var nU=l(c$);a8=u(nU,"top_p"),nU.forEach(t),n8=u(pn,"."),pn.forEach(t),N0.forEach(t),r8=h(B),io=o(B,"TR",{});var x0=l(io);Rf=o(x0,"TD",{align:!0});var rU=l(Rf);o8=u(rU,"temperature"),rU.forEach(t),l8=h(x0),fe=o(x0,"TD",{align:!0});var hn=l(fe);i8=u(hn,"(Default: "),f$=o(hn,"CODE",{});var oU=l(f$);u8=u(oU,"1.0"),oU.forEach(t),c8=u(hn,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),p$=o(hn,"CODE",{});var lU=l(p$);f8=u(lU,"0"),lU.forEach(t),p8=u(hn," means always take the highest score, "),h$=o(hn,"CODE",{});var iU=l(h$);h8=u(iU,"100.0"),iU.forEach(t),d8=u(hn," is getting closer to uniform probability."),hn.forEach(t),x0.forEach(t),g8=h(B),uo=o(B,"TR",{});var I0=l(uo);Sf=o(I0,"TD",{align:!0});var uU=l(Sf);m8=u(uU,"repetition_penalty"),uU.forEach(t),$8=h(I0),_s=o(I0,"TD",{align:!0});var H0=l(_s);q8=u(H0,"(Default: "),d$=o(H0,"CODE",{});var cU=l(d$);_8=u(cU,"None"),cU.forEach(t),v8=u(H0,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),H0.forEach(t),I0.forEach(t),y8=h(B),co=o(B,"TR",{});var B0=l(co);Nf=o(B0,"TD",{align:!0});var fU=l(Nf);E8=u(fU,"max_new_tokens"),fU.forEach(t),w8=h(B0),we=o(B0,"TD",{align:!0});var eg=l(we);b8=u(eg,"(Default: "),g$=o(eg,"CODE",{});var pU=l(g$);T8=u(pU,"None"),pU.forEach(t),j8=u(eg,"). Int (0-250). The amount of new tokens to be generated, this does "),m$=o(eg,"STRONG",{});var hU=l(m$);k8=u(hU,"not"),hU.forEach(t),A8=u(eg," include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),eg.forEach(t),B0.forEach(t),D8=h(B),fo=o(B,"TR",{});var C0=l(fo);xf=o(C0,"TD",{align:!0});var dU=l(xf);O8=u(dU,"max_time"),dU.forEach(t),P8=h(C0),be=o(C0,"TD",{align:!0});var tg=l(be);R8=u(tg,"(Default: "),$$=o(tg,"CODE",{});var gU=l($$);S8=u(gU,"None"),gU.forEach(t),N8=u(tg,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),q$=o(tg,"CODE",{});var mU=l(q$);x8=u(mU,"max_new_tokens"),mU.forEach(t),I8=u(tg," for best results."),tg.forEach(t),C0.forEach(t),H8=h(B),po=o(B,"TR",{});var G0=l(po);If=o(G0,"TD",{align:!0});var $U=l(If);B8=u($U,"return_full_text"),$U.forEach(t),C8=h(G0),Te=o(G0,"TD",{align:!0});var sg=l(Te);G8=u(sg,"(Default: "),_$=o(sg,"CODE",{});var qU=l(_$);L8=u(qU,"True"),qU.forEach(t),M8=u(sg,"). Bool. If set to False, the return results will "),v$=o(sg,"STRONG",{});var _U=l(v$);U8=u(_U,"not"),_U.forEach(t),z8=u(sg," contain the original query making it easier for prompting."),sg.forEach(t),G0.forEach(t),K8=h(B),ho=o(B,"TR",{});var L0=l(ho);Hf=o(L0,"TD",{align:!0});var vU=l(Hf);F8=u(vU,"num_return_sequences"),vU.forEach(t),J8=h(L0),vs=o(L0,"TD",{align:!0});var M0=l(vs);W8=u(M0,"(Default: "),y$=o(M0,"CODE",{});var yU=l(y$);Y8=u(yU,"1"),yU.forEach(t),V8=u(M0,"). Integer. The number of proposition you want to be returned."),M0.forEach(t),L0.forEach(t),X8=h(B),go=o(B,"TR",{});var U0=l(go);Bf=o(U0,"TD",{align:!0});var EU=l(Bf);Q8=u(EU,"do_sample"),EU.forEach(t),Z8=h(U0),ys=o(U0,"TD",{align:!0});var z0=l(ys);eA=u(z0,"(Optional: "),E$=o(z0,"CODE",{});var wU=l(E$);tA=u(wU,"True"),wU.forEach(t),sA=u(z0,"). Bool. Whether or not to use sampling, use greedy decoding otherwise."),z0.forEach(t),U0.forEach(t),aA=h(B),mo=o(B,"TR",{});var K0=l(mo);Cf=o(K0,"TD",{align:!0});var bU=l(Cf);w$=o(bU,"STRONG",{});var TU=l(w$);nA=u(TU,"options"),TU.forEach(t),bU.forEach(t),rA=h(K0),Gf=o(K0,"TD",{align:!0});var jU=l(Gf);oA=u(jU,"a dict containing the following keys:"),jU.forEach(t),K0.forEach(t),lA=h(B),$o=o(B,"TR",{});var F0=l($o);Lf=o(F0,"TD",{align:!0});var kU=l(Lf);iA=u(kU,"use_gpu"),kU.forEach(t),uA=h(F0),Es=o(F0,"TD",{align:!0});var J0=l(Es);cA=u(J0,"(Default: "),b$=o(J0,"CODE",{});var AU=l(b$);fA=u(AU,"false"),AU.forEach(t),pA=u(J0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),J0.forEach(t),F0.forEach(t),hA=h(B),qo=o(B,"TR",{});var W0=l(qo);Mf=o(W0,"TD",{align:!0});var DU=l(Mf);dA=u(DU,"use_cache"),DU.forEach(t),gA=h(W0),ws=o(W0,"TD",{align:!0});var Y0=l(ws);mA=u(Y0,"(Default: "),T$=o(Y0,"CODE",{});var OU=l(T$);$A=u(OU,"true"),OU.forEach(t),qA=u(Y0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Y0.forEach(t),W0.forEach(t),_A=h(B),_o=o(B,"TR",{});var V0=l(_o);Uf=o(V0,"TD",{align:!0});var PU=l(Uf);vA=u(PU,"wait_for_model"),PU.forEach(t),yA=h(V0),bs=o(V0,"TD",{align:!0});var X0=l(bs);EA=u(X0,"(Default: "),j$=o(X0,"CODE",{});var RU=l(j$);wA=u(RU,"false"),RU.forEach(t),bA=u(X0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),X0.forEach(t),V0.forEach(t),B.forEach(t),D0.forEach(t),Nv=h(a),zf=o(a,"P",{});var SU=l(zf);TA=u(SU,"Return value is either a dict or a list of dicts if you sent a list of inputs"),SU.forEach(t),xv=h(a),v(Ts.$$.fragment,a),Iv=h(a),js=o(a,"TABLE",{});var Q0=l(js);k$=o(Q0,"THEAD",{});var NU=l(k$);vo=o(NU,"TR",{});var Z0=l(vo);Kf=o(Z0,"TH",{align:!0});var xU=l(Kf);jA=u(xU,"Returned values"),xU.forEach(t),kA=h(Z0),A$=o(Z0,"TH",{align:!0}),l(A$).forEach(t),Z0.forEach(t),NU.forEach(t),AA=h(Q0),D$=o(Q0,"TBODY",{});var IU=l(D$);yo=o(IU,"TR",{});var eb=l(yo);Ff=o(eb,"TD",{align:!0});var HU=l(Ff);O$=o(HU,"STRONG",{});var BU=l(O$);DA=u(BU,"generated_text"),BU.forEach(t),HU.forEach(t),OA=h(eb),Jf=o(eb,"TD",{align:!0});var CU=l(Jf);PA=u(CU,"The continuated string"),CU.forEach(t),eb.forEach(t),IU.forEach(t),Q0.forEach(t),Hv=h(a),Ke=o(a,"H3",{class:!0});var tb=l(Ke);ks=o(tb,"A",{id:!0,class:!0,href:!0});var GU=l(ks);P$=o(GU,"SPAN",{});var LU=l(P$);v(Eo.$$.fragment,LU),LU.forEach(t),GU.forEach(t),RA=h(tb),R$=o(tb,"SPAN",{});var MU=l(R$);SA=u(MU,"Text2Text Generation task"),MU.forEach(t),tb.forEach(t),Bv=h(a),As=o(a,"P",{});var sb=l(As);NA=u(sb,"Essentially "),Wf=o(sb,"A",{href:!0});var UU=l(Wf);xA=u(UU,"Text-generation task"),UU.forEach(t),IA=u(sb,`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),sb.forEach(t),Cv=h(a),Fe=o(a,"H3",{class:!0});var ab=l(Fe);Ds=o(ab,"A",{id:!0,class:!0,href:!0});var zU=l(Ds);S$=o(zU,"SPAN",{});var KU=l(S$);v(wo.$$.fragment,KU),KU.forEach(t),zU.forEach(t),HA=h(ab),N$=o(ab,"SPAN",{});var FU=l(N$);BA=u(FU,"Token Classification task"),FU.forEach(t),ab.forEach(t),Gv=h(a),Yf=o(a,"P",{});var JU=l(Yf);CA=u(JU,`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),JU.forEach(t),Lv=h(a),v(Os.$$.fragment,a),Mv=h(a),Je=o(a,"P",{});var u2=l(Je);GA=u(u2,"Available with: "),bo=o(u2,"A",{href:!0,rel:!0});var WU=l(bo);LA=u(WU,"\u{1F917} Transformers"),WU.forEach(t),MA=u(u2,`,
`),To=o(u2,"A",{href:!0,rel:!0});var YU=l(To);UA=u(YU,"Flair"),YU.forEach(t),u2.forEach(t),Uv=h(a),Vf=o(a,"P",{});var VU=l(Vf);zA=u(VU,"Example:"),VU.forEach(t),zv=h(a),v(Ps.$$.fragment,a),Kv=h(a),Xf=o(a,"P",{});var XU=l(Xf);KA=u(XU,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),XU.forEach(t),Fv=h(a),Rs=o(a,"TABLE",{});var nb=l(Rs);x$=o(nb,"THEAD",{});var QU=l(x$);jo=o(QU,"TR",{});var rb=l(jo);Qf=o(rb,"TH",{align:!0});var ZU=l(Qf);FA=u(ZU,"All parameters"),ZU.forEach(t),JA=h(rb),I$=o(rb,"TH",{align:!0}),l(I$).forEach(t),rb.forEach(t),QU.forEach(t),WA=h(nb),W=o(nb,"TBODY",{});var Z=l(W);ko=o(Z,"TR",{});var ob=l(ko);Ao=o(ob,"TD",{align:!0});var JB=l(Ao);H$=o(JB,"STRONG",{});var ez=l(H$);YA=u(ez,"inputs"),ez.forEach(t),VA=u(JB," (required)"),JB.forEach(t),XA=h(ob),Zf=o(ob,"TD",{align:!0});var tz=l(Zf);QA=u(tz,"a string to be classified"),tz.forEach(t),ob.forEach(t),ZA=h(Z),Do=o(Z,"TR",{});var lb=l(Do);ep=o(lb,"TD",{align:!0});var sz=l(ep);B$=o(sz,"STRONG",{});var az=l(B$);eD=u(az,"parameters"),az.forEach(t),sz.forEach(t),tD=h(lb),tp=o(lb,"TD",{align:!0});var nz=l(tp);sD=u(nz,"a dict containing the following key:"),nz.forEach(t),lb.forEach(t),aD=h(Z),Oo=o(Z,"TR",{});var ib=l(Oo);sp=o(ib,"TD",{align:!0});var rz=l(sp);nD=u(rz,"aggregation_strategy"),rz.forEach(t),rD=h(ib),x=o(ib,"TD",{align:!0});var C=l(x);oD=u(C,"(Default: "),C$=o(C,"CODE",{});var oz=l(C$);lD=u(oz,"simple"),oz.forEach(t),iD=u(C,"). There are several aggregation strategies: "),uD=o(C,"BR",{}),cD=h(C),G$=o(C,"CODE",{});var lz=l(G$);fD=u(lz,"none"),lz.forEach(t),pD=u(C,": Every token gets classified without further aggregation. "),hD=o(C,"BR",{}),dD=h(C),L$=o(C,"CODE",{});var iz=l(L$);gD=u(iz,"simple"),iz.forEach(t),mD=u(C,": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),$D=o(C,"BR",{}),qD=h(C),M$=o(C,"CODE",{});var uz=l(M$);_D=u(uz,"first"),uz.forEach(t),vD=u(C,": Same as the "),U$=o(C,"CODE",{});var cz=l(U$);yD=u(cz,"simple"),cz.forEach(t),ED=u(C," strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),wD=o(C,"BR",{}),bD=h(C),z$=o(C,"CODE",{});var fz=l(z$);TD=u(fz,"average"),fz.forEach(t),jD=u(C,": Same as the "),K$=o(C,"CODE",{});var pz=l(K$);kD=u(pz,"simple"),pz.forEach(t),AD=u(C," strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),DD=o(C,"BR",{}),OD=h(C),F$=o(C,"CODE",{});var hz=l(F$);PD=u(hz,"max"),hz.forEach(t),RD=u(C,": Same as the "),J$=o(C,"CODE",{});var dz=l(J$);SD=u(dz,"simple"),dz.forEach(t),ND=u(C," strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),C.forEach(t),ib.forEach(t),xD=h(Z),Po=o(Z,"TR",{});var ub=l(Po);ap=o(ub,"TD",{align:!0});var gz=l(ap);W$=o(gz,"STRONG",{});var mz=l(W$);ID=u(mz,"options"),mz.forEach(t),gz.forEach(t),HD=h(ub),np=o(ub,"TD",{align:!0});var $z=l(np);BD=u($z,"a dict containing the following keys:"),$z.forEach(t),ub.forEach(t),CD=h(Z),Ro=o(Z,"TR",{});var cb=l(Ro);rp=o(cb,"TD",{align:!0});var qz=l(rp);GD=u(qz,"use_gpu"),qz.forEach(t),LD=h(cb),Ss=o(cb,"TD",{align:!0});var fb=l(Ss);MD=u(fb,"(Default: "),Y$=o(fb,"CODE",{});var _z=l(Y$);UD=u(_z,"false"),_z.forEach(t),zD=u(fb,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),fb.forEach(t),cb.forEach(t),KD=h(Z),So=o(Z,"TR",{});var pb=l(So);op=o(pb,"TD",{align:!0});var vz=l(op);FD=u(vz,"use_cache"),vz.forEach(t),JD=h(pb),Ns=o(pb,"TD",{align:!0});var hb=l(Ns);WD=u(hb,"(Default: "),V$=o(hb,"CODE",{});var yz=l(V$);YD=u(yz,"true"),yz.forEach(t),VD=u(hb,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),hb.forEach(t),pb.forEach(t),XD=h(Z),No=o(Z,"TR",{});var db=l(No);lp=o(db,"TD",{align:!0});var Ez=l(lp);QD=u(Ez,"wait_for_model"),Ez.forEach(t),ZD=h(db),xs=o(db,"TD",{align:!0});var gb=l(xs);eO=u(gb,"(Default: "),X$=o(gb,"CODE",{});var wz=l(X$);tO=u(wz,"false"),wz.forEach(t),sO=u(gb,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),gb.forEach(t),db.forEach(t),Z.forEach(t),nb.forEach(t),Jv=h(a),ip=o(a,"P",{});var bz=l(ip);aO=u(bz,"Return value is either a dict or a list of dicts if you sent a list of inputs"),bz.forEach(t),Wv=h(a),v(Is.$$.fragment,a),Yv=h(a),Hs=o(a,"TABLE",{});var mb=l(Hs);Q$=o(mb,"THEAD",{});var Tz=l(Q$);xo=o(Tz,"TR",{});var $b=l(xo);up=o($b,"TH",{align:!0});var jz=l(up);nO=u(jz,"Returned values"),jz.forEach(t),rO=h($b),Z$=o($b,"TH",{align:!0}),l(Z$).forEach(t),$b.forEach(t),Tz.forEach(t),oO=h(mb),ae=o(mb,"TBODY",{});var Pe=l(ae);Io=o(Pe,"TR",{});var qb=l(Io);cp=o(qb,"TD",{align:!0});var kz=l(cp);eq=o(kz,"STRONG",{});var Az=l(eq);lO=u(Az,"entity_group"),Az.forEach(t),kz.forEach(t),iO=h(qb),fp=o(qb,"TD",{align:!0});var Dz=l(fp);uO=u(Dz,"The type for the entity being recognized (model specific)."),Dz.forEach(t),qb.forEach(t),cO=h(Pe),Ho=o(Pe,"TR",{});var _b=l(Ho);pp=o(_b,"TD",{align:!0});var Oz=l(pp);tq=o(Oz,"STRONG",{});var Pz=l(tq);fO=u(Pz,"score"),Pz.forEach(t),Oz.forEach(t),pO=h(_b),hp=o(_b,"TD",{align:!0});var Rz=l(hp);hO=u(Rz,"How likely the entity was recognized."),Rz.forEach(t),_b.forEach(t),dO=h(Pe),Bo=o(Pe,"TR",{});var vb=l(Bo);dp=o(vb,"TD",{align:!0});var Sz=l(dp);sq=o(Sz,"STRONG",{});var Nz=l(sq);gO=u(Nz,"word"),Nz.forEach(t),Sz.forEach(t),mO=h(vb),gp=o(vb,"TD",{align:!0});var xz=l(gp);$O=u(xz,"The string that was captured"),xz.forEach(t),vb.forEach(t),qO=h(Pe),Co=o(Pe,"TR",{});var yb=l(Co);mp=o(yb,"TD",{align:!0});var Iz=l(mp);aq=o(Iz,"STRONG",{});var Hz=l(aq);_O=u(Hz,"start"),Hz.forEach(t),Iz.forEach(t),vO=h(yb),Bs=o(yb,"TD",{align:!0});var Eb=l(Bs);yO=u(Eb,"The offset stringwise where the answer is located. Useful to disambiguate if "),nq=o(Eb,"CODE",{});var Bz=l(nq);EO=u(Bz,"word"),Bz.forEach(t),wO=u(Eb," occurs multiple times."),Eb.forEach(t),yb.forEach(t),bO=h(Pe),Go=o(Pe,"TR",{});var wb=l(Go);$p=o(wb,"TD",{align:!0});var Cz=l($p);rq=o(Cz,"STRONG",{});var Gz=l(rq);TO=u(Gz,"end"),Gz.forEach(t),Cz.forEach(t),jO=h(wb),Cs=o(wb,"TD",{align:!0});var bb=l(Cs);kO=u(bb,"The offset stringwise where the answer is located. Useful to disambiguate if "),oq=o(bb,"CODE",{});var Lz=l(oq);AO=u(Lz,"word"),Lz.forEach(t),DO=u(bb," occurs multiple times."),bb.forEach(t),wb.forEach(t),Pe.forEach(t),mb.forEach(t),Vv=h(a),We=o(a,"H3",{class:!0});var Tb=l(We);Gs=o(Tb,"A",{id:!0,class:!0,href:!0});var Mz=l(Gs);lq=o(Mz,"SPAN",{});var Uz=l(lq);v(Lo.$$.fragment,Uz),Uz.forEach(t),Mz.forEach(t),OO=h(Tb),iq=o(Tb,"SPAN",{});var zz=l(iq);PO=u(zz,"Named Entity Recognition (NER) task"),zz.forEach(t),Tb.forEach(t),Xv=h(a),Mo=o(a,"P",{});var WB=l(Mo);RO=u(WB,"See "),qp=o(WB,"A",{href:!0});var Kz=l(qp);SO=u(Kz,"Token-classification task"),Kz.forEach(t),WB.forEach(t),Qv=h(a),Ye=o(a,"H3",{class:!0});var jb=l(Ye);Ls=o(jb,"A",{id:!0,class:!0,href:!0});var Fz=l(Ls);uq=o(Fz,"SPAN",{});var Jz=l(uq);v(Uo.$$.fragment,Jz),Jz.forEach(t),Fz.forEach(t),NO=h(jb),cq=o(jb,"SPAN",{});var Wz=l(cq);xO=u(Wz,"Translation task"),Wz.forEach(t),jb.forEach(t),Zv=h(a),_p=o(a,"P",{});var Yz=l(_p);IO=u(Yz,"This task is well known to translate text from one language to another"),Yz.forEach(t),ey=h(a),v(Ms.$$.fragment,a),ty=h(a),zo=o(a,"P",{});var YB=l(zo);HO=u(YB,"Available with: "),Ko=o(YB,"A",{href:!0,rel:!0});var Vz=l(Ko);BO=u(Vz,"\u{1F917} Transformers"),Vz.forEach(t),YB.forEach(t),sy=h(a),vp=o(a,"P",{});var Xz=l(vp);CO=u(Xz,"Example:"),Xz.forEach(t),ay=h(a),v(Us.$$.fragment,a),ny=h(a),yp=o(a,"P",{});var Qz=l(yp);GO=u(Qz,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Qz.forEach(t),ry=h(a),zs=o(a,"TABLE",{});var kb=l(zs);fq=o(kb,"THEAD",{});var Zz=l(fq);Fo=o(Zz,"TR",{});var Ab=l(Fo);Ep=o(Ab,"TH",{align:!0});var eK=l(Ep);LO=u(eK,"All parameters"),eK.forEach(t),MO=h(Ab),pq=o(Ab,"TH",{align:!0}),l(pq).forEach(t),Ab.forEach(t),Zz.forEach(t),UO=h(kb),ne=o(kb,"TBODY",{});var Re=l(ne);Jo=o(Re,"TR",{});var Db=l(Jo);Wo=o(Db,"TD",{align:!0});var VB=l(Wo);hq=o(VB,"STRONG",{});var tK=l(hq);zO=u(tK,"inputs"),tK.forEach(t),KO=u(VB," (required)"),VB.forEach(t),FO=h(Db),wp=o(Db,"TD",{align:!0});var sK=l(wp);JO=u(sK,"a string to be translated in the original languages"),sK.forEach(t),Db.forEach(t),WO=h(Re),Yo=o(Re,"TR",{});var Ob=l(Yo);bp=o(Ob,"TD",{align:!0});var aK=l(bp);dq=o(aK,"STRONG",{});var nK=l(dq);YO=u(nK,"options"),nK.forEach(t),aK.forEach(t),VO=h(Ob),Tp=o(Ob,"TD",{align:!0});var rK=l(Tp);XO=u(rK,"a dict containing the following keys:"),rK.forEach(t),Ob.forEach(t),QO=h(Re),Vo=o(Re,"TR",{});var Pb=l(Vo);jp=o(Pb,"TD",{align:!0});var oK=l(jp);ZO=u(oK,"use_gpu"),oK.forEach(t),eP=h(Pb),Ks=o(Pb,"TD",{align:!0});var Rb=l(Ks);tP=u(Rb,"(Default: "),gq=o(Rb,"CODE",{});var lK=l(gq);sP=u(lK,"false"),lK.forEach(t),aP=u(Rb,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Rb.forEach(t),Pb.forEach(t),nP=h(Re),Xo=o(Re,"TR",{});var Sb=l(Xo);kp=o(Sb,"TD",{align:!0});var iK=l(kp);rP=u(iK,"use_cache"),iK.forEach(t),oP=h(Sb),Fs=o(Sb,"TD",{align:!0});var Nb=l(Fs);lP=u(Nb,"(Default: "),mq=o(Nb,"CODE",{});var uK=l(mq);iP=u(uK,"true"),uK.forEach(t),uP=u(Nb,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Nb.forEach(t),Sb.forEach(t),cP=h(Re),Qo=o(Re,"TR",{});var xb=l(Qo);Ap=o(xb,"TD",{align:!0});var cK=l(Ap);fP=u(cK,"wait_for_model"),cK.forEach(t),pP=h(xb),Js=o(xb,"TD",{align:!0});var Ib=l(Js);hP=u(Ib,"(Default: "),$q=o(Ib,"CODE",{});var fK=l($q);dP=u(fK,"false"),fK.forEach(t),gP=u(Ib,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Ib.forEach(t),xb.forEach(t),Re.forEach(t),kb.forEach(t),oy=h(a),Dp=o(a,"P",{});var pK=l(Dp);mP=u(pK,"Return value is either a dict or a list of dicts if you sent a list of inputs"),pK.forEach(t),ly=h(a),Ws=o(a,"TABLE",{});var Hb=l(Ws);qq=o(Hb,"THEAD",{});var hK=l(qq);Zo=o(hK,"TR",{});var Bb=l(Zo);Op=o(Bb,"TH",{align:!0});var dK=l(Op);$P=u(dK,"Returned values"),dK.forEach(t),qP=h(Bb),_q=o(Bb,"TH",{align:!0}),l(_q).forEach(t),Bb.forEach(t),hK.forEach(t),_P=h(Hb),vq=o(Hb,"TBODY",{});var gK=l(vq);el=o(gK,"TR",{});var Cb=l(el);Pp=o(Cb,"TD",{align:!0});var mK=l(Pp);yq=o(mK,"STRONG",{});var $K=l(yq);vP=u($K,"translation_text"),$K.forEach(t),mK.forEach(t),yP=h(Cb),Rp=o(Cb,"TD",{align:!0});var qK=l(Rp);EP=u(qK,"The string after translation"),qK.forEach(t),Cb.forEach(t),gK.forEach(t),Hb.forEach(t),iy=h(a),Ve=o(a,"H3",{class:!0});var Gb=l(Ve);Ys=o(Gb,"A",{id:!0,class:!0,href:!0});var _K=l(Ys);Eq=o(_K,"SPAN",{});var vK=l(Eq);v(tl.$$.fragment,vK),vK.forEach(t),_K.forEach(t),wP=h(Gb),wq=o(Gb,"SPAN",{});var yK=l(wq);bP=u(yK,"Zero-Shot Classification task"),yK.forEach(t),Gb.forEach(t),uy=h(a),Sp=o(a,"P",{});var EK=l(Sp);TP=u(EK,`This task is super useful to try out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence, and you get a result.`),EK.forEach(t),cy=h(a),v(Vs.$$.fragment,a),fy=h(a),sl=o(a,"P",{});var XB=l(sl);jP=u(XB,"Available with: "),al=o(XB,"A",{href:!0,rel:!0});var wK=l(al);kP=u(wK,"\u{1F917} Transformers"),wK.forEach(t),XB.forEach(t),py=h(a),Np=o(a,"P",{});var bK=l(Np);AP=u(bK,"Request:"),bK.forEach(t),hy=h(a),v(Xs.$$.fragment,a),dy=h(a),xp=o(a,"P",{});var TK=l(xp);DP=u(TK,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),TK.forEach(t),gy=h(a),Qs=o(a,"TABLE",{});var Lb=l(Qs);bq=o(Lb,"THEAD",{});var jK=l(bq);nl=o(jK,"TR",{});var Mb=l(nl);Ip=o(Mb,"TH",{align:!0});var kK=l(Ip);OP=u(kK,"All parameters"),kK.forEach(t),PP=h(Mb),Tq=o(Mb,"TH",{align:!0}),l(Tq).forEach(t),Mb.forEach(t),jK.forEach(t),RP=h(Lb),z=o(Lb,"TBODY",{});var Y=l(z);rl=o(Y,"TR",{});var Ub=l(rl);ol=o(Ub,"TD",{align:!0});var QB=l(ol);jq=o(QB,"STRONG",{});var AK=l(jq);SP=u(AK,"inputs"),AK.forEach(t),NP=u(QB," (required)"),QB.forEach(t),xP=h(Ub),Hp=o(Ub,"TD",{align:!0});var DK=l(Hp);IP=u(DK,"a string or list of strings"),DK.forEach(t),Ub.forEach(t),HP=h(Y),ll=o(Y,"TR",{});var zb=l(ll);il=o(zb,"TD",{align:!0});var ZB=l(il);kq=o(ZB,"STRONG",{});var OK=l(kq);BP=u(OK,"parameters"),OK.forEach(t),CP=u(ZB," (required)"),ZB.forEach(t),GP=h(zb),Bp=o(zb,"TD",{align:!0});var PK=l(Bp);LP=u(PK,"a dict containing the following keys:"),PK.forEach(t),zb.forEach(t),MP=h(Y),ul=o(Y,"TR",{});var Kb=l(ul);Cp=o(Kb,"TD",{align:!0});var RK=l(Cp);UP=u(RK,"candidate_labels (required)"),RK.forEach(t),zP=h(Kb),je=o(Kb,"TD",{align:!0});var ag=l(je);KP=u(ag,"a list of strings that are potential classes for "),Aq=o(ag,"CODE",{});var SK=l(Aq);FP=u(SK,"inputs"),SK.forEach(t),JP=u(ag,". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),Dq=o(ag,"CODE",{});var NK=l(Dq);WP=u(NK,"multi_label=True"),NK.forEach(t),YP=u(ag," and do the scaling on your end. )"),ag.forEach(t),Kb.forEach(t),VP=h(Y),cl=o(Y,"TR",{});var Fb=l(cl);Gp=o(Fb,"TD",{align:!0});var xK=l(Gp);XP=u(xK,"multi_label"),xK.forEach(t),QP=h(Fb),Zs=o(Fb,"TD",{align:!0});var Jb=l(Zs);ZP=u(Jb,"(Default: "),Oq=o(Jb,"CODE",{});var IK=l(Oq);eR=u(IK,"false"),IK.forEach(t),tR=u(Jb,") Boolean that is set to True if classes can overlap"),Jb.forEach(t),Fb.forEach(t),sR=h(Y),fl=o(Y,"TR",{});var Wb=l(fl);Lp=o(Wb,"TD",{align:!0});var HK=l(Lp);Pq=o(HK,"STRONG",{});var BK=l(Pq);aR=u(BK,"options"),BK.forEach(t),HK.forEach(t),nR=h(Wb),Mp=o(Wb,"TD",{align:!0});var CK=l(Mp);rR=u(CK,"a dict containing the following keys:"),CK.forEach(t),Wb.forEach(t),oR=h(Y),pl=o(Y,"TR",{});var Yb=l(pl);Up=o(Yb,"TD",{align:!0});var GK=l(Up);lR=u(GK,"use_gpu"),GK.forEach(t),iR=h(Yb),ea=o(Yb,"TD",{align:!0});var Vb=l(ea);uR=u(Vb,"(Default: "),Rq=o(Vb,"CODE",{});var LK=l(Rq);cR=u(LK,"false"),LK.forEach(t),fR=u(Vb,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Vb.forEach(t),Yb.forEach(t),pR=h(Y),hl=o(Y,"TR",{});var Xb=l(hl);zp=o(Xb,"TD",{align:!0});var MK=l(zp);hR=u(MK,"use_cache"),MK.forEach(t),dR=h(Xb),ta=o(Xb,"TD",{align:!0});var Qb=l(ta);gR=u(Qb,"(Default: "),Sq=o(Qb,"CODE",{});var UK=l(Sq);mR=u(UK,"true"),UK.forEach(t),$R=u(Qb,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Qb.forEach(t),Xb.forEach(t),qR=h(Y),dl=o(Y,"TR",{});var Zb=l(dl);Kp=o(Zb,"TD",{align:!0});var zK=l(Kp);_R=u(zK,"wait_for_model"),zK.forEach(t),vR=h(Zb),sa=o(Zb,"TD",{align:!0});var e3=l(sa);yR=u(e3,"(Default: "),Nq=o(e3,"CODE",{});var KK=l(Nq);ER=u(KK,"false"),KK.forEach(t),wR=u(e3,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),e3.forEach(t),Zb.forEach(t),Y.forEach(t),Lb.forEach(t),my=h(a),Fp=o(a,"P",{});var FK=l(Fp);bR=u(FK,"Return value is either a dict or a list of dicts if you sent a list of inputs"),FK.forEach(t),$y=h(a),Jp=o(a,"P",{});var JK=l(Jp);TR=u(JK,"Response:"),JK.forEach(t),qy=h(a),v(aa.$$.fragment,a),_y=h(a),na=o(a,"TABLE",{});var t3=l(na);xq=o(t3,"THEAD",{});var WK=l(xq);gl=o(WK,"TR",{});var s3=l(gl);Wp=o(s3,"TH",{align:!0});var YK=l(Wp);jR=u(YK,"Returned values"),YK.forEach(t),kR=h(s3),Iq=o(s3,"TH",{align:!0}),l(Iq).forEach(t),s3.forEach(t),WK.forEach(t),AR=h(t3),Xe=o(t3,"TBODY",{});var ng=l(Xe);ml=o(ng,"TR",{});var a3=l(ml);Yp=o(a3,"TD",{align:!0});var VK=l(Yp);Hq=o(VK,"STRONG",{});var XK=l(Hq);DR=u(XK,"sequence"),XK.forEach(t),VK.forEach(t),OR=h(a3),Vp=o(a3,"TD",{align:!0});var QK=l(Vp);PR=u(QK,"The string sent as an input"),QK.forEach(t),a3.forEach(t),RR=h(ng),$l=o(ng,"TR",{});var n3=l($l);Xp=o(n3,"TD",{align:!0});var ZK=l(Xp);Bq=o(ZK,"STRONG",{});var eF=l(Bq);SR=u(eF,"labels"),eF.forEach(t),ZK.forEach(t),NR=h(n3),Qp=o(n3,"TD",{align:!0});var tF=l(Qp);xR=u(tF,"The list of strings for labels that you sent (in order)"),tF.forEach(t),n3.forEach(t),IR=h(ng),ql=o(ng,"TR",{});var r3=l(ql);Zp=o(r3,"TD",{align:!0});var sF=l(Zp);Cq=o(sF,"STRONG",{});var aF=l(Cq);HR=u(aF,"scores"),aF.forEach(t),sF.forEach(t),BR=h(r3),ra=o(r3,"TD",{align:!0});var o3=l(ra);CR=u(o3,"a list of floats that correspond the the probability of label, in the same order as "),Gq=o(o3,"CODE",{});var nF=l(Gq);GR=u(nF,"labels"),nF.forEach(t),LR=u(o3,"."),o3.forEach(t),r3.forEach(t),ng.forEach(t),t3.forEach(t),vy=h(a),Qe=o(a,"H3",{class:!0});var l3=l(Qe);oa=o(l3,"A",{id:!0,class:!0,href:!0});var rF=l(oa);Lq=o(rF,"SPAN",{});var oF=l(Lq);v(_l.$$.fragment,oF),oF.forEach(t),rF.forEach(t),MR=h(l3),Mq=o(l3,"SPAN",{});var lF=l(Mq);UR=u(lF,"Conversational task"),lF.forEach(t),l3.forEach(t),yy=h(a),eh=o(a,"P",{});var iF=l(eh);zR=u(iF,`This task corresponds to any chatbot like structure. Models tend to have
shorter max_length, so please check with caution when using a given
model if you need long range dependency or not.`),iF.forEach(t),Ey=h(a),v(la.$$.fragment,a),wy=h(a),vl=o(a,"P",{});var eC=l(vl);KR=u(eC,"Available with: "),yl=o(eC,"A",{href:!0,rel:!0});var uF=l(yl);FR=u(uF,"\u{1F917} Transformers"),uF.forEach(t),eC.forEach(t),by=h(a),th=o(a,"P",{});var cF=l(th);JR=u(cF,"Example:"),cF.forEach(t),Ty=h(a),v(ia.$$.fragment,a),jy=h(a),sh=o(a,"P",{});var fF=l(sh);WR=u(fF,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),fF.forEach(t),ky=h(a),ua=o(a,"TABLE",{});var i3=l(ua);Uq=o(i3,"THEAD",{});var pF=l(Uq);El=o(pF,"TR",{});var u3=l(El);ah=o(u3,"TH",{align:!0});var hF=l(ah);YR=u(hF,"All parameters"),hF.forEach(t),VR=h(u3),zq=o(u3,"TH",{align:!0}),l(zq).forEach(t),u3.forEach(t),pF.forEach(t),XR=h(i3),N=o(i3,"TBODY",{});var H=l(N);wl=o(H,"TR",{});var c3=l(wl);bl=o(c3,"TD",{align:!0});var tC=l(bl);Kq=o(tC,"STRONG",{});var dF=l(Kq);QR=u(dF,"inputs"),dF.forEach(t),ZR=u(tC," (required)"),tC.forEach(t),eS=h(c3),Fq=o(c3,"TD",{align:!0}),l(Fq).forEach(t),c3.forEach(t),tS=h(H),Tl=o(H,"TR",{});var f3=l(Tl);nh=o(f3,"TD",{align:!0});var gF=l(nh);sS=u(gF,"text (required)"),gF.forEach(t),aS=h(f3),rh=o(f3,"TD",{align:!0});var mF=l(rh);nS=u(mF,"The last input from the user in the conversation."),mF.forEach(t),f3.forEach(t),rS=h(H),jl=o(H,"TR",{});var p3=l(jl);oh=o(p3,"TD",{align:!0});var $F=l(oh);oS=u($F,"generated_responses"),$F.forEach(t),lS=h(p3),lh=o(p3,"TD",{align:!0});var qF=l(lh);iS=u(qF,"A list of strings corresponding to the earlier replies from the model."),qF.forEach(t),p3.forEach(t),uS=h(H),kl=o(H,"TR",{});var h3=l(kl);ih=o(h3,"TD",{align:!0});var _F=l(ih);cS=u(_F,"past_user_inputs"),_F.forEach(t),fS=h(h3),ca=o(h3,"TD",{align:!0});var d3=l(ca);pS=u(d3,"A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),Jq=o(d3,"CODE",{});var vF=l(Jq);hS=u(vF,"generated_responses"),vF.forEach(t),dS=u(d3,"."),d3.forEach(t),h3.forEach(t),gS=h(H),Al=o(H,"TR",{});var g3=l(Al);uh=o(g3,"TD",{align:!0});var yF=l(uh);Wq=o(yF,"STRONG",{});var EF=l(Wq);mS=u(EF,"parameters"),EF.forEach(t),yF.forEach(t),$S=h(g3),ch=o(g3,"TD",{align:!0});var wF=l(ch);qS=u(wF,"a dict containing the following keys:"),wF.forEach(t),g3.forEach(t),_S=h(H),Dl=o(H,"TR",{});var m3=l(Dl);fh=o(m3,"TD",{align:!0});var bF=l(fh);vS=u(bF,"min_length"),bF.forEach(t),yS=h(m3),ke=o(m3,"TD",{align:!0});var rg=l(ke);ES=u(rg,"(Default: "),Yq=o(rg,"CODE",{});var TF=l(Yq);wS=u(TF,"None"),TF.forEach(t),bS=u(rg,"). Integer to define the minimum length "),Vq=o(rg,"STRONG",{});var jF=l(Vq);TS=u(jF,"in tokens"),jF.forEach(t),jS=u(rg," of the output summary."),rg.forEach(t),m3.forEach(t),kS=h(H),Ol=o(H,"TR",{});var $3=l(Ol);ph=o($3,"TD",{align:!0});var kF=l(ph);AS=u(kF,"max_length"),kF.forEach(t),DS=h($3),Ae=o($3,"TD",{align:!0});var og=l(Ae);OS=u(og,"(Default: "),Xq=o(og,"CODE",{});var AF=l(Xq);PS=u(AF,"None"),AF.forEach(t),RS=u(og,"). Integer to define the maximum length "),Qq=o(og,"STRONG",{});var DF=l(Qq);SS=u(DF,"in tokens"),DF.forEach(t),NS=u(og," of the output summary."),og.forEach(t),$3.forEach(t),xS=h(H),Pl=o(H,"TR",{});var q3=l(Pl);hh=o(q3,"TD",{align:!0});var OF=l(hh);IS=u(OF,"top_k"),OF.forEach(t),HS=h(q3),De=o(q3,"TD",{align:!0});var lg=l(De);BS=u(lg,"(Default: "),Zq=o(lg,"CODE",{});var PF=l(Zq);CS=u(PF,"None"),PF.forEach(t),GS=u(lg,"). Integer to define the top tokens considered within the "),e_=o(lg,"CODE",{});var RF=l(e_);LS=u(RF,"sample"),RF.forEach(t),MS=u(lg," operation to create new text."),lg.forEach(t),q3.forEach(t),US=h(H),Rl=o(H,"TR",{});var _3=l(Rl);dh=o(_3,"TD",{align:!0});var SF=l(dh);zS=u(SF,"top_p"),SF.forEach(t),KS=h(_3),pe=o(_3,"TD",{align:!0});var dn=l(pe);FS=u(dn,"(Default: "),t_=o(dn,"CODE",{});var NF=l(t_);JS=u(NF,"None"),NF.forEach(t),WS=u(dn,"). Float to define the tokens that are within the "),s_=o(dn,"CODE",{});var xF=l(s_);YS=u(xF,"sample"),xF.forEach(t),VS=u(dn," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),a_=o(dn,"CODE",{});var IF=l(a_);XS=u(IF,"top_p"),IF.forEach(t),QS=u(dn,"."),dn.forEach(t),_3.forEach(t),ZS=h(H),Sl=o(H,"TR",{});var v3=l(Sl);gh=o(v3,"TD",{align:!0});var HF=l(gh);eN=u(HF,"temperature"),HF.forEach(t),tN=h(v3),he=o(v3,"TD",{align:!0});var gn=l(he);sN=u(gn,"(Default: "),n_=o(gn,"CODE",{});var BF=l(n_);aN=u(BF,"1.0"),BF.forEach(t),nN=u(gn,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),r_=o(gn,"CODE",{});var CF=l(r_);rN=u(CF,"0"),CF.forEach(t),oN=u(gn," means always take the highest score, "),o_=o(gn,"CODE",{});var GF=l(o_);lN=u(GF,"100.0"),GF.forEach(t),iN=u(gn," is getting closer to uniform probability."),gn.forEach(t),v3.forEach(t),uN=h(H),Nl=o(H,"TR",{});var y3=l(Nl);mh=o(y3,"TD",{align:!0});var LF=l(mh);cN=u(LF,"repetition_penalty"),LF.forEach(t),fN=h(y3),fa=o(y3,"TD",{align:!0});var E3=l(fa);pN=u(E3,"(Default: "),l_=o(E3,"CODE",{});var MF=l(l_);hN=u(MF,"None"),MF.forEach(t),dN=u(E3,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),E3.forEach(t),y3.forEach(t),gN=h(H),xl=o(H,"TR",{});var w3=l(xl);$h=o(w3,"TD",{align:!0});var UF=l($h);mN=u(UF,"max_time"),UF.forEach(t),$N=h(w3),pa=o(w3,"TD",{align:!0});var b3=l(pa);qN=u(b3,"(Default: "),i_=o(b3,"CODE",{});var zF=l(i_);_N=u(zF,"None"),zF.forEach(t),vN=u(b3,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),b3.forEach(t),w3.forEach(t),yN=h(H),Il=o(H,"TR",{});var T3=l(Il);qh=o(T3,"TD",{align:!0});var KF=l(qh);u_=o(KF,"STRONG",{});var FF=l(u_);EN=u(FF,"options"),FF.forEach(t),KF.forEach(t),wN=h(T3),_h=o(T3,"TD",{align:!0});var JF=l(_h);bN=u(JF,"a dict containing the following keys:"),JF.forEach(t),T3.forEach(t),TN=h(H),Hl=o(H,"TR",{});var j3=l(Hl);vh=o(j3,"TD",{align:!0});var WF=l(vh);jN=u(WF,"use_gpu"),WF.forEach(t),kN=h(j3),ha=o(j3,"TD",{align:!0});var k3=l(ha);AN=u(k3,"(Default: "),c_=o(k3,"CODE",{});var YF=l(c_);DN=u(YF,"false"),YF.forEach(t),ON=u(k3,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),k3.forEach(t),j3.forEach(t),PN=h(H),Bl=o(H,"TR",{});var A3=l(Bl);yh=o(A3,"TD",{align:!0});var VF=l(yh);RN=u(VF,"use_cache"),VF.forEach(t),SN=h(A3),da=o(A3,"TD",{align:!0});var D3=l(da);NN=u(D3,"(Default: "),f_=o(D3,"CODE",{});var XF=l(f_);xN=u(XF,"true"),XF.forEach(t),IN=u(D3,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),D3.forEach(t),A3.forEach(t),HN=h(H),Cl=o(H,"TR",{});var O3=l(Cl);Eh=o(O3,"TD",{align:!0});var QF=l(Eh);BN=u(QF,"wait_for_model"),QF.forEach(t),CN=h(O3),ga=o(O3,"TD",{align:!0});var P3=l(ga);GN=u(P3,"(Default: "),p_=o(P3,"CODE",{});var ZF=l(p_);LN=u(ZF,"false"),ZF.forEach(t),MN=u(P3,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),P3.forEach(t),O3.forEach(t),H.forEach(t),i3.forEach(t),Ay=h(a),wh=o(a,"P",{});var eJ=l(wh);UN=u(eJ,"Return value is either a dict or a list of dicts if you sent a list of inputs"),eJ.forEach(t),Dy=h(a),ma=o(a,"TABLE",{});var R3=l(ma);h_=o(R3,"THEAD",{});var tJ=l(h_);Gl=o(tJ,"TR",{});var S3=l(Gl);bh=o(S3,"TH",{align:!0});var sJ=l(bh);zN=u(sJ,"Returned values"),sJ.forEach(t),KN=h(S3),d_=o(S3,"TH",{align:!0}),l(d_).forEach(t),S3.forEach(t),tJ.forEach(t),FN=h(R3),$e=o(R3,"TBODY",{});var mn=l($e);Ll=o(mn,"TR",{});var N3=l(Ll);Th=o(N3,"TD",{align:!0});var aJ=l(Th);g_=o(aJ,"STRONG",{});var nJ=l(g_);JN=u(nJ,"generated_text"),nJ.forEach(t),aJ.forEach(t),WN=h(N3),jh=o(N3,"TD",{align:!0});var rJ=l(jh);YN=u(rJ,"The answer of the bot"),rJ.forEach(t),N3.forEach(t),VN=h(mn),Ml=o(mn,"TR",{});var x3=l(Ml);kh=o(x3,"TD",{align:!0});var oJ=l(kh);m_=o(oJ,"STRONG",{});var lJ=l(m_);XN=u(lJ,"conversation"),lJ.forEach(t),oJ.forEach(t),QN=h(x3),Ah=o(x3,"TD",{align:!0});var iJ=l(Ah);ZN=u(iJ,"A facility dictionnary to send back for the next input (with the new user input addition)."),iJ.forEach(t),x3.forEach(t),ex=h(mn),Ul=o(mn,"TR",{});var I3=l(Ul);Dh=o(I3,"TD",{align:!0});var uJ=l(Dh);tx=u(uJ,"past_user_inputs"),uJ.forEach(t),sx=h(I3),Oh=o(I3,"TD",{align:!0});var cJ=l(Oh);ax=u(cJ,"List of strings. The last inputs from the user in the conversation, <em>after the model has run."),cJ.forEach(t),I3.forEach(t),nx=h(mn),zl=o(mn,"TR",{});var H3=l(zl);Ph=o(H3,"TD",{align:!0});var fJ=l(Ph);rx=u(fJ,"generated_responses"),fJ.forEach(t),ox=h(H3),Rh=o(H3,"TD",{align:!0});var pJ=l(Rh);lx=u(pJ,"List of strings. The last outputs from the model in the conversation, <em>after the model has run."),pJ.forEach(t),H3.forEach(t),mn.forEach(t),R3.forEach(t),Oy=h(a),Ze=o(a,"H3",{class:!0});var B3=l(Ze);$a=o(B3,"A",{id:!0,class:!0,href:!0});var hJ=l($a);$_=o(hJ,"SPAN",{});var dJ=l($_);v(Kl.$$.fragment,dJ),dJ.forEach(t),hJ.forEach(t),ix=h(B3),q_=o(B3,"SPAN",{});var gJ=l(q_);ux=u(gJ,"Feature Extraction task"),gJ.forEach(t),B3.forEach(t),Py=h(a),Sh=o(a,"P",{});var mJ=l(Sh);cx=u(mJ,`This task reads some text and outputs raw float values, that are usually
consumed as part of a semantic database/semantic search.`),mJ.forEach(t),Ry=h(a),v(qa.$$.fragment,a),Sy=h(a),et=o(a,"P",{});var c2=l(et);fx=u(c2,"Available with: "),Fl=o(c2,"A",{href:!0,rel:!0});var $J=l(Fl);px=u($J,"\u{1F917} Transformers"),$J.forEach(t),hx=h(c2),Jl=o(c2,"A",{href:!0,rel:!0});var qJ=l(Jl);dx=u(qJ,"Sentence-transformers"),qJ.forEach(t),c2.forEach(t),Ny=h(a),Nh=o(a,"P",{});var _J=l(Nh);gx=u(_J,"Request:"),_J.forEach(t),xy=h(a),_a=o(a,"TABLE",{});var C3=l(_a);__=o(C3,"THEAD",{});var vJ=l(__);Wl=o(vJ,"TR",{});var G3=l(Wl);xh=o(G3,"TH",{align:!0});var yJ=l(xh);mx=u(yJ,"All parameters"),yJ.forEach(t),$x=h(G3),v_=o(G3,"TH",{align:!0}),l(v_).forEach(t),G3.forEach(t),vJ.forEach(t),qx=h(C3),re=o(C3,"TBODY",{});var Se=l(re);Yl=o(Se,"TR",{});var L3=l(Yl);Vl=o(L3,"TD",{align:!0});var sC=l(Vl);y_=o(sC,"STRONG",{});var EJ=l(y_);_x=u(EJ,"inputs"),EJ.forEach(t),vx=u(sC," (required):"),sC.forEach(t),yx=h(L3),Ih=o(L3,"TD",{align:!0});var wJ=l(Ih);Ex=u(wJ,"a string or a list of strings to get the features from."),wJ.forEach(t),L3.forEach(t),wx=h(Se),Xl=o(Se,"TR",{});var M3=l(Xl);Hh=o(M3,"TD",{align:!0});var bJ=l(Hh);E_=o(bJ,"STRONG",{});var TJ=l(E_);bx=u(TJ,"options"),TJ.forEach(t),bJ.forEach(t),Tx=h(M3),Bh=o(M3,"TD",{align:!0});var jJ=l(Bh);jx=u(jJ,"a dict containing the following keys:"),jJ.forEach(t),M3.forEach(t),kx=h(Se),Ql=o(Se,"TR",{});var U3=l(Ql);Ch=o(U3,"TD",{align:!0});var kJ=l(Ch);Ax=u(kJ,"use_gpu"),kJ.forEach(t),Dx=h(U3),va=o(U3,"TD",{align:!0});var z3=l(va);Ox=u(z3,"(Default: "),w_=o(z3,"CODE",{});var AJ=l(w_);Px=u(AJ,"false"),AJ.forEach(t),Rx=u(z3,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),z3.forEach(t),U3.forEach(t),Sx=h(Se),Zl=o(Se,"TR",{});var K3=l(Zl);Gh=o(K3,"TD",{align:!0});var DJ=l(Gh);Nx=u(DJ,"use_cache"),DJ.forEach(t),xx=h(K3),ya=o(K3,"TD",{align:!0});var F3=l(ya);Ix=u(F3,"(Default: "),b_=o(F3,"CODE",{});var OJ=l(b_);Hx=u(OJ,"true"),OJ.forEach(t),Bx=u(F3,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),F3.forEach(t),K3.forEach(t),Cx=h(Se),ei=o(Se,"TR",{});var J3=l(ei);Lh=o(J3,"TD",{align:!0});var PJ=l(Lh);Gx=u(PJ,"wait_for_model"),PJ.forEach(t),Lx=h(J3),Ea=o(J3,"TD",{align:!0});var W3=l(Ea);Mx=u(W3,"(Default: "),T_=o(W3,"CODE",{});var RJ=l(T_);Ux=u(RJ,"false"),RJ.forEach(t),zx=u(W3,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),W3.forEach(t),J3.forEach(t),Se.forEach(t),C3.forEach(t),Iy=h(a),Mh=o(a,"P",{});var SJ=l(Mh);Kx=u(SJ,"Return value is either a dict or a list of dicts if you sent a list of inputs"),SJ.forEach(t),Hy=h(a),wa=o(a,"TABLE",{});var Y3=l(wa);j_=o(Y3,"THEAD",{});var NJ=l(j_);ti=o(NJ,"TR",{});var V3=l(ti);Uh=o(V3,"TH",{align:!0});var xJ=l(Uh);Fx=u(xJ,"Returned values"),xJ.forEach(t),Jx=h(V3),k_=o(V3,"TH",{align:!0}),l(k_).forEach(t),V3.forEach(t),NJ.forEach(t),Wx=h(Y3),A_=o(Y3,"TBODY",{});var IJ=l(A_);si=o(IJ,"TR",{});var X3=l(si);zh=o(X3,"TD",{align:!0});var HJ=l(zh);D_=o(HJ,"STRONG",{});var BJ=l(D_);Yx=u(BJ,"A list of float (or list of list of floats)"),BJ.forEach(t),HJ.forEach(t),Vx=h(X3),Kh=o(X3,"TD",{align:!0});var CJ=l(Kh);Xx=u(CJ,"The numbers that are the representation features of the input."),CJ.forEach(t),X3.forEach(t),IJ.forEach(t),Y3.forEach(t),By=h(a),Fh=o(a,"SMALL",{});var GJ=l(Fh);Qx=u(GJ,`Returned values are a list of floats, or a list of list of floats (depending
  on if you sent a string or a list of string, and if the automatic reduction,
  usually mean_pooling for instance was applied for you or not. This should be
  explained on the model's README.`),GJ.forEach(t),Cy=h(a),tt=o(a,"H2",{class:!0});var Q3=l(tt);ba=o(Q3,"A",{id:!0,class:!0,href:!0});var LJ=l(ba);O_=o(LJ,"SPAN",{});var MJ=l(O_);v(ai.$$.fragment,MJ),MJ.forEach(t),LJ.forEach(t),Zx=h(Q3),P_=o(Q3,"SPAN",{});var UJ=l(P_);eI=u(UJ,"Audio"),UJ.forEach(t),Q3.forEach(t),Gy=h(a),st=o(a,"H3",{class:!0});var Z3=l(st);Ta=o(Z3,"A",{id:!0,class:!0,href:!0});var zJ=l(Ta);R_=o(zJ,"SPAN",{});var KJ=l(R_);v(ni.$$.fragment,KJ),KJ.forEach(t),zJ.forEach(t),tI=h(Z3),S_=o(Z3,"SPAN",{});var FJ=l(S_);sI=u(FJ,"Automatic Speech Recognition task"),FJ.forEach(t),Z3.forEach(t),Ly=h(a),Jh=o(a,"P",{});var JJ=l(Jh);aI=u(JJ,`This task reads some audio input and outputs the said words within the
audio files.`),JJ.forEach(t),My=h(a),v(ja.$$.fragment,a),Uy=h(a),v(ka.$$.fragment,a),zy=h(a),qe=o(a,"P",{});var $u=l(qe);nI=u($u,"Available with: "),ri=o($u,"A",{href:!0,rel:!0});var WJ=l(ri);rI=u(WJ,"\u{1F917} Transformers"),WJ.forEach(t),oI=h($u),oi=o($u,"A",{href:!0,rel:!0});var YJ=l(oi);lI=u(YJ,"ESPnet"),YJ.forEach(t),iI=u($u,` and
`),li=o($u,"A",{href:!0,rel:!0});var VJ=l(li);uI=u(VJ,"SpeechBrain"),VJ.forEach(t),$u.forEach(t),Ky=h(a),Wh=o(a,"P",{});var XJ=l(Wh);cI=u(XJ,"Request:"),XJ.forEach(t),Fy=h(a),v(Aa.$$.fragment,a),Jy=h(a),Yh=o(a,"P",{});var QJ=l(Yh);fI=u(QJ,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),QJ.forEach(t),Wy=h(a),Da=o(a,"TABLE",{});var eT=l(Da);N_=o(eT,"THEAD",{});var ZJ=l(N_);ii=o(ZJ,"TR",{});var tT=l(ii);Vh=o(tT,"TH",{align:!0});var eW=l(Vh);pI=u(eW,"All parameters"),eW.forEach(t),hI=h(tT),x_=o(tT,"TH",{align:!0}),l(x_).forEach(t),tT.forEach(t),ZJ.forEach(t),dI=h(eT),I_=o(eT,"TBODY",{});var tW=l(I_);ui=o(tW,"TR",{});var sT=l(ui);ci=o(sT,"TD",{align:!0});var aC=l(ci);H_=o(aC,"STRONG",{});var sW=l(H_);gI=u(sW,"no parameter"),sW.forEach(t),mI=u(aC," (required)"),aC.forEach(t),$I=h(sT),Xh=o(sT,"TD",{align:!0});var aW=l(Xh);qI=u(aW,"a binary representation of the audio file. No other parameters are currently allowed."),aW.forEach(t),sT.forEach(t),tW.forEach(t),eT.forEach(t),Yy=h(a),Qh=o(a,"P",{});var nW=l(Qh);_I=u(nW,"Return value is either a dict or a list of dicts if you sent a list of inputs"),nW.forEach(t),Vy=h(a),Zh=o(a,"P",{});var rW=l(Zh);vI=u(rW,"Response:"),rW.forEach(t),Xy=h(a),v(Oa.$$.fragment,a),Qy=h(a),Pa=o(a,"TABLE",{});var aT=l(Pa);B_=o(aT,"THEAD",{});var oW=l(B_);fi=o(oW,"TR",{});var nT=l(fi);ed=o(nT,"TH",{align:!0});var lW=l(ed);yI=u(lW,"Returned values"),lW.forEach(t),EI=h(nT),C_=o(nT,"TH",{align:!0}),l(C_).forEach(t),nT.forEach(t),oW.forEach(t),wI=h(aT),G_=o(aT,"TBODY",{});var iW=l(G_);pi=o(iW,"TR",{});var rT=l(pi);td=o(rT,"TD",{align:!0});var uW=l(td);L_=o(uW,"STRONG",{});var cW=l(L_);bI=u(cW,"text"),cW.forEach(t),uW.forEach(t),TI=h(rT),sd=o(rT,"TD",{align:!0});var fW=l(sd);jI=u(fW,"The string that was recognized within the audio file."),fW.forEach(t),rT.forEach(t),iW.forEach(t),aT.forEach(t),Zy=h(a),at=o(a,"H3",{class:!0});var oT=l(at);Ra=o(oT,"A",{id:!0,class:!0,href:!0});var pW=l(Ra);M_=o(pW,"SPAN",{});var hW=l(M_);v(hi.$$.fragment,hW),hW.forEach(t),pW.forEach(t),kI=h(oT),U_=o(oT,"SPAN",{});var dW=l(U_);AI=u(dW,"Audio Classification task"),dW.forEach(t),oT.forEach(t),eE=h(a),ad=o(a,"P",{});var gW=l(ad);DI=u(gW,"This task reads some audio input and outputs the likelihood of classes."),gW.forEach(t),tE=h(a),v(Sa.$$.fragment,a),sE=h(a),nt=o(a,"P",{});var f2=l(nt);OI=u(f2,"Available with: "),di=o(f2,"A",{href:!0,rel:!0});var mW=l(di);PI=u(mW,"\u{1F917} Transformers"),mW.forEach(t),RI=h(f2),gi=o(f2,"A",{href:!0,rel:!0});var $W=l(gi);SI=u($W,"SpeechBrain"),$W.forEach(t),f2.forEach(t),aE=h(a),nd=o(a,"P",{});var qW=l(nd);NI=u(qW,"Request:"),qW.forEach(t),nE=h(a),v(Na.$$.fragment,a),rE=h(a),rd=o(a,"P",{});var _W=l(rd);xI=u(_W,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),_W.forEach(t),oE=h(a),xa=o(a,"TABLE",{});var lT=l(xa);z_=o(lT,"THEAD",{});var vW=l(z_);mi=o(vW,"TR",{});var iT=l(mi);od=o(iT,"TH",{align:!0});var yW=l(od);II=u(yW,"All parameters"),yW.forEach(t),HI=h(iT),K_=o(iT,"TH",{align:!0}),l(K_).forEach(t),iT.forEach(t),vW.forEach(t),BI=h(lT),F_=o(lT,"TBODY",{});var EW=l(F_);$i=o(EW,"TR",{});var uT=l($i);qi=o(uT,"TD",{align:!0});var nC=l(qi);J_=o(nC,"STRONG",{});var wW=l(J_);CI=u(wW,"no parameter"),wW.forEach(t),GI=u(nC," (required)"),nC.forEach(t),LI=h(uT),ld=o(uT,"TD",{align:!0});var bW=l(ld);MI=u(bW,"a binary representation of the audio file. No other parameters are currently allowed."),bW.forEach(t),uT.forEach(t),EW.forEach(t),lT.forEach(t),lE=h(a),id=o(a,"P",{});var TW=l(id);UI=u(TW,"Return value is a dict"),TW.forEach(t),iE=h(a),v(Ia.$$.fragment,a),uE=h(a),Ha=o(a,"TABLE",{});var cT=l(Ha);W_=o(cT,"THEAD",{});var jW=l(W_);_i=o(jW,"TR",{});var fT=l(_i);ud=o(fT,"TH",{align:!0});var kW=l(ud);zI=u(kW,"Returned values"),kW.forEach(t),KI=h(fT),Y_=o(fT,"TH",{align:!0}),l(Y_).forEach(t),fT.forEach(t),jW.forEach(t),FI=h(cT),vi=o(cT,"TBODY",{});var pT=l(vi);yi=o(pT,"TR",{});var hT=l(yi);cd=o(hT,"TD",{align:!0});var AW=l(cd);V_=o(AW,"STRONG",{});var DW=l(V_);JI=u(DW,"label"),DW.forEach(t),AW.forEach(t),WI=h(hT),fd=o(hT,"TD",{align:!0});var OW=l(fd);YI=u(OW,"The label for the class (model specific)"),OW.forEach(t),hT.forEach(t),VI=h(pT),Ei=o(pT,"TR",{});var dT=l(Ei);pd=o(dT,"TD",{align:!0});var PW=l(pd);X_=o(PW,"STRONG",{});var RW=l(X_);XI=u(RW,"score"),RW.forEach(t),PW.forEach(t),QI=h(dT),hd=o(dT,"TD",{align:!0});var SW=l(hd);ZI=u(SW,"A float that represents how likely it is that the audio file belongs to this class."),SW.forEach(t),dT.forEach(t),pT.forEach(t),cT.forEach(t),cE=h(a),rt=o(a,"H2",{class:!0});var gT=l(rt);Ba=o(gT,"A",{id:!0,class:!0,href:!0});var NW=l(Ba);Q_=o(NW,"SPAN",{});var xW=l(Q_);v(wi.$$.fragment,xW),xW.forEach(t),NW.forEach(t),eH=h(gT),Z_=o(gT,"SPAN",{});var IW=l(Z_);tH=u(IW,"Computer Vision"),IW.forEach(t),gT.forEach(t),fE=h(a),ot=o(a,"H3",{class:!0});var mT=l(ot);Ca=o(mT,"A",{id:!0,class:!0,href:!0});var HW=l(Ca);e1=o(HW,"SPAN",{});var BW=l(e1);v(bi.$$.fragment,BW),BW.forEach(t),HW.forEach(t),sH=h(mT),t1=o(mT,"SPAN",{});var CW=l(t1);aH=u(CW,"Image Classification task"),CW.forEach(t),mT.forEach(t),pE=h(a),dd=o(a,"P",{});var GW=l(dd);nH=u(GW,"This task reads some image input and outputs the likelihood of classes."),GW.forEach(t),hE=h(a),v(Ga.$$.fragment,a),dE=h(a),Ti=o(a,"P",{});var rC=l(Ti);rH=u(rC,"Available with: "),ji=o(rC,"A",{href:!0,rel:!0});var LW=l(ji);oH=u(LW,"\u{1F917} Transformers"),LW.forEach(t),rC.forEach(t),gE=h(a),gd=o(a,"P",{});var MW=l(gd);lH=u(MW,"Request:"),MW.forEach(t),mE=h(a),v(La.$$.fragment,a),$E=h(a),Ma=o(a,"P",{});var $T=l(Ma);iH=u($T,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),ki=o($T,"A",{href:!0,rel:!0});var UW=l(ki);uH=u(UW,`Pillow
supports`),UW.forEach(t),cH=u($T,"."),$T.forEach(t),qE=h(a),Ua=o(a,"TABLE",{});var qT=l(Ua);s1=o(qT,"THEAD",{});var zW=l(s1);Ai=o(zW,"TR",{});var _T=l(Ai);md=o(_T,"TH",{align:!0});var KW=l(md);fH=u(KW,"All parameters"),KW.forEach(t),pH=h(_T),a1=o(_T,"TH",{align:!0}),l(a1).forEach(t),_T.forEach(t),zW.forEach(t),hH=h(qT),n1=o(qT,"TBODY",{});var FW=l(n1);Di=o(FW,"TR",{});var vT=l(Di);Oi=o(vT,"TD",{align:!0});var oC=l(Oi);r1=o(oC,"STRONG",{});var JW=l(r1);dH=u(JW,"no parameter"),JW.forEach(t),gH=u(oC," (required)"),oC.forEach(t),mH=h(vT),$d=o(vT,"TD",{align:!0});var WW=l($d);$H=u(WW,"a binary representation of the image file. No other parameters are currently allowed."),WW.forEach(t),vT.forEach(t),FW.forEach(t),qT.forEach(t),_E=h(a),qd=o(a,"P",{});var YW=l(qd);qH=u(YW,"Return value is a dict"),YW.forEach(t),vE=h(a),v(za.$$.fragment,a),yE=h(a),Ka=o(a,"TABLE",{});var yT=l(Ka);o1=o(yT,"THEAD",{});var VW=l(o1);Pi=o(VW,"TR",{});var ET=l(Pi);_d=o(ET,"TH",{align:!0});var XW=l(_d);_H=u(XW,"Returned values"),XW.forEach(t),vH=h(ET),l1=o(ET,"TH",{align:!0}),l(l1).forEach(t),ET.forEach(t),VW.forEach(t),yH=h(yT),Ri=o(yT,"TBODY",{});var wT=l(Ri);Si=o(wT,"TR",{});var bT=l(Si);vd=o(bT,"TD",{align:!0});var QW=l(vd);i1=o(QW,"STRONG",{});var ZW=l(i1);EH=u(ZW,"label"),ZW.forEach(t),QW.forEach(t),wH=h(bT),yd=o(bT,"TD",{align:!0});var eY=l(yd);bH=u(eY,"The label for the class (model specific)"),eY.forEach(t),bT.forEach(t),TH=h(wT),Ni=o(wT,"TR",{});var TT=l(Ni);Ed=o(TT,"TD",{align:!0});var tY=l(Ed);u1=o(tY,"STRONG",{});var sY=l(u1);jH=u(sY,"score"),sY.forEach(t),tY.forEach(t),kH=h(TT),wd=o(TT,"TD",{align:!0});var aY=l(wd);AH=u(aY,"A float that represents how likely it is that the image file belongs to this class."),aY.forEach(t),TT.forEach(t),wT.forEach(t),yT.forEach(t),EE=h(a),lt=o(a,"H3",{class:!0});var jT=l(lt);Fa=o(jT,"A",{id:!0,class:!0,href:!0});var nY=l(Fa);c1=o(nY,"SPAN",{});var rY=l(c1);v(xi.$$.fragment,rY),rY.forEach(t),nY.forEach(t),DH=h(jT),f1=o(jT,"SPAN",{});var oY=l(f1);OH=u(oY,"Object Detection task"),oY.forEach(t),jT.forEach(t),wE=h(a),bd=o(a,"P",{});var lY=l(bd);PH=u(lY,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),lY.forEach(t),bE=h(a),v(Ja.$$.fragment,a),TE=h(a),Ii=o(a,"P",{});var lC=l(Ii);RH=u(lC,"Available with: "),Hi=o(lC,"A",{href:!0,rel:!0});var iY=l(Hi);SH=u(iY,"\u{1F917} Transformers"),iY.forEach(t),lC.forEach(t),jE=h(a),Td=o(a,"P",{});var uY=l(Td);NH=u(uY,"Request:"),uY.forEach(t),kE=h(a),v(Wa.$$.fragment,a),AE=h(a),Ya=o(a,"P",{});var kT=l(Ya);xH=u(kT,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Bi=o(kT,"A",{href:!0,rel:!0});var cY=l(Bi);IH=u(cY,`Pillow
supports`),cY.forEach(t),HH=u(kT,"."),kT.forEach(t),DE=h(a),Va=o(a,"TABLE",{});var AT=l(Va);p1=o(AT,"THEAD",{});var fY=l(p1);Ci=o(fY,"TR",{});var DT=l(Ci);jd=o(DT,"TH",{align:!0});var pY=l(jd);BH=u(pY,"All parameters"),pY.forEach(t),CH=h(DT),h1=o(DT,"TH",{align:!0}),l(h1).forEach(t),DT.forEach(t),fY.forEach(t),GH=h(AT),d1=o(AT,"TBODY",{});var hY=l(d1);Gi=o(hY,"TR",{});var OT=l(Gi);Li=o(OT,"TD",{align:!0});var iC=l(Li);g1=o(iC,"STRONG",{});var dY=l(g1);LH=u(dY,"no parameter"),dY.forEach(t),MH=u(iC," (required)"),iC.forEach(t),UH=h(OT),kd=o(OT,"TD",{align:!0});var gY=l(kd);zH=u(gY,"a binary representation of the image file. No other parameters are currently allowed."),gY.forEach(t),OT.forEach(t),hY.forEach(t),AT.forEach(t),OE=h(a),Ad=o(a,"P",{});var mY=l(Ad);KH=u(mY,"Return value is a dict"),mY.forEach(t),PE=h(a),v(Xa.$$.fragment,a),RE=h(a),Qa=o(a,"TABLE",{});var PT=l(Qa);m1=o(PT,"THEAD",{});var $Y=l(m1);Mi=o($Y,"TR",{});var RT=l(Mi);Dd=o(RT,"TH",{align:!0});var qY=l(Dd);FH=u(qY,"Returned values"),qY.forEach(t),JH=h(RT),$1=o(RT,"TH",{align:!0}),l($1).forEach(t),RT.forEach(t),$Y.forEach(t),WH=h(PT),it=o(PT,"TBODY",{});var ig=l(it);Ui=o(ig,"TR",{});var ST=l(Ui);Od=o(ST,"TD",{align:!0});var _Y=l(Od);q1=o(_Y,"STRONG",{});var vY=l(q1);YH=u(vY,"label"),vY.forEach(t),_Y.forEach(t),VH=h(ST),Pd=o(ST,"TD",{align:!0});var yY=l(Pd);XH=u(yY,"The label for the class (model specific) of a detected object."),yY.forEach(t),ST.forEach(t),QH=h(ig),zi=o(ig,"TR",{});var NT=l(zi);Rd=o(NT,"TD",{align:!0});var EY=l(Rd);_1=o(EY,"STRONG",{});var wY=l(_1);ZH=u(wY,"score"),wY.forEach(t),EY.forEach(t),eB=h(NT),Sd=o(NT,"TD",{align:!0});var bY=l(Sd);tB=u(bY,"A float that represents how likely it is that the detected object belongs to the given class."),bY.forEach(t),NT.forEach(t),sB=h(ig),Ki=o(ig,"TR",{});var xT=l(Ki);Nd=o(xT,"TD",{align:!0});var TY=l(Nd);v1=o(TY,"STRONG",{});var jY=l(v1);aB=u(jY,"box"),jY.forEach(t),TY.forEach(t),nB=h(xT),xd=o(xT,"TD",{align:!0});var kY=l(xd);rB=u(kY,"A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),kY.forEach(t),xT.forEach(t),ig.forEach(t),PT.forEach(t),SE=h(a),ut=o(a,"H3",{class:!0});var IT=l(ut);Za=o(IT,"A",{id:!0,class:!0,href:!0});var AY=l(Za);y1=o(AY,"SPAN",{});var DY=l(y1);v(Fi.$$.fragment,DY),DY.forEach(t),AY.forEach(t),oB=h(IT),E1=o(IT,"SPAN",{});var OY=l(E1);lB=u(OY,"Image Segmentation task"),OY.forEach(t),IT.forEach(t),NE=h(a),Id=o(a,"P",{});var PY=l(Id);iB=u(PY,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),PY.forEach(t),xE=h(a),v(en.$$.fragment,a),IE=h(a),Ji=o(a,"P",{});var uC=l(Ji);uB=u(uC,"Available with: "),Wi=o(uC,"A",{href:!0,rel:!0});var RY=l(Wi);cB=u(RY,"\u{1F917} Transformers"),RY.forEach(t),uC.forEach(t),HE=h(a),Hd=o(a,"P",{});var SY=l(Hd);fB=u(SY,"Request:"),SY.forEach(t),BE=h(a),v(tn.$$.fragment,a),CE=h(a),sn=o(a,"P",{});var HT=l(sn);pB=u(HT,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),Yi=o(HT,"A",{href:!0,rel:!0});var NY=l(Yi);hB=u(NY,`Pillow
supports`),NY.forEach(t),dB=u(HT,"."),HT.forEach(t),GE=h(a),an=o(a,"TABLE",{});var BT=l(an);w1=o(BT,"THEAD",{});var xY=l(w1);Vi=o(xY,"TR",{});var CT=l(Vi);Bd=o(CT,"TH",{align:!0});var IY=l(Bd);gB=u(IY,"All parameters"),IY.forEach(t),mB=h(CT),b1=o(CT,"TH",{align:!0}),l(b1).forEach(t),CT.forEach(t),xY.forEach(t),$B=h(BT),T1=o(BT,"TBODY",{});var HY=l(T1);Xi=o(HY,"TR",{});var GT=l(Xi);Qi=o(GT,"TD",{align:!0});var cC=l(Qi);j1=o(cC,"STRONG",{});var BY=l(j1);qB=u(BY,"no parameter"),BY.forEach(t),_B=u(cC," (required)"),cC.forEach(t),vB=h(GT),Cd=o(GT,"TD",{align:!0});var CY=l(Cd);yB=u(CY,"a binary representation of the image file. No other parameters are currently allowed."),CY.forEach(t),GT.forEach(t),HY.forEach(t),BT.forEach(t),LE=h(a),Gd=o(a,"P",{});var GY=l(Gd);EB=u(GY,"Return value is a dict"),GY.forEach(t),ME=h(a),v(nn.$$.fragment,a),UE=h(a),rn=o(a,"TABLE",{});var LT=l(rn);k1=o(LT,"THEAD",{});var LY=l(k1);Zi=o(LY,"TR",{});var MT=l(Zi);Ld=o(MT,"TH",{align:!0});var MY=l(Ld);wB=u(MY,"Returned values"),MY.forEach(t),bB=h(MT),A1=o(MT,"TH",{align:!0}),l(A1).forEach(t),MT.forEach(t),LY.forEach(t),TB=h(LT),ct=o(LT,"TBODY",{});var ug=l(ct);eu=o(ug,"TR",{});var UT=l(eu);Md=o(UT,"TD",{align:!0});var UY=l(Md);D1=o(UY,"STRONG",{});var zY=l(D1);jB=u(zY,"label"),zY.forEach(t),UY.forEach(t),kB=h(UT),Ud=o(UT,"TD",{align:!0});var KY=l(Ud);AB=u(KY,"The label for the class (model specific) of a segment."),KY.forEach(t),UT.forEach(t),DB=h(ug),tu=o(ug,"TR",{});var zT=l(tu);zd=o(zT,"TD",{align:!0});var FY=l(zd);O1=o(FY,"STRONG",{});var JY=l(O1);OB=u(JY,"score"),JY.forEach(t),FY.forEach(t),PB=h(zT),Kd=o(zT,"TD",{align:!0});var WY=l(Kd);RB=u(WY,"A float that represents how likely it is that the segment belongs to the given class."),WY.forEach(t),zT.forEach(t),SB=h(ug),su=o(ug,"TR",{});var KT=l(su);Fd=o(KT,"TD",{align:!0});var YY=l(Fd);P1=o(YY,"STRONG",{});var VY=l(P1);NB=u(VY,"mask"),VY.forEach(t),YY.forEach(t),xB=h(KT),Jd=o(KT,"TD",{align:!0});var XY=l(Jd);IB=u(XY,"A str (base64 str of a single channel black-and-white img) representing the mask of a segment."),XY.forEach(t),KT.forEach(t),ug.forEach(t),LT.forEach(t),this.h()},h(){f(n,"name","hf:doc:metadata"),f(n,"content",JSON.stringify(HQ)),f(d,"id","detailed-parameters"),f(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(d,"href","#detailed-parameters"),f(s,"class","relative group"),f(le,"id","which-task-is-used-by-this-model"),f(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(le,"href","#which-task-is-used-by-this-model"),f(D,"class","relative group"),f(pt,"class","block dark:hidden"),QY(pt.src,fC="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task.png")||f(pt,"src",fC),f(pt,"width","300"),f(ht,"class","hidden dark:block invert"),QY(ht.src,pC="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task-dark.png")||f(ht,"src",pC),f(ht,"width","300"),f(dt,"id","natural-language-processing"),f(dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(dt,"href","#natural-language-processing"),f(Ie,"class","relative group"),f(gt,"id","fill-mask-task"),f(gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(gt,"href","#fill-mask-task"),f(He,"class","relative group"),f(yn,"href","https://github.com/huggingface/transformers"),f(yn,"rel","nofollow"),f(wu,"align","left"),f(gg,"align","left"),f(bn,"align","left"),f(bu,"align","left"),f(Tu,"align","left"),f(ju,"align","left"),f(ku,"align","left"),f(_t,"align","left"),f(Au,"align","left"),f(vt,"align","left"),f(Du,"align","left"),f(yt,"align","left"),f(Pu,"align","left"),f(Eg,"align","left"),f(Ru,"align","left"),f(Su,"align","left"),f(Nu,"align","left"),f(xu,"align","left"),f(Iu,"align","left"),f(Hu,"align","left"),f(Bu,"align","left"),f(Cu,"align","left"),f(bt,"id","summarization-task"),f(bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(bt,"href","#summarization-task"),f(Be,"class","relative group"),f(Gu,"href","mailto:api-enterprise@huggingface.co"),f(In,"href","https://github.com/huggingface/transformers"),f(In,"rel","nofollow"),f(Uu,"align","left"),f(Og,"align","left"),f(Cn,"align","left"),f(zu,"align","left"),f(Ku,"align","left"),f(Fu,"align","left"),f(Ju,"align","left"),f(_e,"align","left"),f(Wu,"align","left"),f(ve,"align","left"),f(Yu,"align","left"),f(ye,"align","left"),f(Vu,"align","left"),f(ie,"align","left"),f(Xu,"align","left"),f(ue,"align","left"),f(Qu,"align","left"),f(Dt,"align","left"),f(Zu,"align","left"),f(Ot,"align","left"),f(ec,"align","left"),f(tc,"align","left"),f(sc,"align","left"),f(Pt,"align","left"),f(ac,"align","left"),f(Rt,"align","left"),f(nc,"align","left"),f(St,"align","left"),f(oc,"align","left"),f(Qg,"align","left"),f(lc,"align","left"),f(ic,"align","left"),f(xt,"id","question-answering-task"),f(xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(xt,"href","#question-answering-task"),f(Ce,"class","relative group"),f(tr,"href","https://github.com/huggingface/transformers"),f(tr,"rel","nofollow"),f(sr,"href","https://github.com/allenai/allennlp"),f(sr,"rel","nofollow"),f(hc,"align","left"),f(nm,"align","left"),f(dc,"align","left"),f(gc,"align","left"),f(mc,"align","left"),f($c,"align","left"),f(qc,"align","left"),f(Gt,"align","left"),f(_c,"align","left"),f(Lt,"align","left"),f(Mt,"id","table-question-answering-task"),f(Mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Mt,"href","#table-question-answering-task"),f(Le,"class","relative group"),f(cr,"href","https://github.com/huggingface/transformers"),f(cr,"rel","nofollow"),f(wc,"align","left"),f(dm,"align","left"),f(hr,"align","left"),f(mm,"align","left"),f(bc,"align","left"),f(Tc,"align","left"),f(jc,"align","left"),f(kc,"align","left"),f(Ac,"align","left"),f(Dc,"align","left"),f(Oc,"align","left"),f(Ft,"align","left"),f(Pc,"align","left"),f(Jt,"align","left"),f(Rc,"align","left"),f(Wt,"align","left"),f(Nc,"align","left"),f(Em,"align","left"),f(xc,"align","left"),f(Ic,"align","left"),f(Hc,"align","left"),f(Bc,"align","left"),f(Cc,"align","left"),f(Gc,"align","left"),f(Lc,"align","left"),f(Mc,"align","left"),f(Xt,"id","sentence-similarity-task"),f(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Xt,"href","#sentence-similarity-task"),f(Me,"class","relative group"),f(kr,"href","https://www.sbert.net/index.html"),f(kr,"rel","nofollow"),f(Fc,"align","left"),f(Om,"align","left"),f(Or,"align","left"),f(Rm,"align","left"),f(Jc,"align","left"),f(Wc,"align","left"),f(Yc,"align","left"),f(Vc,"align","left"),f(Xc,"align","left"),f(Qc,"align","left"),f(Zc,"align","left"),f(ts,"align","left"),f(ef,"align","left"),f(ss,"align","left"),f(tf,"align","left"),f(as,"align","left"),f(af,"align","left"),f(Bm,"align","left"),f(nf,"align","left"),f(rf,"align","left"),f(os,"id","text-classification-task"),f(os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(os,"href","#text-classification-task"),f(Ue,"class","relative group"),f(Lr,"href","https://github.com/huggingface/transformers"),f(Lr,"rel","nofollow"),f(cf,"align","left"),f(zm,"align","left"),f(zr,"align","left"),f(ff,"align","left"),f(pf,"align","left"),f(hf,"align","left"),f(df,"align","left"),f(cs,"align","left"),f(gf,"align","left"),f(fs,"align","left"),f(mf,"align","left"),f(ps,"align","left"),f(qf,"align","left"),f(Xm,"align","left"),f(_f,"align","left"),f(vf,"align","left"),f(yf,"align","left"),f(Ef,"align","left"),f(gs,"id","text-generation-task"),f(gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(gs,"href","#text-generation-task"),f(ze,"class","relative group"),f(to,"href","https://github.com/huggingface/transformers"),f(to,"rel","nofollow"),f(jf,"align","left"),f(a$,"align","left"),f(no,"align","left"),f(kf,"align","left"),f(Af,"align","left"),f(Df,"align","left"),f(Of,"align","left"),f(Ee,"align","left"),f(Pf,"align","left"),f(ce,"align","left"),f(Rf,"align","left"),f(fe,"align","left"),f(Sf,"align","left"),f(_s,"align","left"),f(Nf,"align","left"),f(we,"align","left"),f(xf,"align","left"),f(be,"align","left"),f(If,"align","left"),f(Te,"align","left"),f(Hf,"align","left"),f(vs,"align","left"),f(Bf,"align","left"),f(ys,"align","left"),f(Cf,"align","left"),f(Gf,"align","left"),f(Lf,"align","left"),f(Es,"align","left"),f(Mf,"align","left"),f(ws,"align","left"),f(Uf,"align","left"),f(bs,"align","left"),f(Kf,"align","left"),f(A$,"align","left"),f(Ff,"align","left"),f(Jf,"align","left"),f(ks,"id","text2text-generation-task"),f(ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ks,"href","#text2text-generation-task"),f(Ke,"class","relative group"),f(Wf,"href","#text-generation-task"),f(Ds,"id","token-classification-task"),f(Ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ds,"href","#token-classification-task"),f(Fe,"class","relative group"),f(bo,"href","https://github.com/huggingface/transformers"),f(bo,"rel","nofollow"),f(To,"href","https://github.com/flairNLP/flair"),f(To,"rel","nofollow"),f(Qf,"align","left"),f(I$,"align","left"),f(Ao,"align","left"),f(Zf,"align","left"),f(ep,"align","left"),f(tp,"align","left"),f(sp,"align","left"),f(x,"align","left"),f(ap,"align","left"),f(np,"align","left"),f(rp,"align","left"),f(Ss,"align","left"),f(op,"align","left"),f(Ns,"align","left"),f(lp,"align","left"),f(xs,"align","left"),f(up,"align","left"),f(Z$,"align","left"),f(cp,"align","left"),f(fp,"align","left"),f(pp,"align","left"),f(hp,"align","left"),f(dp,"align","left"),f(gp,"align","left"),f(mp,"align","left"),f(Bs,"align","left"),f($p,"align","left"),f(Cs,"align","left"),f(Gs,"id","named-entity-recognition-ner-task"),f(Gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Gs,"href","#named-entity-recognition-ner-task"),f(We,"class","relative group"),f(qp,"href","#token-classification-task"),f(Ls,"id","translation-task"),f(Ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ls,"href","#translation-task"),f(Ye,"class","relative group"),f(Ko,"href","https://github.com/huggingface/transformers"),f(Ko,"rel","nofollow"),f(Ep,"align","left"),f(pq,"align","left"),f(Wo,"align","left"),f(wp,"align","left"),f(bp,"align","left"),f(Tp,"align","left"),f(jp,"align","left"),f(Ks,"align","left"),f(kp,"align","left"),f(Fs,"align","left"),f(Ap,"align","left"),f(Js,"align","left"),f(Op,"align","left"),f(_q,"align","left"),f(Pp,"align","left"),f(Rp,"align","left"),f(Ys,"id","zeroshot-classification-task"),f(Ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ys,"href","#zeroshot-classification-task"),f(Ve,"class","relative group"),f(al,"href","https://github.com/huggingface/transformers"),f(al,"rel","nofollow"),f(Ip,"align","left"),f(Tq,"align","left"),f(ol,"align","left"),f(Hp,"align","left"),f(il,"align","left"),f(Bp,"align","left"),f(Cp,"align","left"),f(je,"align","left"),f(Gp,"align","left"),f(Zs,"align","left"),f(Lp,"align","left"),f(Mp,"align","left"),f(Up,"align","left"),f(ea,"align","left"),f(zp,"align","left"),f(ta,"align","left"),f(Kp,"align","left"),f(sa,"align","left"),f(Wp,"align","left"),f(Iq,"align","left"),f(Yp,"align","left"),f(Vp,"align","left"),f(Xp,"align","left"),f(Qp,"align","left"),f(Zp,"align","left"),f(ra,"align","left"),f(oa,"id","conversational-task"),f(oa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(oa,"href","#conversational-task"),f(Qe,"class","relative group"),f(yl,"href","https://github.com/huggingface/transformers"),f(yl,"rel","nofollow"),f(ah,"align","left"),f(zq,"align","left"),f(bl,"align","left"),f(Fq,"align","left"),f(nh,"align","left"),f(rh,"align","left"),f(oh,"align","left"),f(lh,"align","left"),f(ih,"align","left"),f(ca,"align","left"),f(uh,"align","left"),f(ch,"align","left"),f(fh,"align","left"),f(ke,"align","left"),f(ph,"align","left"),f(Ae,"align","left"),f(hh,"align","left"),f(De,"align","left"),f(dh,"align","left"),f(pe,"align","left"),f(gh,"align","left"),f(he,"align","left"),f(mh,"align","left"),f(fa,"align","left"),f($h,"align","left"),f(pa,"align","left"),f(qh,"align","left"),f(_h,"align","left"),f(vh,"align","left"),f(ha,"align","left"),f(yh,"align","left"),f(da,"align","left"),f(Eh,"align","left"),f(ga,"align","left"),f(bh,"align","left"),f(d_,"align","left"),f(Th,"align","left"),f(jh,"align","left"),f(kh,"align","left"),f(Ah,"align","left"),f(Dh,"align","left"),f(Oh,"align","left"),f(Ph,"align","left"),f(Rh,"align","left"),f($a,"id","feature-extraction-task"),f($a,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f($a,"href","#feature-extraction-task"),f(Ze,"class","relative group"),f(Fl,"href","https://github.com/huggingface/transformers"),f(Fl,"rel","nofollow"),f(Jl,"href","https://github.com/UKPLab/sentence-transformers"),f(Jl,"rel","nofollow"),f(xh,"align","left"),f(v_,"align","left"),f(Vl,"align","left"),f(Ih,"align","left"),f(Hh,"align","left"),f(Bh,"align","left"),f(Ch,"align","left"),f(va,"align","left"),f(Gh,"align","left"),f(ya,"align","left"),f(Lh,"align","left"),f(Ea,"align","left"),f(Uh,"align","left"),f(k_,"align","left"),f(zh,"align","left"),f(Kh,"align","left"),f(ba,"id","audio"),f(ba,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ba,"href","#audio"),f(tt,"class","relative group"),f(Ta,"id","automatic-speech-recognition-task"),f(Ta,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ta,"href","#automatic-speech-recognition-task"),f(st,"class","relative group"),f(ri,"href","https://github.com/huggingface/transformers"),f(ri,"rel","nofollow"),f(oi,"href","https://github.com/espnet/espnet"),f(oi,"rel","nofollow"),f(li,"href","https://github.com/speechbrain/speechbrain"),f(li,"rel","nofollow"),f(Vh,"align","left"),f(x_,"align","left"),f(ci,"align","left"),f(Xh,"align","left"),f(ed,"align","left"),f(C_,"align","left"),f(td,"align","left"),f(sd,"align","left"),f(Ra,"id","audio-classification-task"),f(Ra,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ra,"href","#audio-classification-task"),f(at,"class","relative group"),f(di,"href","https://github.com/huggingface/transformers"),f(di,"rel","nofollow"),f(gi,"href","https://github.com/speechbrain/speechbrain"),f(gi,"rel","nofollow"),f(od,"align","left"),f(K_,"align","left"),f(qi,"align","left"),f(ld,"align","left"),f(ud,"align","left"),f(Y_,"align","left"),f(cd,"align","left"),f(fd,"align","left"),f(pd,"align","left"),f(hd,"align","left"),f(Ba,"id","computer-vision"),f(Ba,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ba,"href","#computer-vision"),f(rt,"class","relative group"),f(Ca,"id","image-classification-task"),f(Ca,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ca,"href","#image-classification-task"),f(ot,"class","relative group"),f(ji,"href","https://github.com/huggingface/transformers"),f(ji,"rel","nofollow"),f(ki,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),f(ki,"rel","nofollow"),f(md,"align","left"),f(a1,"align","left"),f(Oi,"align","left"),f($d,"align","left"),f(_d,"align","left"),f(l1,"align","left"),f(vd,"align","left"),f(yd,"align","left"),f(Ed,"align","left"),f(wd,"align","left"),f(Fa,"id","object-detection-task"),f(Fa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Fa,"href","#object-detection-task"),f(lt,"class","relative group"),f(Hi,"href","https://github.com/huggingface/transformers"),f(Hi,"rel","nofollow"),f(Bi,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),f(Bi,"rel","nofollow"),f(jd,"align","left"),f(h1,"align","left"),f(Li,"align","left"),f(kd,"align","left"),f(Dd,"align","left"),f($1,"align","left"),f(Od,"align","left"),f(Pd,"align","left"),f(Rd,"align","left"),f(Sd,"align","left"),f(Nd,"align","left"),f(xd,"align","left"),f(Za,"id","image-segmentation-task"),f(Za,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Za,"href","#image-segmentation-task"),f(ut,"class","relative group"),f(Wi,"href","https://github.com/huggingface/transformers"),f(Wi,"rel","nofollow"),f(Yi,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),f(Yi,"rel","nofollow"),f(Bd,"align","left"),f(b1,"align","left"),f(Qi,"align","left"),f(Cd,"align","left"),f(Ld,"align","left"),f(A1,"align","left"),f(Md,"align","left"),f(Ud,"align","left"),f(zd,"align","left"),f(Kd,"align","left"),f(Fd,"align","left"),f(Jd,"align","left")},m(a,g){e(document.head,n),m(a,c,g),m(a,s,g),e(s,d),e(d,$),y(k,$,null),e(s,A),e(s,j),e(j,T),m(a,S,g),m(a,D,g),e(D,le),e(le,Ne),y(ee,Ne,null),e(D,V),e(D,ft),e(ft,qu),m(a,$n,g),m(a,xe,g),e(xe,FT),m(a,p2,g),m(a,_u,g),e(_u,JT),m(a,h2,g),m(a,pt,g),m(a,d2,g),m(a,ht,g),m(a,g2,g),m(a,Ie,g),e(Ie,dt),e(dt,cg),y(qn,cg,null),e(Ie,WT),e(Ie,fg),e(fg,YT),m(a,m2,g),m(a,He,g),e(He,gt),e(gt,pg),y(_n,pg,null),e(He,VT),e(He,hg),e(hg,XT),m(a,$2,g),m(a,vu,g),e(vu,QT),m(a,q2,g),y(mt,a,g),m(a,_2,g),m(a,vn,g),e(vn,ZT),e(vn,yn),e(yn,ej),m(a,v2,g),m(a,yu,g),e(yu,tj),m(a,y2,g),y($t,a,g),m(a,E2,g),m(a,Eu,g),e(Eu,sj),m(a,w2,g),m(a,qt,g),e(qt,dg),e(dg,En),e(En,wu),e(wu,aj),e(En,nj),e(En,gg),e(qt,rj),e(qt,te),e(te,wn),e(wn,bn),e(bn,mg),e(mg,oj),e(bn,lj),e(wn,ij),e(wn,bu),e(bu,uj),e(te,cj),e(te,Tn),e(Tn,Tu),e(Tu,$g),e($g,fj),e(Tn,pj),e(Tn,ju),e(ju,hj),e(te,dj),e(te,jn),e(jn,ku),e(ku,gj),e(jn,mj),e(jn,_t),e(_t,$j),e(_t,qg),e(qg,qj),e(_t,_j),e(te,vj),e(te,kn),e(kn,Au),e(Au,yj),e(kn,Ej),e(kn,vt),e(vt,wj),e(vt,_g),e(_g,bj),e(vt,Tj),e(te,jj),e(te,An),e(An,Du),e(Du,kj),e(An,Aj),e(An,yt),e(yt,Dj),e(yt,vg),e(vg,Oj),e(yt,Pj),m(a,b2,g),m(a,Ou,g),e(Ou,Rj),m(a,T2,g),y(Et,a,g),m(a,j2,g),m(a,wt,g),e(wt,yg),e(yg,Dn),e(Dn,Pu),e(Pu,Sj),e(Dn,Nj),e(Dn,Eg),e(wt,xj),e(wt,de),e(de,On),e(On,Ru),e(Ru,wg),e(wg,Ij),e(On,Hj),e(On,Su),e(Su,Bj),e(de,Cj),e(de,Pn),e(Pn,Nu),e(Nu,bg),e(bg,Gj),e(Pn,Lj),e(Pn,xu),e(xu,Mj),e(de,Uj),e(de,Rn),e(Rn,Iu),e(Iu,Tg),e(Tg,zj),e(Rn,Kj),e(Rn,Hu),e(Hu,Fj),e(de,Jj),e(de,Sn),e(Sn,Bu),e(Bu,jg),e(jg,Wj),e(Sn,Yj),e(Sn,Cu),e(Cu,Vj),m(a,k2,g),m(a,Be,g),e(Be,bt),e(bt,kg),y(Nn,kg,null),e(Be,Xj),e(Be,Ag),e(Ag,Qj),m(a,A2,g),m(a,Tt,g),e(Tt,Zj),e(Tt,Gu),e(Gu,e4),e(Tt,t4),m(a,D2,g),y(jt,a,g),m(a,O2,g),m(a,xn,g),e(xn,s4),e(xn,In),e(In,a4),m(a,P2,g),m(a,Lu,g),e(Lu,n4),m(a,R2,g),y(kt,a,g),m(a,S2,g),m(a,Mu,g),e(Mu,r4),m(a,N2,g),m(a,At,g),e(At,Dg),e(Dg,Hn),e(Hn,Uu),e(Uu,o4),e(Hn,l4),e(Hn,Og),e(At,i4),e(At,G),e(G,Bn),e(Bn,Cn),e(Cn,Pg),e(Pg,u4),e(Cn,c4),e(Bn,f4),e(Bn,zu),e(zu,p4),e(G,h4),e(G,Gn),e(Gn,Ku),e(Ku,Rg),e(Rg,d4),e(Gn,g4),e(Gn,Fu),e(Fu,m4),e(G,$4),e(G,Ln),e(Ln,Ju),e(Ju,q4),e(Ln,_4),e(Ln,_e),e(_e,v4),e(_e,Sg),e(Sg,y4),e(_e,E4),e(_e,Ng),e(Ng,w4),e(_e,b4),e(G,T4),e(G,Mn),e(Mn,Wu),e(Wu,j4),e(Mn,k4),e(Mn,ve),e(ve,A4),e(ve,xg),e(xg,D4),e(ve,O4),e(ve,Ig),e(Ig,P4),e(ve,R4),e(G,S4),e(G,Un),e(Un,Yu),e(Yu,N4),e(Un,x4),e(Un,ye),e(ye,I4),e(ye,Hg),e(Hg,H4),e(ye,B4),e(ye,Bg),e(Bg,C4),e(ye,G4),e(G,L4),e(G,zn),e(zn,Vu),e(Vu,M4),e(zn,U4),e(zn,ie),e(ie,z4),e(ie,Cg),e(Cg,K4),e(ie,F4),e(ie,Gg),e(Gg,J4),e(ie,W4),e(ie,Lg),e(Lg,Y4),e(ie,V4),e(G,X4),e(G,Kn),e(Kn,Xu),e(Xu,Q4),e(Kn,Z4),e(Kn,ue),e(ue,e5),e(ue,Mg),e(Mg,t5),e(ue,s5),e(ue,Ug),e(Ug,a5),e(ue,n5),e(ue,zg),e(zg,r5),e(ue,o5),e(G,l5),e(G,Fn),e(Fn,Qu),e(Qu,i5),e(Fn,u5),e(Fn,Dt),e(Dt,c5),e(Dt,Kg),e(Kg,f5),e(Dt,p5),e(G,h5),e(G,Jn),e(Jn,Zu),e(Zu,d5),e(Jn,g5),e(Jn,Ot),e(Ot,m5),e(Ot,Fg),e(Fg,$5),e(Ot,q5),e(G,_5),e(G,Wn),e(Wn,ec),e(ec,Jg),e(Jg,v5),e(Wn,y5),e(Wn,tc),e(tc,E5),e(G,w5),e(G,Yn),e(Yn,sc),e(sc,b5),e(Yn,T5),e(Yn,Pt),e(Pt,j5),e(Pt,Wg),e(Wg,k5),e(Pt,A5),e(G,D5),e(G,Vn),e(Vn,ac),e(ac,O5),e(Vn,P5),e(Vn,Rt),e(Rt,R5),e(Rt,Yg),e(Yg,S5),e(Rt,N5),e(G,x5),e(G,Xn),e(Xn,nc),e(nc,I5),e(Xn,H5),e(Xn,St),e(St,B5),e(St,Vg),e(Vg,C5),e(St,G5),m(a,x2,g),m(a,rc,g),e(rc,L5),m(a,I2,g),m(a,Nt,g),e(Nt,Xg),e(Xg,Qn),e(Qn,oc),e(oc,M5),e(Qn,U5),e(Qn,Qg),e(Nt,z5),e(Nt,Zg),e(Zg,Zn),e(Zn,lc),e(lc,em),e(em,K5),e(Zn,F5),e(Zn,ic),e(ic,J5),m(a,H2,g),m(a,Ce,g),e(Ce,xt),e(xt,tm),y(er,tm,null),e(Ce,W5),e(Ce,sm),e(sm,Y5),m(a,B2,g),m(a,uc,g),e(uc,V5),m(a,C2,g),y(It,a,g),m(a,G2,g),m(a,Ge,g),e(Ge,X5),e(Ge,tr),e(tr,Q5),e(Ge,Z5),e(Ge,sr),e(sr,ek),m(a,L2,g),m(a,cc,g),e(cc,tk),m(a,M2,g),y(Ht,a,g),m(a,U2,g),m(a,fc,g),e(fc,sk),m(a,z2,g),m(a,pc,g),e(pc,ak),m(a,K2,g),y(Bt,a,g),m(a,F2,g),m(a,Ct,g),e(Ct,am),e(am,ar),e(ar,hc),e(hc,nk),e(ar,rk),e(ar,nm),e(Ct,ok),e(Ct,ge),e(ge,nr),e(nr,dc),e(dc,rm),e(rm,lk),e(nr,ik),e(nr,gc),e(gc,uk),e(ge,ck),e(ge,rr),e(rr,mc),e(mc,om),e(om,fk),e(rr,pk),e(rr,$c),e($c,hk),e(ge,dk),e(ge,or),e(or,qc),e(qc,lm),e(lm,gk),e(or,mk),e(or,Gt),e(Gt,$k),e(Gt,im),e(im,qk),e(Gt,_k),e(ge,vk),e(ge,lr),e(lr,_c),e(_c,um),e(um,yk),e(lr,Ek),e(lr,Lt),e(Lt,wk),e(Lt,cm),e(cm,bk),e(Lt,Tk),m(a,J2,g),m(a,Le,g),e(Le,Mt),e(Mt,fm),y(ir,fm,null),e(Le,jk),e(Le,pm),e(pm,kk),m(a,W2,g),m(a,vc,g),e(vc,Ak),m(a,Y2,g),y(Ut,a,g),m(a,V2,g),m(a,ur,g),e(ur,Dk),e(ur,cr),e(cr,Ok),m(a,X2,g),m(a,yc,g),e(yc,Pk),m(a,Q2,g),y(zt,a,g),m(a,Z2,g),m(a,Ec,g),e(Ec,Rk),m(a,ev,g),m(a,Kt,g),e(Kt,hm),e(hm,fr),e(fr,wc),e(wc,Sk),e(fr,Nk),e(fr,dm),e(Kt,xk),e(Kt,F),e(F,pr),e(pr,hr),e(hr,gm),e(gm,Ik),e(hr,Hk),e(pr,Bk),e(pr,mm),e(F,Ck),e(F,dr),e(dr,bc),e(bc,Gk),e(dr,Lk),e(dr,Tc),e(Tc,Mk),e(F,Uk),e(F,gr),e(gr,jc),e(jc,zk),e(gr,Kk),e(gr,kc),e(kc,Fk),e(F,Jk),e(F,mr),e(mr,Ac),e(Ac,$m),e($m,Wk),e(mr,Yk),e(mr,Dc),e(Dc,Vk),e(F,Xk),e(F,$r),e($r,Oc),e(Oc,Qk),e($r,Zk),e($r,Ft),e(Ft,e6),e(Ft,qm),e(qm,t6),e(Ft,s6),e(F,a6),e(F,qr),e(qr,Pc),e(Pc,n6),e(qr,r6),e(qr,Jt),e(Jt,o6),e(Jt,_m),e(_m,l6),e(Jt,i6),e(F,u6),e(F,_r),e(_r,Rc),e(Rc,c6),e(_r,f6),e(_r,Wt),e(Wt,p6),e(Wt,vm),e(vm,h6),e(Wt,d6),m(a,tv,g),m(a,Sc,g),e(Sc,g6),m(a,sv,g),y(Yt,a,g),m(a,av,g),m(a,Vt,g),e(Vt,ym),e(ym,vr),e(vr,Nc),e(Nc,m6),e(vr,$6),e(vr,Em),e(Vt,q6),e(Vt,me),e(me,yr),e(yr,xc),e(xc,wm),e(wm,_6),e(yr,v6),e(yr,Ic),e(Ic,y6),e(me,E6),e(me,Er),e(Er,Hc),e(Hc,bm),e(bm,w6),e(Er,b6),e(Er,Bc),e(Bc,T6),e(me,j6),e(me,wr),e(wr,Cc),e(Cc,Tm),e(Tm,k6),e(wr,A6),e(wr,Gc),e(Gc,D6),e(me,O6),e(me,br),e(br,Lc),e(Lc,jm),e(jm,P6),e(br,R6),e(br,Mc),e(Mc,S6),m(a,nv,g),m(a,Me,g),e(Me,Xt),e(Xt,km),y(Tr,km,null),e(Me,N6),e(Me,Am),e(Am,x6),m(a,rv,g),m(a,Uc,g),e(Uc,I6),m(a,ov,g),y(Qt,a,g),m(a,lv,g),m(a,jr,g),e(jr,H6),e(jr,kr),e(kr,B6),m(a,iv,g),m(a,zc,g),e(zc,C6),m(a,uv,g),y(Zt,a,g),m(a,cv,g),m(a,Kc,g),e(Kc,G6),m(a,fv,g),m(a,es,g),e(es,Dm),e(Dm,Ar),e(Ar,Fc),e(Fc,L6),e(Ar,M6),e(Ar,Om),e(es,U6),e(es,J),e(J,Dr),e(Dr,Or),e(Or,Pm),e(Pm,z6),e(Or,K6),e(Dr,F6),e(Dr,Rm),e(J,J6),e(J,Pr),e(Pr,Jc),e(Jc,W6),e(Pr,Y6),e(Pr,Wc),e(Wc,V6),e(J,X6),e(J,Rr),e(Rr,Yc),e(Yc,Q6),e(Rr,Z6),e(Rr,Vc),e(Vc,e7),e(J,t7),e(J,Sr),e(Sr,Xc),e(Xc,Sm),e(Sm,s7),e(Sr,a7),e(Sr,Qc),e(Qc,n7),e(J,r7),e(J,Nr),e(Nr,Zc),e(Zc,o7),e(Nr,l7),e(Nr,ts),e(ts,i7),e(ts,Nm),e(Nm,u7),e(ts,c7),e(J,f7),e(J,xr),e(xr,ef),e(ef,p7),e(xr,h7),e(xr,ss),e(ss,d7),e(ss,xm),e(xm,g7),e(ss,m7),e(J,$7),e(J,Ir),e(Ir,tf),e(tf,q7),e(Ir,_7),e(Ir,as),e(as,v7),e(as,Im),e(Im,y7),e(as,E7),m(a,pv,g),m(a,sf,g),e(sf,w7),m(a,hv,g),y(ns,a,g),m(a,dv,g),m(a,rs,g),e(rs,Hm),e(Hm,Hr),e(Hr,af),e(af,b7),e(Hr,T7),e(Hr,Bm),e(rs,j7),e(rs,Cm),e(Cm,Br),e(Br,nf),e(nf,Gm),e(Gm,k7),e(Br,A7),e(Br,rf),e(rf,D7),m(a,gv,g),m(a,Ue,g),e(Ue,os),e(os,Lm),y(Cr,Lm,null),e(Ue,O7),e(Ue,Mm),e(Mm,P7),m(a,mv,g),m(a,of,g),e(of,R7),m(a,$v,g),y(ls,a,g),m(a,qv,g),m(a,Gr,g),e(Gr,S7),e(Gr,Lr),e(Lr,N7),m(a,_v,g),m(a,lf,g),e(lf,x7),m(a,vv,g),y(is,a,g),m(a,yv,g),m(a,uf,g),e(uf,I7),m(a,Ev,g),m(a,us,g),e(us,Um),e(Um,Mr),e(Mr,cf),e(cf,H7),e(Mr,B7),e(Mr,zm),e(us,C7),e(us,se),e(se,Ur),e(Ur,zr),e(zr,Km),e(Km,G7),e(zr,L7),e(Ur,M7),e(Ur,ff),e(ff,U7),e(se,z7),e(se,Kr),e(Kr,pf),e(pf,Fm),e(Fm,K7),e(Kr,F7),e(Kr,hf),e(hf,J7),e(se,W7),e(se,Fr),e(Fr,df),e(df,Y7),e(Fr,V7),e(Fr,cs),e(cs,X7),e(cs,Jm),e(Jm,Q7),e(cs,Z7),e(se,e9),e(se,Jr),e(Jr,gf),e(gf,t9),e(Jr,s9),e(Jr,fs),e(fs,a9),e(fs,Wm),e(Wm,n9),e(fs,r9),e(se,o9),e(se,Wr),e(Wr,mf),e(mf,l9),e(Wr,i9),e(Wr,ps),e(ps,u9),e(ps,Ym),e(Ym,c9),e(ps,f9),m(a,wv,g),m(a,$f,g),e($f,p9),m(a,bv,g),y(hs,a,g),m(a,Tv,g),m(a,ds,g),e(ds,Vm),e(Vm,Yr),e(Yr,qf),e(qf,h9),e(Yr,d9),e(Yr,Xm),e(ds,g9),e(ds,Vr),e(Vr,Xr),e(Xr,_f),e(_f,Qm),e(Qm,m9),e(Xr,$9),e(Xr,vf),e(vf,q9),e(Vr,_9),e(Vr,Qr),e(Qr,yf),e(yf,Zm),e(Zm,v9),e(Qr,y9),e(Qr,Ef),e(Ef,E9),m(a,jv,g),m(a,ze,g),e(ze,gs),e(gs,e$),y(Zr,e$,null),e(ze,w9),e(ze,t$),e(t$,b9),m(a,kv,g),m(a,wf,g),e(wf,T9),m(a,Av,g),y(ms,a,g),m(a,Dv,g),m(a,eo,g),e(eo,j9),e(eo,to),e(to,k9),m(a,Ov,g),m(a,bf,g),e(bf,A9),m(a,Pv,g),y($s,a,g),m(a,Rv,g),m(a,Tf,g),e(Tf,D9),m(a,Sv,g),m(a,qs,g),e(qs,s$),e(s$,so),e(so,jf),e(jf,O9),e(so,P9),e(so,a$),e(qs,R9),e(qs,I),e(I,ao),e(ao,no),e(no,n$),e(n$,S9),e(no,N9),e(ao,x9),e(ao,kf),e(kf,I9),e(I,H9),e(I,ro),e(ro,Af),e(Af,r$),e(r$,B9),e(ro,C9),e(ro,Df),e(Df,G9),e(I,L9),e(I,oo),e(oo,Of),e(Of,M9),e(oo,U9),e(oo,Ee),e(Ee,z9),e(Ee,o$),e(o$,K9),e(Ee,F9),e(Ee,l$),e(l$,J9),e(Ee,W9),e(I,Y9),e(I,lo),e(lo,Pf),e(Pf,V9),e(lo,X9),e(lo,ce),e(ce,Q9),e(ce,i$),e(i$,Z9),e(ce,e8),e(ce,u$),e(u$,t8),e(ce,s8),e(ce,c$),e(c$,a8),e(ce,n8),e(I,r8),e(I,io),e(io,Rf),e(Rf,o8),e(io,l8),e(io,fe),e(fe,i8),e(fe,f$),e(f$,u8),e(fe,c8),e(fe,p$),e(p$,f8),e(fe,p8),e(fe,h$),e(h$,h8),e(fe,d8),e(I,g8),e(I,uo),e(uo,Sf),e(Sf,m8),e(uo,$8),e(uo,_s),e(_s,q8),e(_s,d$),e(d$,_8),e(_s,v8),e(I,y8),e(I,co),e(co,Nf),e(Nf,E8),e(co,w8),e(co,we),e(we,b8),e(we,g$),e(g$,T8),e(we,j8),e(we,m$),e(m$,k8),e(we,A8),e(I,D8),e(I,fo),e(fo,xf),e(xf,O8),e(fo,P8),e(fo,be),e(be,R8),e(be,$$),e($$,S8),e(be,N8),e(be,q$),e(q$,x8),e(be,I8),e(I,H8),e(I,po),e(po,If),e(If,B8),e(po,C8),e(po,Te),e(Te,G8),e(Te,_$),e(_$,L8),e(Te,M8),e(Te,v$),e(v$,U8),e(Te,z8),e(I,K8),e(I,ho),e(ho,Hf),e(Hf,F8),e(ho,J8),e(ho,vs),e(vs,W8),e(vs,y$),e(y$,Y8),e(vs,V8),e(I,X8),e(I,go),e(go,Bf),e(Bf,Q8),e(go,Z8),e(go,ys),e(ys,eA),e(ys,E$),e(E$,tA),e(ys,sA),e(I,aA),e(I,mo),e(mo,Cf),e(Cf,w$),e(w$,nA),e(mo,rA),e(mo,Gf),e(Gf,oA),e(I,lA),e(I,$o),e($o,Lf),e(Lf,iA),e($o,uA),e($o,Es),e(Es,cA),e(Es,b$),e(b$,fA),e(Es,pA),e(I,hA),e(I,qo),e(qo,Mf),e(Mf,dA),e(qo,gA),e(qo,ws),e(ws,mA),e(ws,T$),e(T$,$A),e(ws,qA),e(I,_A),e(I,_o),e(_o,Uf),e(Uf,vA),e(_o,yA),e(_o,bs),e(bs,EA),e(bs,j$),e(j$,wA),e(bs,bA),m(a,Nv,g),m(a,zf,g),e(zf,TA),m(a,xv,g),y(Ts,a,g),m(a,Iv,g),m(a,js,g),e(js,k$),e(k$,vo),e(vo,Kf),e(Kf,jA),e(vo,kA),e(vo,A$),e(js,AA),e(js,D$),e(D$,yo),e(yo,Ff),e(Ff,O$),e(O$,DA),e(yo,OA),e(yo,Jf),e(Jf,PA),m(a,Hv,g),m(a,Ke,g),e(Ke,ks),e(ks,P$),y(Eo,P$,null),e(Ke,RA),e(Ke,R$),e(R$,SA),m(a,Bv,g),m(a,As,g),e(As,NA),e(As,Wf),e(Wf,xA),e(As,IA),m(a,Cv,g),m(a,Fe,g),e(Fe,Ds),e(Ds,S$),y(wo,S$,null),e(Fe,HA),e(Fe,N$),e(N$,BA),m(a,Gv,g),m(a,Yf,g),e(Yf,CA),m(a,Lv,g),y(Os,a,g),m(a,Mv,g),m(a,Je,g),e(Je,GA),e(Je,bo),e(bo,LA),e(Je,MA),e(Je,To),e(To,UA),m(a,Uv,g),m(a,Vf,g),e(Vf,zA),m(a,zv,g),y(Ps,a,g),m(a,Kv,g),m(a,Xf,g),e(Xf,KA),m(a,Fv,g),m(a,Rs,g),e(Rs,x$),e(x$,jo),e(jo,Qf),e(Qf,FA),e(jo,JA),e(jo,I$),e(Rs,WA),e(Rs,W),e(W,ko),e(ko,Ao),e(Ao,H$),e(H$,YA),e(Ao,VA),e(ko,XA),e(ko,Zf),e(Zf,QA),e(W,ZA),e(W,Do),e(Do,ep),e(ep,B$),e(B$,eD),e(Do,tD),e(Do,tp),e(tp,sD),e(W,aD),e(W,Oo),e(Oo,sp),e(sp,nD),e(Oo,rD),e(Oo,x),e(x,oD),e(x,C$),e(C$,lD),e(x,iD),e(x,uD),e(x,cD),e(x,G$),e(G$,fD),e(x,pD),e(x,hD),e(x,dD),e(x,L$),e(L$,gD),e(x,mD),e(x,$D),e(x,qD),e(x,M$),e(M$,_D),e(x,vD),e(x,U$),e(U$,yD),e(x,ED),e(x,wD),e(x,bD),e(x,z$),e(z$,TD),e(x,jD),e(x,K$),e(K$,kD),e(x,AD),e(x,DD),e(x,OD),e(x,F$),e(F$,PD),e(x,RD),e(x,J$),e(J$,SD),e(x,ND),e(W,xD),e(W,Po),e(Po,ap),e(ap,W$),e(W$,ID),e(Po,HD),e(Po,np),e(np,BD),e(W,CD),e(W,Ro),e(Ro,rp),e(rp,GD),e(Ro,LD),e(Ro,Ss),e(Ss,MD),e(Ss,Y$),e(Y$,UD),e(Ss,zD),e(W,KD),e(W,So),e(So,op),e(op,FD),e(So,JD),e(So,Ns),e(Ns,WD),e(Ns,V$),e(V$,YD),e(Ns,VD),e(W,XD),e(W,No),e(No,lp),e(lp,QD),e(No,ZD),e(No,xs),e(xs,eO),e(xs,X$),e(X$,tO),e(xs,sO),m(a,Jv,g),m(a,ip,g),e(ip,aO),m(a,Wv,g),y(Is,a,g),m(a,Yv,g),m(a,Hs,g),e(Hs,Q$),e(Q$,xo),e(xo,up),e(up,nO),e(xo,rO),e(xo,Z$),e(Hs,oO),e(Hs,ae),e(ae,Io),e(Io,cp),e(cp,eq),e(eq,lO),e(Io,iO),e(Io,fp),e(fp,uO),e(ae,cO),e(ae,Ho),e(Ho,pp),e(pp,tq),e(tq,fO),e(Ho,pO),e(Ho,hp),e(hp,hO),e(ae,dO),e(ae,Bo),e(Bo,dp),e(dp,sq),e(sq,gO),e(Bo,mO),e(Bo,gp),e(gp,$O),e(ae,qO),e(ae,Co),e(Co,mp),e(mp,aq),e(aq,_O),e(Co,vO),e(Co,Bs),e(Bs,yO),e(Bs,nq),e(nq,EO),e(Bs,wO),e(ae,bO),e(ae,Go),e(Go,$p),e($p,rq),e(rq,TO),e(Go,jO),e(Go,Cs),e(Cs,kO),e(Cs,oq),e(oq,AO),e(Cs,DO),m(a,Vv,g),m(a,We,g),e(We,Gs),e(Gs,lq),y(Lo,lq,null),e(We,OO),e(We,iq),e(iq,PO),m(a,Xv,g),m(a,Mo,g),e(Mo,RO),e(Mo,qp),e(qp,SO),m(a,Qv,g),m(a,Ye,g),e(Ye,Ls),e(Ls,uq),y(Uo,uq,null),e(Ye,NO),e(Ye,cq),e(cq,xO),m(a,Zv,g),m(a,_p,g),e(_p,IO),m(a,ey,g),y(Ms,a,g),m(a,ty,g),m(a,zo,g),e(zo,HO),e(zo,Ko),e(Ko,BO),m(a,sy,g),m(a,vp,g),e(vp,CO),m(a,ay,g),y(Us,a,g),m(a,ny,g),m(a,yp,g),e(yp,GO),m(a,ry,g),m(a,zs,g),e(zs,fq),e(fq,Fo),e(Fo,Ep),e(Ep,LO),e(Fo,MO),e(Fo,pq),e(zs,UO),e(zs,ne),e(ne,Jo),e(Jo,Wo),e(Wo,hq),e(hq,zO),e(Wo,KO),e(Jo,FO),e(Jo,wp),e(wp,JO),e(ne,WO),e(ne,Yo),e(Yo,bp),e(bp,dq),e(dq,YO),e(Yo,VO),e(Yo,Tp),e(Tp,XO),e(ne,QO),e(ne,Vo),e(Vo,jp),e(jp,ZO),e(Vo,eP),e(Vo,Ks),e(Ks,tP),e(Ks,gq),e(gq,sP),e(Ks,aP),e(ne,nP),e(ne,Xo),e(Xo,kp),e(kp,rP),e(Xo,oP),e(Xo,Fs),e(Fs,lP),e(Fs,mq),e(mq,iP),e(Fs,uP),e(ne,cP),e(ne,Qo),e(Qo,Ap),e(Ap,fP),e(Qo,pP),e(Qo,Js),e(Js,hP),e(Js,$q),e($q,dP),e(Js,gP),m(a,oy,g),m(a,Dp,g),e(Dp,mP),m(a,ly,g),m(a,Ws,g),e(Ws,qq),e(qq,Zo),e(Zo,Op),e(Op,$P),e(Zo,qP),e(Zo,_q),e(Ws,_P),e(Ws,vq),e(vq,el),e(el,Pp),e(Pp,yq),e(yq,vP),e(el,yP),e(el,Rp),e(Rp,EP),m(a,iy,g),m(a,Ve,g),e(Ve,Ys),e(Ys,Eq),y(tl,Eq,null),e(Ve,wP),e(Ve,wq),e(wq,bP),m(a,uy,g),m(a,Sp,g),e(Sp,TP),m(a,cy,g),y(Vs,a,g),m(a,fy,g),m(a,sl,g),e(sl,jP),e(sl,al),e(al,kP),m(a,py,g),m(a,Np,g),e(Np,AP),m(a,hy,g),y(Xs,a,g),m(a,dy,g),m(a,xp,g),e(xp,DP),m(a,gy,g),m(a,Qs,g),e(Qs,bq),e(bq,nl),e(nl,Ip),e(Ip,OP),e(nl,PP),e(nl,Tq),e(Qs,RP),e(Qs,z),e(z,rl),e(rl,ol),e(ol,jq),e(jq,SP),e(ol,NP),e(rl,xP),e(rl,Hp),e(Hp,IP),e(z,HP),e(z,ll),e(ll,il),e(il,kq),e(kq,BP),e(il,CP),e(ll,GP),e(ll,Bp),e(Bp,LP),e(z,MP),e(z,ul),e(ul,Cp),e(Cp,UP),e(ul,zP),e(ul,je),e(je,KP),e(je,Aq),e(Aq,FP),e(je,JP),e(je,Dq),e(Dq,WP),e(je,YP),e(z,VP),e(z,cl),e(cl,Gp),e(Gp,XP),e(cl,QP),e(cl,Zs),e(Zs,ZP),e(Zs,Oq),e(Oq,eR),e(Zs,tR),e(z,sR),e(z,fl),e(fl,Lp),e(Lp,Pq),e(Pq,aR),e(fl,nR),e(fl,Mp),e(Mp,rR),e(z,oR),e(z,pl),e(pl,Up),e(Up,lR),e(pl,iR),e(pl,ea),e(ea,uR),e(ea,Rq),e(Rq,cR),e(ea,fR),e(z,pR),e(z,hl),e(hl,zp),e(zp,hR),e(hl,dR),e(hl,ta),e(ta,gR),e(ta,Sq),e(Sq,mR),e(ta,$R),e(z,qR),e(z,dl),e(dl,Kp),e(Kp,_R),e(dl,vR),e(dl,sa),e(sa,yR),e(sa,Nq),e(Nq,ER),e(sa,wR),m(a,my,g),m(a,Fp,g),e(Fp,bR),m(a,$y,g),m(a,Jp,g),e(Jp,TR),m(a,qy,g),y(aa,a,g),m(a,_y,g),m(a,na,g),e(na,xq),e(xq,gl),e(gl,Wp),e(Wp,jR),e(gl,kR),e(gl,Iq),e(na,AR),e(na,Xe),e(Xe,ml),e(ml,Yp),e(Yp,Hq),e(Hq,DR),e(ml,OR),e(ml,Vp),e(Vp,PR),e(Xe,RR),e(Xe,$l),e($l,Xp),e(Xp,Bq),e(Bq,SR),e($l,NR),e($l,Qp),e(Qp,xR),e(Xe,IR),e(Xe,ql),e(ql,Zp),e(Zp,Cq),e(Cq,HR),e(ql,BR),e(ql,ra),e(ra,CR),e(ra,Gq),e(Gq,GR),e(ra,LR),m(a,vy,g),m(a,Qe,g),e(Qe,oa),e(oa,Lq),y(_l,Lq,null),e(Qe,MR),e(Qe,Mq),e(Mq,UR),m(a,yy,g),m(a,eh,g),e(eh,zR),m(a,Ey,g),y(la,a,g),m(a,wy,g),m(a,vl,g),e(vl,KR),e(vl,yl),e(yl,FR),m(a,by,g),m(a,th,g),e(th,JR),m(a,Ty,g),y(ia,a,g),m(a,jy,g),m(a,sh,g),e(sh,WR),m(a,ky,g),m(a,ua,g),e(ua,Uq),e(Uq,El),e(El,ah),e(ah,YR),e(El,VR),e(El,zq),e(ua,XR),e(ua,N),e(N,wl),e(wl,bl),e(bl,Kq),e(Kq,QR),e(bl,ZR),e(wl,eS),e(wl,Fq),e(N,tS),e(N,Tl),e(Tl,nh),e(nh,sS),e(Tl,aS),e(Tl,rh),e(rh,nS),e(N,rS),e(N,jl),e(jl,oh),e(oh,oS),e(jl,lS),e(jl,lh),e(lh,iS),e(N,uS),e(N,kl),e(kl,ih),e(ih,cS),e(kl,fS),e(kl,ca),e(ca,pS),e(ca,Jq),e(Jq,hS),e(ca,dS),e(N,gS),e(N,Al),e(Al,uh),e(uh,Wq),e(Wq,mS),e(Al,$S),e(Al,ch),e(ch,qS),e(N,_S),e(N,Dl),e(Dl,fh),e(fh,vS),e(Dl,yS),e(Dl,ke),e(ke,ES),e(ke,Yq),e(Yq,wS),e(ke,bS),e(ke,Vq),e(Vq,TS),e(ke,jS),e(N,kS),e(N,Ol),e(Ol,ph),e(ph,AS),e(Ol,DS),e(Ol,Ae),e(Ae,OS),e(Ae,Xq),e(Xq,PS),e(Ae,RS),e(Ae,Qq),e(Qq,SS),e(Ae,NS),e(N,xS),e(N,Pl),e(Pl,hh),e(hh,IS),e(Pl,HS),e(Pl,De),e(De,BS),e(De,Zq),e(Zq,CS),e(De,GS),e(De,e_),e(e_,LS),e(De,MS),e(N,US),e(N,Rl),e(Rl,dh),e(dh,zS),e(Rl,KS),e(Rl,pe),e(pe,FS),e(pe,t_),e(t_,JS),e(pe,WS),e(pe,s_),e(s_,YS),e(pe,VS),e(pe,a_),e(a_,XS),e(pe,QS),e(N,ZS),e(N,Sl),e(Sl,gh),e(gh,eN),e(Sl,tN),e(Sl,he),e(he,sN),e(he,n_),e(n_,aN),e(he,nN),e(he,r_),e(r_,rN),e(he,oN),e(he,o_),e(o_,lN),e(he,iN),e(N,uN),e(N,Nl),e(Nl,mh),e(mh,cN),e(Nl,fN),e(Nl,fa),e(fa,pN),e(fa,l_),e(l_,hN),e(fa,dN),e(N,gN),e(N,xl),e(xl,$h),e($h,mN),e(xl,$N),e(xl,pa),e(pa,qN),e(pa,i_),e(i_,_N),e(pa,vN),e(N,yN),e(N,Il),e(Il,qh),e(qh,u_),e(u_,EN),e(Il,wN),e(Il,_h),e(_h,bN),e(N,TN),e(N,Hl),e(Hl,vh),e(vh,jN),e(Hl,kN),e(Hl,ha),e(ha,AN),e(ha,c_),e(c_,DN),e(ha,ON),e(N,PN),e(N,Bl),e(Bl,yh),e(yh,RN),e(Bl,SN),e(Bl,da),e(da,NN),e(da,f_),e(f_,xN),e(da,IN),e(N,HN),e(N,Cl),e(Cl,Eh),e(Eh,BN),e(Cl,CN),e(Cl,ga),e(ga,GN),e(ga,p_),e(p_,LN),e(ga,MN),m(a,Ay,g),m(a,wh,g),e(wh,UN),m(a,Dy,g),m(a,ma,g),e(ma,h_),e(h_,Gl),e(Gl,bh),e(bh,zN),e(Gl,KN),e(Gl,d_),e(ma,FN),e(ma,$e),e($e,Ll),e(Ll,Th),e(Th,g_),e(g_,JN),e(Ll,WN),e(Ll,jh),e(jh,YN),e($e,VN),e($e,Ml),e(Ml,kh),e(kh,m_),e(m_,XN),e(Ml,QN),e(Ml,Ah),e(Ah,ZN),e($e,ex),e($e,Ul),e(Ul,Dh),e(Dh,tx),e(Ul,sx),e(Ul,Oh),e(Oh,ax),e($e,nx),e($e,zl),e(zl,Ph),e(Ph,rx),e(zl,ox),e(zl,Rh),e(Rh,lx),m(a,Oy,g),m(a,Ze,g),e(Ze,$a),e($a,$_),y(Kl,$_,null),e(Ze,ix),e(Ze,q_),e(q_,ux),m(a,Py,g),m(a,Sh,g),e(Sh,cx),m(a,Ry,g),y(qa,a,g),m(a,Sy,g),m(a,et,g),e(et,fx),e(et,Fl),e(Fl,px),e(et,hx),e(et,Jl),e(Jl,dx),m(a,Ny,g),m(a,Nh,g),e(Nh,gx),m(a,xy,g),m(a,_a,g),e(_a,__),e(__,Wl),e(Wl,xh),e(xh,mx),e(Wl,$x),e(Wl,v_),e(_a,qx),e(_a,re),e(re,Yl),e(Yl,Vl),e(Vl,y_),e(y_,_x),e(Vl,vx),e(Yl,yx),e(Yl,Ih),e(Ih,Ex),e(re,wx),e(re,Xl),e(Xl,Hh),e(Hh,E_),e(E_,bx),e(Xl,Tx),e(Xl,Bh),e(Bh,jx),e(re,kx),e(re,Ql),e(Ql,Ch),e(Ch,Ax),e(Ql,Dx),e(Ql,va),e(va,Ox),e(va,w_),e(w_,Px),e(va,Rx),e(re,Sx),e(re,Zl),e(Zl,Gh),e(Gh,Nx),e(Zl,xx),e(Zl,ya),e(ya,Ix),e(ya,b_),e(b_,Hx),e(ya,Bx),e(re,Cx),e(re,ei),e(ei,Lh),e(Lh,Gx),e(ei,Lx),e(ei,Ea),e(Ea,Mx),e(Ea,T_),e(T_,Ux),e(Ea,zx),m(a,Iy,g),m(a,Mh,g),e(Mh,Kx),m(a,Hy,g),m(a,wa,g),e(wa,j_),e(j_,ti),e(ti,Uh),e(Uh,Fx),e(ti,Jx),e(ti,k_),e(wa,Wx),e(wa,A_),e(A_,si),e(si,zh),e(zh,D_),e(D_,Yx),e(si,Vx),e(si,Kh),e(Kh,Xx),m(a,By,g),m(a,Fh,g),e(Fh,Qx),m(a,Cy,g),m(a,tt,g),e(tt,ba),e(ba,O_),y(ai,O_,null),e(tt,Zx),e(tt,P_),e(P_,eI),m(a,Gy,g),m(a,st,g),e(st,Ta),e(Ta,R_),y(ni,R_,null),e(st,tI),e(st,S_),e(S_,sI),m(a,Ly,g),m(a,Jh,g),e(Jh,aI),m(a,My,g),y(ja,a,g),m(a,Uy,g),y(ka,a,g),m(a,zy,g),m(a,qe,g),e(qe,nI),e(qe,ri),e(ri,rI),e(qe,oI),e(qe,oi),e(oi,lI),e(qe,iI),e(qe,li),e(li,uI),m(a,Ky,g),m(a,Wh,g),e(Wh,cI),m(a,Fy,g),y(Aa,a,g),m(a,Jy,g),m(a,Yh,g),e(Yh,fI),m(a,Wy,g),m(a,Da,g),e(Da,N_),e(N_,ii),e(ii,Vh),e(Vh,pI),e(ii,hI),e(ii,x_),e(Da,dI),e(Da,I_),e(I_,ui),e(ui,ci),e(ci,H_),e(H_,gI),e(ci,mI),e(ui,$I),e(ui,Xh),e(Xh,qI),m(a,Yy,g),m(a,Qh,g),e(Qh,_I),m(a,Vy,g),m(a,Zh,g),e(Zh,vI),m(a,Xy,g),y(Oa,a,g),m(a,Qy,g),m(a,Pa,g),e(Pa,B_),e(B_,fi),e(fi,ed),e(ed,yI),e(fi,EI),e(fi,C_),e(Pa,wI),e(Pa,G_),e(G_,pi),e(pi,td),e(td,L_),e(L_,bI),e(pi,TI),e(pi,sd),e(sd,jI),m(a,Zy,g),m(a,at,g),e(at,Ra),e(Ra,M_),y(hi,M_,null),e(at,kI),e(at,U_),e(U_,AI),m(a,eE,g),m(a,ad,g),e(ad,DI),m(a,tE,g),y(Sa,a,g),m(a,sE,g),m(a,nt,g),e(nt,OI),e(nt,di),e(di,PI),e(nt,RI),e(nt,gi),e(gi,SI),m(a,aE,g),m(a,nd,g),e(nd,NI),m(a,nE,g),y(Na,a,g),m(a,rE,g),m(a,rd,g),e(rd,xI),m(a,oE,g),m(a,xa,g),e(xa,z_),e(z_,mi),e(mi,od),e(od,II),e(mi,HI),e(mi,K_),e(xa,BI),e(xa,F_),e(F_,$i),e($i,qi),e(qi,J_),e(J_,CI),e(qi,GI),e($i,LI),e($i,ld),e(ld,MI),m(a,lE,g),m(a,id,g),e(id,UI),m(a,iE,g),y(Ia,a,g),m(a,uE,g),m(a,Ha,g),e(Ha,W_),e(W_,_i),e(_i,ud),e(ud,zI),e(_i,KI),e(_i,Y_),e(Ha,FI),e(Ha,vi),e(vi,yi),e(yi,cd),e(cd,V_),e(V_,JI),e(yi,WI),e(yi,fd),e(fd,YI),e(vi,VI),e(vi,Ei),e(Ei,pd),e(pd,X_),e(X_,XI),e(Ei,QI),e(Ei,hd),e(hd,ZI),m(a,cE,g),m(a,rt,g),e(rt,Ba),e(Ba,Q_),y(wi,Q_,null),e(rt,eH),e(rt,Z_),e(Z_,tH),m(a,fE,g),m(a,ot,g),e(ot,Ca),e(Ca,e1),y(bi,e1,null),e(ot,sH),e(ot,t1),e(t1,aH),m(a,pE,g),m(a,dd,g),e(dd,nH),m(a,hE,g),y(Ga,a,g),m(a,dE,g),m(a,Ti,g),e(Ti,rH),e(Ti,ji),e(ji,oH),m(a,gE,g),m(a,gd,g),e(gd,lH),m(a,mE,g),y(La,a,g),m(a,$E,g),m(a,Ma,g),e(Ma,iH),e(Ma,ki),e(ki,uH),e(Ma,cH),m(a,qE,g),m(a,Ua,g),e(Ua,s1),e(s1,Ai),e(Ai,md),e(md,fH),e(Ai,pH),e(Ai,a1),e(Ua,hH),e(Ua,n1),e(n1,Di),e(Di,Oi),e(Oi,r1),e(r1,dH),e(Oi,gH),e(Di,mH),e(Di,$d),e($d,$H),m(a,_E,g),m(a,qd,g),e(qd,qH),m(a,vE,g),y(za,a,g),m(a,yE,g),m(a,Ka,g),e(Ka,o1),e(o1,Pi),e(Pi,_d),e(_d,_H),e(Pi,vH),e(Pi,l1),e(Ka,yH),e(Ka,Ri),e(Ri,Si),e(Si,vd),e(vd,i1),e(i1,EH),e(Si,wH),e(Si,yd),e(yd,bH),e(Ri,TH),e(Ri,Ni),e(Ni,Ed),e(Ed,u1),e(u1,jH),e(Ni,kH),e(Ni,wd),e(wd,AH),m(a,EE,g),m(a,lt,g),e(lt,Fa),e(Fa,c1),y(xi,c1,null),e(lt,DH),e(lt,f1),e(f1,OH),m(a,wE,g),m(a,bd,g),e(bd,PH),m(a,bE,g),y(Ja,a,g),m(a,TE,g),m(a,Ii,g),e(Ii,RH),e(Ii,Hi),e(Hi,SH),m(a,jE,g),m(a,Td,g),e(Td,NH),m(a,kE,g),y(Wa,a,g),m(a,AE,g),m(a,Ya,g),e(Ya,xH),e(Ya,Bi),e(Bi,IH),e(Ya,HH),m(a,DE,g),m(a,Va,g),e(Va,p1),e(p1,Ci),e(Ci,jd),e(jd,BH),e(Ci,CH),e(Ci,h1),e(Va,GH),e(Va,d1),e(d1,Gi),e(Gi,Li),e(Li,g1),e(g1,LH),e(Li,MH),e(Gi,UH),e(Gi,kd),e(kd,zH),m(a,OE,g),m(a,Ad,g),e(Ad,KH),m(a,PE,g),y(Xa,a,g),m(a,RE,g),m(a,Qa,g),e(Qa,m1),e(m1,Mi),e(Mi,Dd),e(Dd,FH),e(Mi,JH),e(Mi,$1),e(Qa,WH),e(Qa,it),e(it,Ui),e(Ui,Od),e(Od,q1),e(q1,YH),e(Ui,VH),e(Ui,Pd),e(Pd,XH),e(it,QH),e(it,zi),e(zi,Rd),e(Rd,_1),e(_1,ZH),e(zi,eB),e(zi,Sd),e(Sd,tB),e(it,sB),e(it,Ki),e(Ki,Nd),e(Nd,v1),e(v1,aB),e(Ki,nB),e(Ki,xd),e(xd,rB),m(a,SE,g),m(a,ut,g),e(ut,Za),e(Za,y1),y(Fi,y1,null),e(ut,oB),e(ut,E1),e(E1,lB),m(a,NE,g),m(a,Id,g),e(Id,iB),m(a,xE,g),y(en,a,g),m(a,IE,g),m(a,Ji,g),e(Ji,uB),e(Ji,Wi),e(Wi,cB),m(a,HE,g),m(a,Hd,g),e(Hd,fB),m(a,BE,g),y(tn,a,g),m(a,CE,g),m(a,sn,g),e(sn,pB),e(sn,Yi),e(Yi,hB),e(sn,dB),m(a,GE,g),m(a,an,g),e(an,w1),e(w1,Vi),e(Vi,Bd),e(Bd,gB),e(Vi,mB),e(Vi,b1),e(an,$B),e(an,T1),e(T1,Xi),e(Xi,Qi),e(Qi,j1),e(j1,qB),e(Qi,_B),e(Xi,vB),e(Xi,Cd),e(Cd,yB),m(a,LE,g),m(a,Gd,g),e(Gd,EB),m(a,ME,g),y(nn,a,g),m(a,UE,g),m(a,rn,g),e(rn,k1),e(k1,Zi),e(Zi,Ld),e(Ld,wB),e(Zi,bB),e(Zi,A1),e(rn,TB),e(rn,ct),e(ct,eu),e(eu,Md),e(Md,D1),e(D1,jB),e(eu,kB),e(eu,Ud),e(Ud,AB),e(ct,DB),e(ct,tu),e(tu,zd),e(zd,O1),e(O1,OB),e(tu,PB),e(tu,Kd),e(Kd,RB),e(ct,SB),e(ct,su),e(su,Fd),e(Fd,P1),e(P1,NB),e(su,xB),e(su,Jd),e(Jd,IB),zE=!0},p(a,[g]){const au={};g&2&&(au.$$scope={dirty:g,ctx:a}),mt.$set(au);const R1={};g&2&&(R1.$$scope={dirty:g,ctx:a}),$t.$set(R1);const S1={};g&2&&(S1.$$scope={dirty:g,ctx:a}),Et.$set(S1);const N1={};g&2&&(N1.$$scope={dirty:g,ctx:a}),jt.$set(N1);const nu={};g&2&&(nu.$$scope={dirty:g,ctx:a}),kt.$set(nu);const x1={};g&2&&(x1.$$scope={dirty:g,ctx:a}),It.$set(x1);const I1={};g&2&&(I1.$$scope={dirty:g,ctx:a}),Ht.$set(I1);const H1={};g&2&&(H1.$$scope={dirty:g,ctx:a}),Bt.$set(H1);const B1={};g&2&&(B1.$$scope={dirty:g,ctx:a}),Ut.$set(B1);const C1={};g&2&&(C1.$$scope={dirty:g,ctx:a}),zt.$set(C1);const ru={};g&2&&(ru.$$scope={dirty:g,ctx:a}),Yt.$set(ru);const G1={};g&2&&(G1.$$scope={dirty:g,ctx:a}),Qt.$set(G1);const L1={};g&2&&(L1.$$scope={dirty:g,ctx:a}),Zt.$set(L1);const M1={};g&2&&(M1.$$scope={dirty:g,ctx:a}),ns.$set(M1);const ou={};g&2&&(ou.$$scope={dirty:g,ctx:a}),ls.$set(ou);const U1={};g&2&&(U1.$$scope={dirty:g,ctx:a}),is.$set(U1);const z1={};g&2&&(z1.$$scope={dirty:g,ctx:a}),hs.$set(z1);const K1={};g&2&&(K1.$$scope={dirty:g,ctx:a}),ms.$set(K1);const F1={};g&2&&(F1.$$scope={dirty:g,ctx:a}),$s.$set(F1);const Wd={};g&2&&(Wd.$$scope={dirty:g,ctx:a}),Ts.$set(Wd);const J1={};g&2&&(J1.$$scope={dirty:g,ctx:a}),Os.$set(J1);const W1={};g&2&&(W1.$$scope={dirty:g,ctx:a}),Ps.$set(W1);const Y1={};g&2&&(Y1.$$scope={dirty:g,ctx:a}),Is.$set(Y1);const lu={};g&2&&(lu.$$scope={dirty:g,ctx:a}),Ms.$set(lu);const V1={};g&2&&(V1.$$scope={dirty:g,ctx:a}),Us.$set(V1);const iu={};g&2&&(iu.$$scope={dirty:g,ctx:a}),Vs.$set(iu);const X1={};g&2&&(X1.$$scope={dirty:g,ctx:a}),Xs.$set(X1);const oe={};g&2&&(oe.$$scope={dirty:g,ctx:a}),aa.$set(oe);const uu={};g&2&&(uu.$$scope={dirty:g,ctx:a}),la.$set(uu);const Yd={};g&2&&(Yd.$$scope={dirty:g,ctx:a}),ia.$set(Yd);const Q1={};g&2&&(Q1.$$scope={dirty:g,ctx:a}),qa.$set(Q1);const Z1={};g&2&&(Z1.$$scope={dirty:g,ctx:a}),ja.$set(Z1);const cu={};g&2&&(cu.$$scope={dirty:g,ctx:a}),ka.$set(cu);const e2={};g&2&&(e2.$$scope={dirty:g,ctx:a}),Aa.$set(e2);const t2={};g&2&&(t2.$$scope={dirty:g,ctx:a}),Oa.$set(t2);const s2={};g&2&&(s2.$$scope={dirty:g,ctx:a}),Sa.$set(s2);const fu={};g&2&&(fu.$$scope={dirty:g,ctx:a}),Na.$set(fu);const a2={};g&2&&(a2.$$scope={dirty:g,ctx:a}),Ia.$set(a2);const pu={};g&2&&(pu.$$scope={dirty:g,ctx:a}),Ga.$set(pu);const n2={};g&2&&(n2.$$scope={dirty:g,ctx:a}),La.$set(n2);const hu={};g&2&&(hu.$$scope={dirty:g,ctx:a}),za.$set(hu);const r2={};g&2&&(r2.$$scope={dirty:g,ctx:a}),Ja.$set(r2);const du={};g&2&&(du.$$scope={dirty:g,ctx:a}),Wa.$set(du);const o2={};g&2&&(o2.$$scope={dirty:g,ctx:a}),Xa.$set(o2);const gu={};g&2&&(gu.$$scope={dirty:g,ctx:a}),en.$set(gu);const l2={};g&2&&(l2.$$scope={dirty:g,ctx:a}),tn.$set(l2);const mu={};g&2&&(mu.$$scope={dirty:g,ctx:a}),nn.$set(mu)},i(a){zE||(E(k.$$.fragment,a),E(ee.$$.fragment,a),E(qn.$$.fragment,a),E(_n.$$.fragment,a),E(mt.$$.fragment,a),E($t.$$.fragment,a),E(Et.$$.fragment,a),E(Nn.$$.fragment,a),E(jt.$$.fragment,a),E(kt.$$.fragment,a),E(er.$$.fragment,a),E(It.$$.fragment,a),E(Ht.$$.fragment,a),E(Bt.$$.fragment,a),E(ir.$$.fragment,a),E(Ut.$$.fragment,a),E(zt.$$.fragment,a),E(Yt.$$.fragment,a),E(Tr.$$.fragment,a),E(Qt.$$.fragment,a),E(Zt.$$.fragment,a),E(ns.$$.fragment,a),E(Cr.$$.fragment,a),E(ls.$$.fragment,a),E(is.$$.fragment,a),E(hs.$$.fragment,a),E(Zr.$$.fragment,a),E(ms.$$.fragment,a),E($s.$$.fragment,a),E(Ts.$$.fragment,a),E(Eo.$$.fragment,a),E(wo.$$.fragment,a),E(Os.$$.fragment,a),E(Ps.$$.fragment,a),E(Is.$$.fragment,a),E(Lo.$$.fragment,a),E(Uo.$$.fragment,a),E(Ms.$$.fragment,a),E(Us.$$.fragment,a),E(tl.$$.fragment,a),E(Vs.$$.fragment,a),E(Xs.$$.fragment,a),E(aa.$$.fragment,a),E(_l.$$.fragment,a),E(la.$$.fragment,a),E(ia.$$.fragment,a),E(Kl.$$.fragment,a),E(qa.$$.fragment,a),E(ai.$$.fragment,a),E(ni.$$.fragment,a),E(ja.$$.fragment,a),E(ka.$$.fragment,a),E(Aa.$$.fragment,a),E(Oa.$$.fragment,a),E(hi.$$.fragment,a),E(Sa.$$.fragment,a),E(Na.$$.fragment,a),E(Ia.$$.fragment,a),E(wi.$$.fragment,a),E(bi.$$.fragment,a),E(Ga.$$.fragment,a),E(La.$$.fragment,a),E(za.$$.fragment,a),E(xi.$$.fragment,a),E(Ja.$$.fragment,a),E(Wa.$$.fragment,a),E(Xa.$$.fragment,a),E(Fi.$$.fragment,a),E(en.$$.fragment,a),E(tn.$$.fragment,a),E(nn.$$.fragment,a),zE=!0)},o(a){w(k.$$.fragment,a),w(ee.$$.fragment,a),w(qn.$$.fragment,a),w(_n.$$.fragment,a),w(mt.$$.fragment,a),w($t.$$.fragment,a),w(Et.$$.fragment,a),w(Nn.$$.fragment,a),w(jt.$$.fragment,a),w(kt.$$.fragment,a),w(er.$$.fragment,a),w(It.$$.fragment,a),w(Ht.$$.fragment,a),w(Bt.$$.fragment,a),w(ir.$$.fragment,a),w(Ut.$$.fragment,a),w(zt.$$.fragment,a),w(Yt.$$.fragment,a),w(Tr.$$.fragment,a),w(Qt.$$.fragment,a),w(Zt.$$.fragment,a),w(ns.$$.fragment,a),w(Cr.$$.fragment,a),w(ls.$$.fragment,a),w(is.$$.fragment,a),w(hs.$$.fragment,a),w(Zr.$$.fragment,a),w(ms.$$.fragment,a),w($s.$$.fragment,a),w(Ts.$$.fragment,a),w(Eo.$$.fragment,a),w(wo.$$.fragment,a),w(Os.$$.fragment,a),w(Ps.$$.fragment,a),w(Is.$$.fragment,a),w(Lo.$$.fragment,a),w(Uo.$$.fragment,a),w(Ms.$$.fragment,a),w(Us.$$.fragment,a),w(tl.$$.fragment,a),w(Vs.$$.fragment,a),w(Xs.$$.fragment,a),w(aa.$$.fragment,a),w(_l.$$.fragment,a),w(la.$$.fragment,a),w(ia.$$.fragment,a),w(Kl.$$.fragment,a),w(qa.$$.fragment,a),w(ai.$$.fragment,a),w(ni.$$.fragment,a),w(ja.$$.fragment,a),w(ka.$$.fragment,a),w(Aa.$$.fragment,a),w(Oa.$$.fragment,a),w(hi.$$.fragment,a),w(Sa.$$.fragment,a),w(Na.$$.fragment,a),w(Ia.$$.fragment,a),w(wi.$$.fragment,a),w(bi.$$.fragment,a),w(Ga.$$.fragment,a),w(La.$$.fragment,a),w(za.$$.fragment,a),w(xi.$$.fragment,a),w(Ja.$$.fragment,a),w(Wa.$$.fragment,a),w(Xa.$$.fragment,a),w(Fi.$$.fragment,a),w(en.$$.fragment,a),w(tn.$$.fragment,a),w(nn.$$.fragment,a),zE=!1},d(a){t(n),a&&t(c),a&&t(s),b(k),a&&t(S),a&&t(D),b(ee),a&&t($n),a&&t(xe),a&&t(p2),a&&t(_u),a&&t(h2),a&&t(pt),a&&t(d2),a&&t(ht),a&&t(g2),a&&t(Ie),b(qn),a&&t(m2),a&&t(He),b(_n),a&&t($2),a&&t(vu),a&&t(q2),b(mt,a),a&&t(_2),a&&t(vn),a&&t(v2),a&&t(yu),a&&t(y2),b($t,a),a&&t(E2),a&&t(Eu),a&&t(w2),a&&t(qt),a&&t(b2),a&&t(Ou),a&&t(T2),b(Et,a),a&&t(j2),a&&t(wt),a&&t(k2),a&&t(Be),b(Nn),a&&t(A2),a&&t(Tt),a&&t(D2),b(jt,a),a&&t(O2),a&&t(xn),a&&t(P2),a&&t(Lu),a&&t(R2),b(kt,a),a&&t(S2),a&&t(Mu),a&&t(N2),a&&t(At),a&&t(x2),a&&t(rc),a&&t(I2),a&&t(Nt),a&&t(H2),a&&t(Ce),b(er),a&&t(B2),a&&t(uc),a&&t(C2),b(It,a),a&&t(G2),a&&t(Ge),a&&t(L2),a&&t(cc),a&&t(M2),b(Ht,a),a&&t(U2),a&&t(fc),a&&t(z2),a&&t(pc),a&&t(K2),b(Bt,a),a&&t(F2),a&&t(Ct),a&&t(J2),a&&t(Le),b(ir),a&&t(W2),a&&t(vc),a&&t(Y2),b(Ut,a),a&&t(V2),a&&t(ur),a&&t(X2),a&&t(yc),a&&t(Q2),b(zt,a),a&&t(Z2),a&&t(Ec),a&&t(ev),a&&t(Kt),a&&t(tv),a&&t(Sc),a&&t(sv),b(Yt,a),a&&t(av),a&&t(Vt),a&&t(nv),a&&t(Me),b(Tr),a&&t(rv),a&&t(Uc),a&&t(ov),b(Qt,a),a&&t(lv),a&&t(jr),a&&t(iv),a&&t(zc),a&&t(uv),b(Zt,a),a&&t(cv),a&&t(Kc),a&&t(fv),a&&t(es),a&&t(pv),a&&t(sf),a&&t(hv),b(ns,a),a&&t(dv),a&&t(rs),a&&t(gv),a&&t(Ue),b(Cr),a&&t(mv),a&&t(of),a&&t($v),b(ls,a),a&&t(qv),a&&t(Gr),a&&t(_v),a&&t(lf),a&&t(vv),b(is,a),a&&t(yv),a&&t(uf),a&&t(Ev),a&&t(us),a&&t(wv),a&&t($f),a&&t(bv),b(hs,a),a&&t(Tv),a&&t(ds),a&&t(jv),a&&t(ze),b(Zr),a&&t(kv),a&&t(wf),a&&t(Av),b(ms,a),a&&t(Dv),a&&t(eo),a&&t(Ov),a&&t(bf),a&&t(Pv),b($s,a),a&&t(Rv),a&&t(Tf),a&&t(Sv),a&&t(qs),a&&t(Nv),a&&t(zf),a&&t(xv),b(Ts,a),a&&t(Iv),a&&t(js),a&&t(Hv),a&&t(Ke),b(Eo),a&&t(Bv),a&&t(As),a&&t(Cv),a&&t(Fe),b(wo),a&&t(Gv),a&&t(Yf),a&&t(Lv),b(Os,a),a&&t(Mv),a&&t(Je),a&&t(Uv),a&&t(Vf),a&&t(zv),b(Ps,a),a&&t(Kv),a&&t(Xf),a&&t(Fv),a&&t(Rs),a&&t(Jv),a&&t(ip),a&&t(Wv),b(Is,a),a&&t(Yv),a&&t(Hs),a&&t(Vv),a&&t(We),b(Lo),a&&t(Xv),a&&t(Mo),a&&t(Qv),a&&t(Ye),b(Uo),a&&t(Zv),a&&t(_p),a&&t(ey),b(Ms,a),a&&t(ty),a&&t(zo),a&&t(sy),a&&t(vp),a&&t(ay),b(Us,a),a&&t(ny),a&&t(yp),a&&t(ry),a&&t(zs),a&&t(oy),a&&t(Dp),a&&t(ly),a&&t(Ws),a&&t(iy),a&&t(Ve),b(tl),a&&t(uy),a&&t(Sp),a&&t(cy),b(Vs,a),a&&t(fy),a&&t(sl),a&&t(py),a&&t(Np),a&&t(hy),b(Xs,a),a&&t(dy),a&&t(xp),a&&t(gy),a&&t(Qs),a&&t(my),a&&t(Fp),a&&t($y),a&&t(Jp),a&&t(qy),b(aa,a),a&&t(_y),a&&t(na),a&&t(vy),a&&t(Qe),b(_l),a&&t(yy),a&&t(eh),a&&t(Ey),b(la,a),a&&t(wy),a&&t(vl),a&&t(by),a&&t(th),a&&t(Ty),b(ia,a),a&&t(jy),a&&t(sh),a&&t(ky),a&&t(ua),a&&t(Ay),a&&t(wh),a&&t(Dy),a&&t(ma),a&&t(Oy),a&&t(Ze),b(Kl),a&&t(Py),a&&t(Sh),a&&t(Ry),b(qa,a),a&&t(Sy),a&&t(et),a&&t(Ny),a&&t(Nh),a&&t(xy),a&&t(_a),a&&t(Iy),a&&t(Mh),a&&t(Hy),a&&t(wa),a&&t(By),a&&t(Fh),a&&t(Cy),a&&t(tt),b(ai),a&&t(Gy),a&&t(st),b(ni),a&&t(Ly),a&&t(Jh),a&&t(My),b(ja,a),a&&t(Uy),b(ka,a),a&&t(zy),a&&t(qe),a&&t(Ky),a&&t(Wh),a&&t(Fy),b(Aa,a),a&&t(Jy),a&&t(Yh),a&&t(Wy),a&&t(Da),a&&t(Yy),a&&t(Qh),a&&t(Vy),a&&t(Zh),a&&t(Xy),b(Oa,a),a&&t(Qy),a&&t(Pa),a&&t(Zy),a&&t(at),b(hi),a&&t(eE),a&&t(ad),a&&t(tE),b(Sa,a),a&&t(sE),a&&t(nt),a&&t(aE),a&&t(nd),a&&t(nE),b(Na,a),a&&t(rE),a&&t(rd),a&&t(oE),a&&t(xa),a&&t(lE),a&&t(id),a&&t(iE),b(Ia,a),a&&t(uE),a&&t(Ha),a&&t(cE),a&&t(rt),b(wi),a&&t(fE),a&&t(ot),b(bi),a&&t(pE),a&&t(dd),a&&t(hE),b(Ga,a),a&&t(dE),a&&t(Ti),a&&t(gE),a&&t(gd),a&&t(mE),b(La,a),a&&t($E),a&&t(Ma),a&&t(qE),a&&t(Ua),a&&t(_E),a&&t(qd),a&&t(vE),b(za,a),a&&t(yE),a&&t(Ka),a&&t(EE),a&&t(lt),b(xi),a&&t(wE),a&&t(bd),a&&t(bE),b(Ja,a),a&&t(TE),a&&t(Ii),a&&t(jE),a&&t(Td),a&&t(kE),b(Wa,a),a&&t(AE),a&&t(Ya),a&&t(DE),a&&t(Va),a&&t(OE),a&&t(Ad),a&&t(PE),b(Xa,a),a&&t(RE),a&&t(Qa),a&&t(SE),a&&t(ut),b(Fi),a&&t(NE),a&&t(Id),a&&t(xE),b(en,a),a&&t(IE),a&&t(Ji),a&&t(HE),a&&t(Hd),a&&t(BE),b(tn,a),a&&t(CE),a&&t(sn),a&&t(GE),a&&t(an),a&&t(LE),a&&t(Gd),a&&t(ME),b(nn,a),a&&t(UE),a&&t(rn)}}}const HQ={local:"detailed-parameters",sections:[{local:"which-task-is-used-by-this-model",title:"Which task is used by this model ?"},{local:"natural-language-processing",sections:[{local:"fill-mask-task",title:"Fill Mask task"},{local:"summarization-task",title:"Summarization task"},{local:"question-answering-task",title:"Question Answering task"},{local:"table-question-answering-task",title:"Table Question Answering task"},{local:"sentence-similarity-task",title:"Sentence Similarity task"},{local:"text-classification-task",title:"Text Classification task"},{local:"text-generation-task",title:"Text Generation task"},{local:"text2text-generation-task",title:"Text2Text Generation task"},{local:"token-classification-task",title:"Token Classification task"},{local:"named-entity-recognition-ner-task",title:"Named Entity Recognition (NER) task"},{local:"translation-task",title:"Translation task"},{local:"zeroshot-classification-task",title:"Zero-Shot Classification task"},{local:"conversational-task",title:"Conversational task"},{local:"feature-extraction-task",title:"Feature Extraction task"}],title:"Natural Language Processing"},{local:"audio",sections:[{local:"automatic-speech-recognition-task",title:"Automatic Speech Recognition task"},{local:"audio-classification-task",title:"Audio Classification task"}],title:"Audio"},{local:"computer-vision",sections:[{local:"image-classification-task",title:"Image Classification task"},{local:"object-detection-task",title:"Object Detection task"},{local:"image-segmentation-task",title:"Image Segmentation task"}],title:"Computer Vision"}],title:"Detailed parameters"};function BQ(q){return aV(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class UQ extends ZY{constructor(n){super();eV(this,n,BQ,IQ,tV,{})}}export{UQ as default,HQ as metadata};
