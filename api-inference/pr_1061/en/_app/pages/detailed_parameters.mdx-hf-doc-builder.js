import{S as YF,i as VF,s as XF,e as n,k as p,w as _,t as i,M as QF,c as o,d as t,m as h,a as l,x as v,h as u,b as f,N as WF,G as e,g as m,y as E,q as y,o as w,B as b,v as ZF,L as O}from"../chunks/vendor-hf-doc-builder.js";import{T as J}from"../chunks/Tip-hf-doc-builder.js";import{I as z}from"../chunks/IconCopyLink-hf-doc-builder.js";import{I as L,M as P,C as R}from"../chunks/InferenceApi-hf-doc-builder.js";function eJ(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("bert-base-uncased"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"bert-base-uncased"),D.forEach(t),A=u(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/bert-base-uncased"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function tJ(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/bert-base-uncased"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is [MASK]."})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is [MASK].&quot;</span>})`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function sJ(q){let r,c;return r=new P({props:{$$slots:{default:[tJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function aJ(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/bert-base-uncased",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is [MASK]."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;The answer to the universe is [MASK].&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:<span class="hljs-number">0.16963955760002136</span>,&quot;token&quot;:<span class="hljs-number">2053</span>,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:<span class="hljs-number">0.07344776391983032</span>,&quot;token&quot;:<span class="hljs-number">2498</span>,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:<span class="hljs-number">0.05803241208195686</span>,&quot;token&quot;:<span class="hljs-number">2748</span>,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:<span class="hljs-number">0.043957844376564026</span>,&quot;token&quot;:<span class="hljs-number">4242</span>,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:<span class="hljs-number">0.04015745222568512</span>,&quot;token&quot;:<span class="hljs-number">3722</span>,&quot;token_str&quot;:&quot;simple&quot;}]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function rJ(q){let r,c;return r=new P({props:{$$slots:{default:[aJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function nJ(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is [MASK]."}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is [MASK].&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:0.16963955760002136,&quot;token&quot;:2053,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:0.07344776391983032,&quot;token&quot;:2498,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:0.05803241208195686,&quot;token&quot;:2748,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:0.043957844376564026,&quot;token&quot;:4242,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:0.04015745222568512,&quot;token&quot;:3722,&quot;token_str&quot;:&quot;simple&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function oJ(q){let r,c;return r=new P({props:{$$slots:{default:[nJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function lJ(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "sequence": "the answer to the universe is no.",
            "score": 0.1696,
            "token": 2053,
            "token_str": "no",
        },
        {
            "sequence": "the answer to the universe is nothing.",
            "score": 0.0734,
            "token": 2498,
            "token_str": "nothing",
        },
        {
            "sequence": "the answer to the universe is yes.",
            "score": 0.0580,
            "token": 2748,
            "token_str": "yes",
        },
        {
            "sequence": "the answer to the universe is unknown.",
            "score": 0.044,
            "token": 4242,
            "token_str": "unknown",
        },
        {
            "sequence": "the answer to the universe is simple.",
            "score": 0.0402,
            "token": 3722,
            "token_str": "simple",
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is no.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.1696</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2053</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;no&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is nothing.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0734</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2498</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;nothing&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is yes.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0580</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2748</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;yes&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is unknown.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.044</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">4242</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;unknown&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is simple.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0402</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">3722</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;simple&quot;</span>,
        },
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function iJ(q){let r,c;return r=new P({props:{$$slots:{default:[lJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function uJ(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("facebook/bart-large-cnn"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/bart-large-cnn"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/bart-large-cnn"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function cJ(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",
        "parameters": {"do_sample": False},
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "summary_text": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;do_sample&quot;</span>: <span class="hljs-literal">False</span>},
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;summary_text&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;</span>,
        },
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function fJ(q){let r,c;return r=new P({props:{$$slots:{default:[cJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function pJ(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-cnn",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres."}]`,highlighted:`import fetch from <span class="hljs-comment">&quot;node-fetch&quot;</span>;
async function query(data) {
    const response = await fetch(
        <span class="hljs-comment">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>,
        {
            headers: { <span class="hljs-type">Authorization</span>: \`<span class="hljs-type">Bearer</span> <span class="hljs-string">\${</span><span class="hljs-type">API_TOKEN</span>}\` },
            method: <span class="hljs-comment">&quot;POST&quot;</span>,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: <span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>}).then((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{<span class="hljs-comment">&quot;summary_text&quot;</span>:<span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres.&quot;</span>}]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function hJ(q){let r,c;return r=new P({props:{$$slots:{default:[pJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function dJ(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d '{"inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.", "parameters": {"do_sample": false}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world."}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;, &quot;parameters&quot;: {&quot;do_sample&quot;: false}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;summary_text&quot;:&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function gJ(q){let r,c;return r=new P({props:{$$slots:{default:[dJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function mJ(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("deepset/roberta-base-squad2"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"deepset/roberta-base-squad2"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/deepset/roberta-base-squad2"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function $J(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "question": "What's my name?",
            "context": "My name is Clara and I live in Berkeley.",
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;question&quot;</span>: <span class="hljs-string">&quot;What&#x27;s my name?&quot;</span>,
            <span class="hljs-string">&quot;context&quot;</span>: <span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>,
        }
    }
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function qJ(q){let r,c;return r=new P({props:{$$slots:{default:[$J]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function _J(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{question:"What is my name?",context:"My name is Clara and I live in Berkeley."}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:{<span class="hljs-attr">question</span>:<span class="hljs-string">&quot;What is my name?&quot;</span>,<span class="hljs-attr">context</span>:<span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function vJ(q){let r,c;return r=new P({props:{$$slots:{default:[_J]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function EJ(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d '{"inputs":{"question":"What is my name?","context":"My name is Clara and I live in Berkeley."}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;question&quot;:&quot;What is my name?&quot;,&quot;context&quot;:&quot;My name is Clara and I live in Berkeley.&quot;}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function yJ(q){let r,c;return r=new P({props:{$$slots:{default:[EJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function wJ(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data),
    {"score": 0.9327, "start": 11, "end": 16, "answer": "Clara"},
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9327</span>, <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;Clara&quot;</span>},
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function bJ(q){let r,c;return r=new P({props:{$$slots:{default:[wJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function TJ(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("google/tapas-base-finetuned-wtq"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"google/tapas-base-finetuned-wtq"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/google/tapas-base-finetuned-wtq"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function jJ(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "query": "How many stars does the transformers repository have?",
            "table": {
                "Repository": ["Transformers", "Datasets", "Tokenizers"],
                "Stars": ["36542", "4512", "3934"],
                "Contributors": ["651", "77", "34"],
                "Programming language": [
                    "Python",
                    "Python",
                    "Rust, Python and NodeJS",
                ],
            },
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;query&quot;</span>: <span class="hljs-string">&quot;How many stars does the transformers repository have?&quot;</span>,
            <span class="hljs-string">&quot;table&quot;</span>: {
                <span class="hljs-string">&quot;Repository&quot;</span>: [<span class="hljs-string">&quot;Transformers&quot;</span>, <span class="hljs-string">&quot;Datasets&quot;</span>, <span class="hljs-string">&quot;Tokenizers&quot;</span>],
                <span class="hljs-string">&quot;Stars&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>, <span class="hljs-string">&quot;4512&quot;</span>, <span class="hljs-string">&quot;3934&quot;</span>],
                <span class="hljs-string">&quot;Contributors&quot;</span>: [<span class="hljs-string">&quot;651&quot;</span>, <span class="hljs-string">&quot;77&quot;</span>, <span class="hljs-string">&quot;34&quot;</span>],
                <span class="hljs-string">&quot;Programming language&quot;</span>: [
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Rust, Python and NodeJS&quot;</span>,
                ],
            },
        }
    }
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function kJ(q){let r,c;return r=new P({props:{$$slots:{default:[jJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function AJ(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{query:"How many stars does the transformers repository have?",table:{Repository:["Transformers","Datasets","Tokenizers"],Stars:["36542","4512","3934"],Contributors:["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:{query:&quot;How many stars does the transformers repository have?&quot;,<span class="hljs-keyword">table</span>:{Repository:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],Stars:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],Contributors:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function DJ(q){let r,c;return r=new P({props:{$$slots:{default:[AJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function OJ(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d '{"inputs":{"query":"How many stars does the transformers repository have?","table":{"Repository":["Transformers","Datasets","Tokenizers"],"Stars":["36542","4512","3934"],"Contributors":["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;query&quot;:&quot;How many stars does the transformers repository have?&quot;,&quot;table&quot;:{&quot;Repository&quot;:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],&quot;Stars&quot;:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],&quot;Contributors&quot;:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[0,1]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function PJ(q){let r,c;return r=new P({props:{$$slots:{default:[OJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function RJ(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    data,
    {
        "answer": "AVERAGE > 36542",
        "coordinates": [[0, 1]],
        "cells": ["36542"],
        "aggregator": "AVERAGE",
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;AVERAGE &gt; 36542&quot;</span>,
        <span class="hljs-string">&quot;coordinates&quot;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]],
        <span class="hljs-string">&quot;cells&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>],
        <span class="hljs-string">&quot;aggregator&quot;</span>: <span class="hljs-string">&quot;AVERAGE&quot;</span>,
    },
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function SJ(q){let r,c;return r=new P({props:{$$slots:{default:[RJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function NJ(q){let r,c,s,d,$,k;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("distilbert-base-uncased-finetuned-sst-2-english"),this.h()},l(A){r=o(A,"P",{});var j=l(r);c=o(j,"STRONG",{});var T=l(c);s=u(T,"Recommended model"),T.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var S=l($);k=u(S,"distilbert-base-uncased-finetuned-sst-2-english"),S.forEach(t),j.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),f($,"rel","nofollow")},m(A,j){m(A,r,j),e(r,c),e(c,s),e(r,d),e(r,$),e($,k)},d(A){A&&t(r)}}}function xJ(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "I like you. I love you"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;I like you. I love you&quot;</span>})`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function IJ(q){let r,c;return r=new P({props:{$$slots:{default:[xJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function HJ(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"I like you. I love you"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;I like you. I love you&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [[{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053},{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553}]]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function BJ(q){let r,c;return r=new P({props:{$$slots:{default:[HJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function CJ(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d '{"inputs":"I like you. I love you"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;I like you. I love you&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [[{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053},{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553}]]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function GJ(q){let r,c;return r=new P({props:{$$slots:{default:[CJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function LJ(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data),
    [
        [
            {"label": "NEGATIVE", "score": 0.0001},
            {"label": "POSITIVE", "score": 0.9999},
        ]
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        [
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;NEGATIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0001</span>},
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9999</span>},
        ]
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function UJ(q){let r,c;return r=new P({props:{$$slots:{default:[LJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function zJ(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(": "),$=n("a"),k=i("gpt2"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,": "),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"gpt2"),D.forEach(t),A=u(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/gpt2"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function MJ(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/gpt2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is&quot;</span>})`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function KJ(q){let r,c;return r=new P({props:{$$slots:{default:[MJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function FJ(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/gpt2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;The answer to the universe is&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function JJ(q){let r,c;return r=new P({props:{$$slots:{default:[FJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function WJ(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function YJ(q){let r,c;return r=new P({props:{$$slots:{default:[WJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function VJ(q){let r,c;return r=new R({props:{code:`data == [
    {
        "generated_text": 'The answer to the universe is that we are the creation of the entire universe," says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.'
    }
]`,highlighted:`data == [
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&#x27;The answer to the universe is that we are the creation of the entire universe,&quot; says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.&#x27;</span>
    }
]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function XJ(q){let r,c;return r=new P({props:{$$slots:{default:[VJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function QJ(q){let r,c,s,d,$,k;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("dbmdz/bert-large-cased-finetuned-conll03-english"),this.h()},l(A){r=o(A,"P",{});var j=l(r);c=o(j,"STRONG",{});var T=l(c);s=u(T,"Recommended model"),T.forEach(t),d=u(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var S=l($);k=u(S,"dbmdz/bert-large-cased-finetuned-conll03-english"),S.forEach(t),j.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"),f($,"rel","nofollow")},m(A,j){m(A,r,j),e(r,c),e(c,s),e(r,d),e(r,$),e($,k)},d(A){A&&t(r)}}}function ZJ(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "My name is Sarah Jessica Parker but you can call me Jessica"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;</span>})`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function eW(q){let r,c;return r=new P({props:{$$slots:{default:[ZJ]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function tW(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"My name is Sarah Jessica Parker but you can call me Jessica"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9991337060928345</span>,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:<span class="hljs-number">11</span>,&quot;end&quot;:<span class="hljs-number">31</span>},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9979912042617798</span>,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:<span class="hljs-number">52</span>,&quot;end&quot;:<span class="hljs-number">59</span>}]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function sW(q){let r,c;return r=new P({props:{$$slots:{default:[tW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function aW(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d '{"inputs":"My name is Sarah Jessica Parker but you can call me Jessica"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9991337060928345,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:11,&quot;end&quot;:31},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9979912042617798,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:52,&quot;end&quot;:59}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function rW(q){let r,c;return r=new P({props:{$$slots:{default:[aW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function nW(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "entity_group": "PER",
            "score": 0.9991,
            "word": "Sarah Jessica Parker",
            "start": 11,
            "end": 31,
        },
        {
            "entity_group": "PER",
            "score": 0.998,
            "word": "Jessica",
            "start": 52,
            "end": 59,
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9991</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Sarah Jessica Parker&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">31</span>,
        },
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.998</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Jessica&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">52</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">59</span>,
        },
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function oW(q){let r,c;return r=new P({props:{$$slots:{default:[nW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function lW(q){let r,c,s,d,$,k,A,j,T,S,D,ne,Re;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("Helsinki-NLP/opus-mt-ru-en"),A=i(`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=n("strong"),T=i("Recommended model"),S=i(": "),D=n("a"),ne=i("t5-base"),Re=i("."),this.h()},l(Q){r=o(Q,"P",{});var Y=l(r);c=o(Y,"STRONG",{});var it=l(c);s=u(it,"Recommended model"),it.forEach(t),d=u(Y,`:
`),$=o(Y,"A",{href:!0,rel:!0});var Mi=l($);k=u(Mi,"Helsinki-NLP/opus-mt-ru-en"),Mi.forEach(t),A=u(Y,`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=o(Y,"STRONG",{});var rr=l(j);T=u(rr,"Recommended model"),rr.forEach(t),S=u(Y,": "),D=o(Y,"A",{href:!0,rel:!0});var Se=l(D);ne=u(Se,"t5-base"),Se.forEach(t),Re=u(Y,"."),Y.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/Helsinki-NLP/opus-mt-ru-en"),f($,"rel","nofollow"),f(D,"href","https://huggingface.co/t5-base"),f(D,"rel","nofollow")},m(Q,Y){m(Q,r,Y),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A),e(r,j),e(j,T),e(r,S),e(r,D),e(D,ne),e(r,Re)},d(Q){Q&&t(r)}}}function iW(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435",
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "translation_text": "My name is Wolfgang and I live in Berlin.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>,
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;translation_text&quot;</span>: <span class="hljs-string">&quot;My name is Wolfgang and I live in Berlin.&quot;</span>,
        },
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function uW(q){let r,c;return r=new P({props:{$$slots:{default:[iW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function cW(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function fW(q){let r,c;return r=new P({props:{$$slots:{default:[cW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function pW(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d '{"inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function hW(q){let r,c;return r=new P({props:{$$slots:{default:[pW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function dW(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("facebook/bart-large-mnli"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/bart-large-mnli"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/bart-large-mnli"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function gW(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-mnli"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "parameters": {"candidate_labels": ["refund", "legal", "faq"]},
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;candidate_labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]},
    }
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function mW(q){let r,c;return r=new P({props:{$$slots:{default:[gW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function $W(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-mnli",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", parameters: {candidate_labels: ["refund", "legal", "faq"]}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>, <span class="hljs-attr">parameters</span>: {<span class="hljs-attr">candidate_labels</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function qW(q){let r,c;return r=new P({props:{$$slots:{default:[$W]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function _W(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d '{"inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;, &quot;parameters&quot;: {&quot;candidate_labels&quot;: [&quot;refund&quot;, &quot;legal&quot;, &quot;faq&quot;]}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function vW(q){let r,c;return r=new P({props:{$$slots:{default:[_W]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function EW(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data),
    {
        "sequence": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "labels": ["refund", "faq", "legal"],
        "scores": [
            # 88% refund
            0.8778,
            0.1052,
            0.017,
        ],
    },
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {
        <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>],
        <span class="hljs-string">&quot;scores&quot;</span>: [
            <span class="hljs-comment"># 88% refund</span>
            <span class="hljs-number">0.8778</span>,
            <span class="hljs-number">0.1052</span>,
            <span class="hljs-number">0.017</span>,
        ],
    },
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function yW(q){let r,c;return r=new P({props:{$$slots:{default:[EW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function wW(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("microsoft/DialoGPT-large"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"microsoft/DialoGPT-large"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/microsoft/DialoGPT-large"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function bW(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "past_user_inputs": ["Which movie is the best ?"],
            "generated_responses": ["It's Die Hard for sure."],
            "text": "Can you explain why ?",
        },
    }
)
# Response
self.assertEqual(
    data,
    {
        "generated_text": "It's the best movie ever.",
        "conversation": {
            "past_user_inputs": [
                "Which movie is the best ?",
                "Can you explain why ?",
            ],
            "generated_responses": [
                "It's Die Hard for sure.",
                "It's the best movie ever.",
            ],
        },
        "warnings": ["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."],
    },
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [<span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>],
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
        },
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
        <span class="hljs-string">&quot;conversation&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [
                <span class="hljs-string">&quot;Which movie is the best ?&quot;</span>,
                <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
            ],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [
                <span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>,
                <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
            ],
        },
        <span class="hljs-string">&quot;warnings&quot;</span>: [<span class="hljs-string">&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;</span>],
    },
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function TW(q){let r,c;return r=new P({props:{$$slots:{default:[bW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function jW(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: {past_user_inputs: ["Which movie is the best ?"], generated_responses: ["It is Die Hard for sure."], text:"Can you explain why ?"}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs: {past_user_inputs: [&quot;Which movie is the best ?&quot;], generated_responses: [&quot;It is Die Hard for sure.&quot;], <span class="hljs-type">text</span>:&quot;Can you explain why ?&quot;}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;]}`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function kW(q){let r,c;return r=new P({props:{$$slots:{default:[jW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function AW(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\
        -X POST \\
        -d '{"inputs": {"past_user_inputs": ["Which movie is the best ?"], "generated_responses": ["It is Die Hard for sure."], "text":"Can you explain why ?"}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:'curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\\n        -X POST \\\n        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: {&quot;past_user_inputs&quot;: [&quot;Which movie is the best ?&quot;], &quot;generated_responses&quot;: [&quot;It is Die Hard for sure.&quot;], &quot;text&quot;:&quot;Can you explain why ?&quot;}}&#x27;</span> \\\n        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">${HF_API_TOKEN}</span>&quot;</span>\n<span class="hljs-comment"># {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.&quot;]}</span>'}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function DW(q){let r,c;return r=new P({props:{$$slots:{default:[AW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function OW(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("Sentence-transformers"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"Sentence-transformers"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function PW(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(": "),$=n("a"),k=i(`Check your
langage`),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,": "),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,`Check your
langage`),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function RW(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("English"),d=i(`:
`),$=n("a"),k=i("facebook/wav2vec2-large-960h-lv60-self"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"English"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/wav2vec2-large-960h-lv60-self"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function SW(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("sample1.flac")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function NW(q){let r,c;return r=new P({props:{$$slots:{default:[SW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function xW(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("sample1.flac").then((response) => {
    console.log(JSON.stringify(response));
});
// {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;fs&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>) {
    <span class="hljs-keyword">const</span> data = fs.<span class="hljs-title function_">readFileSync</span>(filename);
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: data,
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>(<span class="hljs-string">&quot;sample1.flac&quot;</span>).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function IW(q){let r,c;return r=new P({props:{$$slots:{default:[xW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function HW(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
        -X POST \\
        --data-binary '@sample1.flac' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function BW(q){let r,c;return r=new P({props:{$$slots:{default:[HW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function CW(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    data,
    {
        "text": "GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;</span>
    },
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function GW(q){let r,c;return r=new P({props:{$$slots:{default:[CW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function LW(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("superb/hubert-large-superb-er"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"superb/hubert-large-superb-er"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/superb/hubert-large-superb-er"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function UW(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("sample1.flac")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function zW(q){let r,c;return r=new P({props:{$$slots:{default:[UW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function MW(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("sample1.flac").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;sample1.flac&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.5927661657333374</span>,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:<span class="hljs-number">0.2002529799938202</span>,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:<span class="hljs-number">0.12795612215995789</span>,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:<span class="hljs-number">0.07902472466230392</span>,&quot;label&quot;:&quot;sad&quot;}]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function KW(q){let r,c;return r=new P({props:{$$slots:{default:[MW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function FW(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
        -X POST \\
        --data-binary '@sample1.flac' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.5927661657333374,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:0.2002529799938202,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:0.12795612215995789,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:0.07902472466230392,&quot;label&quot;:&quot;sad&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function JW(q){let r,c;return r=new P({props:{$$slots:{default:[FW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function WW(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {"score": 0.5928, "label": "neu"},
        {"score": 0.2003, "label": "hap"},
        {"score": 0.128, "label": "ang"},
        {"score": 0.079, "label": "sad"},
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.5928</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;neu&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.2003</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;hap&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.128</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;ang&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.079</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;sad&quot;</span>},
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function YW(q){let r,c;return r=new P({props:{$$slots:{default:[WW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function VW(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("google/vit-base-patch16-224"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"google/vit-base-patch16-224"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/google/vit-base-patch16-224"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function XW(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/vit-base-patch16-224"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/vit-base-patch16-224&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function QW(q){let r,c;return r=new P({props:{$$slots:{default:[XW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function ZW(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/vit-base-patch16-224",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.9374412894248962,"label":"Egyptian cat"},{"score":0.03844260051846504,"label":"tabby, tabby cat"},{"score":0.014411412179470062,"label":"tiger cat"},{"score":0.003274323185905814,"label":"lynx, catamount"},{"score":0.0006795919616706669,"label":"Siamese cat, Siamese"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/vit-base-patch16-224&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.9374412894248962</span>,&quot;label&quot;:&quot;Egyptian cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.03844260051846504</span>,&quot;label&quot;:&quot;tabby, tabby cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.014411412179470062</span>,&quot;label&quot;:&quot;tiger cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.003274323185905814</span>,&quot;label&quot;:&quot;lynx, catamount&quot;},{&quot;score&quot;:<span class="hljs-number">0.0006795919616706669</span>,&quot;label&quot;:&quot;Siamese cat, Siamese&quot;}]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function eY(q){let r,c;return r=new P({props:{$$slots:{default:[ZW]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function tY(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/google/vit-base-patch16-224 \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.9374412894248962,"label":"Egyptian cat"},{"score":0.03844260051846504,"label":"tabby, tabby cat"},{"score":0.014411412179470062,"label":"tiger cat"},{"score":0.003274323185905814,"label":"lynx, catamount"},{"score":0.0006795919616706669,"label":"Siamese cat, Siamese"}]`,highlighted:`curl https://api-inference.huggingface.co/models/google/vit-base-patch16-224 \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.9374412894248962,&quot;label&quot;:&quot;Egyptian cat&quot;},{&quot;score&quot;:0.03844260051846504,&quot;label&quot;:&quot;tabby, tabby cat&quot;},{&quot;score&quot;:0.014411412179470062,&quot;label&quot;:&quot;tiger cat&quot;},{&quot;score&quot;:0.003274323185905814,&quot;label&quot;:&quot;lynx, catamount&quot;},{&quot;score&quot;:0.0006795919616706669,&quot;label&quot;:&quot;Siamese cat, Siamese&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function sY(q){let r,c;return r=new P({props:{$$slots:{default:[tY]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function aY(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {"score": 0.9374, "label": "Egyptian cat"},
        {"score": 0.0384, "label": "tabby, tabby cat"},
        {"score": 0.0144, "label": "tiger cat"},
        {"score": 0.0033, "label": "lynx, catamount"},
        {"score": 0.0007, "label": "Siamese cat, Siamese"},
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9374</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;Egyptian cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0384</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;tabby, tabby cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0144</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;tiger cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0033</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;lynx, catamount&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0007</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;Siamese cat, Siamese&quot;</span>},
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function rY(q){let r,c;return r=new P({props:{$$slots:{default:[aY]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function nY(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("facebook/detr-resnet-50"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/detr-resnet-50"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/detr-resnet-50"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function oY(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function lY(q){let r,c;return r=new P({props:{$$slots:{default:[oY]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function iY(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/detr-resnet-50",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.9982201457023621</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">40</span>,&quot;ymin&quot;:<span class="hljs-number">70</span>,&quot;xmax&quot;:<span class="hljs-number">175</span>,&quot;ymax&quot;:<span class="hljs-number">117</span>}},{&quot;score&quot;:<span class="hljs-number">0.9960021376609802</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">333</span>,&quot;ymin&quot;:<span class="hljs-number">72</span>,&quot;xmax&quot;:<span class="hljs-number">368</span>,&quot;ymax&quot;:<span class="hljs-number">187</span>}},{&quot;score&quot;:<span class="hljs-number">0.9954745173454285</span>,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">0</span>,&quot;ymin&quot;:<span class="hljs-number">1</span>,&quot;xmax&quot;:<span class="hljs-number">639</span>,&quot;ymax&quot;:<span class="hljs-number">473</span>}},{&quot;score&quot;:<span class="hljs-number">0.9988006353378296</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">13</span>,&quot;ymin&quot;:<span class="hljs-number">52</span>,&quot;xmax&quot;:<span class="hljs-number">314</span>,&quot;ymax&quot;:<span class="hljs-number">470</span>}},{&quot;score&quot;:<span class="hljs-number">0.9986783862113953</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">345</span>,&quot;ymin&quot;:<span class="hljs-number">23</span>,&quot;xmax&quot;:<span class="hljs-number">640</span>,&quot;ymax&quot;:<span class="hljs-number">368</span>}}]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function uY(q){let r,c;return r=new P({props:{$$slots:{default:[iY]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function cY(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.9982201457023621,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:40,&quot;ymin&quot;:70,&quot;xmax&quot;:175,&quot;ymax&quot;:117}},{&quot;score&quot;:0.9960021376609802,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:333,&quot;ymin&quot;:72,&quot;xmax&quot;:368,&quot;ymax&quot;:187}},{&quot;score&quot;:0.9954745173454285,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:0,&quot;ymin&quot;:1,&quot;xmax&quot;:639,&quot;ymax&quot;:473}},{&quot;score&quot;:0.9988006353378296,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:13,&quot;ymin&quot;:52,&quot;xmax&quot;:314,&quot;ymax&quot;:470}},{&quot;score&quot;:0.9986783862113953,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:345,&quot;ymin&quot;:23,&quot;xmax&quot;:640,&quot;ymax&quot;:368}}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function fY(q){let r,c;return r=new P({props:{$$slots:{default:[cY]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function pY(q){let r,c;return r=new R({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {
            "score": 0.9982,
            "label": "remote",
            "box": {"xmin": 40, "ymin": 70, "xmax": 175, "ymax": 117},
        },
        {
            "score": 0.9960,
            "label": "remote",
            "box": {"xmin": 333, "ymin": 72, "xmax": 368, "ymax": 187},
        },
        {
            "score": 0.9955,
            "label": "couch",
            "box": {"xmin": 0, "ymin": 1, "xmax": 639, "ymax": 473},
        },
        {
            "score": 0.9988,
            "label": "cat",
            "box": {"xmin": 13, "ymin": 52, "xmax": 314, "ymax": 470},
        },
        {
            "score": 0.9987,
            "label": "cat",
            "box": {"xmin": 345, "ymin": 23, "xmax": 640, "ymax": 368},
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9982</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">40</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">70</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">175</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">117</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9960</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">333</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">72</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">368</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">187</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9955</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;couch&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">639</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">473</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9988</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">52</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">314</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">470</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9987</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">345</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">23</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">640</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">368</span>},
        },
    ],
)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function hY(q){let r,c;return r=new P({props:{$$slots:{default:[pY]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function dY(q){let r,c,s,d,$,k,A;return{c(){r=n("p"),c=n("strong"),s=i("Recommended model"),d=i(`:
`),$=n("a"),k=i("facebook/detr-resnet-50-panoptic"),A=i("."),this.h()},l(j){r=o(j,"P",{});var T=l(r);c=o(T,"STRONG",{});var S=l(c);s=u(S,"Recommended model"),S.forEach(t),d=u(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=u(D,"facebook/detr-resnet-50-panoptic"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){f($,"href","https://huggingface.co/facebook/detr-resnet-50-panoptic"),f($,"rel","nofollow")},m(j,T){m(j,r,T),e(r,c),e(c,s),e(r,d),e(r,$),e($,k),e(r,A)},d(j){j&&t(r)}}}function gY(q){let r,c;return r=new R({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function mY(q){let r,c;return r=new P({props:{$$slots:{default:[gY]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function $Y(q){let r,c;return r=new R({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score": 0.9094282388687134, "label": "blanket", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9940965175628662, "label": "cat", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9986692667007446, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994757771492004, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9722068309783936, "label": "couch", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994235038757324, "label": "cat", "mask": "BASE64ENCODED_MASK"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;: <span class="hljs-number">0.9094282388687134</span>, &quot;label&quot;: &quot;blanket&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9940965175628662</span>, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9986692667007446</span>, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9994757771492004</span>, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9722068309783936</span>, &quot;label&quot;: &quot;couch&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9994235038757324</span>, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}]`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function qY(q){let r,c;return r=new P({props:{$$slots:{default:[$Y]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function _Y(q){let r,c;return r=new R({props:{code:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score": 0.9094282388687134, "label": "blanket", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9940965175628662, "label": "cat", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9986692667007446, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994757771492004, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9722068309783936, "label": "couch", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994235038757324, "label": "cat", "mask": "BASE64ENCODED_MASK"}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;: 0.9094282388687134, &quot;label&quot;: &quot;blanket&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9940965175628662, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9986692667007446, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9994757771492004, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9722068309783936, &quot;label&quot;: &quot;couch&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9994235038757324, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}]</span>`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function vY(q){let r,c;return r=new P({props:{$$slots:{default:[_Y]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function EY(q){let r,c;return r=new R({props:{code:`import base64
from io import BytesIO
from PIL import Image
with Image.open("cats.jpg") as img:
    masks = [d["mask"] for d in data]
    self.assertEqual(img.size, (640, 480))
    mask_imgs = [Image.open(BytesIO(base64.b64decode(mask))) for mask in masks]
    for mask_img in mask_imgs:
        self.assertEqual(mask_img.size, img.size)
        self.assertEqual(mask_img.mode, "L")  # L (8-bit pixels, black and white)
    first_mask_img = mask_imgs[0]
    min_pxl_val, max_pxl_val = first_mask_img.getextrema()
    self.assertGreaterEqual(min_pxl_val, 0)
    self.assertLessEqual(max_pxl_val, 255)`,highlighted:`<span class="hljs-keyword">import</span> base64
<span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">with</span> Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;cats.jpg&quot;</span>) <span class="hljs-keyword">as</span> img:
    masks = [d[<span class="hljs-string">&quot;mask&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> data]
    self.assertEqual(img.size, (<span class="hljs-number">640</span>, <span class="hljs-number">480</span>))
    mask_imgs = [Image.<span class="hljs-built_in">open</span>(BytesIO(base64.b64decode(mask))) <span class="hljs-keyword">for</span> mask <span class="hljs-keyword">in</span> masks]
    <span class="hljs-keyword">for</span> mask_img <span class="hljs-keyword">in</span> mask_imgs:
        self.assertEqual(mask_img.size, img.size)
        self.assertEqual(mask_img.mode, <span class="hljs-string">&quot;L&quot;</span>)  <span class="hljs-comment"># L (8-bit pixels, black and white)</span>
    first_mask_img = mask_imgs[<span class="hljs-number">0</span>]
    min_pxl_val, max_pxl_val = first_mask_img.getextrema()
    self.assertGreaterEqual(min_pxl_val, <span class="hljs-number">0</span>)
    self.assertLessEqual(max_pxl_val, <span class="hljs-number">255</span>)`}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p:O,i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function yY(q){let r,c;return r=new P({props:{$$slots:{default:[EY]},$$scope:{ctx:q}}}),{c(){_(r.$$.fragment)},l(s){v(r.$$.fragment,s)},m(s,d){E(r,s,d),c=!0},p(s,d){const $={};d&2&&($.$$scope={dirty:d,ctx:s}),r.$set($)},i(s){c||(y(r.$$.fragment,s),c=!0)},o(s){w(r.$$.fragment,s),c=!1},d(s){b(r,s)}}}function wY(q){let r,c,s,d,$,k,A,j,T,S,D,ne,Re,Q,Y,it,Mi,rr,Se,g3,o1,Ki,m3,l1,ut,II,i1,ct,HI,u1,Ne,ft,_d,nr,$3,vd,q3,c1,xe,pt,Ed,or,_3,yd,v3,f1,Fi,E3,p1,ht,h1,lr,y3,ir,w3,d1,Ji,b3,g1,dt,m1,Wi,T3,$1,gt,wd,ur,Yi,j3,k3,bd,A3,Z,cr,fr,Td,D3,O3,P3,Vi,R3,S3,pr,Xi,jd,N3,x3,Qi,I3,H3,hr,Zi,B3,C3,mt,G3,kd,L3,U3,z3,dr,eu,M3,K3,$t,F3,Ad,J3,W3,Y3,gr,tu,V3,X3,qt,Q3,Dd,Z3,eT,q1,su,tT,_1,_t,v1,vt,Od,mr,au,sT,aT,Pd,rT,pe,$r,ru,Rd,nT,oT,nu,lT,iT,qr,ou,Sd,uT,cT,lu,fT,pT,_r,iu,Nd,hT,dT,uu,gT,mT,vr,cu,xd,$T,qT,fu,_T,E1,Ie,Et,Id,Er,vT,Hd,ET,y1,yt,yT,pu,wT,bT,w1,wt,b1,yr,TT,wr,jT,T1,hu,kT,j1,bt,k1,du,AT,A1,Tt,Bd,br,gu,DT,OT,Cd,PT,G,Tr,jr,Gd,RT,ST,NT,mu,xT,IT,kr,$u,Ld,HT,BT,qu,CT,GT,Ar,_u,LT,UT,$e,zT,Ud,MT,KT,zd,FT,JT,WT,Dr,vu,YT,VT,qe,XT,Md,QT,ZT,Kd,ej,tj,sj,Or,Eu,aj,rj,_e,nj,Fd,oj,lj,Jd,ij,uj,cj,Pr,yu,fj,pj,oe,hj,Wd,dj,gj,Yd,mj,$j,Vd,qj,_j,vj,Rr,wu,Ej,yj,le,wj,Xd,bj,Tj,Qd,jj,kj,Zd,Aj,Dj,Oj,Sr,bu,Pj,Rj,jt,Sj,eg,Nj,xj,Ij,Nr,Tu,Hj,Bj,kt,Cj,tg,Gj,Lj,Uj,xr,ju,sg,zj,Mj,ku,Kj,Fj,Ir,Au,Jj,Wj,At,Yj,ag,Vj,Xj,Qj,Hr,Du,Zj,e4,Dt,t4,rg,s4,a4,r4,Br,Ou,n4,o4,Ot,l4,ng,i4,u4,D1,Pu,c4,O1,Pt,og,Cr,Ru,f4,p4,lg,h4,ig,Gr,Su,ug,d4,g4,Nu,m4,P1,He,Rt,cg,Lr,$4,fg,q4,R1,xu,_4,S1,St,N1,Be,v4,Ur,E4,y4,zr,w4,x1,Iu,b4,I1,Nt,H1,Hu,T4,B1,Bu,j4,C1,xt,G1,It,pg,Mr,Cu,k4,A4,hg,D4,he,Kr,Gu,dg,O4,P4,Lu,R4,S4,Fr,Uu,gg,N4,x4,zu,I4,H4,Jr,Mu,mg,B4,C4,Ht,G4,$g,L4,U4,z4,Wr,Ku,qg,M4,K4,Bt,F4,_g,J4,W4,L1,Ce,Ct,vg,Yr,Y4,Eg,V4,U1,Fu,X4,z1,Gt,M1,Vr,Q4,Xr,Z4,K1,Ju,ek,F1,Lt,J1,Wu,tk,W1,Ut,yg,Qr,Yu,sk,ak,wg,rk,K,Zr,en,bg,nk,ok,lk,Tg,ik,tn,Vu,uk,ck,Xu,fk,pk,sn,Qu,hk,dk,Zu,gk,mk,an,ec,jg,$k,qk,tc,_k,vk,rn,sc,Ek,yk,zt,wk,kg,bk,Tk,jk,nn,ac,kk,Ak,Mt,Dk,Ag,Ok,Pk,Rk,on,rc,Sk,Nk,Kt,xk,Dg,Ik,Hk,Y1,nc,Bk,V1,Ft,X1,Jt,Og,ln,oc,Ck,Gk,Pg,Lk,de,un,lc,Rg,Uk,zk,ic,Mk,Kk,cn,uc,Sg,Fk,Jk,cc,Wk,Yk,fn,fc,Ng,Vk,Xk,pc,Qk,Zk,pn,hc,xg,e5,t5,dc,s5,Q1,Ge,Wt,Ig,hn,a5,Hg,r5,Z1,gc,n5,ev,Yt,tv,dn,o5,gn,l5,sv,mc,i5,av,Vt,rv,$c,u5,nv,Xt,Bg,mn,qc,c5,f5,Cg,p5,ee,$n,qn,Gg,h5,d5,g5,_c,m5,$5,_n,vc,Lg,q5,_5,Ec,v5,E5,vn,yc,y5,w5,Qt,b5,Ug,T5,j5,k5,En,wc,A5,D5,Zt,O5,zg,P5,R5,S5,yn,bc,N5,x5,es,I5,Mg,H5,B5,ov,Tc,C5,lv,ts,iv,ss,Kg,wn,jc,G5,L5,Fg,U5,bn,Tn,kc,Jg,z5,M5,Ac,K5,F5,jn,Dc,Wg,J5,W5,Oc,Y5,uv,Le,as,Yg,kn,V5,Vg,X5,cv,Pc,Q5,fv,rs,pv,An,Z5,Dn,e6,hv,Rc,t6,dv,ns,gv,Sc,s6,mv,os,Xg,On,Nc,a6,r6,Qg,n6,I,Pn,Rn,Zg,o6,l6,i6,xc,u6,c6,Sn,Ic,em,f6,p6,Hc,h6,d6,Nn,Bc,g6,m6,ve,$6,tm,q6,_6,sm,v6,E6,y6,xn,Cc,w6,b6,ie,T6,am,j6,k6,rm,A6,D6,nm,O6,P6,R6,In,Gc,S6,N6,ue,x6,om,I6,H6,lm,B6,C6,im,G6,L6,U6,Hn,Lc,z6,M6,ls,K6,um,F6,J6,W6,Bn,Uc,Y6,V6,Ee,X6,cm,Q6,Z6,fm,e7,t7,s7,Cn,zc,a7,r7,ye,n7,pm,o7,l7,hm,i7,u7,c7,Gn,Mc,f7,p7,we,h7,dm,d7,g7,gm,m7,$7,q7,Ln,Kc,_7,v7,is,E7,mm,y7,w7,b7,Un,Fc,T7,j7,us,k7,$m,A7,D7,O7,zn,Jc,qm,P7,R7,Wc,S7,N7,Mn,Yc,x7,I7,cs,H7,_m,B7,C7,G7,Kn,Vc,L7,U7,fs,z7,vm,M7,K7,F7,Fn,Xc,J7,W7,ps,Y7,Em,V7,X7,$v,Qc,Q7,qv,hs,_v,ds,ym,Jn,Zc,Z7,e9,wm,t9,bm,Wn,ef,Tm,s9,a9,tf,r9,vv,Ue,gs,jm,Yn,n9,km,o9,Ev,ms,l9,sf,i9,u9,yv,ze,$s,Am,Vn,c9,Dm,f9,wv,af,p9,bv,qs,Tv,Me,h9,Xn,d9,g9,Qn,m9,jv,rf,$9,kv,_s,Av,nf,q9,Dv,vs,Om,Zn,of,_9,v9,Pm,E9,F,eo,to,Rm,y9,w9,b9,lf,T9,j9,so,uf,Sm,k9,A9,cf,D9,O9,ao,ff,P9,R9,x,S9,Nm,N9,x9,I9,H9,xm,B9,C9,G9,L9,Im,U9,z9,M9,K9,Hm,F9,J9,Bm,W9,Y9,V9,X9,Cm,Q9,Z9,Gm,e8,t8,s8,a8,Lm,r8,n8,Um,o8,l8,i8,ro,pf,zm,u8,c8,hf,f8,p8,no,df,h8,d8,Es,g8,Mm,m8,$8,q8,oo,gf,_8,v8,ys,E8,Km,y8,w8,b8,lo,mf,T8,j8,ws,k8,Fm,A8,D8,Ov,$f,O8,Pv,bs,Rv,Ts,Jm,io,qf,P8,R8,Wm,S8,te,uo,_f,Ym,N8,x8,vf,I8,H8,co,Ef,Vm,B8,C8,yf,G8,L8,fo,wf,Xm,U8,z8,bf,M8,K8,po,Tf,Qm,F8,J8,js,W8,Zm,Y8,V8,X8,ho,jf,e$,Q8,Z8,ks,eA,t$,tA,sA,Sv,Ke,As,s$,go,aA,a$,rA,Nv,mo,nA,kf,oA,xv,Fe,Ds,r$,$o,lA,n$,iA,Iv,Af,uA,Hv,Os,Bv,qo,cA,_o,fA,Cv,Df,pA,Gv,Ps,Lv,Of,hA,Uv,Rs,o$,vo,Pf,dA,gA,l$,mA,se,Eo,yo,i$,$A,qA,_A,Rf,vA,EA,wo,Sf,u$,yA,wA,Nf,bA,TA,bo,xf,jA,kA,Ss,AA,c$,DA,OA,PA,To,If,RA,SA,Ns,NA,f$,xA,IA,HA,jo,Hf,BA,CA,xs,GA,p$,LA,UA,zv,Bf,zA,Mv,Is,h$,ko,Cf,MA,KA,d$,FA,g$,Ao,Gf,m$,JA,WA,Lf,YA,Kv,Je,Hs,$$,Do,VA,q$,XA,Fv,Uf,QA,Jv,Bs,Wv,Oo,ZA,Po,eD,Yv,zf,tD,Vv,Cs,Xv,Mf,sD,Qv,Gs,_$,Ro,Kf,aD,rD,v$,nD,M,So,No,E$,oD,lD,iD,Ff,uD,cD,xo,Io,y$,fD,pD,hD,Jf,dD,gD,Ho,Wf,mD,$D,be,qD,w$,_D,vD,b$,ED,yD,wD,Bo,Yf,bD,TD,Ls,jD,T$,kD,AD,DD,Co,Vf,j$,OD,PD,Xf,RD,SD,Go,Qf,ND,xD,Us,ID,k$,HD,BD,CD,Lo,Zf,GD,LD,zs,UD,A$,zD,MD,KD,Uo,ep,FD,JD,Ms,WD,D$,YD,VD,Zv,tp,XD,e2,sp,QD,t2,Ks,s2,Fs,O$,zo,ap,ZD,eO,P$,tO,We,Mo,rp,R$,sO,aO,np,rO,nO,Ko,op,S$,oO,lO,lp,iO,uO,Fo,ip,N$,cO,fO,Js,pO,x$,hO,dO,a2,Ye,Ws,I$,Jo,gO,H$,mO,r2,up,$O,n2,Ys,o2,Wo,qO,Yo,_O,l2,cp,vO,i2,Vs,u2,fp,EO,c2,Xs,B$,Vo,pp,yO,wO,C$,bO,N,Xo,Qo,G$,TO,jO,kO,L$,AO,Zo,hp,DO,OO,dp,PO,RO,el,gp,SO,NO,mp,xO,IO,tl,$p,HO,BO,Qs,CO,U$,GO,LO,UO,sl,qp,z$,zO,MO,_p,KO,FO,al,vp,JO,WO,Te,YO,M$,VO,XO,K$,QO,ZO,eP,rl,Ep,tP,sP,je,aP,F$,rP,nP,J$,oP,lP,iP,nl,yp,uP,cP,ke,fP,W$,pP,hP,Y$,dP,gP,mP,ol,wp,$P,qP,ce,_P,V$,vP,EP,X$,yP,wP,Q$,bP,TP,jP,ll,bp,kP,AP,fe,DP,Z$,OP,PP,eq,RP,SP,tq,NP,xP,IP,il,Tp,HP,BP,Zs,CP,sq,GP,LP,UP,ul,jp,zP,MP,ea,KP,aq,FP,JP,WP,cl,kp,rq,YP,VP,Ap,XP,QP,fl,Dp,ZP,eR,ta,tR,nq,sR,aR,rR,pl,Op,nR,oR,sa,lR,oq,iR,uR,cR,hl,Pp,fR,pR,aa,hR,lq,dR,gR,f2,Rp,mR,p2,ra,iq,dl,Sp,$R,qR,uq,_R,ge,gl,Np,cq,vR,ER,xp,yR,wR,ml,Ip,fq,bR,TR,Hp,jR,kR,$l,Bp,AR,DR,Cp,OR,PR,ql,Gp,RR,SR,Lp,NR,h2,Ve,na,pq,_l,xR,hq,IR,d2,Up,HR,g2,oa,m2,Xe,BR,vl,CR,GR,El,LR,$2,zp,UR,q2,la,dq,yl,Mp,zR,MR,gq,KR,ae,wl,bl,mq,FR,JR,WR,Kp,YR,VR,Tl,Fp,$q,XR,QR,Jp,ZR,eS,jl,Wp,tS,sS,ia,aS,qq,rS,nS,oS,kl,Yp,lS,iS,ua,uS,_q,cS,fS,pS,Al,Vp,hS,dS,ca,gS,vq,mS,$S,_2,Xp,qS,v2,fa,Eq,Dl,Qp,_S,vS,yq,ES,wq,Ol,Zp,bq,yS,wS,eh,bS,E2,th,TS,y2,Qe,pa,Tq,Pl,jS,jq,kS,w2,Ze,ha,kq,Rl,AS,Aq,DS,b2,sh,OS,T2,da,j2,ga,k2,me,PS,Sl,RS,SS,Nl,NS,xS,xl,IS,A2,ah,HS,D2,ma,O2,rh,BS,P2,$a,Dq,Il,nh,CS,GS,Oq,LS,Pq,Hl,Bl,Rq,US,zS,MS,oh,KS,R2,lh,FS,S2,ih,JS,N2,qa,x2,_a,Sq,Cl,uh,WS,YS,Nq,VS,xq,Gl,ch,Iq,XS,QS,fh,ZS,I2,et,va,Hq,Ll,eN,Bq,tN,H2,ph,sN,B2,Ea,C2,tt,aN,Ul,rN,nN,zl,oN,G2,hh,lN,L2,ya,U2,dh,iN,z2,wa,Cq,Ml,gh,uN,cN,Gq,fN,Lq,Kl,Fl,Uq,pN,hN,dN,mh,gN,M2,$h,mN,K2,ba,F2,Ta,zq,Jl,qh,$N,qN,Mq,_N,Wl,Yl,_h,Kq,vN,EN,vh,yN,wN,Vl,Eh,Fq,bN,TN,yh,jN,J2,st,ja,Jq,Xl,kN,Wq,AN,W2,at,ka,Yq,Ql,DN,Vq,ON,Y2,wh,PN,V2,Aa,X2,Zl,RN,ei,SN,Q2,bh,NN,Z2,Da,eE,Oa,xN,ti,IN,HN,tE,Pa,Xq,si,Th,BN,CN,Qq,GN,Zq,ai,ri,e_,LN,UN,zN,jh,MN,sE,kh,KN,aE,Ra,rE,Sa,t_,ni,Ah,FN,JN,s_,WN,oi,li,Dh,a_,YN,VN,Oh,XN,QN,ii,Ph,r_,ZN,ex,Rh,tx,nE,rt,Na,n_,ui,sx,o_,ax,oE,Sh,rx,lE,xa,iE,ci,nx,fi,ox,uE,Nh,lx,cE,Ia,fE,Ha,ix,pi,ux,cx,pE,Ba,l_,hi,xh,fx,px,i_,hx,u_,di,gi,c_,dx,gx,mx,Ih,$x,hE,Hh,qx,dE,Ca,gE,Ga,f_,mi,Bh,_x,vx,p_,Ex,nt,$i,Ch,h_,yx,wx,Gh,bx,Tx,qi,Lh,d_,jx,kx,Uh,Ax,Dx,_i,zh,g_,Ox,Px,Mh,Rx,mE,ot,La,m_,vi,Sx,$_,Nx,$E,Kh,xx,qE,Ua,_E,Ei,Ix,yi,Hx,vE,Fh,Bx,EE,za,yE,Ma,Cx,wi,Gx,Lx,wE,Ka,q_,bi,Jh,Ux,zx,__,Mx,v_,Ti,ji,E_,Kx,Fx,Jx,Wh,Wx,bE,Yh,Yx,TE,Fa,jE,Ja,y_,ki,Vh,Vx,Xx,w_,Qx,lt,Ai,Xh,b_,Zx,eI,Qh,tI,sI,Di,Zh,T_,aI,rI,ed,nI,oI,Oi,td,j_,lI,iI,sd,uI,kE;return k=new z({}),Q=new z({}),nr=new z({}),or=new z({}),ht=new J({props:{$$slots:{default:[eJ]},$$scope:{ctx:q}}}),dt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[oJ],js:[rJ],python:[sJ]},$$scope:{ctx:q}}}),_t=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[iJ]},$$scope:{ctx:q}}}),Er=new z({}),wt=new J({props:{$$slots:{default:[uJ]},$$scope:{ctx:q}}}),bt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[gJ],js:[hJ],python:[fJ]},$$scope:{ctx:q}}}),Lr=new z({}),St=new J({props:{$$slots:{default:[mJ]},$$scope:{ctx:q}}}),Nt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[yJ],js:[vJ],python:[qJ]},$$scope:{ctx:q}}}),xt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[bJ]},$$scope:{ctx:q}}}),Yr=new z({}),Gt=new J({props:{$$slots:{default:[TJ]},$$scope:{ctx:q}}}),Lt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[PJ],js:[DJ],python:[kJ]},$$scope:{ctx:q}}}),Ft=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[SJ]},$$scope:{ctx:q}}}),hn=new z({}),Yt=new J({props:{$$slots:{default:[NJ]},$$scope:{ctx:q}}}),Vt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[GJ],js:[BJ],python:[IJ]},$$scope:{ctx:q}}}),ts=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[UJ]},$$scope:{ctx:q}}}),kn=new z({}),rs=new J({props:{$$slots:{default:[zJ]},$$scope:{ctx:q}}}),ns=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[YJ],js:[JJ],python:[KJ]},$$scope:{ctx:q}}}),hs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[XJ]},$$scope:{ctx:q}}}),Yn=new z({}),Vn=new z({}),qs=new J({props:{$$slots:{default:[QJ]},$$scope:{ctx:q}}}),_s=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[rW],js:[sW],python:[eW]},$$scope:{ctx:q}}}),bs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[oW]},$$scope:{ctx:q}}}),go=new z({}),$o=new z({}),Os=new J({props:{$$slots:{default:[lW]},$$scope:{ctx:q}}}),Ps=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[hW],js:[fW],python:[uW]},$$scope:{ctx:q}}}),Do=new z({}),Bs=new J({props:{$$slots:{default:[dW]},$$scope:{ctx:q}}}),Cs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[vW],js:[qW],python:[mW]},$$scope:{ctx:q}}}),Ks=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[yW]},$$scope:{ctx:q}}}),Jo=new z({}),Ys=new J({props:{$$slots:{default:[wW]},$$scope:{ctx:q}}}),Vs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[DW],js:[kW],python:[TW]},$$scope:{ctx:q}}}),_l=new z({}),oa=new J({props:{$$slots:{default:[OW]},$$scope:{ctx:q}}}),Pl=new z({}),Rl=new z({}),da=new J({props:{$$slots:{default:[PW]},$$scope:{ctx:q}}}),ga=new J({props:{$$slots:{default:[RW]},$$scope:{ctx:q}}}),ma=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[BW],js:[IW],python:[NW]},$$scope:{ctx:q}}}),qa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[GW]},$$scope:{ctx:q}}}),Ll=new z({}),Ea=new J({props:{$$slots:{default:[LW]},$$scope:{ctx:q}}}),ya=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[JW],js:[KW],python:[zW]},$$scope:{ctx:q}}}),ba=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[YW]},$$scope:{ctx:q}}}),Xl=new z({}),Ql=new z({}),Aa=new J({props:{$$slots:{default:[VW]},$$scope:{ctx:q}}}),Da=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[sY],js:[eY],python:[QW]},$$scope:{ctx:q}}}),Ra=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[rY]},$$scope:{ctx:q}}}),ui=new z({}),xa=new J({props:{$$slots:{default:[nY]},$$scope:{ctx:q}}}),Ia=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[fY],js:[uY],python:[lY]},$$scope:{ctx:q}}}),Ca=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[hY]},$$scope:{ctx:q}}}),vi=new z({}),Ua=new J({props:{$$slots:{default:[dY]},$$scope:{ctx:q}}}),za=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[vY],js:[qY],python:[mY]},$$scope:{ctx:q}}}),Fa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[yY]},$$scope:{ctx:q}}}),{c(){r=n("meta"),c=p(),s=n("h1"),d=n("a"),$=n("span"),_(k.$$.fragment),A=p(),j=n("span"),T=i("Detailed parameters"),S=p(),D=n("h2"),ne=n("a"),Re=n("span"),_(Q.$$.fragment),Y=p(),it=n("span"),Mi=i("Which task is used by this model ?"),rr=p(),Se=n("p"),g3=i(`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),o1=p(),Ki=n("p"),m3=i("The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),l1=p(),ut=n("img"),i1=p(),ct=n("img"),u1=p(),Ne=n("h2"),ft=n("a"),_d=n("span"),_(nr.$$.fragment),$3=p(),vd=n("span"),q3=i("Natural Language Processing"),c1=p(),xe=n("h3"),pt=n("a"),Ed=n("span"),_(or.$$.fragment),_3=p(),yd=n("span"),v3=i("Fill Mask task"),f1=p(),Fi=n("p"),E3=i(`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),p1=p(),_(ht.$$.fragment),h1=p(),lr=n("p"),y3=i("Available with: "),ir=n("a"),w3=i("\u{1F917} Transformers"),d1=p(),Ji=n("p"),b3=i("Example:"),g1=p(),_(dt.$$.fragment),m1=p(),Wi=n("p"),T3=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),$1=p(),gt=n("table"),wd=n("thead"),ur=n("tr"),Yi=n("th"),j3=i("All parameters"),k3=p(),bd=n("th"),A3=p(),Z=n("tbody"),cr=n("tr"),fr=n("td"),Td=n("strong"),D3=i("inputs"),O3=i(" (required):"),P3=p(),Vi=n("td"),R3=i("a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),S3=p(),pr=n("tr"),Xi=n("td"),jd=n("strong"),N3=i("options"),x3=p(),Qi=n("td"),I3=i("a dict containing the following keys:"),H3=p(),hr=n("tr"),Zi=n("td"),B3=i("use_gpu"),C3=p(),mt=n("td"),G3=i("(Default: "),kd=n("code"),L3=i("false"),U3=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),z3=p(),dr=n("tr"),eu=n("td"),M3=i("use_cache"),K3=p(),$t=n("td"),F3=i("(Default: "),Ad=n("code"),J3=i("true"),W3=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Y3=p(),gr=n("tr"),tu=n("td"),V3=i("wait_for_model"),X3=p(),qt=n("td"),Q3=i("(Default: "),Dd=n("code"),Z3=i("false"),eT=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),q1=p(),su=n("p"),tT=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),_1=p(),_(_t.$$.fragment),v1=p(),vt=n("table"),Od=n("thead"),mr=n("tr"),au=n("th"),sT=i("Returned values"),aT=p(),Pd=n("th"),rT=p(),pe=n("tbody"),$r=n("tr"),ru=n("td"),Rd=n("strong"),nT=i("sequence"),oT=p(),nu=n("td"),lT=i("The actual sequence of tokens that ran against the model (may contain special tokens)"),iT=p(),qr=n("tr"),ou=n("td"),Sd=n("strong"),uT=i("score"),cT=p(),lu=n("td"),fT=i("The probability for this token."),pT=p(),_r=n("tr"),iu=n("td"),Nd=n("strong"),hT=i("token"),dT=p(),uu=n("td"),gT=i("The id of the token"),mT=p(),vr=n("tr"),cu=n("td"),xd=n("strong"),$T=i("token_str"),qT=p(),fu=n("td"),_T=i("The string representation of the token"),E1=p(),Ie=n("h3"),Et=n("a"),Id=n("span"),_(Er.$$.fragment),vT=p(),Hd=n("span"),ET=i("Summarization task"),y1=p(),yt=n("p"),yT=i(`This task is well known to summarize longer text into shorter text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss your summarization needs,
please get in touch with us: <`),pu=n("a"),wT=i("api-enterprise@huggingface.co"),bT=i(">"),w1=p(),_(wt.$$.fragment),b1=p(),yr=n("p"),TT=i("Available with: "),wr=n("a"),jT=i("\u{1F917} Transformers"),T1=p(),hu=n("p"),kT=i("Example:"),j1=p(),_(bt.$$.fragment),k1=p(),du=n("p"),AT=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),A1=p(),Tt=n("table"),Bd=n("thead"),br=n("tr"),gu=n("th"),DT=i("All parameters"),OT=p(),Cd=n("th"),PT=p(),G=n("tbody"),Tr=n("tr"),jr=n("td"),Gd=n("strong"),RT=i("inputs"),ST=i(" (required)"),NT=p(),mu=n("td"),xT=i("a string to be summarized"),IT=p(),kr=n("tr"),$u=n("td"),Ld=n("strong"),HT=i("parameters"),BT=p(),qu=n("td"),CT=i("a dict containing the following keys:"),GT=p(),Ar=n("tr"),_u=n("td"),LT=i("min_length"),UT=p(),$e=n("td"),zT=i("(Default: "),Ud=n("code"),MT=i("None"),KT=i("). Integer to define the minimum length "),zd=n("strong"),FT=i("in tokens"),JT=i(" of the output summary."),WT=p(),Dr=n("tr"),vu=n("td"),YT=i("max_length"),VT=p(),qe=n("td"),XT=i("(Default: "),Md=n("code"),QT=i("None"),ZT=i("). Integer to define the maximum length "),Kd=n("strong"),ej=i("in tokens"),tj=i(" of the output summary."),sj=p(),Or=n("tr"),Eu=n("td"),aj=i("top_k"),rj=p(),_e=n("td"),nj=i("(Default: "),Fd=n("code"),oj=i("None"),lj=i("). Integer to define the top tokens considered within the "),Jd=n("code"),ij=i("sample"),uj=i(" operation to create new text."),cj=p(),Pr=n("tr"),yu=n("td"),fj=i("top_p"),pj=p(),oe=n("td"),hj=i("(Default: "),Wd=n("code"),dj=i("None"),gj=i("). Float to define the tokens that are within the "),Yd=n("code"),mj=i("sample"),$j=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Vd=n("code"),qj=i("top_p"),_j=i("."),vj=p(),Rr=n("tr"),wu=n("td"),Ej=i("temperature"),yj=p(),le=n("td"),wj=i("(Default: "),Xd=n("code"),bj=i("1.0"),Tj=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),Qd=n("code"),jj=i("0"),kj=i(" means always take the highest score, "),Zd=n("code"),Aj=i("100.0"),Dj=i(" is getting closer to uniform probability."),Oj=p(),Sr=n("tr"),bu=n("td"),Pj=i("repetition_penalty"),Rj=p(),jt=n("td"),Sj=i("(Default: "),eg=n("code"),Nj=i("None"),xj=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),Ij=p(),Nr=n("tr"),Tu=n("td"),Hj=i("max_time"),Bj=p(),kt=n("td"),Cj=i("(Default: "),tg=n("code"),Gj=i("None"),Lj=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),Uj=p(),xr=n("tr"),ju=n("td"),sg=n("strong"),zj=i("options"),Mj=p(),ku=n("td"),Kj=i("a dict containing the following keys:"),Fj=p(),Ir=n("tr"),Au=n("td"),Jj=i("use_gpu"),Wj=p(),At=n("td"),Yj=i("(Default: "),ag=n("code"),Vj=i("false"),Xj=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Qj=p(),Hr=n("tr"),Du=n("td"),Zj=i("use_cache"),e4=p(),Dt=n("td"),t4=i("(Default: "),rg=n("code"),s4=i("true"),a4=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),r4=p(),Br=n("tr"),Ou=n("td"),n4=i("wait_for_model"),o4=p(),Ot=n("td"),l4=i("(Default: "),ng=n("code"),i4=i("false"),u4=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),D1=p(),Pu=n("p"),c4=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),O1=p(),Pt=n("table"),og=n("thead"),Cr=n("tr"),Ru=n("th"),f4=i("Returned values"),p4=p(),lg=n("th"),h4=p(),ig=n("tbody"),Gr=n("tr"),Su=n("td"),ug=n("strong"),d4=i("summarization_text"),g4=p(),Nu=n("td"),m4=i("The string after translation"),P1=p(),He=n("h3"),Rt=n("a"),cg=n("span"),_(Lr.$$.fragment),$4=p(),fg=n("span"),q4=i("Question Answering task"),R1=p(),xu=n("p"),_4=i("Want to have a nice know-it-all bot that can answer any question?"),S1=p(),_(St.$$.fragment),N1=p(),Be=n("p"),v4=i("Available with: "),Ur=n("a"),E4=i("\u{1F917}Transformers"),y4=i(` and
`),zr=n("a"),w4=i("AllenNLP"),x1=p(),Iu=n("p"),b4=i("Example:"),I1=p(),_(Nt.$$.fragment),H1=p(),Hu=n("p"),T4=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),B1=p(),Bu=n("p"),j4=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),C1=p(),_(xt.$$.fragment),G1=p(),It=n("table"),pg=n("thead"),Mr=n("tr"),Cu=n("th"),k4=i("Returned values"),A4=p(),hg=n("th"),D4=p(),he=n("tbody"),Kr=n("tr"),Gu=n("td"),dg=n("strong"),O4=i("answer"),P4=p(),Lu=n("td"),R4=i("A string that\u2019s the answer within the text."),S4=p(),Fr=n("tr"),Uu=n("td"),gg=n("strong"),N4=i("score"),x4=p(),zu=n("td"),I4=i("A float that represents how likely that the answer is correct"),H4=p(),Jr=n("tr"),Mu=n("td"),mg=n("strong"),B4=i("start"),C4=p(),Ht=n("td"),G4=i("The index (string wise) of the start of the answer within "),$g=n("code"),L4=i("context"),U4=i("."),z4=p(),Wr=n("tr"),Ku=n("td"),qg=n("strong"),M4=i("stop"),K4=p(),Bt=n("td"),F4=i("The index (string wise) of the stop of the answer within "),_g=n("code"),J4=i("context"),W4=i("."),L1=p(),Ce=n("h3"),Ct=n("a"),vg=n("span"),_(Yr.$$.fragment),Y4=p(),Eg=n("span"),V4=i("Table Question Answering task"),U1=p(),Fu=n("p"),X4=i(`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),z1=p(),_(Gt.$$.fragment),M1=p(),Vr=n("p"),Q4=i("Available with: "),Xr=n("a"),Z4=i("\u{1F917} Transformers"),K1=p(),Ju=n("p"),ek=i("Example:"),F1=p(),_(Lt.$$.fragment),J1=p(),Wu=n("p"),tk=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),W1=p(),Ut=n("table"),yg=n("thead"),Qr=n("tr"),Yu=n("th"),sk=i("All parameters"),ak=p(),wg=n("th"),rk=p(),K=n("tbody"),Zr=n("tr"),en=n("td"),bg=n("strong"),nk=i("inputs"),ok=i(" (required)"),lk=p(),Tg=n("td"),ik=p(),tn=n("tr"),Vu=n("td"),uk=i("query (required)"),ck=p(),Xu=n("td"),fk=i("The query in plain text that you want to ask the table"),pk=p(),sn=n("tr"),Qu=n("td"),hk=i("table (required)"),dk=p(),Zu=n("td"),gk=i("A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),mk=p(),an=n("tr"),ec=n("td"),jg=n("strong"),$k=i("options"),qk=p(),tc=n("td"),_k=i("a dict containing the following keys:"),vk=p(),rn=n("tr"),sc=n("td"),Ek=i("use_gpu"),yk=p(),zt=n("td"),wk=i("(Default: "),kg=n("code"),bk=i("false"),Tk=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),jk=p(),nn=n("tr"),ac=n("td"),kk=i("use_cache"),Ak=p(),Mt=n("td"),Dk=i("(Default: "),Ag=n("code"),Ok=i("true"),Pk=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Rk=p(),on=n("tr"),rc=n("td"),Sk=i("wait_for_model"),Nk=p(),Kt=n("td"),xk=i("(Default: "),Dg=n("code"),Ik=i("false"),Hk=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Y1=p(),nc=n("p"),Bk=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),V1=p(),_(Ft.$$.fragment),X1=p(),Jt=n("table"),Og=n("thead"),ln=n("tr"),oc=n("th"),Ck=i("Returned values"),Gk=p(),Pg=n("th"),Lk=p(),de=n("tbody"),un=n("tr"),lc=n("td"),Rg=n("strong"),Uk=i("answer"),zk=p(),ic=n("td"),Mk=i("The plaintext answer"),Kk=p(),cn=n("tr"),uc=n("td"),Sg=n("strong"),Fk=i("coordinates"),Jk=p(),cc=n("td"),Wk=i("a list of coordinates of the cells referenced in the answer"),Yk=p(),fn=n("tr"),fc=n("td"),Ng=n("strong"),Vk=i("cells"),Xk=p(),pc=n("td"),Qk=i("a list of coordinates of the cells contents"),Zk=p(),pn=n("tr"),hc=n("td"),xg=n("strong"),e5=i("aggregator"),t5=p(),dc=n("td"),s5=i("The aggregator used to get the answer"),Q1=p(),Ge=n("h3"),Wt=n("a"),Ig=n("span"),_(hn.$$.fragment),a5=p(),Hg=n("span"),r5=i("Text Classification task"),Z1=p(),gc=n("p"),n5=i(`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),ev=p(),_(Yt.$$.fragment),tv=p(),dn=n("p"),o5=i("Available with: "),gn=n("a"),l5=i("\u{1F917} Transformers"),sv=p(),mc=n("p"),i5=i("Example:"),av=p(),_(Vt.$$.fragment),rv=p(),$c=n("p"),u5=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),nv=p(),Xt=n("table"),Bg=n("thead"),mn=n("tr"),qc=n("th"),c5=i("All parameters"),f5=p(),Cg=n("th"),p5=p(),ee=n("tbody"),$n=n("tr"),qn=n("td"),Gg=n("strong"),h5=i("inputs"),d5=i(" (required)"),g5=p(),_c=n("td"),m5=i("a string to be classified"),$5=p(),_n=n("tr"),vc=n("td"),Lg=n("strong"),q5=i("options"),_5=p(),Ec=n("td"),v5=i("a dict containing the following keys:"),E5=p(),vn=n("tr"),yc=n("td"),y5=i("use_gpu"),w5=p(),Qt=n("td"),b5=i("(Default: "),Ug=n("code"),T5=i("false"),j5=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),k5=p(),En=n("tr"),wc=n("td"),A5=i("use_cache"),D5=p(),Zt=n("td"),O5=i("(Default: "),zg=n("code"),P5=i("true"),R5=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),S5=p(),yn=n("tr"),bc=n("td"),N5=i("wait_for_model"),x5=p(),es=n("td"),I5=i("(Default: "),Mg=n("code"),H5=i("false"),B5=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),ov=p(),Tc=n("p"),C5=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),lv=p(),_(ts.$$.fragment),iv=p(),ss=n("table"),Kg=n("thead"),wn=n("tr"),jc=n("th"),G5=i("Returned values"),L5=p(),Fg=n("th"),U5=p(),bn=n("tbody"),Tn=n("tr"),kc=n("td"),Jg=n("strong"),z5=i("label"),M5=p(),Ac=n("td"),K5=i("The label for the class (model specific)"),F5=p(),jn=n("tr"),Dc=n("td"),Wg=n("strong"),J5=i("score"),W5=p(),Oc=n("td"),Y5=i("A floats that represents how likely is that the text belongs the this class."),uv=p(),Le=n("h3"),as=n("a"),Yg=n("span"),_(kn.$$.fragment),V5=p(),Vg=n("span"),X5=i("Text Generation task"),cv=p(),Pc=n("p"),Q5=i("Use to continue text from a prompt. This is a very generic task."),fv=p(),_(rs.$$.fragment),pv=p(),An=n("p"),Z5=i("Available with: "),Dn=n("a"),e6=i("\u{1F917} Transformers"),hv=p(),Rc=n("p"),t6=i("Example:"),dv=p(),_(ns.$$.fragment),gv=p(),Sc=n("p"),s6=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),mv=p(),os=n("table"),Xg=n("thead"),On=n("tr"),Nc=n("th"),a6=i("All parameters"),r6=p(),Qg=n("th"),n6=p(),I=n("tbody"),Pn=n("tr"),Rn=n("td"),Zg=n("strong"),o6=i("inputs"),l6=i(" (required):"),i6=p(),xc=n("td"),u6=i("a string to be generated from"),c6=p(),Sn=n("tr"),Ic=n("td"),em=n("strong"),f6=i("parameters"),p6=p(),Hc=n("td"),h6=i("dict containing the following keys:"),d6=p(),Nn=n("tr"),Bc=n("td"),g6=i("top_k"),m6=p(),ve=n("td"),$6=i("(Default: "),tm=n("code"),q6=i("None"),_6=i("). Integer to define the top tokens considered within the "),sm=n("code"),v6=i("sample"),E6=i(" operation to create new text."),y6=p(),xn=n("tr"),Cc=n("td"),w6=i("top_p"),b6=p(),ie=n("td"),T6=i("(Default: "),am=n("code"),j6=i("None"),k6=i("). Float to define the tokens that are within the "),rm=n("code"),A6=i("sample"),D6=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),nm=n("code"),O6=i("top_p"),P6=i("."),R6=p(),In=n("tr"),Gc=n("td"),S6=i("temperature"),N6=p(),ue=n("td"),x6=i("(Default: "),om=n("code"),I6=i("1.0"),H6=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),lm=n("code"),B6=i("0"),C6=i(" means always take the highest score, "),im=n("code"),G6=i("100.0"),L6=i(" is getting closer to uniform probability."),U6=p(),Hn=n("tr"),Lc=n("td"),z6=i("repetition_penalty"),M6=p(),ls=n("td"),K6=i("(Default: "),um=n("code"),F6=i("None"),J6=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),W6=p(),Bn=n("tr"),Uc=n("td"),Y6=i("max_new_tokens"),V6=p(),Ee=n("td"),X6=i("(Default: "),cm=n("code"),Q6=i("None"),Z6=i("). Int (0-250). The amount of new tokens to be generated, this does "),fm=n("strong"),e7=i("not"),t7=i(" include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),s7=p(),Cn=n("tr"),zc=n("td"),a7=i("max_time"),r7=p(),ye=n("td"),n7=i("(Default: "),pm=n("code"),o7=i("None"),l7=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),hm=n("code"),i7=i("max_new_tokens"),u7=i(" for best results."),c7=p(),Gn=n("tr"),Mc=n("td"),f7=i("return_full_text"),p7=p(),we=n("td"),h7=i("(Default: "),dm=n("code"),d7=i("True"),g7=i("). Bool. If set to False, the return results will "),gm=n("strong"),m7=i("not"),$7=i(" contain the original query making it easier for prompting."),q7=p(),Ln=n("tr"),Kc=n("td"),_7=i("num_return_sequences"),v7=p(),is=n("td"),E7=i("(Default: "),mm=n("code"),y7=i("1"),w7=i("). Integer. The number of proposition you want to be returned."),b7=p(),Un=n("tr"),Fc=n("td"),T7=i("do_sample"),j7=p(),us=n("td"),k7=i("(Optional: "),$m=n("code"),A7=i("True"),D7=i("). Bool. Whether or not to use sampling, use greedy decoding otherwise."),O7=p(),zn=n("tr"),Jc=n("td"),qm=n("strong"),P7=i("options"),R7=p(),Wc=n("td"),S7=i("a dict containing the following keys:"),N7=p(),Mn=n("tr"),Yc=n("td"),x7=i("use_gpu"),I7=p(),cs=n("td"),H7=i("(Default: "),_m=n("code"),B7=i("false"),C7=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),G7=p(),Kn=n("tr"),Vc=n("td"),L7=i("use_cache"),U7=p(),fs=n("td"),z7=i("(Default: "),vm=n("code"),M7=i("true"),K7=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),F7=p(),Fn=n("tr"),Xc=n("td"),J7=i("wait_for_model"),W7=p(),ps=n("td"),Y7=i("(Default: "),Em=n("code"),V7=i("false"),X7=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),$v=p(),Qc=n("p"),Q7=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),qv=p(),_(hs.$$.fragment),_v=p(),ds=n("table"),ym=n("thead"),Jn=n("tr"),Zc=n("th"),Z7=i("Returned values"),e9=p(),wm=n("th"),t9=p(),bm=n("tbody"),Wn=n("tr"),ef=n("td"),Tm=n("strong"),s9=i("generated_text"),a9=p(),tf=n("td"),r9=i("The continuated string"),vv=p(),Ue=n("h3"),gs=n("a"),jm=n("span"),_(Yn.$$.fragment),n9=p(),km=n("span"),o9=i("Text2Text Generation task"),Ev=p(),ms=n("p"),l9=i("Essentially "),sf=n("a"),i9=i("Text-generation task"),u9=i(`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),yv=p(),ze=n("h3"),$s=n("a"),Am=n("span"),_(Vn.$$.fragment),c9=p(),Dm=n("span"),f9=i("Token Classification task"),wv=p(),af=n("p"),p9=i(`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),bv=p(),_(qs.$$.fragment),Tv=p(),Me=n("p"),h9=i("Available with: "),Xn=n("a"),d9=i("\u{1F917} Transformers"),g9=i(`,
`),Qn=n("a"),m9=i("Flair"),jv=p(),rf=n("p"),$9=i("Example:"),kv=p(),_(_s.$$.fragment),Av=p(),nf=n("p"),q9=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Dv=p(),vs=n("table"),Om=n("thead"),Zn=n("tr"),of=n("th"),_9=i("All parameters"),v9=p(),Pm=n("th"),E9=p(),F=n("tbody"),eo=n("tr"),to=n("td"),Rm=n("strong"),y9=i("inputs"),w9=i(" (required)"),b9=p(),lf=n("td"),T9=i("a string to be classified"),j9=p(),so=n("tr"),uf=n("td"),Sm=n("strong"),k9=i("parameters"),A9=p(),cf=n("td"),D9=i("a dict containing the following key:"),O9=p(),ao=n("tr"),ff=n("td"),P9=i("aggregation_strategy"),R9=p(),x=n("td"),S9=i("(Default: "),Nm=n("code"),N9=i("simple"),x9=i("). There are several aggregation strategies: "),I9=n("br"),H9=p(),xm=n("code"),B9=i("none"),C9=i(": Every token gets classified without further aggregation. "),G9=n("br"),L9=p(),Im=n("code"),U9=i("simple"),z9=i(": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),M9=n("br"),K9=p(),Hm=n("code"),F9=i("first"),J9=i(": Same as the "),Bm=n("code"),W9=i("simple"),Y9=i(" strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),V9=n("br"),X9=p(),Cm=n("code"),Q9=i("average"),Z9=i(": Same as the "),Gm=n("code"),e8=i("simple"),t8=i(" strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),s8=n("br"),a8=p(),Lm=n("code"),r8=i("max"),n8=i(": Same as the "),Um=n("code"),o8=i("simple"),l8=i(" strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),i8=p(),ro=n("tr"),pf=n("td"),zm=n("strong"),u8=i("options"),c8=p(),hf=n("td"),f8=i("a dict containing the following keys:"),p8=p(),no=n("tr"),df=n("td"),h8=i("use_gpu"),d8=p(),Es=n("td"),g8=i("(Default: "),Mm=n("code"),m8=i("false"),$8=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),q8=p(),oo=n("tr"),gf=n("td"),_8=i("use_cache"),v8=p(),ys=n("td"),E8=i("(Default: "),Km=n("code"),y8=i("true"),w8=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),b8=p(),lo=n("tr"),mf=n("td"),T8=i("wait_for_model"),j8=p(),ws=n("td"),k8=i("(Default: "),Fm=n("code"),A8=i("false"),D8=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Ov=p(),$f=n("p"),O8=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Pv=p(),_(bs.$$.fragment),Rv=p(),Ts=n("table"),Jm=n("thead"),io=n("tr"),qf=n("th"),P8=i("Returned values"),R8=p(),Wm=n("th"),S8=p(),te=n("tbody"),uo=n("tr"),_f=n("td"),Ym=n("strong"),N8=i("entity_group"),x8=p(),vf=n("td"),I8=i("The type for the entity being recognized (model specific)."),H8=p(),co=n("tr"),Ef=n("td"),Vm=n("strong"),B8=i("score"),C8=p(),yf=n("td"),G8=i("How likely the entity was recognized."),L8=p(),fo=n("tr"),wf=n("td"),Xm=n("strong"),U8=i("word"),z8=p(),bf=n("td"),M8=i("The string that was captured"),K8=p(),po=n("tr"),Tf=n("td"),Qm=n("strong"),F8=i("start"),J8=p(),js=n("td"),W8=i("The offset stringwise where the answer is located. Useful to disambiguate if "),Zm=n("code"),Y8=i("word"),V8=i(" occurs multiple times."),X8=p(),ho=n("tr"),jf=n("td"),e$=n("strong"),Q8=i("end"),Z8=p(),ks=n("td"),eA=i("The offset stringwise where the answer is located. Useful to disambiguate if "),t$=n("code"),tA=i("word"),sA=i(" occurs multiple times."),Sv=p(),Ke=n("h3"),As=n("a"),s$=n("span"),_(go.$$.fragment),aA=p(),a$=n("span"),rA=i("Named Entity Recognition (NER) task"),Nv=p(),mo=n("p"),nA=i("See "),kf=n("a"),oA=i("Token-classification task"),xv=p(),Fe=n("h3"),Ds=n("a"),r$=n("span"),_($o.$$.fragment),lA=p(),n$=n("span"),iA=i("Translation task"),Iv=p(),Af=n("p"),uA=i("This task is well known to translate text from one language to another"),Hv=p(),_(Os.$$.fragment),Bv=p(),qo=n("p"),cA=i("Available with: "),_o=n("a"),fA=i("\u{1F917} Transformers"),Cv=p(),Df=n("p"),pA=i("Example:"),Gv=p(),_(Ps.$$.fragment),Lv=p(),Of=n("p"),hA=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Uv=p(),Rs=n("table"),o$=n("thead"),vo=n("tr"),Pf=n("th"),dA=i("All parameters"),gA=p(),l$=n("th"),mA=p(),se=n("tbody"),Eo=n("tr"),yo=n("td"),i$=n("strong"),$A=i("inputs"),qA=i(" (required)"),_A=p(),Rf=n("td"),vA=i("a string to be translated in the original languages"),EA=p(),wo=n("tr"),Sf=n("td"),u$=n("strong"),yA=i("options"),wA=p(),Nf=n("td"),bA=i("a dict containing the following keys:"),TA=p(),bo=n("tr"),xf=n("td"),jA=i("use_gpu"),kA=p(),Ss=n("td"),AA=i("(Default: "),c$=n("code"),DA=i("false"),OA=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),PA=p(),To=n("tr"),If=n("td"),RA=i("use_cache"),SA=p(),Ns=n("td"),NA=i("(Default: "),f$=n("code"),xA=i("true"),IA=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),HA=p(),jo=n("tr"),Hf=n("td"),BA=i("wait_for_model"),CA=p(),xs=n("td"),GA=i("(Default: "),p$=n("code"),LA=i("false"),UA=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),zv=p(),Bf=n("p"),zA=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Mv=p(),Is=n("table"),h$=n("thead"),ko=n("tr"),Cf=n("th"),MA=i("Returned values"),KA=p(),d$=n("th"),FA=p(),g$=n("tbody"),Ao=n("tr"),Gf=n("td"),m$=n("strong"),JA=i("translation_text"),WA=p(),Lf=n("td"),YA=i("The string after translation"),Kv=p(),Je=n("h3"),Hs=n("a"),$$=n("span"),_(Do.$$.fragment),VA=p(),q$=n("span"),XA=i("Zero-Shot Classification task"),Fv=p(),Uf=n("p"),QA=i(`This task is super useful to try out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence, and you get a result.`),Jv=p(),_(Bs.$$.fragment),Wv=p(),Oo=n("p"),ZA=i("Available with: "),Po=n("a"),eD=i("\u{1F917} Transformers"),Yv=p(),zf=n("p"),tD=i("Request:"),Vv=p(),_(Cs.$$.fragment),Xv=p(),Mf=n("p"),sD=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Qv=p(),Gs=n("table"),_$=n("thead"),Ro=n("tr"),Kf=n("th"),aD=i("All parameters"),rD=p(),v$=n("th"),nD=p(),M=n("tbody"),So=n("tr"),No=n("td"),E$=n("strong"),oD=i("inputs"),lD=i(" (required)"),iD=p(),Ff=n("td"),uD=i("a string or list of strings"),cD=p(),xo=n("tr"),Io=n("td"),y$=n("strong"),fD=i("parameters"),pD=i(" (required)"),hD=p(),Jf=n("td"),dD=i("a dict containing the following keys:"),gD=p(),Ho=n("tr"),Wf=n("td"),mD=i("candidate_labels (required)"),$D=p(),be=n("td"),qD=i("a list of strings that are potential classes for "),w$=n("code"),_D=i("inputs"),vD=i(". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),b$=n("code"),ED=i("multi_label=True"),yD=i(" and do the scaling on your end. )"),wD=p(),Bo=n("tr"),Yf=n("td"),bD=i("multi_label"),TD=p(),Ls=n("td"),jD=i("(Default: "),T$=n("code"),kD=i("false"),AD=i(") Boolean that is set to True if classes can overlap"),DD=p(),Co=n("tr"),Vf=n("td"),j$=n("strong"),OD=i("options"),PD=p(),Xf=n("td"),RD=i("a dict containing the following keys:"),SD=p(),Go=n("tr"),Qf=n("td"),ND=i("use_gpu"),xD=p(),Us=n("td"),ID=i("(Default: "),k$=n("code"),HD=i("false"),BD=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),CD=p(),Lo=n("tr"),Zf=n("td"),GD=i("use_cache"),LD=p(),zs=n("td"),UD=i("(Default: "),A$=n("code"),zD=i("true"),MD=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),KD=p(),Uo=n("tr"),ep=n("td"),FD=i("wait_for_model"),JD=p(),Ms=n("td"),WD=i("(Default: "),D$=n("code"),YD=i("false"),VD=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Zv=p(),tp=n("p"),XD=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),e2=p(),sp=n("p"),QD=i("Response:"),t2=p(),_(Ks.$$.fragment),s2=p(),Fs=n("table"),O$=n("thead"),zo=n("tr"),ap=n("th"),ZD=i("Returned values"),eO=p(),P$=n("th"),tO=p(),We=n("tbody"),Mo=n("tr"),rp=n("td"),R$=n("strong"),sO=i("sequence"),aO=p(),np=n("td"),rO=i("The string sent as an input"),nO=p(),Ko=n("tr"),op=n("td"),S$=n("strong"),oO=i("labels"),lO=p(),lp=n("td"),iO=i("The list of strings for labels that you sent (in order)"),uO=p(),Fo=n("tr"),ip=n("td"),N$=n("strong"),cO=i("scores"),fO=p(),Js=n("td"),pO=i("a list of floats that correspond the the probability of label, in the same order as "),x$=n("code"),hO=i("labels"),dO=i("."),a2=p(),Ye=n("h3"),Ws=n("a"),I$=n("span"),_(Jo.$$.fragment),gO=p(),H$=n("span"),mO=i("Conversational task"),r2=p(),up=n("p"),$O=i(`This task corresponds to any chatbot like structure. Models tend to have
shorter max_length, so please check with caution when using a given
model if you need long range dependency or not.`),n2=p(),_(Ys.$$.fragment),o2=p(),Wo=n("p"),qO=i("Available with: "),Yo=n("a"),_O=i("\u{1F917} Transformers"),l2=p(),cp=n("p"),vO=i("Example:"),i2=p(),_(Vs.$$.fragment),u2=p(),fp=n("p"),EO=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),c2=p(),Xs=n("table"),B$=n("thead"),Vo=n("tr"),pp=n("th"),yO=i("All parameters"),wO=p(),C$=n("th"),bO=p(),N=n("tbody"),Xo=n("tr"),Qo=n("td"),G$=n("strong"),TO=i("inputs"),jO=i(" (required)"),kO=p(),L$=n("td"),AO=p(),Zo=n("tr"),hp=n("td"),DO=i("text (required)"),OO=p(),dp=n("td"),PO=i("The last input from the user in the conversation."),RO=p(),el=n("tr"),gp=n("td"),SO=i("generated_responses"),NO=p(),mp=n("td"),xO=i("A list of strings corresponding to the earlier replies from the model."),IO=p(),tl=n("tr"),$p=n("td"),HO=i("past_user_inputs"),BO=p(),Qs=n("td"),CO=i("A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),U$=n("code"),GO=i("generated_responses"),LO=i("."),UO=p(),sl=n("tr"),qp=n("td"),z$=n("strong"),zO=i("parameters"),MO=p(),_p=n("td"),KO=i("a dict containing the following keys:"),FO=p(),al=n("tr"),vp=n("td"),JO=i("min_length"),WO=p(),Te=n("td"),YO=i("(Default: "),M$=n("code"),VO=i("None"),XO=i("). Integer to define the minimum length "),K$=n("strong"),QO=i("in tokens"),ZO=i(" of the output summary."),eP=p(),rl=n("tr"),Ep=n("td"),tP=i("max_length"),sP=p(),je=n("td"),aP=i("(Default: "),F$=n("code"),rP=i("None"),nP=i("). Integer to define the maximum length "),J$=n("strong"),oP=i("in tokens"),lP=i(" of the output summary."),iP=p(),nl=n("tr"),yp=n("td"),uP=i("top_k"),cP=p(),ke=n("td"),fP=i("(Default: "),W$=n("code"),pP=i("None"),hP=i("). Integer to define the top tokens considered within the "),Y$=n("code"),dP=i("sample"),gP=i(" operation to create new text."),mP=p(),ol=n("tr"),wp=n("td"),$P=i("top_p"),qP=p(),ce=n("td"),_P=i("(Default: "),V$=n("code"),vP=i("None"),EP=i("). Float to define the tokens that are within the "),X$=n("code"),yP=i("sample"),wP=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Q$=n("code"),bP=i("top_p"),TP=i("."),jP=p(),ll=n("tr"),bp=n("td"),kP=i("temperature"),AP=p(),fe=n("td"),DP=i("(Default: "),Z$=n("code"),OP=i("1.0"),PP=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),eq=n("code"),RP=i("0"),SP=i(" means always take the highest score, "),tq=n("code"),NP=i("100.0"),xP=i(" is getting closer to uniform probability."),IP=p(),il=n("tr"),Tp=n("td"),HP=i("repetition_penalty"),BP=p(),Zs=n("td"),CP=i("(Default: "),sq=n("code"),GP=i("None"),LP=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),UP=p(),ul=n("tr"),jp=n("td"),zP=i("max_time"),MP=p(),ea=n("td"),KP=i("(Default: "),aq=n("code"),FP=i("None"),JP=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),WP=p(),cl=n("tr"),kp=n("td"),rq=n("strong"),YP=i("options"),VP=p(),Ap=n("td"),XP=i("a dict containing the following keys:"),QP=p(),fl=n("tr"),Dp=n("td"),ZP=i("use_gpu"),eR=p(),ta=n("td"),tR=i("(Default: "),nq=n("code"),sR=i("false"),aR=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),rR=p(),pl=n("tr"),Op=n("td"),nR=i("use_cache"),oR=p(),sa=n("td"),lR=i("(Default: "),oq=n("code"),iR=i("true"),uR=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),cR=p(),hl=n("tr"),Pp=n("td"),fR=i("wait_for_model"),pR=p(),aa=n("td"),hR=i("(Default: "),lq=n("code"),dR=i("false"),gR=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),f2=p(),Rp=n("p"),mR=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),p2=p(),ra=n("table"),iq=n("thead"),dl=n("tr"),Sp=n("th"),$R=i("Returned values"),qR=p(),uq=n("th"),_R=p(),ge=n("tbody"),gl=n("tr"),Np=n("td"),cq=n("strong"),vR=i("generated_text"),ER=p(),xp=n("td"),yR=i("The answer of the bot"),wR=p(),ml=n("tr"),Ip=n("td"),fq=n("strong"),bR=i("conversation"),TR=p(),Hp=n("td"),jR=i("A facility dictionnary to send back for the next input (with the new user input addition)."),kR=p(),$l=n("tr"),Bp=n("td"),AR=i("past_user_inputs"),DR=p(),Cp=n("td"),OR=i("List of strings. The last inputs from the user in the conversation, <em>after the model has run."),PR=p(),ql=n("tr"),Gp=n("td"),RR=i("generated_responses"),SR=p(),Lp=n("td"),NR=i("List of strings. The last outputs from the model in the conversation, <em>after the model has run."),h2=p(),Ve=n("h3"),na=n("a"),pq=n("span"),_(_l.$$.fragment),xR=p(),hq=n("span"),IR=i("Feature Extraction task"),d2=p(),Up=n("p"),HR=i(`This task reads some text and outputs raw float values, that are usually
consumed as part of a semantic database/semantic search.`),g2=p(),_(oa.$$.fragment),m2=p(),Xe=n("p"),BR=i("Available with: "),vl=n("a"),CR=i("\u{1F917} Transformers"),GR=p(),El=n("a"),LR=i("Sentence-transformers"),$2=p(),zp=n("p"),UR=i("Request:"),q2=p(),la=n("table"),dq=n("thead"),yl=n("tr"),Mp=n("th"),zR=i("All parameters"),MR=p(),gq=n("th"),KR=p(),ae=n("tbody"),wl=n("tr"),bl=n("td"),mq=n("strong"),FR=i("inputs"),JR=i(" (required):"),WR=p(),Kp=n("td"),YR=i("a string or a list of strings to get the features from."),VR=p(),Tl=n("tr"),Fp=n("td"),$q=n("strong"),XR=i("options"),QR=p(),Jp=n("td"),ZR=i("a dict containing the following keys:"),eS=p(),jl=n("tr"),Wp=n("td"),tS=i("use_gpu"),sS=p(),ia=n("td"),aS=i("(Default: "),qq=n("code"),rS=i("false"),nS=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),oS=p(),kl=n("tr"),Yp=n("td"),lS=i("use_cache"),iS=p(),ua=n("td"),uS=i("(Default: "),_q=n("code"),cS=i("true"),fS=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),pS=p(),Al=n("tr"),Vp=n("td"),hS=i("wait_for_model"),dS=p(),ca=n("td"),gS=i("(Default: "),vq=n("code"),mS=i("false"),$S=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),_2=p(),Xp=n("p"),qS=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),v2=p(),fa=n("table"),Eq=n("thead"),Dl=n("tr"),Qp=n("th"),_S=i("Returned values"),vS=p(),yq=n("th"),ES=p(),wq=n("tbody"),Ol=n("tr"),Zp=n("td"),bq=n("strong"),yS=i("A list of float (or list of list of floats)"),wS=p(),eh=n("td"),bS=i("The numbers that are the representation features of the input."),E2=p(),th=n("small"),TS=i(`Returned values are a list of floats, or a list of list of floats (depending
  on if you sent a string or a list of string, and if the automatic reduction,
  usually mean_pooling for instance was applied for you or not. This should be
  explained on the model's README.`),y2=p(),Qe=n("h2"),pa=n("a"),Tq=n("span"),_(Pl.$$.fragment),jS=p(),jq=n("span"),kS=i("Audio"),w2=p(),Ze=n("h3"),ha=n("a"),kq=n("span"),_(Rl.$$.fragment),AS=p(),Aq=n("span"),DS=i("Automatic Speech Recognition task"),b2=p(),sh=n("p"),OS=i(`This task reads some audio input and outputs the said words within the
audio files.`),T2=p(),_(da.$$.fragment),j2=p(),_(ga.$$.fragment),k2=p(),me=n("p"),PS=i("Available with: "),Sl=n("a"),RS=i("\u{1F917} Transformers"),SS=p(),Nl=n("a"),NS=i("ESPnet"),xS=i(` and
`),xl=n("a"),IS=i("SpeechBrain"),A2=p(),ah=n("p"),HS=i("Request:"),D2=p(),_(ma.$$.fragment),O2=p(),rh=n("p"),BS=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),P2=p(),$a=n("table"),Dq=n("thead"),Il=n("tr"),nh=n("th"),CS=i("All parameters"),GS=p(),Oq=n("th"),LS=p(),Pq=n("tbody"),Hl=n("tr"),Bl=n("td"),Rq=n("strong"),US=i("no parameter"),zS=i(" (required)"),MS=p(),oh=n("td"),KS=i("a binary representation of the audio file. No other parameters are currently allowed."),R2=p(),lh=n("p"),FS=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),S2=p(),ih=n("p"),JS=i("Response:"),N2=p(),_(qa.$$.fragment),x2=p(),_a=n("table"),Sq=n("thead"),Cl=n("tr"),uh=n("th"),WS=i("Returned values"),YS=p(),Nq=n("th"),VS=p(),xq=n("tbody"),Gl=n("tr"),ch=n("td"),Iq=n("strong"),XS=i("text"),QS=p(),fh=n("td"),ZS=i("The string that was recognized within the audio file."),I2=p(),et=n("h3"),va=n("a"),Hq=n("span"),_(Ll.$$.fragment),eN=p(),Bq=n("span"),tN=i("Audio Classification task"),H2=p(),ph=n("p"),sN=i("This task reads some audio input and outputs the likelihood of classes."),B2=p(),_(Ea.$$.fragment),C2=p(),tt=n("p"),aN=i("Available with: "),Ul=n("a"),rN=i("\u{1F917} Transformers"),nN=p(),zl=n("a"),oN=i("SpeechBrain"),G2=p(),hh=n("p"),lN=i("Request:"),L2=p(),_(ya.$$.fragment),U2=p(),dh=n("p"),iN=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),z2=p(),wa=n("table"),Cq=n("thead"),Ml=n("tr"),gh=n("th"),uN=i("All parameters"),cN=p(),Gq=n("th"),fN=p(),Lq=n("tbody"),Kl=n("tr"),Fl=n("td"),Uq=n("strong"),pN=i("no parameter"),hN=i(" (required)"),dN=p(),mh=n("td"),gN=i("a binary representation of the audio file. No other parameters are currently allowed."),M2=p(),$h=n("p"),mN=i("Return value is a dict"),K2=p(),_(ba.$$.fragment),F2=p(),Ta=n("table"),zq=n("thead"),Jl=n("tr"),qh=n("th"),$N=i("Returned values"),qN=p(),Mq=n("th"),_N=p(),Wl=n("tbody"),Yl=n("tr"),_h=n("td"),Kq=n("strong"),vN=i("label"),EN=p(),vh=n("td"),yN=i("The label for the class (model specific)"),wN=p(),Vl=n("tr"),Eh=n("td"),Fq=n("strong"),bN=i("score"),TN=p(),yh=n("td"),jN=i("A float that represents how likely it is that the audio file belongs to this class."),J2=p(),st=n("h2"),ja=n("a"),Jq=n("span"),_(Xl.$$.fragment),kN=p(),Wq=n("span"),AN=i("Computer Vision"),W2=p(),at=n("h3"),ka=n("a"),Yq=n("span"),_(Ql.$$.fragment),DN=p(),Vq=n("span"),ON=i("Image Classification task"),Y2=p(),wh=n("p"),PN=i("This task reads some image input and outputs the likelihood of classes."),V2=p(),_(Aa.$$.fragment),X2=p(),Zl=n("p"),RN=i("Available with: "),ei=n("a"),SN=i("\u{1F917} Transformers"),Q2=p(),bh=n("p"),NN=i("Request:"),Z2=p(),_(Da.$$.fragment),eE=p(),Oa=n("p"),xN=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),ti=n("a"),IN=i(`Pillow
supports`),HN=i("."),tE=p(),Pa=n("table"),Xq=n("thead"),si=n("tr"),Th=n("th"),BN=i("All parameters"),CN=p(),Qq=n("th"),GN=p(),Zq=n("tbody"),ai=n("tr"),ri=n("td"),e_=n("strong"),LN=i("no parameter"),UN=i(" (required)"),zN=p(),jh=n("td"),MN=i("a binary representation of the image file. No other parameters are currently allowed."),sE=p(),kh=n("p"),KN=i("Return value is a dict"),aE=p(),_(Ra.$$.fragment),rE=p(),Sa=n("table"),t_=n("thead"),ni=n("tr"),Ah=n("th"),FN=i("Returned values"),JN=p(),s_=n("th"),WN=p(),oi=n("tbody"),li=n("tr"),Dh=n("td"),a_=n("strong"),YN=i("label"),VN=p(),Oh=n("td"),XN=i("The label for the class (model specific)"),QN=p(),ii=n("tr"),Ph=n("td"),r_=n("strong"),ZN=i("score"),ex=p(),Rh=n("td"),tx=i("A float that represents how likely it is that the image file belongs to this class."),nE=p(),rt=n("h3"),Na=n("a"),n_=n("span"),_(ui.$$.fragment),sx=p(),o_=n("span"),ax=i("Object Detection task"),oE=p(),Sh=n("p"),rx=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),lE=p(),_(xa.$$.fragment),iE=p(),ci=n("p"),nx=i("Available with: "),fi=n("a"),ox=i("\u{1F917} Transformers"),uE=p(),Nh=n("p"),lx=i("Request:"),cE=p(),_(Ia.$$.fragment),fE=p(),Ha=n("p"),ix=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),pi=n("a"),ux=i(`Pillow
supports`),cx=i("."),pE=p(),Ba=n("table"),l_=n("thead"),hi=n("tr"),xh=n("th"),fx=i("All parameters"),px=p(),i_=n("th"),hx=p(),u_=n("tbody"),di=n("tr"),gi=n("td"),c_=n("strong"),dx=i("no parameter"),gx=i(" (required)"),mx=p(),Ih=n("td"),$x=i("a binary representation of the image file. No other parameters are currently allowed."),hE=p(),Hh=n("p"),qx=i("Return value is a dict"),dE=p(),_(Ca.$$.fragment),gE=p(),Ga=n("table"),f_=n("thead"),mi=n("tr"),Bh=n("th"),_x=i("Returned values"),vx=p(),p_=n("th"),Ex=p(),nt=n("tbody"),$i=n("tr"),Ch=n("td"),h_=n("strong"),yx=i("label"),wx=p(),Gh=n("td"),bx=i("The label for the class (model specific) of a detected object."),Tx=p(),qi=n("tr"),Lh=n("td"),d_=n("strong"),jx=i("score"),kx=p(),Uh=n("td"),Ax=i("A float that represents how likely it is that the detected object belongs to the given class."),Dx=p(),_i=n("tr"),zh=n("td"),g_=n("strong"),Ox=i("box"),Px=p(),Mh=n("td"),Rx=i("A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),mE=p(),ot=n("h3"),La=n("a"),m_=n("span"),_(vi.$$.fragment),Sx=p(),$_=n("span"),Nx=i("Image Segmentation task"),$E=p(),Kh=n("p"),xx=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),qE=p(),_(Ua.$$.fragment),_E=p(),Ei=n("p"),Ix=i("Available with: "),yi=n("a"),Hx=i("\u{1F917} Transformers"),vE=p(),Fh=n("p"),Bx=i("Request:"),EE=p(),_(za.$$.fragment),yE=p(),Ma=n("p"),Cx=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),wi=n("a"),Gx=i(`Pillow
supports`),Lx=i("."),wE=p(),Ka=n("table"),q_=n("thead"),bi=n("tr"),Jh=n("th"),Ux=i("All parameters"),zx=p(),__=n("th"),Mx=p(),v_=n("tbody"),Ti=n("tr"),ji=n("td"),E_=n("strong"),Kx=i("no parameter"),Fx=i(" (required)"),Jx=p(),Wh=n("td"),Wx=i("a binary representation of the image file. No other parameters are currently allowed."),bE=p(),Yh=n("p"),Yx=i("Return value is a dict"),TE=p(),_(Fa.$$.fragment),jE=p(),Ja=n("table"),y_=n("thead"),ki=n("tr"),Vh=n("th"),Vx=i("Returned values"),Xx=p(),w_=n("th"),Qx=p(),lt=n("tbody"),Ai=n("tr"),Xh=n("td"),b_=n("strong"),Zx=i("label"),eI=p(),Qh=n("td"),tI=i("The label for the class (model specific) of a segment."),sI=p(),Di=n("tr"),Zh=n("td"),T_=n("strong"),aI=i("score"),rI=p(),ed=n("td"),nI=i("A float that represents how likely it is that the segment belongs to the given class."),oI=p(),Oi=n("tr"),td=n("td"),j_=n("strong"),lI=i("mask"),iI=p(),sd=n("td"),uI=i("A str (base64 str of a single channel black-and-white img) representing the mask of a segment."),this.h()},l(a){const g=QF('[data-svelte="svelte-1phssyn"]',document.head);r=o(g,"META",{name:!0,content:!0}),g.forEach(t),c=h(a),s=o(a,"H1",{class:!0});var Pi=l(s);d=o(Pi,"A",{id:!0,class:!0,href:!0});var k_=l(d);$=o(k_,"SPAN",{});var A_=l($);v(k.$$.fragment,A_),A_.forEach(t),k_.forEach(t),A=h(Pi),j=o(Pi,"SPAN",{});var D_=l(j);T=u(D_,"Detailed parameters"),D_.forEach(t),Pi.forEach(t),S=h(a),D=o(a,"H2",{class:!0});var Ri=l(D);ne=o(Ri,"A",{id:!0,class:!0,href:!0});var O_=l(ne);Re=o(O_,"SPAN",{});var P_=l(Re);v(Q.$$.fragment,P_),P_.forEach(t),O_.forEach(t),Y=h(Ri),it=o(Ri,"SPAN",{});var R_=l(it);Mi=u(R_,"Which task is used by this model ?"),R_.forEach(t),Ri.forEach(t),rr=h(a),Se=o(a,"P",{});var S_=l(Se);g3=u(S_,`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),S_.forEach(t),o1=h(a),Ki=o(a,"P",{});var N_=l(Ki);m3=u(N_,"The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),N_.forEach(t),l1=h(a),ut=o(a,"IMG",{class:!0,src:!0,width:!0}),i1=h(a),ct=o(a,"IMG",{class:!0,src:!0,width:!0}),u1=h(a),Ne=o(a,"H2",{class:!0});var Si=l(Ne);ft=o(Si,"A",{id:!0,class:!0,href:!0});var x_=l(ft);_d=o(x_,"SPAN",{});var I_=l(_d);v(nr.$$.fragment,I_),I_.forEach(t),x_.forEach(t),$3=h(Si),vd=o(Si,"SPAN",{});var H_=l(vd);q3=u(H_,"Natural Language Processing"),H_.forEach(t),Si.forEach(t),c1=h(a),xe=o(a,"H3",{class:!0});var Ni=l(xe);pt=o(Ni,"A",{id:!0,class:!0,href:!0});var B_=l(pt);Ed=o(B_,"SPAN",{});var C_=l(Ed);v(or.$$.fragment,C_),C_.forEach(t),B_.forEach(t),_3=h(Ni),yd=o(Ni,"SPAN",{});var G_=l(yd);v3=u(G_,"Fill Mask task"),G_.forEach(t),Ni.forEach(t),f1=h(a),Fi=o(a,"P",{});var L_=l(Fi);E3=u(L_,`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),L_.forEach(t),p1=h(a),v(ht.$$.fragment,a),h1=h(a),lr=o(a,"P",{});var ad=l(lr);y3=u(ad,"Available with: "),ir=o(ad,"A",{href:!0,rel:!0});var U_=l(ir);w3=u(U_,"\u{1F917} Transformers"),U_.forEach(t),ad.forEach(t),d1=h(a),Ji=o(a,"P",{});var z_=l(Ji);b3=u(z_,"Example:"),z_.forEach(t),g1=h(a),v(dt.$$.fragment,a),m1=h(a),Wi=o(a,"P",{});var M_=l(Wi);T3=u(M_,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),M_.forEach(t),$1=h(a),gt=o(a,"TABLE",{});var xi=l(gt);wd=o(xi,"THEAD",{});var K_=l(wd);ur=o(K_,"TR",{});var Ii=l(ur);Yi=o(Ii,"TH",{align:!0});var F_=l(Yi);j3=u(F_,"All parameters"),F_.forEach(t),k3=h(Ii),bd=o(Ii,"TH",{align:!0}),l(bd).forEach(t),Ii.forEach(t),K_.forEach(t),A3=h(xi),Z=o(xi,"TBODY",{});var re=l(Z);cr=o(re,"TR",{});var Hi=l(cr);fr=o(Hi,"TD",{align:!0});var rd=l(fr);Td=o(rd,"STRONG",{});var J_=l(Td);D3=u(J_,"inputs"),J_.forEach(t),O3=u(rd," (required):"),rd.forEach(t),P3=h(Hi),Vi=o(Hi,"TD",{align:!0});var W_=l(Vi);R3=u(W_,"a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),W_.forEach(t),Hi.forEach(t),S3=h(re),pr=o(re,"TR",{});var Bi=l(pr);Xi=o(Bi,"TD",{align:!0});var Y_=l(Xi);jd=o(Y_,"STRONG",{});var V_=l(jd);N3=u(V_,"options"),V_.forEach(t),Y_.forEach(t),x3=h(Bi),Qi=o(Bi,"TD",{align:!0});var X_=l(Qi);I3=u(X_,"a dict containing the following keys:"),X_.forEach(t),Bi.forEach(t),H3=h(re),hr=o(re,"TR",{});var Ci=l(hr);Zi=o(Ci,"TD",{align:!0});var Q_=l(Zi);B3=u(Q_,"use_gpu"),Q_.forEach(t),C3=h(Ci),mt=o(Ci,"TD",{align:!0});var Gi=l(mt);G3=u(Gi,"(Default: "),kd=o(Gi,"CODE",{});var Z_=l(kd);L3=u(Z_,"false"),Z_.forEach(t),U3=u(Gi,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Gi.forEach(t),Ci.forEach(t),z3=h(re),dr=o(re,"TR",{});var Li=l(dr);eu=o(Li,"TD",{align:!0});var e1=l(eu);M3=u(e1,"use_cache"),e1.forEach(t),K3=h(Li),$t=o(Li,"TD",{align:!0});var Ui=l($t);F3=u(Ui,"(Default: "),Ad=o(Ui,"CODE",{});var t1=l(Ad);J3=u(t1,"true"),t1.forEach(t),W3=u(Ui,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Ui.forEach(t),Li.forEach(t),Y3=h(re),gr=o(re,"TR",{});var AE=l(gr);tu=o(AE,"TD",{align:!0});var BI=l(tu);V3=u(BI,"wait_for_model"),BI.forEach(t),X3=h(AE),qt=o(AE,"TD",{align:!0});var DE=l(qt);Q3=u(DE,"(Default: "),Dd=o(DE,"CODE",{});var CI=l(Dd);Z3=u(CI,"false"),CI.forEach(t),eT=u(DE,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),DE.forEach(t),AE.forEach(t),re.forEach(t),xi.forEach(t),q1=h(a),su=o(a,"P",{});var GI=l(su);tT=u(GI,"Return value is either a dict or a list of dicts if you sent a list of inputs"),GI.forEach(t),_1=h(a),v(_t.$$.fragment,a),v1=h(a),vt=o(a,"TABLE",{});var OE=l(vt);Od=o(OE,"THEAD",{});var LI=l(Od);mr=o(LI,"TR",{});var PE=l(mr);au=o(PE,"TH",{align:!0});var UI=l(au);sT=u(UI,"Returned values"),UI.forEach(t),aT=h(PE),Pd=o(PE,"TH",{align:!0}),l(Pd).forEach(t),PE.forEach(t),LI.forEach(t),rT=h(OE),pe=o(OE,"TBODY",{});var Wa=l(pe);$r=o(Wa,"TR",{});var RE=l($r);ru=o(RE,"TD",{align:!0});var zI=l(ru);Rd=o(zI,"STRONG",{});var MI=l(Rd);nT=u(MI,"sequence"),MI.forEach(t),zI.forEach(t),oT=h(RE),nu=o(RE,"TD",{align:!0});var KI=l(nu);lT=u(KI,"The actual sequence of tokens that ran against the model (may contain special tokens)"),KI.forEach(t),RE.forEach(t),iT=h(Wa),qr=o(Wa,"TR",{});var SE=l(qr);ou=o(SE,"TD",{align:!0});var FI=l(ou);Sd=o(FI,"STRONG",{});var JI=l(Sd);uT=u(JI,"score"),JI.forEach(t),FI.forEach(t),cT=h(SE),lu=o(SE,"TD",{align:!0});var WI=l(lu);fT=u(WI,"The probability for this token."),WI.forEach(t),SE.forEach(t),pT=h(Wa),_r=o(Wa,"TR",{});var NE=l(_r);iu=o(NE,"TD",{align:!0});var YI=l(iu);Nd=o(YI,"STRONG",{});var VI=l(Nd);hT=u(VI,"token"),VI.forEach(t),YI.forEach(t),dT=h(NE),uu=o(NE,"TD",{align:!0});var XI=l(uu);gT=u(XI,"The id of the token"),XI.forEach(t),NE.forEach(t),mT=h(Wa),vr=o(Wa,"TR",{});var xE=l(vr);cu=o(xE,"TD",{align:!0});var QI=l(cu);xd=o(QI,"STRONG",{});var ZI=l(xd);$T=u(ZI,"token_str"),ZI.forEach(t),QI.forEach(t),qT=h(xE),fu=o(xE,"TD",{align:!0});var eH=l(fu);_T=u(eH,"The string representation of the token"),eH.forEach(t),xE.forEach(t),Wa.forEach(t),OE.forEach(t),E1=h(a),Ie=o(a,"H3",{class:!0});var IE=l(Ie);Et=o(IE,"A",{id:!0,class:!0,href:!0});var tH=l(Et);Id=o(tH,"SPAN",{});var sH=l(Id);v(Er.$$.fragment,sH),sH.forEach(t),tH.forEach(t),vT=h(IE),Hd=o(IE,"SPAN",{});var aH=l(Hd);ET=u(aH,"Summarization task"),aH.forEach(t),IE.forEach(t),y1=h(a),yt=o(a,"P",{});var HE=l(yt);yT=u(HE,`This task is well known to summarize longer text into shorter text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss your summarization needs,
please get in touch with us: <`),pu=o(HE,"A",{href:!0});var rH=l(pu);wT=u(rH,"api-enterprise@huggingface.co"),rH.forEach(t),bT=u(HE,">"),HE.forEach(t),w1=h(a),v(wt.$$.fragment,a),b1=h(a),yr=o(a,"P",{});var cI=l(yr);TT=u(cI,"Available with: "),wr=o(cI,"A",{href:!0,rel:!0});var nH=l(wr);jT=u(nH,"\u{1F917} Transformers"),nH.forEach(t),cI.forEach(t),T1=h(a),hu=o(a,"P",{});var oH=l(hu);kT=u(oH,"Example:"),oH.forEach(t),j1=h(a),v(bt.$$.fragment,a),k1=h(a),du=o(a,"P",{});var lH=l(du);AT=u(lH,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),lH.forEach(t),A1=h(a),Tt=o(a,"TABLE",{});var BE=l(Tt);Bd=o(BE,"THEAD",{});var iH=l(Bd);br=o(iH,"TR",{});var CE=l(br);gu=o(CE,"TH",{align:!0});var uH=l(gu);DT=u(uH,"All parameters"),uH.forEach(t),OT=h(CE),Cd=o(CE,"TH",{align:!0}),l(Cd).forEach(t),CE.forEach(t),iH.forEach(t),PT=h(BE),G=o(BE,"TBODY",{});var U=l(G);Tr=o(U,"TR",{});var GE=l(Tr);jr=o(GE,"TD",{align:!0});var fI=l(jr);Gd=o(fI,"STRONG",{});var cH=l(Gd);RT=u(cH,"inputs"),cH.forEach(t),ST=u(fI," (required)"),fI.forEach(t),NT=h(GE),mu=o(GE,"TD",{align:!0});var fH=l(mu);xT=u(fH,"a string to be summarized"),fH.forEach(t),GE.forEach(t),IT=h(U),kr=o(U,"TR",{});var LE=l(kr);$u=o(LE,"TD",{align:!0});var pH=l($u);Ld=o(pH,"STRONG",{});var hH=l(Ld);HT=u(hH,"parameters"),hH.forEach(t),pH.forEach(t),BT=h(LE),qu=o(LE,"TD",{align:!0});var dH=l(qu);CT=u(dH,"a dict containing the following keys:"),dH.forEach(t),LE.forEach(t),GT=h(U),Ar=o(U,"TR",{});var UE=l(Ar);_u=o(UE,"TD",{align:!0});var gH=l(_u);LT=u(gH,"min_length"),gH.forEach(t),UT=h(UE),$e=o(UE,"TD",{align:!0});var nd=l($e);zT=u(nd,"(Default: "),Ud=o(nd,"CODE",{});var mH=l(Ud);MT=u(mH,"None"),mH.forEach(t),KT=u(nd,"). Integer to define the minimum length "),zd=o(nd,"STRONG",{});var $H=l(zd);FT=u($H,"in tokens"),$H.forEach(t),JT=u(nd," of the output summary."),nd.forEach(t),UE.forEach(t),WT=h(U),Dr=o(U,"TR",{});var zE=l(Dr);vu=o(zE,"TD",{align:!0});var qH=l(vu);YT=u(qH,"max_length"),qH.forEach(t),VT=h(zE),qe=o(zE,"TD",{align:!0});var od=l(qe);XT=u(od,"(Default: "),Md=o(od,"CODE",{});var _H=l(Md);QT=u(_H,"None"),_H.forEach(t),ZT=u(od,"). Integer to define the maximum length "),Kd=o(od,"STRONG",{});var vH=l(Kd);ej=u(vH,"in tokens"),vH.forEach(t),tj=u(od," of the output summary."),od.forEach(t),zE.forEach(t),sj=h(U),Or=o(U,"TR",{});var ME=l(Or);Eu=o(ME,"TD",{align:!0});var EH=l(Eu);aj=u(EH,"top_k"),EH.forEach(t),rj=h(ME),_e=o(ME,"TD",{align:!0});var ld=l(_e);nj=u(ld,"(Default: "),Fd=o(ld,"CODE",{});var yH=l(Fd);oj=u(yH,"None"),yH.forEach(t),lj=u(ld,"). Integer to define the top tokens considered within the "),Jd=o(ld,"CODE",{});var wH=l(Jd);ij=u(wH,"sample"),wH.forEach(t),uj=u(ld," operation to create new text."),ld.forEach(t),ME.forEach(t),cj=h(U),Pr=o(U,"TR",{});var KE=l(Pr);yu=o(KE,"TD",{align:!0});var bH=l(yu);fj=u(bH,"top_p"),bH.forEach(t),pj=h(KE),oe=o(KE,"TD",{align:!0});var Ya=l(oe);hj=u(Ya,"(Default: "),Wd=o(Ya,"CODE",{});var TH=l(Wd);dj=u(TH,"None"),TH.forEach(t),gj=u(Ya,"). Float to define the tokens that are within the "),Yd=o(Ya,"CODE",{});var jH=l(Yd);mj=u(jH,"sample"),jH.forEach(t),$j=u(Ya," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Vd=o(Ya,"CODE",{});var kH=l(Vd);qj=u(kH,"top_p"),kH.forEach(t),_j=u(Ya,"."),Ya.forEach(t),KE.forEach(t),vj=h(U),Rr=o(U,"TR",{});var FE=l(Rr);wu=o(FE,"TD",{align:!0});var AH=l(wu);Ej=u(AH,"temperature"),AH.forEach(t),yj=h(FE),le=o(FE,"TD",{align:!0});var Va=l(le);wj=u(Va,"(Default: "),Xd=o(Va,"CODE",{});var DH=l(Xd);bj=u(DH,"1.0"),DH.forEach(t),Tj=u(Va,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),Qd=o(Va,"CODE",{});var OH=l(Qd);jj=u(OH,"0"),OH.forEach(t),kj=u(Va," means always take the highest score, "),Zd=o(Va,"CODE",{});var PH=l(Zd);Aj=u(PH,"100.0"),PH.forEach(t),Dj=u(Va," is getting closer to uniform probability."),Va.forEach(t),FE.forEach(t),Oj=h(U),Sr=o(U,"TR",{});var JE=l(Sr);bu=o(JE,"TD",{align:!0});var RH=l(bu);Pj=u(RH,"repetition_penalty"),RH.forEach(t),Rj=h(JE),jt=o(JE,"TD",{align:!0});var WE=l(jt);Sj=u(WE,"(Default: "),eg=o(WE,"CODE",{});var SH=l(eg);Nj=u(SH,"None"),SH.forEach(t),xj=u(WE,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),WE.forEach(t),JE.forEach(t),Ij=h(U),Nr=o(U,"TR",{});var YE=l(Nr);Tu=o(YE,"TD",{align:!0});var NH=l(Tu);Hj=u(NH,"max_time"),NH.forEach(t),Bj=h(YE),kt=o(YE,"TD",{align:!0});var VE=l(kt);Cj=u(VE,"(Default: "),tg=o(VE,"CODE",{});var xH=l(tg);Gj=u(xH,"None"),xH.forEach(t),Lj=u(VE,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),VE.forEach(t),YE.forEach(t),Uj=h(U),xr=o(U,"TR",{});var XE=l(xr);ju=o(XE,"TD",{align:!0});var IH=l(ju);sg=o(IH,"STRONG",{});var HH=l(sg);zj=u(HH,"options"),HH.forEach(t),IH.forEach(t),Mj=h(XE),ku=o(XE,"TD",{align:!0});var BH=l(ku);Kj=u(BH,"a dict containing the following keys:"),BH.forEach(t),XE.forEach(t),Fj=h(U),Ir=o(U,"TR",{});var QE=l(Ir);Au=o(QE,"TD",{align:!0});var CH=l(Au);Jj=u(CH,"use_gpu"),CH.forEach(t),Wj=h(QE),At=o(QE,"TD",{align:!0});var ZE=l(At);Yj=u(ZE,"(Default: "),ag=o(ZE,"CODE",{});var GH=l(ag);Vj=u(GH,"false"),GH.forEach(t),Xj=u(ZE,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),ZE.forEach(t),QE.forEach(t),Qj=h(U),Hr=o(U,"TR",{});var ey=l(Hr);Du=o(ey,"TD",{align:!0});var LH=l(Du);Zj=u(LH,"use_cache"),LH.forEach(t),e4=h(ey),Dt=o(ey,"TD",{align:!0});var ty=l(Dt);t4=u(ty,"(Default: "),rg=o(ty,"CODE",{});var UH=l(rg);s4=u(UH,"true"),UH.forEach(t),a4=u(ty,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),ty.forEach(t),ey.forEach(t),r4=h(U),Br=o(U,"TR",{});var sy=l(Br);Ou=o(sy,"TD",{align:!0});var zH=l(Ou);n4=u(zH,"wait_for_model"),zH.forEach(t),o4=h(sy),Ot=o(sy,"TD",{align:!0});var ay=l(Ot);l4=u(ay,"(Default: "),ng=o(ay,"CODE",{});var MH=l(ng);i4=u(MH,"false"),MH.forEach(t),u4=u(ay,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),ay.forEach(t),sy.forEach(t),U.forEach(t),BE.forEach(t),D1=h(a),Pu=o(a,"P",{});var KH=l(Pu);c4=u(KH,"Return value is either a dict or a list of dicts if you sent a list of inputs"),KH.forEach(t),O1=h(a),Pt=o(a,"TABLE",{});var ry=l(Pt);og=o(ry,"THEAD",{});var FH=l(og);Cr=o(FH,"TR",{});var ny=l(Cr);Ru=o(ny,"TH",{align:!0});var JH=l(Ru);f4=u(JH,"Returned values"),JH.forEach(t),p4=h(ny),lg=o(ny,"TH",{align:!0}),l(lg).forEach(t),ny.forEach(t),FH.forEach(t),h4=h(ry),ig=o(ry,"TBODY",{});var WH=l(ig);Gr=o(WH,"TR",{});var oy=l(Gr);Su=o(oy,"TD",{align:!0});var YH=l(Su);ug=o(YH,"STRONG",{});var VH=l(ug);d4=u(VH,"summarization_text"),VH.forEach(t),YH.forEach(t),g4=h(oy),Nu=o(oy,"TD",{align:!0});var XH=l(Nu);m4=u(XH,"The string after translation"),XH.forEach(t),oy.forEach(t),WH.forEach(t),ry.forEach(t),P1=h(a),He=o(a,"H3",{class:!0});var ly=l(He);Rt=o(ly,"A",{id:!0,class:!0,href:!0});var QH=l(Rt);cg=o(QH,"SPAN",{});var ZH=l(cg);v(Lr.$$.fragment,ZH),ZH.forEach(t),QH.forEach(t),$4=h(ly),fg=o(ly,"SPAN",{});var eB=l(fg);q4=u(eB,"Question Answering task"),eB.forEach(t),ly.forEach(t),R1=h(a),xu=o(a,"P",{});var tB=l(xu);_4=u(tB,"Want to have a nice know-it-all bot that can answer any question?"),tB.forEach(t),S1=h(a),v(St.$$.fragment,a),N1=h(a),Be=o(a,"P",{});var s1=l(Be);v4=u(s1,"Available with: "),Ur=o(s1,"A",{href:!0,rel:!0});var sB=l(Ur);E4=u(sB,"\u{1F917}Transformers"),sB.forEach(t),y4=u(s1,` and
`),zr=o(s1,"A",{href:!0,rel:!0});var aB=l(zr);w4=u(aB,"AllenNLP"),aB.forEach(t),s1.forEach(t),x1=h(a),Iu=o(a,"P",{});var rB=l(Iu);b4=u(rB,"Example:"),rB.forEach(t),I1=h(a),v(Nt.$$.fragment,a),H1=h(a),Hu=o(a,"P",{});var nB=l(Hu);T4=u(nB,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),nB.forEach(t),B1=h(a),Bu=o(a,"P",{});var oB=l(Bu);j4=u(oB,"Return value is either a dict or a list of dicts if you sent a list of inputs"),oB.forEach(t),C1=h(a),v(xt.$$.fragment,a),G1=h(a),It=o(a,"TABLE",{});var iy=l(It);pg=o(iy,"THEAD",{});var lB=l(pg);Mr=o(lB,"TR",{});var uy=l(Mr);Cu=o(uy,"TH",{align:!0});var iB=l(Cu);k4=u(iB,"Returned values"),iB.forEach(t),A4=h(uy),hg=o(uy,"TH",{align:!0}),l(hg).forEach(t),uy.forEach(t),lB.forEach(t),D4=h(iy),he=o(iy,"TBODY",{});var Xa=l(he);Kr=o(Xa,"TR",{});var cy=l(Kr);Gu=o(cy,"TD",{align:!0});var uB=l(Gu);dg=o(uB,"STRONG",{});var cB=l(dg);O4=u(cB,"answer"),cB.forEach(t),uB.forEach(t),P4=h(cy),Lu=o(cy,"TD",{align:!0});var fB=l(Lu);R4=u(fB,"A string that\u2019s the answer within the text."),fB.forEach(t),cy.forEach(t),S4=h(Xa),Fr=o(Xa,"TR",{});var fy=l(Fr);Uu=o(fy,"TD",{align:!0});var pB=l(Uu);gg=o(pB,"STRONG",{});var hB=l(gg);N4=u(hB,"score"),hB.forEach(t),pB.forEach(t),x4=h(fy),zu=o(fy,"TD",{align:!0});var dB=l(zu);I4=u(dB,"A float that represents how likely that the answer is correct"),dB.forEach(t),fy.forEach(t),H4=h(Xa),Jr=o(Xa,"TR",{});var py=l(Jr);Mu=o(py,"TD",{align:!0});var gB=l(Mu);mg=o(gB,"STRONG",{});var mB=l(mg);B4=u(mB,"start"),mB.forEach(t),gB.forEach(t),C4=h(py),Ht=o(py,"TD",{align:!0});var hy=l(Ht);G4=u(hy,"The index (string wise) of the start of the answer within "),$g=o(hy,"CODE",{});var $B=l($g);L4=u($B,"context"),$B.forEach(t),U4=u(hy,"."),hy.forEach(t),py.forEach(t),z4=h(Xa),Wr=o(Xa,"TR",{});var dy=l(Wr);Ku=o(dy,"TD",{align:!0});var qB=l(Ku);qg=o(qB,"STRONG",{});var _B=l(qg);M4=u(_B,"stop"),_B.forEach(t),qB.forEach(t),K4=h(dy),Bt=o(dy,"TD",{align:!0});var gy=l(Bt);F4=u(gy,"The index (string wise) of the stop of the answer within "),_g=o(gy,"CODE",{});var vB=l(_g);J4=u(vB,"context"),vB.forEach(t),W4=u(gy,"."),gy.forEach(t),dy.forEach(t),Xa.forEach(t),iy.forEach(t),L1=h(a),Ce=o(a,"H3",{class:!0});var my=l(Ce);Ct=o(my,"A",{id:!0,class:!0,href:!0});var EB=l(Ct);vg=o(EB,"SPAN",{});var yB=l(vg);v(Yr.$$.fragment,yB),yB.forEach(t),EB.forEach(t),Y4=h(my),Eg=o(my,"SPAN",{});var wB=l(Eg);V4=u(wB,"Table Question Answering task"),wB.forEach(t),my.forEach(t),U1=h(a),Fu=o(a,"P",{});var bB=l(Fu);X4=u(bB,`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),bB.forEach(t),z1=h(a),v(Gt.$$.fragment,a),M1=h(a),Vr=o(a,"P",{});var pI=l(Vr);Q4=u(pI,"Available with: "),Xr=o(pI,"A",{href:!0,rel:!0});var TB=l(Xr);Z4=u(TB,"\u{1F917} Transformers"),TB.forEach(t),pI.forEach(t),K1=h(a),Ju=o(a,"P",{});var jB=l(Ju);ek=u(jB,"Example:"),jB.forEach(t),F1=h(a),v(Lt.$$.fragment,a),J1=h(a),Wu=o(a,"P",{});var kB=l(Wu);tk=u(kB,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),kB.forEach(t),W1=h(a),Ut=o(a,"TABLE",{});var $y=l(Ut);yg=o($y,"THEAD",{});var AB=l(yg);Qr=o(AB,"TR",{});var qy=l(Qr);Yu=o(qy,"TH",{align:!0});var DB=l(Yu);sk=u(DB,"All parameters"),DB.forEach(t),ak=h(qy),wg=o(qy,"TH",{align:!0}),l(wg).forEach(t),qy.forEach(t),AB.forEach(t),rk=h($y),K=o($y,"TBODY",{});var V=l(K);Zr=o(V,"TR",{});var _y=l(Zr);en=o(_y,"TD",{align:!0});var hI=l(en);bg=o(hI,"STRONG",{});var OB=l(bg);nk=u(OB,"inputs"),OB.forEach(t),ok=u(hI," (required)"),hI.forEach(t),lk=h(_y),Tg=o(_y,"TD",{align:!0}),l(Tg).forEach(t),_y.forEach(t),ik=h(V),tn=o(V,"TR",{});var vy=l(tn);Vu=o(vy,"TD",{align:!0});var PB=l(Vu);uk=u(PB,"query (required)"),PB.forEach(t),ck=h(vy),Xu=o(vy,"TD",{align:!0});var RB=l(Xu);fk=u(RB,"The query in plain text that you want to ask the table"),RB.forEach(t),vy.forEach(t),pk=h(V),sn=o(V,"TR",{});var Ey=l(sn);Qu=o(Ey,"TD",{align:!0});var SB=l(Qu);hk=u(SB,"table (required)"),SB.forEach(t),dk=h(Ey),Zu=o(Ey,"TD",{align:!0});var NB=l(Zu);gk=u(NB,"A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),NB.forEach(t),Ey.forEach(t),mk=h(V),an=o(V,"TR",{});var yy=l(an);ec=o(yy,"TD",{align:!0});var xB=l(ec);jg=o(xB,"STRONG",{});var IB=l(jg);$k=u(IB,"options"),IB.forEach(t),xB.forEach(t),qk=h(yy),tc=o(yy,"TD",{align:!0});var HB=l(tc);_k=u(HB,"a dict containing the following keys:"),HB.forEach(t),yy.forEach(t),vk=h(V),rn=o(V,"TR",{});var wy=l(rn);sc=o(wy,"TD",{align:!0});var BB=l(sc);Ek=u(BB,"use_gpu"),BB.forEach(t),yk=h(wy),zt=o(wy,"TD",{align:!0});var by=l(zt);wk=u(by,"(Default: "),kg=o(by,"CODE",{});var CB=l(kg);bk=u(CB,"false"),CB.forEach(t),Tk=u(by,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),by.forEach(t),wy.forEach(t),jk=h(V),nn=o(V,"TR",{});var Ty=l(nn);ac=o(Ty,"TD",{align:!0});var GB=l(ac);kk=u(GB,"use_cache"),GB.forEach(t),Ak=h(Ty),Mt=o(Ty,"TD",{align:!0});var jy=l(Mt);Dk=u(jy,"(Default: "),Ag=o(jy,"CODE",{});var LB=l(Ag);Ok=u(LB,"true"),LB.forEach(t),Pk=u(jy,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),jy.forEach(t),Ty.forEach(t),Rk=h(V),on=o(V,"TR",{});var ky=l(on);rc=o(ky,"TD",{align:!0});var UB=l(rc);Sk=u(UB,"wait_for_model"),UB.forEach(t),Nk=h(ky),Kt=o(ky,"TD",{align:!0});var Ay=l(Kt);xk=u(Ay,"(Default: "),Dg=o(Ay,"CODE",{});var zB=l(Dg);Ik=u(zB,"false"),zB.forEach(t),Hk=u(Ay,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Ay.forEach(t),ky.forEach(t),V.forEach(t),$y.forEach(t),Y1=h(a),nc=o(a,"P",{});var MB=l(nc);Bk=u(MB,"Return value is either a dict or a list of dicts if you sent a list of inputs"),MB.forEach(t),V1=h(a),v(Ft.$$.fragment,a),X1=h(a),Jt=o(a,"TABLE",{});var Dy=l(Jt);Og=o(Dy,"THEAD",{});var KB=l(Og);ln=o(KB,"TR",{});var Oy=l(ln);oc=o(Oy,"TH",{align:!0});var FB=l(oc);Ck=u(FB,"Returned values"),FB.forEach(t),Gk=h(Oy),Pg=o(Oy,"TH",{align:!0}),l(Pg).forEach(t),Oy.forEach(t),KB.forEach(t),Lk=h(Dy),de=o(Dy,"TBODY",{});var Qa=l(de);un=o(Qa,"TR",{});var Py=l(un);lc=o(Py,"TD",{align:!0});var JB=l(lc);Rg=o(JB,"STRONG",{});var WB=l(Rg);Uk=u(WB,"answer"),WB.forEach(t),JB.forEach(t),zk=h(Py),ic=o(Py,"TD",{align:!0});var YB=l(ic);Mk=u(YB,"The plaintext answer"),YB.forEach(t),Py.forEach(t),Kk=h(Qa),cn=o(Qa,"TR",{});var Ry=l(cn);uc=o(Ry,"TD",{align:!0});var VB=l(uc);Sg=o(VB,"STRONG",{});var XB=l(Sg);Fk=u(XB,"coordinates"),XB.forEach(t),VB.forEach(t),Jk=h(Ry),cc=o(Ry,"TD",{align:!0});var QB=l(cc);Wk=u(QB,"a list of coordinates of the cells referenced in the answer"),QB.forEach(t),Ry.forEach(t),Yk=h(Qa),fn=o(Qa,"TR",{});var Sy=l(fn);fc=o(Sy,"TD",{align:!0});var ZB=l(fc);Ng=o(ZB,"STRONG",{});var eC=l(Ng);Vk=u(eC,"cells"),eC.forEach(t),ZB.forEach(t),Xk=h(Sy),pc=o(Sy,"TD",{align:!0});var tC=l(pc);Qk=u(tC,"a list of coordinates of the cells contents"),tC.forEach(t),Sy.forEach(t),Zk=h(Qa),pn=o(Qa,"TR",{});var Ny=l(pn);hc=o(Ny,"TD",{align:!0});var sC=l(hc);xg=o(sC,"STRONG",{});var aC=l(xg);e5=u(aC,"aggregator"),aC.forEach(t),sC.forEach(t),t5=h(Ny),dc=o(Ny,"TD",{align:!0});var rC=l(dc);s5=u(rC,"The aggregator used to get the answer"),rC.forEach(t),Ny.forEach(t),Qa.forEach(t),Dy.forEach(t),Q1=h(a),Ge=o(a,"H3",{class:!0});var xy=l(Ge);Wt=o(xy,"A",{id:!0,class:!0,href:!0});var nC=l(Wt);Ig=o(nC,"SPAN",{});var oC=l(Ig);v(hn.$$.fragment,oC),oC.forEach(t),nC.forEach(t),a5=h(xy),Hg=o(xy,"SPAN",{});var lC=l(Hg);r5=u(lC,"Text Classification task"),lC.forEach(t),xy.forEach(t),Z1=h(a),gc=o(a,"P",{});var iC=l(gc);n5=u(iC,`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),iC.forEach(t),ev=h(a),v(Yt.$$.fragment,a),tv=h(a),dn=o(a,"P",{});var dI=l(dn);o5=u(dI,"Available with: "),gn=o(dI,"A",{href:!0,rel:!0});var uC=l(gn);l5=u(uC,"\u{1F917} Transformers"),uC.forEach(t),dI.forEach(t),sv=h(a),mc=o(a,"P",{});var cC=l(mc);i5=u(cC,"Example:"),cC.forEach(t),av=h(a),v(Vt.$$.fragment,a),rv=h(a),$c=o(a,"P",{});var fC=l($c);u5=u(fC,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),fC.forEach(t),nv=h(a),Xt=o(a,"TABLE",{});var Iy=l(Xt);Bg=o(Iy,"THEAD",{});var pC=l(Bg);mn=o(pC,"TR",{});var Hy=l(mn);qc=o(Hy,"TH",{align:!0});var hC=l(qc);c5=u(hC,"All parameters"),hC.forEach(t),f5=h(Hy),Cg=o(Hy,"TH",{align:!0}),l(Cg).forEach(t),Hy.forEach(t),pC.forEach(t),p5=h(Iy),ee=o(Iy,"TBODY",{});var Ae=l(ee);$n=o(Ae,"TR",{});var By=l($n);qn=o(By,"TD",{align:!0});var gI=l(qn);Gg=o(gI,"STRONG",{});var dC=l(Gg);h5=u(dC,"inputs"),dC.forEach(t),d5=u(gI," (required)"),gI.forEach(t),g5=h(By),_c=o(By,"TD",{align:!0});var gC=l(_c);m5=u(gC,"a string to be classified"),gC.forEach(t),By.forEach(t),$5=h(Ae),_n=o(Ae,"TR",{});var Cy=l(_n);vc=o(Cy,"TD",{align:!0});var mC=l(vc);Lg=o(mC,"STRONG",{});var $C=l(Lg);q5=u($C,"options"),$C.forEach(t),mC.forEach(t),_5=h(Cy),Ec=o(Cy,"TD",{align:!0});var qC=l(Ec);v5=u(qC,"a dict containing the following keys:"),qC.forEach(t),Cy.forEach(t),E5=h(Ae),vn=o(Ae,"TR",{});var Gy=l(vn);yc=o(Gy,"TD",{align:!0});var _C=l(yc);y5=u(_C,"use_gpu"),_C.forEach(t),w5=h(Gy),Qt=o(Gy,"TD",{align:!0});var Ly=l(Qt);b5=u(Ly,"(Default: "),Ug=o(Ly,"CODE",{});var vC=l(Ug);T5=u(vC,"false"),vC.forEach(t),j5=u(Ly,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Ly.forEach(t),Gy.forEach(t),k5=h(Ae),En=o(Ae,"TR",{});var Uy=l(En);wc=o(Uy,"TD",{align:!0});var EC=l(wc);A5=u(EC,"use_cache"),EC.forEach(t),D5=h(Uy),Zt=o(Uy,"TD",{align:!0});var zy=l(Zt);O5=u(zy,"(Default: "),zg=o(zy,"CODE",{});var yC=l(zg);P5=u(yC,"true"),yC.forEach(t),R5=u(zy,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),zy.forEach(t),Uy.forEach(t),S5=h(Ae),yn=o(Ae,"TR",{});var My=l(yn);bc=o(My,"TD",{align:!0});var wC=l(bc);N5=u(wC,"wait_for_model"),wC.forEach(t),x5=h(My),es=o(My,"TD",{align:!0});var Ky=l(es);I5=u(Ky,"(Default: "),Mg=o(Ky,"CODE",{});var bC=l(Mg);H5=u(bC,"false"),bC.forEach(t),B5=u(Ky,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Ky.forEach(t),My.forEach(t),Ae.forEach(t),Iy.forEach(t),ov=h(a),Tc=o(a,"P",{});var TC=l(Tc);C5=u(TC,"Return value is either a dict or a list of dicts if you sent a list of inputs"),TC.forEach(t),lv=h(a),v(ts.$$.fragment,a),iv=h(a),ss=o(a,"TABLE",{});var Fy=l(ss);Kg=o(Fy,"THEAD",{});var jC=l(Kg);wn=o(jC,"TR",{});var Jy=l(wn);jc=o(Jy,"TH",{align:!0});var kC=l(jc);G5=u(kC,"Returned values"),kC.forEach(t),L5=h(Jy),Fg=o(Jy,"TH",{align:!0}),l(Fg).forEach(t),Jy.forEach(t),jC.forEach(t),U5=h(Fy),bn=o(Fy,"TBODY",{});var Wy=l(bn);Tn=o(Wy,"TR",{});var Yy=l(Tn);kc=o(Yy,"TD",{align:!0});var AC=l(kc);Jg=o(AC,"STRONG",{});var DC=l(Jg);z5=u(DC,"label"),DC.forEach(t),AC.forEach(t),M5=h(Yy),Ac=o(Yy,"TD",{align:!0});var OC=l(Ac);K5=u(OC,"The label for the class (model specific)"),OC.forEach(t),Yy.forEach(t),F5=h(Wy),jn=o(Wy,"TR",{});var Vy=l(jn);Dc=o(Vy,"TD",{align:!0});var PC=l(Dc);Wg=o(PC,"STRONG",{});var RC=l(Wg);J5=u(RC,"score"),RC.forEach(t),PC.forEach(t),W5=h(Vy),Oc=o(Vy,"TD",{align:!0});var SC=l(Oc);Y5=u(SC,"A floats that represents how likely is that the text belongs the this class."),SC.forEach(t),Vy.forEach(t),Wy.forEach(t),Fy.forEach(t),uv=h(a),Le=o(a,"H3",{class:!0});var Xy=l(Le);as=o(Xy,"A",{id:!0,class:!0,href:!0});var NC=l(as);Yg=o(NC,"SPAN",{});var xC=l(Yg);v(kn.$$.fragment,xC),xC.forEach(t),NC.forEach(t),V5=h(Xy),Vg=o(Xy,"SPAN",{});var IC=l(Vg);X5=u(IC,"Text Generation task"),IC.forEach(t),Xy.forEach(t),cv=h(a),Pc=o(a,"P",{});var HC=l(Pc);Q5=u(HC,"Use to continue text from a prompt. This is a very generic task."),HC.forEach(t),fv=h(a),v(rs.$$.fragment,a),pv=h(a),An=o(a,"P",{});var mI=l(An);Z5=u(mI,"Available with: "),Dn=o(mI,"A",{href:!0,rel:!0});var BC=l(Dn);e6=u(BC,"\u{1F917} Transformers"),BC.forEach(t),mI.forEach(t),hv=h(a),Rc=o(a,"P",{});var CC=l(Rc);t6=u(CC,"Example:"),CC.forEach(t),dv=h(a),v(ns.$$.fragment,a),gv=h(a),Sc=o(a,"P",{});var GC=l(Sc);s6=u(GC,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),GC.forEach(t),mv=h(a),os=o(a,"TABLE",{});var Qy=l(os);Xg=o(Qy,"THEAD",{});var LC=l(Xg);On=o(LC,"TR",{});var Zy=l(On);Nc=o(Zy,"TH",{align:!0});var UC=l(Nc);a6=u(UC,"All parameters"),UC.forEach(t),r6=h(Zy),Qg=o(Zy,"TH",{align:!0}),l(Qg).forEach(t),Zy.forEach(t),LC.forEach(t),n6=h(Qy),I=o(Qy,"TBODY",{});var B=l(I);Pn=o(B,"TR",{});var ew=l(Pn);Rn=o(ew,"TD",{align:!0});var $I=l(Rn);Zg=o($I,"STRONG",{});var zC=l(Zg);o6=u(zC,"inputs"),zC.forEach(t),l6=u($I," (required):"),$I.forEach(t),i6=h(ew),xc=o(ew,"TD",{align:!0});var MC=l(xc);u6=u(MC,"a string to be generated from"),MC.forEach(t),ew.forEach(t),c6=h(B),Sn=o(B,"TR",{});var tw=l(Sn);Ic=o(tw,"TD",{align:!0});var KC=l(Ic);em=o(KC,"STRONG",{});var FC=l(em);f6=u(FC,"parameters"),FC.forEach(t),KC.forEach(t),p6=h(tw),Hc=o(tw,"TD",{align:!0});var JC=l(Hc);h6=u(JC,"dict containing the following keys:"),JC.forEach(t),tw.forEach(t),d6=h(B),Nn=o(B,"TR",{});var sw=l(Nn);Bc=o(sw,"TD",{align:!0});var WC=l(Bc);g6=u(WC,"top_k"),WC.forEach(t),m6=h(sw),ve=o(sw,"TD",{align:!0});var id=l(ve);$6=u(id,"(Default: "),tm=o(id,"CODE",{});var YC=l(tm);q6=u(YC,"None"),YC.forEach(t),_6=u(id,"). Integer to define the top tokens considered within the "),sm=o(id,"CODE",{});var VC=l(sm);v6=u(VC,"sample"),VC.forEach(t),E6=u(id," operation to create new text."),id.forEach(t),sw.forEach(t),y6=h(B),xn=o(B,"TR",{});var aw=l(xn);Cc=o(aw,"TD",{align:!0});var XC=l(Cc);w6=u(XC,"top_p"),XC.forEach(t),b6=h(aw),ie=o(aw,"TD",{align:!0});var Za=l(ie);T6=u(Za,"(Default: "),am=o(Za,"CODE",{});var QC=l(am);j6=u(QC,"None"),QC.forEach(t),k6=u(Za,"). Float to define the tokens that are within the "),rm=o(Za,"CODE",{});var ZC=l(rm);A6=u(ZC,"sample"),ZC.forEach(t),D6=u(Za," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),nm=o(Za,"CODE",{});var eG=l(nm);O6=u(eG,"top_p"),eG.forEach(t),P6=u(Za,"."),Za.forEach(t),aw.forEach(t),R6=h(B),In=o(B,"TR",{});var rw=l(In);Gc=o(rw,"TD",{align:!0});var tG=l(Gc);S6=u(tG,"temperature"),tG.forEach(t),N6=h(rw),ue=o(rw,"TD",{align:!0});var er=l(ue);x6=u(er,"(Default: "),om=o(er,"CODE",{});var sG=l(om);I6=u(sG,"1.0"),sG.forEach(t),H6=u(er,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),lm=o(er,"CODE",{});var aG=l(lm);B6=u(aG,"0"),aG.forEach(t),C6=u(er," means always take the highest score, "),im=o(er,"CODE",{});var rG=l(im);G6=u(rG,"100.0"),rG.forEach(t),L6=u(er," is getting closer to uniform probability."),er.forEach(t),rw.forEach(t),U6=h(B),Hn=o(B,"TR",{});var nw=l(Hn);Lc=o(nw,"TD",{align:!0});var nG=l(Lc);z6=u(nG,"repetition_penalty"),nG.forEach(t),M6=h(nw),ls=o(nw,"TD",{align:!0});var ow=l(ls);K6=u(ow,"(Default: "),um=o(ow,"CODE",{});var oG=l(um);F6=u(oG,"None"),oG.forEach(t),J6=u(ow,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),ow.forEach(t),nw.forEach(t),W6=h(B),Bn=o(B,"TR",{});var lw=l(Bn);Uc=o(lw,"TD",{align:!0});var lG=l(Uc);Y6=u(lG,"max_new_tokens"),lG.forEach(t),V6=h(lw),Ee=o(lw,"TD",{align:!0});var ud=l(Ee);X6=u(ud,"(Default: "),cm=o(ud,"CODE",{});var iG=l(cm);Q6=u(iG,"None"),iG.forEach(t),Z6=u(ud,"). Int (0-250). The amount of new tokens to be generated, this does "),fm=o(ud,"STRONG",{});var uG=l(fm);e7=u(uG,"not"),uG.forEach(t),t7=u(ud," include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),ud.forEach(t),lw.forEach(t),s7=h(B),Cn=o(B,"TR",{});var iw=l(Cn);zc=o(iw,"TD",{align:!0});var cG=l(zc);a7=u(cG,"max_time"),cG.forEach(t),r7=h(iw),ye=o(iw,"TD",{align:!0});var cd=l(ye);n7=u(cd,"(Default: "),pm=o(cd,"CODE",{});var fG=l(pm);o7=u(fG,"None"),fG.forEach(t),l7=u(cd,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),hm=o(cd,"CODE",{});var pG=l(hm);i7=u(pG,"max_new_tokens"),pG.forEach(t),u7=u(cd," for best results."),cd.forEach(t),iw.forEach(t),c7=h(B),Gn=o(B,"TR",{});var uw=l(Gn);Mc=o(uw,"TD",{align:!0});var hG=l(Mc);f7=u(hG,"return_full_text"),hG.forEach(t),p7=h(uw),we=o(uw,"TD",{align:!0});var fd=l(we);h7=u(fd,"(Default: "),dm=o(fd,"CODE",{});var dG=l(dm);d7=u(dG,"True"),dG.forEach(t),g7=u(fd,"). Bool. If set to False, the return results will "),gm=o(fd,"STRONG",{});var gG=l(gm);m7=u(gG,"not"),gG.forEach(t),$7=u(fd," contain the original query making it easier for prompting."),fd.forEach(t),uw.forEach(t),q7=h(B),Ln=o(B,"TR",{});var cw=l(Ln);Kc=o(cw,"TD",{align:!0});var mG=l(Kc);_7=u(mG,"num_return_sequences"),mG.forEach(t),v7=h(cw),is=o(cw,"TD",{align:!0});var fw=l(is);E7=u(fw,"(Default: "),mm=o(fw,"CODE",{});var $G=l(mm);y7=u($G,"1"),$G.forEach(t),w7=u(fw,"). Integer. The number of proposition you want to be returned."),fw.forEach(t),cw.forEach(t),b7=h(B),Un=o(B,"TR",{});var pw=l(Un);Fc=o(pw,"TD",{align:!0});var qG=l(Fc);T7=u(qG,"do_sample"),qG.forEach(t),j7=h(pw),us=o(pw,"TD",{align:!0});var hw=l(us);k7=u(hw,"(Optional: "),$m=o(hw,"CODE",{});var _G=l($m);A7=u(_G,"True"),_G.forEach(t),D7=u(hw,"). Bool. Whether or not to use sampling, use greedy decoding otherwise."),hw.forEach(t),pw.forEach(t),O7=h(B),zn=o(B,"TR",{});var dw=l(zn);Jc=o(dw,"TD",{align:!0});var vG=l(Jc);qm=o(vG,"STRONG",{});var EG=l(qm);P7=u(EG,"options"),EG.forEach(t),vG.forEach(t),R7=h(dw),Wc=o(dw,"TD",{align:!0});var yG=l(Wc);S7=u(yG,"a dict containing the following keys:"),yG.forEach(t),dw.forEach(t),N7=h(B),Mn=o(B,"TR",{});var gw=l(Mn);Yc=o(gw,"TD",{align:!0});var wG=l(Yc);x7=u(wG,"use_gpu"),wG.forEach(t),I7=h(gw),cs=o(gw,"TD",{align:!0});var mw=l(cs);H7=u(mw,"(Default: "),_m=o(mw,"CODE",{});var bG=l(_m);B7=u(bG,"false"),bG.forEach(t),C7=u(mw,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),mw.forEach(t),gw.forEach(t),G7=h(B),Kn=o(B,"TR",{});var $w=l(Kn);Vc=o($w,"TD",{align:!0});var TG=l(Vc);L7=u(TG,"use_cache"),TG.forEach(t),U7=h($w),fs=o($w,"TD",{align:!0});var qw=l(fs);z7=u(qw,"(Default: "),vm=o(qw,"CODE",{});var jG=l(vm);M7=u(jG,"true"),jG.forEach(t),K7=u(qw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),qw.forEach(t),$w.forEach(t),F7=h(B),Fn=o(B,"TR",{});var _w=l(Fn);Xc=o(_w,"TD",{align:!0});var kG=l(Xc);J7=u(kG,"wait_for_model"),kG.forEach(t),W7=h(_w),ps=o(_w,"TD",{align:!0});var vw=l(ps);Y7=u(vw,"(Default: "),Em=o(vw,"CODE",{});var AG=l(Em);V7=u(AG,"false"),AG.forEach(t),X7=u(vw,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),vw.forEach(t),_w.forEach(t),B.forEach(t),Qy.forEach(t),$v=h(a),Qc=o(a,"P",{});var DG=l(Qc);Q7=u(DG,"Return value is either a dict or a list of dicts if you sent a list of inputs"),DG.forEach(t),qv=h(a),v(hs.$$.fragment,a),_v=h(a),ds=o(a,"TABLE",{});var Ew=l(ds);ym=o(Ew,"THEAD",{});var OG=l(ym);Jn=o(OG,"TR",{});var yw=l(Jn);Zc=o(yw,"TH",{align:!0});var PG=l(Zc);Z7=u(PG,"Returned values"),PG.forEach(t),e9=h(yw),wm=o(yw,"TH",{align:!0}),l(wm).forEach(t),yw.forEach(t),OG.forEach(t),t9=h(Ew),bm=o(Ew,"TBODY",{});var RG=l(bm);Wn=o(RG,"TR",{});var ww=l(Wn);ef=o(ww,"TD",{align:!0});var SG=l(ef);Tm=o(SG,"STRONG",{});var NG=l(Tm);s9=u(NG,"generated_text"),NG.forEach(t),SG.forEach(t),a9=h(ww),tf=o(ww,"TD",{align:!0});var xG=l(tf);r9=u(xG,"The continuated string"),xG.forEach(t),ww.forEach(t),RG.forEach(t),Ew.forEach(t),vv=h(a),Ue=o(a,"H3",{class:!0});var bw=l(Ue);gs=o(bw,"A",{id:!0,class:!0,href:!0});var IG=l(gs);jm=o(IG,"SPAN",{});var HG=l(jm);v(Yn.$$.fragment,HG),HG.forEach(t),IG.forEach(t),n9=h(bw),km=o(bw,"SPAN",{});var BG=l(km);o9=u(BG,"Text2Text Generation task"),BG.forEach(t),bw.forEach(t),Ev=h(a),ms=o(a,"P",{});var Tw=l(ms);l9=u(Tw,"Essentially "),sf=o(Tw,"A",{href:!0});var CG=l(sf);i9=u(CG,"Text-generation task"),CG.forEach(t),u9=u(Tw,`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),Tw.forEach(t),yv=h(a),ze=o(a,"H3",{class:!0});var jw=l(ze);$s=o(jw,"A",{id:!0,class:!0,href:!0});var GG=l($s);Am=o(GG,"SPAN",{});var LG=l(Am);v(Vn.$$.fragment,LG),LG.forEach(t),GG.forEach(t),c9=h(jw),Dm=o(jw,"SPAN",{});var UG=l(Dm);f9=u(UG,"Token Classification task"),UG.forEach(t),jw.forEach(t),wv=h(a),af=o(a,"P",{});var zG=l(af);p9=u(zG,`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),zG.forEach(t),bv=h(a),v(qs.$$.fragment,a),Tv=h(a),Me=o(a,"P",{});var a1=l(Me);h9=u(a1,"Available with: "),Xn=o(a1,"A",{href:!0,rel:!0});var MG=l(Xn);d9=u(MG,"\u{1F917} Transformers"),MG.forEach(t),g9=u(a1,`,
`),Qn=o(a1,"A",{href:!0,rel:!0});var KG=l(Qn);m9=u(KG,"Flair"),KG.forEach(t),a1.forEach(t),jv=h(a),rf=o(a,"P",{});var FG=l(rf);$9=u(FG,"Example:"),FG.forEach(t),kv=h(a),v(_s.$$.fragment,a),Av=h(a),nf=o(a,"P",{});var JG=l(nf);q9=u(JG,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),JG.forEach(t),Dv=h(a),vs=o(a,"TABLE",{});var kw=l(vs);Om=o(kw,"THEAD",{});var WG=l(Om);Zn=o(WG,"TR",{});var Aw=l(Zn);of=o(Aw,"TH",{align:!0});var YG=l(of);_9=u(YG,"All parameters"),YG.forEach(t),v9=h(Aw),Pm=o(Aw,"TH",{align:!0}),l(Pm).forEach(t),Aw.forEach(t),WG.forEach(t),E9=h(kw),F=o(kw,"TBODY",{});var X=l(F);eo=o(X,"TR",{});var Dw=l(eo);to=o(Dw,"TD",{align:!0});var qI=l(to);Rm=o(qI,"STRONG",{});var VG=l(Rm);y9=u(VG,"inputs"),VG.forEach(t),w9=u(qI," (required)"),qI.forEach(t),b9=h(Dw),lf=o(Dw,"TD",{align:!0});var XG=l(lf);T9=u(XG,"a string to be classified"),XG.forEach(t),Dw.forEach(t),j9=h(X),so=o(X,"TR",{});var Ow=l(so);uf=o(Ow,"TD",{align:!0});var QG=l(uf);Sm=o(QG,"STRONG",{});var ZG=l(Sm);k9=u(ZG,"parameters"),ZG.forEach(t),QG.forEach(t),A9=h(Ow),cf=o(Ow,"TD",{align:!0});var eL=l(cf);D9=u(eL,"a dict containing the following key:"),eL.forEach(t),Ow.forEach(t),O9=h(X),ao=o(X,"TR",{});var Pw=l(ao);ff=o(Pw,"TD",{align:!0});var tL=l(ff);P9=u(tL,"aggregation_strategy"),tL.forEach(t),R9=h(Pw),x=o(Pw,"TD",{align:!0});var C=l(x);S9=u(C,"(Default: "),Nm=o(C,"CODE",{});var sL=l(Nm);N9=u(sL,"simple"),sL.forEach(t),x9=u(C,"). There are several aggregation strategies: "),I9=o(C,"BR",{}),H9=h(C),xm=o(C,"CODE",{});var aL=l(xm);B9=u(aL,"none"),aL.forEach(t),C9=u(C,": Every token gets classified without further aggregation. "),G9=o(C,"BR",{}),L9=h(C),Im=o(C,"CODE",{});var rL=l(Im);U9=u(rL,"simple"),rL.forEach(t),z9=u(C,": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),M9=o(C,"BR",{}),K9=h(C),Hm=o(C,"CODE",{});var nL=l(Hm);F9=u(nL,"first"),nL.forEach(t),J9=u(C,": Same as the "),Bm=o(C,"CODE",{});var oL=l(Bm);W9=u(oL,"simple"),oL.forEach(t),Y9=u(C," strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),V9=o(C,"BR",{}),X9=h(C),Cm=o(C,"CODE",{});var lL=l(Cm);Q9=u(lL,"average"),lL.forEach(t),Z9=u(C,": Same as the "),Gm=o(C,"CODE",{});var iL=l(Gm);e8=u(iL,"simple"),iL.forEach(t),t8=u(C," strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),s8=o(C,"BR",{}),a8=h(C),Lm=o(C,"CODE",{});var uL=l(Lm);r8=u(uL,"max"),uL.forEach(t),n8=u(C,": Same as the "),Um=o(C,"CODE",{});var cL=l(Um);o8=u(cL,"simple"),cL.forEach(t),l8=u(C," strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),C.forEach(t),Pw.forEach(t),i8=h(X),ro=o(X,"TR",{});var Rw=l(ro);pf=o(Rw,"TD",{align:!0});var fL=l(pf);zm=o(fL,"STRONG",{});var pL=l(zm);u8=u(pL,"options"),pL.forEach(t),fL.forEach(t),c8=h(Rw),hf=o(Rw,"TD",{align:!0});var hL=l(hf);f8=u(hL,"a dict containing the following keys:"),hL.forEach(t),Rw.forEach(t),p8=h(X),no=o(X,"TR",{});var Sw=l(no);df=o(Sw,"TD",{align:!0});var dL=l(df);h8=u(dL,"use_gpu"),dL.forEach(t),d8=h(Sw),Es=o(Sw,"TD",{align:!0});var Nw=l(Es);g8=u(Nw,"(Default: "),Mm=o(Nw,"CODE",{});var gL=l(Mm);m8=u(gL,"false"),gL.forEach(t),$8=u(Nw,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Nw.forEach(t),Sw.forEach(t),q8=h(X),oo=o(X,"TR",{});var xw=l(oo);gf=o(xw,"TD",{align:!0});var mL=l(gf);_8=u(mL,"use_cache"),mL.forEach(t),v8=h(xw),ys=o(xw,"TD",{align:!0});var Iw=l(ys);E8=u(Iw,"(Default: "),Km=o(Iw,"CODE",{});var $L=l(Km);y8=u($L,"true"),$L.forEach(t),w8=u(Iw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Iw.forEach(t),xw.forEach(t),b8=h(X),lo=o(X,"TR",{});var Hw=l(lo);mf=o(Hw,"TD",{align:!0});var qL=l(mf);T8=u(qL,"wait_for_model"),qL.forEach(t),j8=h(Hw),ws=o(Hw,"TD",{align:!0});var Bw=l(ws);k8=u(Bw,"(Default: "),Fm=o(Bw,"CODE",{});var _L=l(Fm);A8=u(_L,"false"),_L.forEach(t),D8=u(Bw,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Bw.forEach(t),Hw.forEach(t),X.forEach(t),kw.forEach(t),Ov=h(a),$f=o(a,"P",{});var vL=l($f);O8=u(vL,"Return value is either a dict or a list of dicts if you sent a list of inputs"),vL.forEach(t),Pv=h(a),v(bs.$$.fragment,a),Rv=h(a),Ts=o(a,"TABLE",{});var Cw=l(Ts);Jm=o(Cw,"THEAD",{});var EL=l(Jm);io=o(EL,"TR",{});var Gw=l(io);qf=o(Gw,"TH",{align:!0});var yL=l(qf);P8=u(yL,"Returned values"),yL.forEach(t),R8=h(Gw),Wm=o(Gw,"TH",{align:!0}),l(Wm).forEach(t),Gw.forEach(t),EL.forEach(t),S8=h(Cw),te=o(Cw,"TBODY",{});var De=l(te);uo=o(De,"TR",{});var Lw=l(uo);_f=o(Lw,"TD",{align:!0});var wL=l(_f);Ym=o(wL,"STRONG",{});var bL=l(Ym);N8=u(bL,"entity_group"),bL.forEach(t),wL.forEach(t),x8=h(Lw),vf=o(Lw,"TD",{align:!0});var TL=l(vf);I8=u(TL,"The type for the entity being recognized (model specific)."),TL.forEach(t),Lw.forEach(t),H8=h(De),co=o(De,"TR",{});var Uw=l(co);Ef=o(Uw,"TD",{align:!0});var jL=l(Ef);Vm=o(jL,"STRONG",{});var kL=l(Vm);B8=u(kL,"score"),kL.forEach(t),jL.forEach(t),C8=h(Uw),yf=o(Uw,"TD",{align:!0});var AL=l(yf);G8=u(AL,"How likely the entity was recognized."),AL.forEach(t),Uw.forEach(t),L8=h(De),fo=o(De,"TR",{});var zw=l(fo);wf=o(zw,"TD",{align:!0});var DL=l(wf);Xm=o(DL,"STRONG",{});var OL=l(Xm);U8=u(OL,"word"),OL.forEach(t),DL.forEach(t),z8=h(zw),bf=o(zw,"TD",{align:!0});var PL=l(bf);M8=u(PL,"The string that was captured"),PL.forEach(t),zw.forEach(t),K8=h(De),po=o(De,"TR",{});var Mw=l(po);Tf=o(Mw,"TD",{align:!0});var RL=l(Tf);Qm=o(RL,"STRONG",{});var SL=l(Qm);F8=u(SL,"start"),SL.forEach(t),RL.forEach(t),J8=h(Mw),js=o(Mw,"TD",{align:!0});var Kw=l(js);W8=u(Kw,"The offset stringwise where the answer is located. Useful to disambiguate if "),Zm=o(Kw,"CODE",{});var NL=l(Zm);Y8=u(NL,"word"),NL.forEach(t),V8=u(Kw," occurs multiple times."),Kw.forEach(t),Mw.forEach(t),X8=h(De),ho=o(De,"TR",{});var Fw=l(ho);jf=o(Fw,"TD",{align:!0});var xL=l(jf);e$=o(xL,"STRONG",{});var IL=l(e$);Q8=u(IL,"end"),IL.forEach(t),xL.forEach(t),Z8=h(Fw),ks=o(Fw,"TD",{align:!0});var Jw=l(ks);eA=u(Jw,"The offset stringwise where the answer is located. Useful to disambiguate if "),t$=o(Jw,"CODE",{});var HL=l(t$);tA=u(HL,"word"),HL.forEach(t),sA=u(Jw," occurs multiple times."),Jw.forEach(t),Fw.forEach(t),De.forEach(t),Cw.forEach(t),Sv=h(a),Ke=o(a,"H3",{class:!0});var Ww=l(Ke);As=o(Ww,"A",{id:!0,class:!0,href:!0});var BL=l(As);s$=o(BL,"SPAN",{});var CL=l(s$);v(go.$$.fragment,CL),CL.forEach(t),BL.forEach(t),aA=h(Ww),a$=o(Ww,"SPAN",{});var GL=l(a$);rA=u(GL,"Named Entity Recognition (NER) task"),GL.forEach(t),Ww.forEach(t),Nv=h(a),mo=o(a,"P",{});var _I=l(mo);nA=u(_I,"See "),kf=o(_I,"A",{href:!0});var LL=l(kf);oA=u(LL,"Token-classification task"),LL.forEach(t),_I.forEach(t),xv=h(a),Fe=o(a,"H3",{class:!0});var Yw=l(Fe);Ds=o(Yw,"A",{id:!0,class:!0,href:!0});var UL=l(Ds);r$=o(UL,"SPAN",{});var zL=l(r$);v($o.$$.fragment,zL),zL.forEach(t),UL.forEach(t),lA=h(Yw),n$=o(Yw,"SPAN",{});var ML=l(n$);iA=u(ML,"Translation task"),ML.forEach(t),Yw.forEach(t),Iv=h(a),Af=o(a,"P",{});var KL=l(Af);uA=u(KL,"This task is well known to translate text from one language to another"),KL.forEach(t),Hv=h(a),v(Os.$$.fragment,a),Bv=h(a),qo=o(a,"P",{});var vI=l(qo);cA=u(vI,"Available with: "),_o=o(vI,"A",{href:!0,rel:!0});var FL=l(_o);fA=u(FL,"\u{1F917} Transformers"),FL.forEach(t),vI.forEach(t),Cv=h(a),Df=o(a,"P",{});var JL=l(Df);pA=u(JL,"Example:"),JL.forEach(t),Gv=h(a),v(Ps.$$.fragment,a),Lv=h(a),Of=o(a,"P",{});var WL=l(Of);hA=u(WL,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),WL.forEach(t),Uv=h(a),Rs=o(a,"TABLE",{});var Vw=l(Rs);o$=o(Vw,"THEAD",{});var YL=l(o$);vo=o(YL,"TR",{});var Xw=l(vo);Pf=o(Xw,"TH",{align:!0});var VL=l(Pf);dA=u(VL,"All parameters"),VL.forEach(t),gA=h(Xw),l$=o(Xw,"TH",{align:!0}),l(l$).forEach(t),Xw.forEach(t),YL.forEach(t),mA=h(Vw),se=o(Vw,"TBODY",{});var Oe=l(se);Eo=o(Oe,"TR",{});var Qw=l(Eo);yo=o(Qw,"TD",{align:!0});var EI=l(yo);i$=o(EI,"STRONG",{});var XL=l(i$);$A=u(XL,"inputs"),XL.forEach(t),qA=u(EI," (required)"),EI.forEach(t),_A=h(Qw),Rf=o(Qw,"TD",{align:!0});var QL=l(Rf);vA=u(QL,"a string to be translated in the original languages"),QL.forEach(t),Qw.forEach(t),EA=h(Oe),wo=o(Oe,"TR",{});var Zw=l(wo);Sf=o(Zw,"TD",{align:!0});var ZL=l(Sf);u$=o(ZL,"STRONG",{});var eU=l(u$);yA=u(eU,"options"),eU.forEach(t),ZL.forEach(t),wA=h(Zw),Nf=o(Zw,"TD",{align:!0});var tU=l(Nf);bA=u(tU,"a dict containing the following keys:"),tU.forEach(t),Zw.forEach(t),TA=h(Oe),bo=o(Oe,"TR",{});var e0=l(bo);xf=o(e0,"TD",{align:!0});var sU=l(xf);jA=u(sU,"use_gpu"),sU.forEach(t),kA=h(e0),Ss=o(e0,"TD",{align:!0});var t0=l(Ss);AA=u(t0,"(Default: "),c$=o(t0,"CODE",{});var aU=l(c$);DA=u(aU,"false"),aU.forEach(t),OA=u(t0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),t0.forEach(t),e0.forEach(t),PA=h(Oe),To=o(Oe,"TR",{});var s0=l(To);If=o(s0,"TD",{align:!0});var rU=l(If);RA=u(rU,"use_cache"),rU.forEach(t),SA=h(s0),Ns=o(s0,"TD",{align:!0});var a0=l(Ns);NA=u(a0,"(Default: "),f$=o(a0,"CODE",{});var nU=l(f$);xA=u(nU,"true"),nU.forEach(t),IA=u(a0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),a0.forEach(t),s0.forEach(t),HA=h(Oe),jo=o(Oe,"TR",{});var r0=l(jo);Hf=o(r0,"TD",{align:!0});var oU=l(Hf);BA=u(oU,"wait_for_model"),oU.forEach(t),CA=h(r0),xs=o(r0,"TD",{align:!0});var n0=l(xs);GA=u(n0,"(Default: "),p$=o(n0,"CODE",{});var lU=l(p$);LA=u(lU,"false"),lU.forEach(t),UA=u(n0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),n0.forEach(t),r0.forEach(t),Oe.forEach(t),Vw.forEach(t),zv=h(a),Bf=o(a,"P",{});var iU=l(Bf);zA=u(iU,"Return value is either a dict or a list of dicts if you sent a list of inputs"),iU.forEach(t),Mv=h(a),Is=o(a,"TABLE",{});var o0=l(Is);h$=o(o0,"THEAD",{});var uU=l(h$);ko=o(uU,"TR",{});var l0=l(ko);Cf=o(l0,"TH",{align:!0});var cU=l(Cf);MA=u(cU,"Returned values"),cU.forEach(t),KA=h(l0),d$=o(l0,"TH",{align:!0}),l(d$).forEach(t),l0.forEach(t),uU.forEach(t),FA=h(o0),g$=o(o0,"TBODY",{});var fU=l(g$);Ao=o(fU,"TR",{});var i0=l(Ao);Gf=o(i0,"TD",{align:!0});var pU=l(Gf);m$=o(pU,"STRONG",{});var hU=l(m$);JA=u(hU,"translation_text"),hU.forEach(t),pU.forEach(t),WA=h(i0),Lf=o(i0,"TD",{align:!0});var dU=l(Lf);YA=u(dU,"The string after translation"),dU.forEach(t),i0.forEach(t),fU.forEach(t),o0.forEach(t),Kv=h(a),Je=o(a,"H3",{class:!0});var u0=l(Je);Hs=o(u0,"A",{id:!0,class:!0,href:!0});var gU=l(Hs);$$=o(gU,"SPAN",{});var mU=l($$);v(Do.$$.fragment,mU),mU.forEach(t),gU.forEach(t),VA=h(u0),q$=o(u0,"SPAN",{});var $U=l(q$);XA=u($U,"Zero-Shot Classification task"),$U.forEach(t),u0.forEach(t),Fv=h(a),Uf=o(a,"P",{});var qU=l(Uf);QA=u(qU,`This task is super useful to try out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence, and you get a result.`),qU.forEach(t),Jv=h(a),v(Bs.$$.fragment,a),Wv=h(a),Oo=o(a,"P",{});var yI=l(Oo);ZA=u(yI,"Available with: "),Po=o(yI,"A",{href:!0,rel:!0});var _U=l(Po);eD=u(_U,"\u{1F917} Transformers"),_U.forEach(t),yI.forEach(t),Yv=h(a),zf=o(a,"P",{});var vU=l(zf);tD=u(vU,"Request:"),vU.forEach(t),Vv=h(a),v(Cs.$$.fragment,a),Xv=h(a),Mf=o(a,"P",{});var EU=l(Mf);sD=u(EU,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),EU.forEach(t),Qv=h(a),Gs=o(a,"TABLE",{});var c0=l(Gs);_$=o(c0,"THEAD",{});var yU=l(_$);Ro=o(yU,"TR",{});var f0=l(Ro);Kf=o(f0,"TH",{align:!0});var wU=l(Kf);aD=u(wU,"All parameters"),wU.forEach(t),rD=h(f0),v$=o(f0,"TH",{align:!0}),l(v$).forEach(t),f0.forEach(t),yU.forEach(t),nD=h(c0),M=o(c0,"TBODY",{});var W=l(M);So=o(W,"TR",{});var p0=l(So);No=o(p0,"TD",{align:!0});var wI=l(No);E$=o(wI,"STRONG",{});var bU=l(E$);oD=u(bU,"inputs"),bU.forEach(t),lD=u(wI," (required)"),wI.forEach(t),iD=h(p0),Ff=o(p0,"TD",{align:!0});var TU=l(Ff);uD=u(TU,"a string or list of strings"),TU.forEach(t),p0.forEach(t),cD=h(W),xo=o(W,"TR",{});var h0=l(xo);Io=o(h0,"TD",{align:!0});var bI=l(Io);y$=o(bI,"STRONG",{});var jU=l(y$);fD=u(jU,"parameters"),jU.forEach(t),pD=u(bI," (required)"),bI.forEach(t),hD=h(h0),Jf=o(h0,"TD",{align:!0});var kU=l(Jf);dD=u(kU,"a dict containing the following keys:"),kU.forEach(t),h0.forEach(t),gD=h(W),Ho=o(W,"TR",{});var d0=l(Ho);Wf=o(d0,"TD",{align:!0});var AU=l(Wf);mD=u(AU,"candidate_labels (required)"),AU.forEach(t),$D=h(d0),be=o(d0,"TD",{align:!0});var pd=l(be);qD=u(pd,"a list of strings that are potential classes for "),w$=o(pd,"CODE",{});var DU=l(w$);_D=u(DU,"inputs"),DU.forEach(t),vD=u(pd,". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),b$=o(pd,"CODE",{});var OU=l(b$);ED=u(OU,"multi_label=True"),OU.forEach(t),yD=u(pd," and do the scaling on your end. )"),pd.forEach(t),d0.forEach(t),wD=h(W),Bo=o(W,"TR",{});var g0=l(Bo);Yf=o(g0,"TD",{align:!0});var PU=l(Yf);bD=u(PU,"multi_label"),PU.forEach(t),TD=h(g0),Ls=o(g0,"TD",{align:!0});var m0=l(Ls);jD=u(m0,"(Default: "),T$=o(m0,"CODE",{});var RU=l(T$);kD=u(RU,"false"),RU.forEach(t),AD=u(m0,") Boolean that is set to True if classes can overlap"),m0.forEach(t),g0.forEach(t),DD=h(W),Co=o(W,"TR",{});var $0=l(Co);Vf=o($0,"TD",{align:!0});var SU=l(Vf);j$=o(SU,"STRONG",{});var NU=l(j$);OD=u(NU,"options"),NU.forEach(t),SU.forEach(t),PD=h($0),Xf=o($0,"TD",{align:!0});var xU=l(Xf);RD=u(xU,"a dict containing the following keys:"),xU.forEach(t),$0.forEach(t),SD=h(W),Go=o(W,"TR",{});var q0=l(Go);Qf=o(q0,"TD",{align:!0});var IU=l(Qf);ND=u(IU,"use_gpu"),IU.forEach(t),xD=h(q0),Us=o(q0,"TD",{align:!0});var _0=l(Us);ID=u(_0,"(Default: "),k$=o(_0,"CODE",{});var HU=l(k$);HD=u(HU,"false"),HU.forEach(t),BD=u(_0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),_0.forEach(t),q0.forEach(t),CD=h(W),Lo=o(W,"TR",{});var v0=l(Lo);Zf=o(v0,"TD",{align:!0});var BU=l(Zf);GD=u(BU,"use_cache"),BU.forEach(t),LD=h(v0),zs=o(v0,"TD",{align:!0});var E0=l(zs);UD=u(E0,"(Default: "),A$=o(E0,"CODE",{});var CU=l(A$);zD=u(CU,"true"),CU.forEach(t),MD=u(E0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),E0.forEach(t),v0.forEach(t),KD=h(W),Uo=o(W,"TR",{});var y0=l(Uo);ep=o(y0,"TD",{align:!0});var GU=l(ep);FD=u(GU,"wait_for_model"),GU.forEach(t),JD=h(y0),Ms=o(y0,"TD",{align:!0});var w0=l(Ms);WD=u(w0,"(Default: "),D$=o(w0,"CODE",{});var LU=l(D$);YD=u(LU,"false"),LU.forEach(t),VD=u(w0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),w0.forEach(t),y0.forEach(t),W.forEach(t),c0.forEach(t),Zv=h(a),tp=o(a,"P",{});var UU=l(tp);XD=u(UU,"Return value is either a dict or a list of dicts if you sent a list of inputs"),UU.forEach(t),e2=h(a),sp=o(a,"P",{});var zU=l(sp);QD=u(zU,"Response:"),zU.forEach(t),t2=h(a),v(Ks.$$.fragment,a),s2=h(a),Fs=o(a,"TABLE",{});var b0=l(Fs);O$=o(b0,"THEAD",{});var MU=l(O$);zo=o(MU,"TR",{});var T0=l(zo);ap=o(T0,"TH",{align:!0});var KU=l(ap);ZD=u(KU,"Returned values"),KU.forEach(t),eO=h(T0),P$=o(T0,"TH",{align:!0}),l(P$).forEach(t),T0.forEach(t),MU.forEach(t),tO=h(b0),We=o(b0,"TBODY",{});var hd=l(We);Mo=o(hd,"TR",{});var j0=l(Mo);rp=o(j0,"TD",{align:!0});var FU=l(rp);R$=o(FU,"STRONG",{});var JU=l(R$);sO=u(JU,"sequence"),JU.forEach(t),FU.forEach(t),aO=h(j0),np=o(j0,"TD",{align:!0});var WU=l(np);rO=u(WU,"The string sent as an input"),WU.forEach(t),j0.forEach(t),nO=h(hd),Ko=o(hd,"TR",{});var k0=l(Ko);op=o(k0,"TD",{align:!0});var YU=l(op);S$=o(YU,"STRONG",{});var VU=l(S$);oO=u(VU,"labels"),VU.forEach(t),YU.forEach(t),lO=h(k0),lp=o(k0,"TD",{align:!0});var XU=l(lp);iO=u(XU,"The list of strings for labels that you sent (in order)"),XU.forEach(t),k0.forEach(t),uO=h(hd),Fo=o(hd,"TR",{});var A0=l(Fo);ip=o(A0,"TD",{align:!0});var QU=l(ip);N$=o(QU,"STRONG",{});var ZU=l(N$);cO=u(ZU,"scores"),ZU.forEach(t),QU.forEach(t),fO=h(A0),Js=o(A0,"TD",{align:!0});var D0=l(Js);pO=u(D0,"a list of floats that correspond the the probability of label, in the same order as "),x$=o(D0,"CODE",{});var ez=l(x$);hO=u(ez,"labels"),ez.forEach(t),dO=u(D0,"."),D0.forEach(t),A0.forEach(t),hd.forEach(t),b0.forEach(t),a2=h(a),Ye=o(a,"H3",{class:!0});var O0=l(Ye);Ws=o(O0,"A",{id:!0,class:!0,href:!0});var tz=l(Ws);I$=o(tz,"SPAN",{});var sz=l(I$);v(Jo.$$.fragment,sz),sz.forEach(t),tz.forEach(t),gO=h(O0),H$=o(O0,"SPAN",{});var az=l(H$);mO=u(az,"Conversational task"),az.forEach(t),O0.forEach(t),r2=h(a),up=o(a,"P",{});var rz=l(up);$O=u(rz,`This task corresponds to any chatbot like structure. Models tend to have
shorter max_length, so please check with caution when using a given
model if you need long range dependency or not.`),rz.forEach(t),n2=h(a),v(Ys.$$.fragment,a),o2=h(a),Wo=o(a,"P",{});var TI=l(Wo);qO=u(TI,"Available with: "),Yo=o(TI,"A",{href:!0,rel:!0});var nz=l(Yo);_O=u(nz,"\u{1F917} Transformers"),nz.forEach(t),TI.forEach(t),l2=h(a),cp=o(a,"P",{});var oz=l(cp);vO=u(oz,"Example:"),oz.forEach(t),i2=h(a),v(Vs.$$.fragment,a),u2=h(a),fp=o(a,"P",{});var lz=l(fp);EO=u(lz,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),lz.forEach(t),c2=h(a),Xs=o(a,"TABLE",{});var P0=l(Xs);B$=o(P0,"THEAD",{});var iz=l(B$);Vo=o(iz,"TR",{});var R0=l(Vo);pp=o(R0,"TH",{align:!0});var uz=l(pp);yO=u(uz,"All parameters"),uz.forEach(t),wO=h(R0),C$=o(R0,"TH",{align:!0}),l(C$).forEach(t),R0.forEach(t),iz.forEach(t),bO=h(P0),N=o(P0,"TBODY",{});var H=l(N);Xo=o(H,"TR",{});var S0=l(Xo);Qo=o(S0,"TD",{align:!0});var jI=l(Qo);G$=o(jI,"STRONG",{});var cz=l(G$);TO=u(cz,"inputs"),cz.forEach(t),jO=u(jI," (required)"),jI.forEach(t),kO=h(S0),L$=o(S0,"TD",{align:!0}),l(L$).forEach(t),S0.forEach(t),AO=h(H),Zo=o(H,"TR",{});var N0=l(Zo);hp=o(N0,"TD",{align:!0});var fz=l(hp);DO=u(fz,"text (required)"),fz.forEach(t),OO=h(N0),dp=o(N0,"TD",{align:!0});var pz=l(dp);PO=u(pz,"The last input from the user in the conversation."),pz.forEach(t),N0.forEach(t),RO=h(H),el=o(H,"TR",{});var x0=l(el);gp=o(x0,"TD",{align:!0});var hz=l(gp);SO=u(hz,"generated_responses"),hz.forEach(t),NO=h(x0),mp=o(x0,"TD",{align:!0});var dz=l(mp);xO=u(dz,"A list of strings corresponding to the earlier replies from the model."),dz.forEach(t),x0.forEach(t),IO=h(H),tl=o(H,"TR",{});var I0=l(tl);$p=o(I0,"TD",{align:!0});var gz=l($p);HO=u(gz,"past_user_inputs"),gz.forEach(t),BO=h(I0),Qs=o(I0,"TD",{align:!0});var H0=l(Qs);CO=u(H0,"A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),U$=o(H0,"CODE",{});var mz=l(U$);GO=u(mz,"generated_responses"),mz.forEach(t),LO=u(H0,"."),H0.forEach(t),I0.forEach(t),UO=h(H),sl=o(H,"TR",{});var B0=l(sl);qp=o(B0,"TD",{align:!0});var $z=l(qp);z$=o($z,"STRONG",{});var qz=l(z$);zO=u(qz,"parameters"),qz.forEach(t),$z.forEach(t),MO=h(B0),_p=o(B0,"TD",{align:!0});var _z=l(_p);KO=u(_z,"a dict containing the following keys:"),_z.forEach(t),B0.forEach(t),FO=h(H),al=o(H,"TR",{});var C0=l(al);vp=o(C0,"TD",{align:!0});var vz=l(vp);JO=u(vz,"min_length"),vz.forEach(t),WO=h(C0),Te=o(C0,"TD",{align:!0});var dd=l(Te);YO=u(dd,"(Default: "),M$=o(dd,"CODE",{});var Ez=l(M$);VO=u(Ez,"None"),Ez.forEach(t),XO=u(dd,"). Integer to define the minimum length "),K$=o(dd,"STRONG",{});var yz=l(K$);QO=u(yz,"in tokens"),yz.forEach(t),ZO=u(dd," of the output summary."),dd.forEach(t),C0.forEach(t),eP=h(H),rl=o(H,"TR",{});var G0=l(rl);Ep=o(G0,"TD",{align:!0});var wz=l(Ep);tP=u(wz,"max_length"),wz.forEach(t),sP=h(G0),je=o(G0,"TD",{align:!0});var gd=l(je);aP=u(gd,"(Default: "),F$=o(gd,"CODE",{});var bz=l(F$);rP=u(bz,"None"),bz.forEach(t),nP=u(gd,"). Integer to define the maximum length "),J$=o(gd,"STRONG",{});var Tz=l(J$);oP=u(Tz,"in tokens"),Tz.forEach(t),lP=u(gd," of the output summary."),gd.forEach(t),G0.forEach(t),iP=h(H),nl=o(H,"TR",{});var L0=l(nl);yp=o(L0,"TD",{align:!0});var jz=l(yp);uP=u(jz,"top_k"),jz.forEach(t),cP=h(L0),ke=o(L0,"TD",{align:!0});var md=l(ke);fP=u(md,"(Default: "),W$=o(md,"CODE",{});var kz=l(W$);pP=u(kz,"None"),kz.forEach(t),hP=u(md,"). Integer to define the top tokens considered within the "),Y$=o(md,"CODE",{});var Az=l(Y$);dP=u(Az,"sample"),Az.forEach(t),gP=u(md," operation to create new text."),md.forEach(t),L0.forEach(t),mP=h(H),ol=o(H,"TR",{});var U0=l(ol);wp=o(U0,"TD",{align:!0});var Dz=l(wp);$P=u(Dz,"top_p"),Dz.forEach(t),qP=h(U0),ce=o(U0,"TD",{align:!0});var tr=l(ce);_P=u(tr,"(Default: "),V$=o(tr,"CODE",{});var Oz=l(V$);vP=u(Oz,"None"),Oz.forEach(t),EP=u(tr,"). Float to define the tokens that are within the "),X$=o(tr,"CODE",{});var Pz=l(X$);yP=u(Pz,"sample"),Pz.forEach(t),wP=u(tr," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Q$=o(tr,"CODE",{});var Rz=l(Q$);bP=u(Rz,"top_p"),Rz.forEach(t),TP=u(tr,"."),tr.forEach(t),U0.forEach(t),jP=h(H),ll=o(H,"TR",{});var z0=l(ll);bp=o(z0,"TD",{align:!0});var Sz=l(bp);kP=u(Sz,"temperature"),Sz.forEach(t),AP=h(z0),fe=o(z0,"TD",{align:!0});var sr=l(fe);DP=u(sr,"(Default: "),Z$=o(sr,"CODE",{});var Nz=l(Z$);OP=u(Nz,"1.0"),Nz.forEach(t),PP=u(sr,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),eq=o(sr,"CODE",{});var xz=l(eq);RP=u(xz,"0"),xz.forEach(t),SP=u(sr," means always take the highest score, "),tq=o(sr,"CODE",{});var Iz=l(tq);NP=u(Iz,"100.0"),Iz.forEach(t),xP=u(sr," is getting closer to uniform probability."),sr.forEach(t),z0.forEach(t),IP=h(H),il=o(H,"TR",{});var M0=l(il);Tp=o(M0,"TD",{align:!0});var Hz=l(Tp);HP=u(Hz,"repetition_penalty"),Hz.forEach(t),BP=h(M0),Zs=o(M0,"TD",{align:!0});var K0=l(Zs);CP=u(K0,"(Default: "),sq=o(K0,"CODE",{});var Bz=l(sq);GP=u(Bz,"None"),Bz.forEach(t),LP=u(K0,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),K0.forEach(t),M0.forEach(t),UP=h(H),ul=o(H,"TR",{});var F0=l(ul);jp=o(F0,"TD",{align:!0});var Cz=l(jp);zP=u(Cz,"max_time"),Cz.forEach(t),MP=h(F0),ea=o(F0,"TD",{align:!0});var J0=l(ea);KP=u(J0,"(Default: "),aq=o(J0,"CODE",{});var Gz=l(aq);FP=u(Gz,"None"),Gz.forEach(t),JP=u(J0,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),J0.forEach(t),F0.forEach(t),WP=h(H),cl=o(H,"TR",{});var W0=l(cl);kp=o(W0,"TD",{align:!0});var Lz=l(kp);rq=o(Lz,"STRONG",{});var Uz=l(rq);YP=u(Uz,"options"),Uz.forEach(t),Lz.forEach(t),VP=h(W0),Ap=o(W0,"TD",{align:!0});var zz=l(Ap);XP=u(zz,"a dict containing the following keys:"),zz.forEach(t),W0.forEach(t),QP=h(H),fl=o(H,"TR",{});var Y0=l(fl);Dp=o(Y0,"TD",{align:!0});var Mz=l(Dp);ZP=u(Mz,"use_gpu"),Mz.forEach(t),eR=h(Y0),ta=o(Y0,"TD",{align:!0});var V0=l(ta);tR=u(V0,"(Default: "),nq=o(V0,"CODE",{});var Kz=l(nq);sR=u(Kz,"false"),Kz.forEach(t),aR=u(V0,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),V0.forEach(t),Y0.forEach(t),rR=h(H),pl=o(H,"TR",{});var X0=l(pl);Op=o(X0,"TD",{align:!0});var Fz=l(Op);nR=u(Fz,"use_cache"),Fz.forEach(t),oR=h(X0),sa=o(X0,"TD",{align:!0});var Q0=l(sa);lR=u(Q0,"(Default: "),oq=o(Q0,"CODE",{});var Jz=l(oq);iR=u(Jz,"true"),Jz.forEach(t),uR=u(Q0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Q0.forEach(t),X0.forEach(t),cR=h(H),hl=o(H,"TR",{});var Z0=l(hl);Pp=o(Z0,"TD",{align:!0});var Wz=l(Pp);fR=u(Wz,"wait_for_model"),Wz.forEach(t),pR=h(Z0),aa=o(Z0,"TD",{align:!0});var eb=l(aa);hR=u(eb,"(Default: "),lq=o(eb,"CODE",{});var Yz=l(lq);dR=u(Yz,"false"),Yz.forEach(t),gR=u(eb,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),eb.forEach(t),Z0.forEach(t),H.forEach(t),P0.forEach(t),f2=h(a),Rp=o(a,"P",{});var Vz=l(Rp);mR=u(Vz,"Return value is either a dict or a list of dicts if you sent a list of inputs"),Vz.forEach(t),p2=h(a),ra=o(a,"TABLE",{});var tb=l(ra);iq=o(tb,"THEAD",{});var Xz=l(iq);dl=o(Xz,"TR",{});var sb=l(dl);Sp=o(sb,"TH",{align:!0});var Qz=l(Sp);$R=u(Qz,"Returned values"),Qz.forEach(t),qR=h(sb),uq=o(sb,"TH",{align:!0}),l(uq).forEach(t),sb.forEach(t),Xz.forEach(t),_R=h(tb),ge=o(tb,"TBODY",{});var ar=l(ge);gl=o(ar,"TR",{});var ab=l(gl);Np=o(ab,"TD",{align:!0});var Zz=l(Np);cq=o(Zz,"STRONG",{});var eM=l(cq);vR=u(eM,"generated_text"),eM.forEach(t),Zz.forEach(t),ER=h(ab),xp=o(ab,"TD",{align:!0});var tM=l(xp);yR=u(tM,"The answer of the bot"),tM.forEach(t),ab.forEach(t),wR=h(ar),ml=o(ar,"TR",{});var rb=l(ml);Ip=o(rb,"TD",{align:!0});var sM=l(Ip);fq=o(sM,"STRONG",{});var aM=l(fq);bR=u(aM,"conversation"),aM.forEach(t),sM.forEach(t),TR=h(rb),Hp=o(rb,"TD",{align:!0});var rM=l(Hp);jR=u(rM,"A facility dictionnary to send back for the next input (with the new user input addition)."),rM.forEach(t),rb.forEach(t),kR=h(ar),$l=o(ar,"TR",{});var nb=l($l);Bp=o(nb,"TD",{align:!0});var nM=l(Bp);AR=u(nM,"past_user_inputs"),nM.forEach(t),DR=h(nb),Cp=o(nb,"TD",{align:!0});var oM=l(Cp);OR=u(oM,"List of strings. The last inputs from the user in the conversation, <em>after the model has run."),oM.forEach(t),nb.forEach(t),PR=h(ar),ql=o(ar,"TR",{});var ob=l(ql);Gp=o(ob,"TD",{align:!0});var lM=l(Gp);RR=u(lM,"generated_responses"),lM.forEach(t),SR=h(ob),Lp=o(ob,"TD",{align:!0});var iM=l(Lp);NR=u(iM,"List of strings. The last outputs from the model in the conversation, <em>after the model has run."),iM.forEach(t),ob.forEach(t),ar.forEach(t),tb.forEach(t),h2=h(a),Ve=o(a,"H3",{class:!0});var lb=l(Ve);na=o(lb,"A",{id:!0,class:!0,href:!0});var uM=l(na);pq=o(uM,"SPAN",{});var cM=l(pq);v(_l.$$.fragment,cM),cM.forEach(t),uM.forEach(t),xR=h(lb),hq=o(lb,"SPAN",{});var fM=l(hq);IR=u(fM,"Feature Extraction task"),fM.forEach(t),lb.forEach(t),d2=h(a),Up=o(a,"P",{});var pM=l(Up);HR=u(pM,`This task reads some text and outputs raw float values, that are usually
consumed as part of a semantic database/semantic search.`),pM.forEach(t),g2=h(a),v(oa.$$.fragment,a),m2=h(a),Xe=o(a,"P",{});var r1=l(Xe);BR=u(r1,"Available with: "),vl=o(r1,"A",{href:!0,rel:!0});var hM=l(vl);CR=u(hM,"\u{1F917} Transformers"),hM.forEach(t),GR=h(r1),El=o(r1,"A",{href:!0,rel:!0});var dM=l(El);LR=u(dM,"Sentence-transformers"),dM.forEach(t),r1.forEach(t),$2=h(a),zp=o(a,"P",{});var gM=l(zp);UR=u(gM,"Request:"),gM.forEach(t),q2=h(a),la=o(a,"TABLE",{});var ib=l(la);dq=o(ib,"THEAD",{});var mM=l(dq);yl=o(mM,"TR",{});var ub=l(yl);Mp=o(ub,"TH",{align:!0});var $M=l(Mp);zR=u($M,"All parameters"),$M.forEach(t),MR=h(ub),gq=o(ub,"TH",{align:!0}),l(gq).forEach(t),ub.forEach(t),mM.forEach(t),KR=h(ib),ae=o(ib,"TBODY",{});var Pe=l(ae);wl=o(Pe,"TR",{});var cb=l(wl);bl=o(cb,"TD",{align:!0});var kI=l(bl);mq=o(kI,"STRONG",{});var qM=l(mq);FR=u(qM,"inputs"),qM.forEach(t),JR=u(kI," (required):"),kI.forEach(t),WR=h(cb),Kp=o(cb,"TD",{align:!0});var _M=l(Kp);YR=u(_M,"a string or a list of strings to get the features from."),_M.forEach(t),cb.forEach(t),VR=h(Pe),Tl=o(Pe,"TR",{});var fb=l(Tl);Fp=o(fb,"TD",{align:!0});var vM=l(Fp);$q=o(vM,"STRONG",{});var EM=l($q);XR=u(EM,"options"),EM.forEach(t),vM.forEach(t),QR=h(fb),Jp=o(fb,"TD",{align:!0});var yM=l(Jp);ZR=u(yM,"a dict containing the following keys:"),yM.forEach(t),fb.forEach(t),eS=h(Pe),jl=o(Pe,"TR",{});var pb=l(jl);Wp=o(pb,"TD",{align:!0});var wM=l(Wp);tS=u(wM,"use_gpu"),wM.forEach(t),sS=h(pb),ia=o(pb,"TD",{align:!0});var hb=l(ia);aS=u(hb,"(Default: "),qq=o(hb,"CODE",{});var bM=l(qq);rS=u(bM,"false"),bM.forEach(t),nS=u(hb,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),hb.forEach(t),pb.forEach(t),oS=h(Pe),kl=o(Pe,"TR",{});var db=l(kl);Yp=o(db,"TD",{align:!0});var TM=l(Yp);lS=u(TM,"use_cache"),TM.forEach(t),iS=h(db),ua=o(db,"TD",{align:!0});var gb=l(ua);uS=u(gb,"(Default: "),_q=o(gb,"CODE",{});var jM=l(_q);cS=u(jM,"true"),jM.forEach(t),fS=u(gb,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),gb.forEach(t),db.forEach(t),pS=h(Pe),Al=o(Pe,"TR",{});var mb=l(Al);Vp=o(mb,"TD",{align:!0});var kM=l(Vp);hS=u(kM,"wait_for_model"),kM.forEach(t),dS=h(mb),ca=o(mb,"TD",{align:!0});var $b=l(ca);gS=u($b,"(Default: "),vq=o($b,"CODE",{});var AM=l(vq);mS=u(AM,"false"),AM.forEach(t),$S=u($b,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),$b.forEach(t),mb.forEach(t),Pe.forEach(t),ib.forEach(t),_2=h(a),Xp=o(a,"P",{});var DM=l(Xp);qS=u(DM,"Return value is either a dict or a list of dicts if you sent a list of inputs"),DM.forEach(t),v2=h(a),fa=o(a,"TABLE",{});var qb=l(fa);Eq=o(qb,"THEAD",{});var OM=l(Eq);Dl=o(OM,"TR",{});var _b=l(Dl);Qp=o(_b,"TH",{align:!0});var PM=l(Qp);_S=u(PM,"Returned values"),PM.forEach(t),vS=h(_b),yq=o(_b,"TH",{align:!0}),l(yq).forEach(t),_b.forEach(t),OM.forEach(t),ES=h(qb),wq=o(qb,"TBODY",{});var RM=l(wq);Ol=o(RM,"TR",{});var vb=l(Ol);Zp=o(vb,"TD",{align:!0});var SM=l(Zp);bq=o(SM,"STRONG",{});var NM=l(bq);yS=u(NM,"A list of float (or list of list of floats)"),NM.forEach(t),SM.forEach(t),wS=h(vb),eh=o(vb,"TD",{align:!0});var xM=l(eh);bS=u(xM,"The numbers that are the representation features of the input."),xM.forEach(t),vb.forEach(t),RM.forEach(t),qb.forEach(t),E2=h(a),th=o(a,"SMALL",{});var IM=l(th);TS=u(IM,`Returned values are a list of floats, or a list of list of floats (depending
  on if you sent a string or a list of string, and if the automatic reduction,
  usually mean_pooling for instance was applied for you or not. This should be
  explained on the model's README.`),IM.forEach(t),y2=h(a),Qe=o(a,"H2",{class:!0});var Eb=l(Qe);pa=o(Eb,"A",{id:!0,class:!0,href:!0});var HM=l(pa);Tq=o(HM,"SPAN",{});var BM=l(Tq);v(Pl.$$.fragment,BM),BM.forEach(t),HM.forEach(t),jS=h(Eb),jq=o(Eb,"SPAN",{});var CM=l(jq);kS=u(CM,"Audio"),CM.forEach(t),Eb.forEach(t),w2=h(a),Ze=o(a,"H3",{class:!0});var yb=l(Ze);ha=o(yb,"A",{id:!0,class:!0,href:!0});var GM=l(ha);kq=o(GM,"SPAN",{});var LM=l(kq);v(Rl.$$.fragment,LM),LM.forEach(t),GM.forEach(t),AS=h(yb),Aq=o(yb,"SPAN",{});var UM=l(Aq);DS=u(UM,"Automatic Speech Recognition task"),UM.forEach(t),yb.forEach(t),b2=h(a),sh=o(a,"P",{});var zM=l(sh);OS=u(zM,`This task reads some audio input and outputs the said words within the
audio files.`),zM.forEach(t),T2=h(a),v(da.$$.fragment,a),j2=h(a),v(ga.$$.fragment,a),k2=h(a),me=o(a,"P",{});var zi=l(me);PS=u(zi,"Available with: "),Sl=o(zi,"A",{href:!0,rel:!0});var MM=l(Sl);RS=u(MM,"\u{1F917} Transformers"),MM.forEach(t),SS=h(zi),Nl=o(zi,"A",{href:!0,rel:!0});var KM=l(Nl);NS=u(KM,"ESPnet"),KM.forEach(t),xS=u(zi,` and
`),xl=o(zi,"A",{href:!0,rel:!0});var FM=l(xl);IS=u(FM,"SpeechBrain"),FM.forEach(t),zi.forEach(t),A2=h(a),ah=o(a,"P",{});var JM=l(ah);HS=u(JM,"Request:"),JM.forEach(t),D2=h(a),v(ma.$$.fragment,a),O2=h(a),rh=o(a,"P",{});var WM=l(rh);BS=u(WM,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),WM.forEach(t),P2=h(a),$a=o(a,"TABLE",{});var wb=l($a);Dq=o(wb,"THEAD",{});var YM=l(Dq);Il=o(YM,"TR",{});var bb=l(Il);nh=o(bb,"TH",{align:!0});var VM=l(nh);CS=u(VM,"All parameters"),VM.forEach(t),GS=h(bb),Oq=o(bb,"TH",{align:!0}),l(Oq).forEach(t),bb.forEach(t),YM.forEach(t),LS=h(wb),Pq=o(wb,"TBODY",{});var XM=l(Pq);Hl=o(XM,"TR",{});var Tb=l(Hl);Bl=o(Tb,"TD",{align:!0});var AI=l(Bl);Rq=o(AI,"STRONG",{});var QM=l(Rq);US=u(QM,"no parameter"),QM.forEach(t),zS=u(AI," (required)"),AI.forEach(t),MS=h(Tb),oh=o(Tb,"TD",{align:!0});var ZM=l(oh);KS=u(ZM,"a binary representation of the audio file. No other parameters are currently allowed."),ZM.forEach(t),Tb.forEach(t),XM.forEach(t),wb.forEach(t),R2=h(a),lh=o(a,"P",{});var eK=l(lh);FS=u(eK,"Return value is either a dict or a list of dicts if you sent a list of inputs"),eK.forEach(t),S2=h(a),ih=o(a,"P",{});var tK=l(ih);JS=u(tK,"Response:"),tK.forEach(t),N2=h(a),v(qa.$$.fragment,a),x2=h(a),_a=o(a,"TABLE",{});var jb=l(_a);Sq=o(jb,"THEAD",{});var sK=l(Sq);Cl=o(sK,"TR",{});var kb=l(Cl);uh=o(kb,"TH",{align:!0});var aK=l(uh);WS=u(aK,"Returned values"),aK.forEach(t),YS=h(kb),Nq=o(kb,"TH",{align:!0}),l(Nq).forEach(t),kb.forEach(t),sK.forEach(t),VS=h(jb),xq=o(jb,"TBODY",{});var rK=l(xq);Gl=o(rK,"TR",{});var Ab=l(Gl);ch=o(Ab,"TD",{align:!0});var nK=l(ch);Iq=o(nK,"STRONG",{});var oK=l(Iq);XS=u(oK,"text"),oK.forEach(t),nK.forEach(t),QS=h(Ab),fh=o(Ab,"TD",{align:!0});var lK=l(fh);ZS=u(lK,"The string that was recognized within the audio file."),lK.forEach(t),Ab.forEach(t),rK.forEach(t),jb.forEach(t),I2=h(a),et=o(a,"H3",{class:!0});var Db=l(et);va=o(Db,"A",{id:!0,class:!0,href:!0});var iK=l(va);Hq=o(iK,"SPAN",{});var uK=l(Hq);v(Ll.$$.fragment,uK),uK.forEach(t),iK.forEach(t),eN=h(Db),Bq=o(Db,"SPAN",{});var cK=l(Bq);tN=u(cK,"Audio Classification task"),cK.forEach(t),Db.forEach(t),H2=h(a),ph=o(a,"P",{});var fK=l(ph);sN=u(fK,"This task reads some audio input and outputs the likelihood of classes."),fK.forEach(t),B2=h(a),v(Ea.$$.fragment,a),C2=h(a),tt=o(a,"P",{});var n1=l(tt);aN=u(n1,"Available with: "),Ul=o(n1,"A",{href:!0,rel:!0});var pK=l(Ul);rN=u(pK,"\u{1F917} Transformers"),pK.forEach(t),nN=h(n1),zl=o(n1,"A",{href:!0,rel:!0});var hK=l(zl);oN=u(hK,"SpeechBrain"),hK.forEach(t),n1.forEach(t),G2=h(a),hh=o(a,"P",{});var dK=l(hh);lN=u(dK,"Request:"),dK.forEach(t),L2=h(a),v(ya.$$.fragment,a),U2=h(a),dh=o(a,"P",{});var gK=l(dh);iN=u(gK,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),gK.forEach(t),z2=h(a),wa=o(a,"TABLE",{});var Ob=l(wa);Cq=o(Ob,"THEAD",{});var mK=l(Cq);Ml=o(mK,"TR",{});var Pb=l(Ml);gh=o(Pb,"TH",{align:!0});var $K=l(gh);uN=u($K,"All parameters"),$K.forEach(t),cN=h(Pb),Gq=o(Pb,"TH",{align:!0}),l(Gq).forEach(t),Pb.forEach(t),mK.forEach(t),fN=h(Ob),Lq=o(Ob,"TBODY",{});var qK=l(Lq);Kl=o(qK,"TR",{});var Rb=l(Kl);Fl=o(Rb,"TD",{align:!0});var DI=l(Fl);Uq=o(DI,"STRONG",{});var _K=l(Uq);pN=u(_K,"no parameter"),_K.forEach(t),hN=u(DI," (required)"),DI.forEach(t),dN=h(Rb),mh=o(Rb,"TD",{align:!0});var vK=l(mh);gN=u(vK,"a binary representation of the audio file. No other parameters are currently allowed."),vK.forEach(t),Rb.forEach(t),qK.forEach(t),Ob.forEach(t),M2=h(a),$h=o(a,"P",{});var EK=l($h);mN=u(EK,"Return value is a dict"),EK.forEach(t),K2=h(a),v(ba.$$.fragment,a),F2=h(a),Ta=o(a,"TABLE",{});var Sb=l(Ta);zq=o(Sb,"THEAD",{});var yK=l(zq);Jl=o(yK,"TR",{});var Nb=l(Jl);qh=o(Nb,"TH",{align:!0});var wK=l(qh);$N=u(wK,"Returned values"),wK.forEach(t),qN=h(Nb),Mq=o(Nb,"TH",{align:!0}),l(Mq).forEach(t),Nb.forEach(t),yK.forEach(t),_N=h(Sb),Wl=o(Sb,"TBODY",{});var xb=l(Wl);Yl=o(xb,"TR",{});var Ib=l(Yl);_h=o(Ib,"TD",{align:!0});var bK=l(_h);Kq=o(bK,"STRONG",{});var TK=l(Kq);vN=u(TK,"label"),TK.forEach(t),bK.forEach(t),EN=h(Ib),vh=o(Ib,"TD",{align:!0});var jK=l(vh);yN=u(jK,"The label for the class (model specific)"),jK.forEach(t),Ib.forEach(t),wN=h(xb),Vl=o(xb,"TR",{});var Hb=l(Vl);Eh=o(Hb,"TD",{align:!0});var kK=l(Eh);Fq=o(kK,"STRONG",{});var AK=l(Fq);bN=u(AK,"score"),AK.forEach(t),kK.forEach(t),TN=h(Hb),yh=o(Hb,"TD",{align:!0});var DK=l(yh);jN=u(DK,"A float that represents how likely it is that the audio file belongs to this class."),DK.forEach(t),Hb.forEach(t),xb.forEach(t),Sb.forEach(t),J2=h(a),st=o(a,"H2",{class:!0});var Bb=l(st);ja=o(Bb,"A",{id:!0,class:!0,href:!0});var OK=l(ja);Jq=o(OK,"SPAN",{});var PK=l(Jq);v(Xl.$$.fragment,PK),PK.forEach(t),OK.forEach(t),kN=h(Bb),Wq=o(Bb,"SPAN",{});var RK=l(Wq);AN=u(RK,"Computer Vision"),RK.forEach(t),Bb.forEach(t),W2=h(a),at=o(a,"H3",{class:!0});var Cb=l(at);ka=o(Cb,"A",{id:!0,class:!0,href:!0});var SK=l(ka);Yq=o(SK,"SPAN",{});var NK=l(Yq);v(Ql.$$.fragment,NK),NK.forEach(t),SK.forEach(t),DN=h(Cb),Vq=o(Cb,"SPAN",{});var xK=l(Vq);ON=u(xK,"Image Classification task"),xK.forEach(t),Cb.forEach(t),Y2=h(a),wh=o(a,"P",{});var IK=l(wh);PN=u(IK,"This task reads some image input and outputs the likelihood of classes."),IK.forEach(t),V2=h(a),v(Aa.$$.fragment,a),X2=h(a),Zl=o(a,"P",{});var OI=l(Zl);RN=u(OI,"Available with: "),ei=o(OI,"A",{href:!0,rel:!0});var HK=l(ei);SN=u(HK,"\u{1F917} Transformers"),HK.forEach(t),OI.forEach(t),Q2=h(a),bh=o(a,"P",{});var BK=l(bh);NN=u(BK,"Request:"),BK.forEach(t),Z2=h(a),v(Da.$$.fragment,a),eE=h(a),Oa=o(a,"P",{});var Gb=l(Oa);xN=u(Gb,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),ti=o(Gb,"A",{href:!0,rel:!0});var CK=l(ti);IN=u(CK,`Pillow
supports`),CK.forEach(t),HN=u(Gb,"."),Gb.forEach(t),tE=h(a),Pa=o(a,"TABLE",{});var Lb=l(Pa);Xq=o(Lb,"THEAD",{});var GK=l(Xq);si=o(GK,"TR",{});var Ub=l(si);Th=o(Ub,"TH",{align:!0});var LK=l(Th);BN=u(LK,"All parameters"),LK.forEach(t),CN=h(Ub),Qq=o(Ub,"TH",{align:!0}),l(Qq).forEach(t),Ub.forEach(t),GK.forEach(t),GN=h(Lb),Zq=o(Lb,"TBODY",{});var UK=l(Zq);ai=o(UK,"TR",{});var zb=l(ai);ri=o(zb,"TD",{align:!0});var PI=l(ri);e_=o(PI,"STRONG",{});var zK=l(e_);LN=u(zK,"no parameter"),zK.forEach(t),UN=u(PI," (required)"),PI.forEach(t),zN=h(zb),jh=o(zb,"TD",{align:!0});var MK=l(jh);MN=u(MK,"a binary representation of the image file. No other parameters are currently allowed."),MK.forEach(t),zb.forEach(t),UK.forEach(t),Lb.forEach(t),sE=h(a),kh=o(a,"P",{});var KK=l(kh);KN=u(KK,"Return value is a dict"),KK.forEach(t),aE=h(a),v(Ra.$$.fragment,a),rE=h(a),Sa=o(a,"TABLE",{});var Mb=l(Sa);t_=o(Mb,"THEAD",{});var FK=l(t_);ni=o(FK,"TR",{});var Kb=l(ni);Ah=o(Kb,"TH",{align:!0});var JK=l(Ah);FN=u(JK,"Returned values"),JK.forEach(t),JN=h(Kb),s_=o(Kb,"TH",{align:!0}),l(s_).forEach(t),Kb.forEach(t),FK.forEach(t),WN=h(Mb),oi=o(Mb,"TBODY",{});var Fb=l(oi);li=o(Fb,"TR",{});var Jb=l(li);Dh=o(Jb,"TD",{align:!0});var WK=l(Dh);a_=o(WK,"STRONG",{});var YK=l(a_);YN=u(YK,"label"),YK.forEach(t),WK.forEach(t),VN=h(Jb),Oh=o(Jb,"TD",{align:!0});var VK=l(Oh);XN=u(VK,"The label for the class (model specific)"),VK.forEach(t),Jb.forEach(t),QN=h(Fb),ii=o(Fb,"TR",{});var Wb=l(ii);Ph=o(Wb,"TD",{align:!0});var XK=l(Ph);r_=o(XK,"STRONG",{});var QK=l(r_);ZN=u(QK,"score"),QK.forEach(t),XK.forEach(t),ex=h(Wb),Rh=o(Wb,"TD",{align:!0});var ZK=l(Rh);tx=u(ZK,"A float that represents how likely it is that the image file belongs to this class."),ZK.forEach(t),Wb.forEach(t),Fb.forEach(t),Mb.forEach(t),nE=h(a),rt=o(a,"H3",{class:!0});var Yb=l(rt);Na=o(Yb,"A",{id:!0,class:!0,href:!0});var eF=l(Na);n_=o(eF,"SPAN",{});var tF=l(n_);v(ui.$$.fragment,tF),tF.forEach(t),eF.forEach(t),sx=h(Yb),o_=o(Yb,"SPAN",{});var sF=l(o_);ax=u(sF,"Object Detection task"),sF.forEach(t),Yb.forEach(t),oE=h(a),Sh=o(a,"P",{});var aF=l(Sh);rx=u(aF,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),aF.forEach(t),lE=h(a),v(xa.$$.fragment,a),iE=h(a),ci=o(a,"P",{});var RI=l(ci);nx=u(RI,"Available with: "),fi=o(RI,"A",{href:!0,rel:!0});var rF=l(fi);ox=u(rF,"\u{1F917} Transformers"),rF.forEach(t),RI.forEach(t),uE=h(a),Nh=o(a,"P",{});var nF=l(Nh);lx=u(nF,"Request:"),nF.forEach(t),cE=h(a),v(Ia.$$.fragment,a),fE=h(a),Ha=o(a,"P",{});var Vb=l(Ha);ix=u(Vb,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),pi=o(Vb,"A",{href:!0,rel:!0});var oF=l(pi);ux=u(oF,`Pillow
supports`),oF.forEach(t),cx=u(Vb,"."),Vb.forEach(t),pE=h(a),Ba=o(a,"TABLE",{});var Xb=l(Ba);l_=o(Xb,"THEAD",{});var lF=l(l_);hi=o(lF,"TR",{});var Qb=l(hi);xh=o(Qb,"TH",{align:!0});var iF=l(xh);fx=u(iF,"All parameters"),iF.forEach(t),px=h(Qb),i_=o(Qb,"TH",{align:!0}),l(i_).forEach(t),Qb.forEach(t),lF.forEach(t),hx=h(Xb),u_=o(Xb,"TBODY",{});var uF=l(u_);di=o(uF,"TR",{});var Zb=l(di);gi=o(Zb,"TD",{align:!0});var SI=l(gi);c_=o(SI,"STRONG",{});var cF=l(c_);dx=u(cF,"no parameter"),cF.forEach(t),gx=u(SI," (required)"),SI.forEach(t),mx=h(Zb),Ih=o(Zb,"TD",{align:!0});var fF=l(Ih);$x=u(fF,"a binary representation of the image file. No other parameters are currently allowed."),fF.forEach(t),Zb.forEach(t),uF.forEach(t),Xb.forEach(t),hE=h(a),Hh=o(a,"P",{});var pF=l(Hh);qx=u(pF,"Return value is a dict"),pF.forEach(t),dE=h(a),v(Ca.$$.fragment,a),gE=h(a),Ga=o(a,"TABLE",{});var e3=l(Ga);f_=o(e3,"THEAD",{});var hF=l(f_);mi=o(hF,"TR",{});var t3=l(mi);Bh=o(t3,"TH",{align:!0});var dF=l(Bh);_x=u(dF,"Returned values"),dF.forEach(t),vx=h(t3),p_=o(t3,"TH",{align:!0}),l(p_).forEach(t),t3.forEach(t),hF.forEach(t),Ex=h(e3),nt=o(e3,"TBODY",{});var $d=l(nt);$i=o($d,"TR",{});var s3=l($i);Ch=o(s3,"TD",{align:!0});var gF=l(Ch);h_=o(gF,"STRONG",{});var mF=l(h_);yx=u(mF,"label"),mF.forEach(t),gF.forEach(t),wx=h(s3),Gh=o(s3,"TD",{align:!0});var $F=l(Gh);bx=u($F,"The label for the class (model specific) of a detected object."),$F.forEach(t),s3.forEach(t),Tx=h($d),qi=o($d,"TR",{});var a3=l(qi);Lh=o(a3,"TD",{align:!0});var qF=l(Lh);d_=o(qF,"STRONG",{});var _F=l(d_);jx=u(_F,"score"),_F.forEach(t),qF.forEach(t),kx=h(a3),Uh=o(a3,"TD",{align:!0});var vF=l(Uh);Ax=u(vF,"A float that represents how likely it is that the detected object belongs to the given class."),vF.forEach(t),a3.forEach(t),Dx=h($d),_i=o($d,"TR",{});var r3=l(_i);zh=o(r3,"TD",{align:!0});var EF=l(zh);g_=o(EF,"STRONG",{});var yF=l(g_);Ox=u(yF,"box"),yF.forEach(t),EF.forEach(t),Px=h(r3),Mh=o(r3,"TD",{align:!0});var wF=l(Mh);Rx=u(wF,"A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),wF.forEach(t),r3.forEach(t),$d.forEach(t),e3.forEach(t),mE=h(a),ot=o(a,"H3",{class:!0});var n3=l(ot);La=o(n3,"A",{id:!0,class:!0,href:!0});var bF=l(La);m_=o(bF,"SPAN",{});var TF=l(m_);v(vi.$$.fragment,TF),TF.forEach(t),bF.forEach(t),Sx=h(n3),$_=o(n3,"SPAN",{});var jF=l($_);Nx=u(jF,"Image Segmentation task"),jF.forEach(t),n3.forEach(t),$E=h(a),Kh=o(a,"P",{});var kF=l(Kh);xx=u(kF,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),kF.forEach(t),qE=h(a),v(Ua.$$.fragment,a),_E=h(a),Ei=o(a,"P",{});var NI=l(Ei);Ix=u(NI,"Available with: "),yi=o(NI,"A",{href:!0,rel:!0});var AF=l(yi);Hx=u(AF,"\u{1F917} Transformers"),AF.forEach(t),NI.forEach(t),vE=h(a),Fh=o(a,"P",{});var DF=l(Fh);Bx=u(DF,"Request:"),DF.forEach(t),EE=h(a),v(za.$$.fragment,a),yE=h(a),Ma=o(a,"P",{});var o3=l(Ma);Cx=u(o3,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),wi=o(o3,"A",{href:!0,rel:!0});var OF=l(wi);Gx=u(OF,`Pillow
supports`),OF.forEach(t),Lx=u(o3,"."),o3.forEach(t),wE=h(a),Ka=o(a,"TABLE",{});var l3=l(Ka);q_=o(l3,"THEAD",{});var PF=l(q_);bi=o(PF,"TR",{});var i3=l(bi);Jh=o(i3,"TH",{align:!0});var RF=l(Jh);Ux=u(RF,"All parameters"),RF.forEach(t),zx=h(i3),__=o(i3,"TH",{align:!0}),l(__).forEach(t),i3.forEach(t),PF.forEach(t),Mx=h(l3),v_=o(l3,"TBODY",{});var SF=l(v_);Ti=o(SF,"TR",{});var u3=l(Ti);ji=o(u3,"TD",{align:!0});var xI=l(ji);E_=o(xI,"STRONG",{});var NF=l(E_);Kx=u(NF,"no parameter"),NF.forEach(t),Fx=u(xI," (required)"),xI.forEach(t),Jx=h(u3),Wh=o(u3,"TD",{align:!0});var xF=l(Wh);Wx=u(xF,"a binary representation of the image file. No other parameters are currently allowed."),xF.forEach(t),u3.forEach(t),SF.forEach(t),l3.forEach(t),bE=h(a),Yh=o(a,"P",{});var IF=l(Yh);Yx=u(IF,"Return value is a dict"),IF.forEach(t),TE=h(a),v(Fa.$$.fragment,a),jE=h(a),Ja=o(a,"TABLE",{});var c3=l(Ja);y_=o(c3,"THEAD",{});var HF=l(y_);ki=o(HF,"TR",{});var f3=l(ki);Vh=o(f3,"TH",{align:!0});var BF=l(Vh);Vx=u(BF,"Returned values"),BF.forEach(t),Xx=h(f3),w_=o(f3,"TH",{align:!0}),l(w_).forEach(t),f3.forEach(t),HF.forEach(t),Qx=h(c3),lt=o(c3,"TBODY",{});var qd=l(lt);Ai=o(qd,"TR",{});var p3=l(Ai);Xh=o(p3,"TD",{align:!0});var CF=l(Xh);b_=o(CF,"STRONG",{});var GF=l(b_);Zx=u(GF,"label"),GF.forEach(t),CF.forEach(t),eI=h(p3),Qh=o(p3,"TD",{align:!0});var LF=l(Qh);tI=u(LF,"The label for the class (model specific) of a segment."),LF.forEach(t),p3.forEach(t),sI=h(qd),Di=o(qd,"TR",{});var h3=l(Di);Zh=o(h3,"TD",{align:!0});var UF=l(Zh);T_=o(UF,"STRONG",{});var zF=l(T_);aI=u(zF,"score"),zF.forEach(t),UF.forEach(t),rI=h(h3),ed=o(h3,"TD",{align:!0});var MF=l(ed);nI=u(MF,"A float that represents how likely it is that the segment belongs to the given class."),MF.forEach(t),h3.forEach(t),oI=h(qd),Oi=o(qd,"TR",{});var d3=l(Oi);td=o(d3,"TD",{align:!0});var KF=l(td);j_=o(KF,"STRONG",{});var FF=l(j_);lI=u(FF,"mask"),FF.forEach(t),KF.forEach(t),iI=h(d3),sd=o(d3,"TD",{align:!0});var JF=l(sd);uI=u(JF,"A str (base64 str of a single channel black-and-white img) representing the mask of a segment."),JF.forEach(t),d3.forEach(t),qd.forEach(t),c3.forEach(t),this.h()},h(){f(r,"name","hf:doc:metadata"),f(r,"content",JSON.stringify(bY)),f(d,"id","detailed-parameters"),f(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(d,"href","#detailed-parameters"),f(s,"class","relative group"),f(ne,"id","which-task-is-used-by-this-model"),f(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ne,"href","#which-task-is-used-by-this-model"),f(D,"class","relative group"),f(ut,"class","block dark:hidden"),WF(ut.src,II="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task.png")||f(ut,"src",II),f(ut,"width","300"),f(ct,"class","hidden dark:block invert"),WF(ct.src,HI="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task-dark.png")||f(ct,"src",HI),f(ct,"width","300"),f(ft,"id","natural-language-processing"),f(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ft,"href","#natural-language-processing"),f(Ne,"class","relative group"),f(pt,"id","fill-mask-task"),f(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(pt,"href","#fill-mask-task"),f(xe,"class","relative group"),f(ir,"href","https://github.com/huggingface/transformers"),f(ir,"rel","nofollow"),f(Yi,"align","left"),f(bd,"align","left"),f(fr,"align","left"),f(Vi,"align","left"),f(Xi,"align","left"),f(Qi,"align","left"),f(Zi,"align","left"),f(mt,"align","left"),f(eu,"align","left"),f($t,"align","left"),f(tu,"align","left"),f(qt,"align","left"),f(au,"align","left"),f(Pd,"align","left"),f(ru,"align","left"),f(nu,"align","left"),f(ou,"align","left"),f(lu,"align","left"),f(iu,"align","left"),f(uu,"align","left"),f(cu,"align","left"),f(fu,"align","left"),f(Et,"id","summarization-task"),f(Et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Et,"href","#summarization-task"),f(Ie,"class","relative group"),f(pu,"href","mailto:api-enterprise@huggingface.co"),f(wr,"href","https://github.com/huggingface/transformers"),f(wr,"rel","nofollow"),f(gu,"align","left"),f(Cd,"align","left"),f(jr,"align","left"),f(mu,"align","left"),f($u,"align","left"),f(qu,"align","left"),f(_u,"align","left"),f($e,"align","left"),f(vu,"align","left"),f(qe,"align","left"),f(Eu,"align","left"),f(_e,"align","left"),f(yu,"align","left"),f(oe,"align","left"),f(wu,"align","left"),f(le,"align","left"),f(bu,"align","left"),f(jt,"align","left"),f(Tu,"align","left"),f(kt,"align","left"),f(ju,"align","left"),f(ku,"align","left"),f(Au,"align","left"),f(At,"align","left"),f(Du,"align","left"),f(Dt,"align","left"),f(Ou,"align","left"),f(Ot,"align","left"),f(Ru,"align","left"),f(lg,"align","left"),f(Su,"align","left"),f(Nu,"align","left"),f(Rt,"id","question-answering-task"),f(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Rt,"href","#question-answering-task"),f(He,"class","relative group"),f(Ur,"href","https://github.com/huggingface/transformers"),f(Ur,"rel","nofollow"),f(zr,"href","https://github.com/allenai/allennlp"),f(zr,"rel","nofollow"),f(Cu,"align","left"),f(hg,"align","left"),f(Gu,"align","left"),f(Lu,"align","left"),f(Uu,"align","left"),f(zu,"align","left"),f(Mu,"align","left"),f(Ht,"align","left"),f(Ku,"align","left"),f(Bt,"align","left"),f(Ct,"id","table-question-answering-task"),f(Ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ct,"href","#table-question-answering-task"),f(Ce,"class","relative group"),f(Xr,"href","https://github.com/huggingface/transformers"),f(Xr,"rel","nofollow"),f(Yu,"align","left"),f(wg,"align","left"),f(en,"align","left"),f(Tg,"align","left"),f(Vu,"align","left"),f(Xu,"align","left"),f(Qu,"align","left"),f(Zu,"align","left"),f(ec,"align","left"),f(tc,"align","left"),f(sc,"align","left"),f(zt,"align","left"),f(ac,"align","left"),f(Mt,"align","left"),f(rc,"align","left"),f(Kt,"align","left"),f(oc,"align","left"),f(Pg,"align","left"),f(lc,"align","left"),f(ic,"align","left"),f(uc,"align","left"),f(cc,"align","left"),f(fc,"align","left"),f(pc,"align","left"),f(hc,"align","left"),f(dc,"align","left"),f(Wt,"id","text-classification-task"),f(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Wt,"href","#text-classification-task"),f(Ge,"class","relative group"),f(gn,"href","https://github.com/huggingface/transformers"),f(gn,"rel","nofollow"),f(qc,"align","left"),f(Cg,"align","left"),f(qn,"align","left"),f(_c,"align","left"),f(vc,"align","left"),f(Ec,"align","left"),f(yc,"align","left"),f(Qt,"align","left"),f(wc,"align","left"),f(Zt,"align","left"),f(bc,"align","left"),f(es,"align","left"),f(jc,"align","left"),f(Fg,"align","left"),f(kc,"align","left"),f(Ac,"align","left"),f(Dc,"align","left"),f(Oc,"align","left"),f(as,"id","text-generation-task"),f(as,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(as,"href","#text-generation-task"),f(Le,"class","relative group"),f(Dn,"href","https://github.com/huggingface/transformers"),f(Dn,"rel","nofollow"),f(Nc,"align","left"),f(Qg,"align","left"),f(Rn,"align","left"),f(xc,"align","left"),f(Ic,"align","left"),f(Hc,"align","left"),f(Bc,"align","left"),f(ve,"align","left"),f(Cc,"align","left"),f(ie,"align","left"),f(Gc,"align","left"),f(ue,"align","left"),f(Lc,"align","left"),f(ls,"align","left"),f(Uc,"align","left"),f(Ee,"align","left"),f(zc,"align","left"),f(ye,"align","left"),f(Mc,"align","left"),f(we,"align","left"),f(Kc,"align","left"),f(is,"align","left"),f(Fc,"align","left"),f(us,"align","left"),f(Jc,"align","left"),f(Wc,"align","left"),f(Yc,"align","left"),f(cs,"align","left"),f(Vc,"align","left"),f(fs,"align","left"),f(Xc,"align","left"),f(ps,"align","left"),f(Zc,"align","left"),f(wm,"align","left"),f(ef,"align","left"),f(tf,"align","left"),f(gs,"id","text2text-generation-task"),f(gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(gs,"href","#text2text-generation-task"),f(Ue,"class","relative group"),f(sf,"href","#text-generation-task"),f($s,"id","token-classification-task"),f($s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f($s,"href","#token-classification-task"),f(ze,"class","relative group"),f(Xn,"href","https://github.com/huggingface/transformers"),f(Xn,"rel","nofollow"),f(Qn,"href","https://github.com/flairNLP/flair"),f(Qn,"rel","nofollow"),f(of,"align","left"),f(Pm,"align","left"),f(to,"align","left"),f(lf,"align","left"),f(uf,"align","left"),f(cf,"align","left"),f(ff,"align","left"),f(x,"align","left"),f(pf,"align","left"),f(hf,"align","left"),f(df,"align","left"),f(Es,"align","left"),f(gf,"align","left"),f(ys,"align","left"),f(mf,"align","left"),f(ws,"align","left"),f(qf,"align","left"),f(Wm,"align","left"),f(_f,"align","left"),f(vf,"align","left"),f(Ef,"align","left"),f(yf,"align","left"),f(wf,"align","left"),f(bf,"align","left"),f(Tf,"align","left"),f(js,"align","left"),f(jf,"align","left"),f(ks,"align","left"),f(As,"id","named-entity-recognition-ner-task"),f(As,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(As,"href","#named-entity-recognition-ner-task"),f(Ke,"class","relative group"),f(kf,"href","#token-classification-task"),f(Ds,"id","translation-task"),f(Ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ds,"href","#translation-task"),f(Fe,"class","relative group"),f(_o,"href","https://github.com/huggingface/transformers"),f(_o,"rel","nofollow"),f(Pf,"align","left"),f(l$,"align","left"),f(yo,"align","left"),f(Rf,"align","left"),f(Sf,"align","left"),f(Nf,"align","left"),f(xf,"align","left"),f(Ss,"align","left"),f(If,"align","left"),f(Ns,"align","left"),f(Hf,"align","left"),f(xs,"align","left"),f(Cf,"align","left"),f(d$,"align","left"),f(Gf,"align","left"),f(Lf,"align","left"),f(Hs,"id","zeroshot-classification-task"),f(Hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Hs,"href","#zeroshot-classification-task"),f(Je,"class","relative group"),f(Po,"href","https://github.com/huggingface/transformers"),f(Po,"rel","nofollow"),f(Kf,"align","left"),f(v$,"align","left"),f(No,"align","left"),f(Ff,"align","left"),f(Io,"align","left"),f(Jf,"align","left"),f(Wf,"align","left"),f(be,"align","left"),f(Yf,"align","left"),f(Ls,"align","left"),f(Vf,"align","left"),f(Xf,"align","left"),f(Qf,"align","left"),f(Us,"align","left"),f(Zf,"align","left"),f(zs,"align","left"),f(ep,"align","left"),f(Ms,"align","left"),f(ap,"align","left"),f(P$,"align","left"),f(rp,"align","left"),f(np,"align","left"),f(op,"align","left"),f(lp,"align","left"),f(ip,"align","left"),f(Js,"align","left"),f(Ws,"id","conversational-task"),f(Ws,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ws,"href","#conversational-task"),f(Ye,"class","relative group"),f(Yo,"href","https://github.com/huggingface/transformers"),f(Yo,"rel","nofollow"),f(pp,"align","left"),f(C$,"align","left"),f(Qo,"align","left"),f(L$,"align","left"),f(hp,"align","left"),f(dp,"align","left"),f(gp,"align","left"),f(mp,"align","left"),f($p,"align","left"),f(Qs,"align","left"),f(qp,"align","left"),f(_p,"align","left"),f(vp,"align","left"),f(Te,"align","left"),f(Ep,"align","left"),f(je,"align","left"),f(yp,"align","left"),f(ke,"align","left"),f(wp,"align","left"),f(ce,"align","left"),f(bp,"align","left"),f(fe,"align","left"),f(Tp,"align","left"),f(Zs,"align","left"),f(jp,"align","left"),f(ea,"align","left"),f(kp,"align","left"),f(Ap,"align","left"),f(Dp,"align","left"),f(ta,"align","left"),f(Op,"align","left"),f(sa,"align","left"),f(Pp,"align","left"),f(aa,"align","left"),f(Sp,"align","left"),f(uq,"align","left"),f(Np,"align","left"),f(xp,"align","left"),f(Ip,"align","left"),f(Hp,"align","left"),f(Bp,"align","left"),f(Cp,"align","left"),f(Gp,"align","left"),f(Lp,"align","left"),f(na,"id","feature-extraction-task"),f(na,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(na,"href","#feature-extraction-task"),f(Ve,"class","relative group"),f(vl,"href","https://github.com/huggingface/transformers"),f(vl,"rel","nofollow"),f(El,"href","https://github.com/UKPLab/sentence-transformers"),f(El,"rel","nofollow"),f(Mp,"align","left"),f(gq,"align","left"),f(bl,"align","left"),f(Kp,"align","left"),f(Fp,"align","left"),f(Jp,"align","left"),f(Wp,"align","left"),f(ia,"align","left"),f(Yp,"align","left"),f(ua,"align","left"),f(Vp,"align","left"),f(ca,"align","left"),f(Qp,"align","left"),f(yq,"align","left"),f(Zp,"align","left"),f(eh,"align","left"),f(pa,"id","audio"),f(pa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(pa,"href","#audio"),f(Qe,"class","relative group"),f(ha,"id","automatic-speech-recognition-task"),f(ha,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ha,"href","#automatic-speech-recognition-task"),f(Ze,"class","relative group"),f(Sl,"href","https://github.com/huggingface/transformers"),f(Sl,"rel","nofollow"),f(Nl,"href","https://github.com/espnet/espnet"),f(Nl,"rel","nofollow"),f(xl,"href","https://github.com/speechbrain/speechbrain"),f(xl,"rel","nofollow"),f(nh,"align","left"),f(Oq,"align","left"),f(Bl,"align","left"),f(oh,"align","left"),f(uh,"align","left"),f(Nq,"align","left"),f(ch,"align","left"),f(fh,"align","left"),f(va,"id","audio-classification-task"),f(va,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(va,"href","#audio-classification-task"),f(et,"class","relative group"),f(Ul,"href","https://github.com/huggingface/transformers"),f(Ul,"rel","nofollow"),f(zl,"href","https://github.com/speechbrain/speechbrain"),f(zl,"rel","nofollow"),f(gh,"align","left"),f(Gq,"align","left"),f(Fl,"align","left"),f(mh,"align","left"),f(qh,"align","left"),f(Mq,"align","left"),f(_h,"align","left"),f(vh,"align","left"),f(Eh,"align","left"),f(yh,"align","left"),f(ja,"id","computer-vision"),f(ja,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ja,"href","#computer-vision"),f(st,"class","relative group"),f(ka,"id","image-classification-task"),f(ka,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ka,"href","#image-classification-task"),f(at,"class","relative group"),f(ei,"href","https://github.com/huggingface/transformers"),f(ei,"rel","nofollow"),f(ti,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),f(ti,"rel","nofollow"),f(Th,"align","left"),f(Qq,"align","left"),f(ri,"align","left"),f(jh,"align","left"),f(Ah,"align","left"),f(s_,"align","left"),f(Dh,"align","left"),f(Oh,"align","left"),f(Ph,"align","left"),f(Rh,"align","left"),f(Na,"id","object-detection-task"),f(Na,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Na,"href","#object-detection-task"),f(rt,"class","relative group"),f(fi,"href","https://github.com/huggingface/transformers"),f(fi,"rel","nofollow"),f(pi,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),f(pi,"rel","nofollow"),f(xh,"align","left"),f(i_,"align","left"),f(gi,"align","left"),f(Ih,"align","left"),f(Bh,"align","left"),f(p_,"align","left"),f(Ch,"align","left"),f(Gh,"align","left"),f(Lh,"align","left"),f(Uh,"align","left"),f(zh,"align","left"),f(Mh,"align","left"),f(La,"id","image-segmentation-task"),f(La,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(La,"href","#image-segmentation-task"),f(ot,"class","relative group"),f(yi,"href","https://github.com/huggingface/transformers"),f(yi,"rel","nofollow"),f(wi,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),f(wi,"rel","nofollow"),f(Jh,"align","left"),f(__,"align","left"),f(ji,"align","left"),f(Wh,"align","left"),f(Vh,"align","left"),f(w_,"align","left"),f(Xh,"align","left"),f(Qh,"align","left"),f(Zh,"align","left"),f(ed,"align","left"),f(td,"align","left"),f(sd,"align","left")},m(a,g){e(document.head,r),m(a,c,g),m(a,s,g),e(s,d),e(d,$),E(k,$,null),e(s,A),e(s,j),e(j,T),m(a,S,g),m(a,D,g),e(D,ne),e(ne,Re),E(Q,Re,null),e(D,Y),e(D,it),e(it,Mi),m(a,rr,g),m(a,Se,g),e(Se,g3),m(a,o1,g),m(a,Ki,g),e(Ki,m3),m(a,l1,g),m(a,ut,g),m(a,i1,g),m(a,ct,g),m(a,u1,g),m(a,Ne,g),e(Ne,ft),e(ft,_d),E(nr,_d,null),e(Ne,$3),e(Ne,vd),e(vd,q3),m(a,c1,g),m(a,xe,g),e(xe,pt),e(pt,Ed),E(or,Ed,null),e(xe,_3),e(xe,yd),e(yd,v3),m(a,f1,g),m(a,Fi,g),e(Fi,E3),m(a,p1,g),E(ht,a,g),m(a,h1,g),m(a,lr,g),e(lr,y3),e(lr,ir),e(ir,w3),m(a,d1,g),m(a,Ji,g),e(Ji,b3),m(a,g1,g),E(dt,a,g),m(a,m1,g),m(a,Wi,g),e(Wi,T3),m(a,$1,g),m(a,gt,g),e(gt,wd),e(wd,ur),e(ur,Yi),e(Yi,j3),e(ur,k3),e(ur,bd),e(gt,A3),e(gt,Z),e(Z,cr),e(cr,fr),e(fr,Td),e(Td,D3),e(fr,O3),e(cr,P3),e(cr,Vi),e(Vi,R3),e(Z,S3),e(Z,pr),e(pr,Xi),e(Xi,jd),e(jd,N3),e(pr,x3),e(pr,Qi),e(Qi,I3),e(Z,H3),e(Z,hr),e(hr,Zi),e(Zi,B3),e(hr,C3),e(hr,mt),e(mt,G3),e(mt,kd),e(kd,L3),e(mt,U3),e(Z,z3),e(Z,dr),e(dr,eu),e(eu,M3),e(dr,K3),e(dr,$t),e($t,F3),e($t,Ad),e(Ad,J3),e($t,W3),e(Z,Y3),e(Z,gr),e(gr,tu),e(tu,V3),e(gr,X3),e(gr,qt),e(qt,Q3),e(qt,Dd),e(Dd,Z3),e(qt,eT),m(a,q1,g),m(a,su,g),e(su,tT),m(a,_1,g),E(_t,a,g),m(a,v1,g),m(a,vt,g),e(vt,Od),e(Od,mr),e(mr,au),e(au,sT),e(mr,aT),e(mr,Pd),e(vt,rT),e(vt,pe),e(pe,$r),e($r,ru),e(ru,Rd),e(Rd,nT),e($r,oT),e($r,nu),e(nu,lT),e(pe,iT),e(pe,qr),e(qr,ou),e(ou,Sd),e(Sd,uT),e(qr,cT),e(qr,lu),e(lu,fT),e(pe,pT),e(pe,_r),e(_r,iu),e(iu,Nd),e(Nd,hT),e(_r,dT),e(_r,uu),e(uu,gT),e(pe,mT),e(pe,vr),e(vr,cu),e(cu,xd),e(xd,$T),e(vr,qT),e(vr,fu),e(fu,_T),m(a,E1,g),m(a,Ie,g),e(Ie,Et),e(Et,Id),E(Er,Id,null),e(Ie,vT),e(Ie,Hd),e(Hd,ET),m(a,y1,g),m(a,yt,g),e(yt,yT),e(yt,pu),e(pu,wT),e(yt,bT),m(a,w1,g),E(wt,a,g),m(a,b1,g),m(a,yr,g),e(yr,TT),e(yr,wr),e(wr,jT),m(a,T1,g),m(a,hu,g),e(hu,kT),m(a,j1,g),E(bt,a,g),m(a,k1,g),m(a,du,g),e(du,AT),m(a,A1,g),m(a,Tt,g),e(Tt,Bd),e(Bd,br),e(br,gu),e(gu,DT),e(br,OT),e(br,Cd),e(Tt,PT),e(Tt,G),e(G,Tr),e(Tr,jr),e(jr,Gd),e(Gd,RT),e(jr,ST),e(Tr,NT),e(Tr,mu),e(mu,xT),e(G,IT),e(G,kr),e(kr,$u),e($u,Ld),e(Ld,HT),e(kr,BT),e(kr,qu),e(qu,CT),e(G,GT),e(G,Ar),e(Ar,_u),e(_u,LT),e(Ar,UT),e(Ar,$e),e($e,zT),e($e,Ud),e(Ud,MT),e($e,KT),e($e,zd),e(zd,FT),e($e,JT),e(G,WT),e(G,Dr),e(Dr,vu),e(vu,YT),e(Dr,VT),e(Dr,qe),e(qe,XT),e(qe,Md),e(Md,QT),e(qe,ZT),e(qe,Kd),e(Kd,ej),e(qe,tj),e(G,sj),e(G,Or),e(Or,Eu),e(Eu,aj),e(Or,rj),e(Or,_e),e(_e,nj),e(_e,Fd),e(Fd,oj),e(_e,lj),e(_e,Jd),e(Jd,ij),e(_e,uj),e(G,cj),e(G,Pr),e(Pr,yu),e(yu,fj),e(Pr,pj),e(Pr,oe),e(oe,hj),e(oe,Wd),e(Wd,dj),e(oe,gj),e(oe,Yd),e(Yd,mj),e(oe,$j),e(oe,Vd),e(Vd,qj),e(oe,_j),e(G,vj),e(G,Rr),e(Rr,wu),e(wu,Ej),e(Rr,yj),e(Rr,le),e(le,wj),e(le,Xd),e(Xd,bj),e(le,Tj),e(le,Qd),e(Qd,jj),e(le,kj),e(le,Zd),e(Zd,Aj),e(le,Dj),e(G,Oj),e(G,Sr),e(Sr,bu),e(bu,Pj),e(Sr,Rj),e(Sr,jt),e(jt,Sj),e(jt,eg),e(eg,Nj),e(jt,xj),e(G,Ij),e(G,Nr),e(Nr,Tu),e(Tu,Hj),e(Nr,Bj),e(Nr,kt),e(kt,Cj),e(kt,tg),e(tg,Gj),e(kt,Lj),e(G,Uj),e(G,xr),e(xr,ju),e(ju,sg),e(sg,zj),e(xr,Mj),e(xr,ku),e(ku,Kj),e(G,Fj),e(G,Ir),e(Ir,Au),e(Au,Jj),e(Ir,Wj),e(Ir,At),e(At,Yj),e(At,ag),e(ag,Vj),e(At,Xj),e(G,Qj),e(G,Hr),e(Hr,Du),e(Du,Zj),e(Hr,e4),e(Hr,Dt),e(Dt,t4),e(Dt,rg),e(rg,s4),e(Dt,a4),e(G,r4),e(G,Br),e(Br,Ou),e(Ou,n4),e(Br,o4),e(Br,Ot),e(Ot,l4),e(Ot,ng),e(ng,i4),e(Ot,u4),m(a,D1,g),m(a,Pu,g),e(Pu,c4),m(a,O1,g),m(a,Pt,g),e(Pt,og),e(og,Cr),e(Cr,Ru),e(Ru,f4),e(Cr,p4),e(Cr,lg),e(Pt,h4),e(Pt,ig),e(ig,Gr),e(Gr,Su),e(Su,ug),e(ug,d4),e(Gr,g4),e(Gr,Nu),e(Nu,m4),m(a,P1,g),m(a,He,g),e(He,Rt),e(Rt,cg),E(Lr,cg,null),e(He,$4),e(He,fg),e(fg,q4),m(a,R1,g),m(a,xu,g),e(xu,_4),m(a,S1,g),E(St,a,g),m(a,N1,g),m(a,Be,g),e(Be,v4),e(Be,Ur),e(Ur,E4),e(Be,y4),e(Be,zr),e(zr,w4),m(a,x1,g),m(a,Iu,g),e(Iu,b4),m(a,I1,g),E(Nt,a,g),m(a,H1,g),m(a,Hu,g),e(Hu,T4),m(a,B1,g),m(a,Bu,g),e(Bu,j4),m(a,C1,g),E(xt,a,g),m(a,G1,g),m(a,It,g),e(It,pg),e(pg,Mr),e(Mr,Cu),e(Cu,k4),e(Mr,A4),e(Mr,hg),e(It,D4),e(It,he),e(he,Kr),e(Kr,Gu),e(Gu,dg),e(dg,O4),e(Kr,P4),e(Kr,Lu),e(Lu,R4),e(he,S4),e(he,Fr),e(Fr,Uu),e(Uu,gg),e(gg,N4),e(Fr,x4),e(Fr,zu),e(zu,I4),e(he,H4),e(he,Jr),e(Jr,Mu),e(Mu,mg),e(mg,B4),e(Jr,C4),e(Jr,Ht),e(Ht,G4),e(Ht,$g),e($g,L4),e(Ht,U4),e(he,z4),e(he,Wr),e(Wr,Ku),e(Ku,qg),e(qg,M4),e(Wr,K4),e(Wr,Bt),e(Bt,F4),e(Bt,_g),e(_g,J4),e(Bt,W4),m(a,L1,g),m(a,Ce,g),e(Ce,Ct),e(Ct,vg),E(Yr,vg,null),e(Ce,Y4),e(Ce,Eg),e(Eg,V4),m(a,U1,g),m(a,Fu,g),e(Fu,X4),m(a,z1,g),E(Gt,a,g),m(a,M1,g),m(a,Vr,g),e(Vr,Q4),e(Vr,Xr),e(Xr,Z4),m(a,K1,g),m(a,Ju,g),e(Ju,ek),m(a,F1,g),E(Lt,a,g),m(a,J1,g),m(a,Wu,g),e(Wu,tk),m(a,W1,g),m(a,Ut,g),e(Ut,yg),e(yg,Qr),e(Qr,Yu),e(Yu,sk),e(Qr,ak),e(Qr,wg),e(Ut,rk),e(Ut,K),e(K,Zr),e(Zr,en),e(en,bg),e(bg,nk),e(en,ok),e(Zr,lk),e(Zr,Tg),e(K,ik),e(K,tn),e(tn,Vu),e(Vu,uk),e(tn,ck),e(tn,Xu),e(Xu,fk),e(K,pk),e(K,sn),e(sn,Qu),e(Qu,hk),e(sn,dk),e(sn,Zu),e(Zu,gk),e(K,mk),e(K,an),e(an,ec),e(ec,jg),e(jg,$k),e(an,qk),e(an,tc),e(tc,_k),e(K,vk),e(K,rn),e(rn,sc),e(sc,Ek),e(rn,yk),e(rn,zt),e(zt,wk),e(zt,kg),e(kg,bk),e(zt,Tk),e(K,jk),e(K,nn),e(nn,ac),e(ac,kk),e(nn,Ak),e(nn,Mt),e(Mt,Dk),e(Mt,Ag),e(Ag,Ok),e(Mt,Pk),e(K,Rk),e(K,on),e(on,rc),e(rc,Sk),e(on,Nk),e(on,Kt),e(Kt,xk),e(Kt,Dg),e(Dg,Ik),e(Kt,Hk),m(a,Y1,g),m(a,nc,g),e(nc,Bk),m(a,V1,g),E(Ft,a,g),m(a,X1,g),m(a,Jt,g),e(Jt,Og),e(Og,ln),e(ln,oc),e(oc,Ck),e(ln,Gk),e(ln,Pg),e(Jt,Lk),e(Jt,de),e(de,un),e(un,lc),e(lc,Rg),e(Rg,Uk),e(un,zk),e(un,ic),e(ic,Mk),e(de,Kk),e(de,cn),e(cn,uc),e(uc,Sg),e(Sg,Fk),e(cn,Jk),e(cn,cc),e(cc,Wk),e(de,Yk),e(de,fn),e(fn,fc),e(fc,Ng),e(Ng,Vk),e(fn,Xk),e(fn,pc),e(pc,Qk),e(de,Zk),e(de,pn),e(pn,hc),e(hc,xg),e(xg,e5),e(pn,t5),e(pn,dc),e(dc,s5),m(a,Q1,g),m(a,Ge,g),e(Ge,Wt),e(Wt,Ig),E(hn,Ig,null),e(Ge,a5),e(Ge,Hg),e(Hg,r5),m(a,Z1,g),m(a,gc,g),e(gc,n5),m(a,ev,g),E(Yt,a,g),m(a,tv,g),m(a,dn,g),e(dn,o5),e(dn,gn),e(gn,l5),m(a,sv,g),m(a,mc,g),e(mc,i5),m(a,av,g),E(Vt,a,g),m(a,rv,g),m(a,$c,g),e($c,u5),m(a,nv,g),m(a,Xt,g),e(Xt,Bg),e(Bg,mn),e(mn,qc),e(qc,c5),e(mn,f5),e(mn,Cg),e(Xt,p5),e(Xt,ee),e(ee,$n),e($n,qn),e(qn,Gg),e(Gg,h5),e(qn,d5),e($n,g5),e($n,_c),e(_c,m5),e(ee,$5),e(ee,_n),e(_n,vc),e(vc,Lg),e(Lg,q5),e(_n,_5),e(_n,Ec),e(Ec,v5),e(ee,E5),e(ee,vn),e(vn,yc),e(yc,y5),e(vn,w5),e(vn,Qt),e(Qt,b5),e(Qt,Ug),e(Ug,T5),e(Qt,j5),e(ee,k5),e(ee,En),e(En,wc),e(wc,A5),e(En,D5),e(En,Zt),e(Zt,O5),e(Zt,zg),e(zg,P5),e(Zt,R5),e(ee,S5),e(ee,yn),e(yn,bc),e(bc,N5),e(yn,x5),e(yn,es),e(es,I5),e(es,Mg),e(Mg,H5),e(es,B5),m(a,ov,g),m(a,Tc,g),e(Tc,C5),m(a,lv,g),E(ts,a,g),m(a,iv,g),m(a,ss,g),e(ss,Kg),e(Kg,wn),e(wn,jc),e(jc,G5),e(wn,L5),e(wn,Fg),e(ss,U5),e(ss,bn),e(bn,Tn),e(Tn,kc),e(kc,Jg),e(Jg,z5),e(Tn,M5),e(Tn,Ac),e(Ac,K5),e(bn,F5),e(bn,jn),e(jn,Dc),e(Dc,Wg),e(Wg,J5),e(jn,W5),e(jn,Oc),e(Oc,Y5),m(a,uv,g),m(a,Le,g),e(Le,as),e(as,Yg),E(kn,Yg,null),e(Le,V5),e(Le,Vg),e(Vg,X5),m(a,cv,g),m(a,Pc,g),e(Pc,Q5),m(a,fv,g),E(rs,a,g),m(a,pv,g),m(a,An,g),e(An,Z5),e(An,Dn),e(Dn,e6),m(a,hv,g),m(a,Rc,g),e(Rc,t6),m(a,dv,g),E(ns,a,g),m(a,gv,g),m(a,Sc,g),e(Sc,s6),m(a,mv,g),m(a,os,g),e(os,Xg),e(Xg,On),e(On,Nc),e(Nc,a6),e(On,r6),e(On,Qg),e(os,n6),e(os,I),e(I,Pn),e(Pn,Rn),e(Rn,Zg),e(Zg,o6),e(Rn,l6),e(Pn,i6),e(Pn,xc),e(xc,u6),e(I,c6),e(I,Sn),e(Sn,Ic),e(Ic,em),e(em,f6),e(Sn,p6),e(Sn,Hc),e(Hc,h6),e(I,d6),e(I,Nn),e(Nn,Bc),e(Bc,g6),e(Nn,m6),e(Nn,ve),e(ve,$6),e(ve,tm),e(tm,q6),e(ve,_6),e(ve,sm),e(sm,v6),e(ve,E6),e(I,y6),e(I,xn),e(xn,Cc),e(Cc,w6),e(xn,b6),e(xn,ie),e(ie,T6),e(ie,am),e(am,j6),e(ie,k6),e(ie,rm),e(rm,A6),e(ie,D6),e(ie,nm),e(nm,O6),e(ie,P6),e(I,R6),e(I,In),e(In,Gc),e(Gc,S6),e(In,N6),e(In,ue),e(ue,x6),e(ue,om),e(om,I6),e(ue,H6),e(ue,lm),e(lm,B6),e(ue,C6),e(ue,im),e(im,G6),e(ue,L6),e(I,U6),e(I,Hn),e(Hn,Lc),e(Lc,z6),e(Hn,M6),e(Hn,ls),e(ls,K6),e(ls,um),e(um,F6),e(ls,J6),e(I,W6),e(I,Bn),e(Bn,Uc),e(Uc,Y6),e(Bn,V6),e(Bn,Ee),e(Ee,X6),e(Ee,cm),e(cm,Q6),e(Ee,Z6),e(Ee,fm),e(fm,e7),e(Ee,t7),e(I,s7),e(I,Cn),e(Cn,zc),e(zc,a7),e(Cn,r7),e(Cn,ye),e(ye,n7),e(ye,pm),e(pm,o7),e(ye,l7),e(ye,hm),e(hm,i7),e(ye,u7),e(I,c7),e(I,Gn),e(Gn,Mc),e(Mc,f7),e(Gn,p7),e(Gn,we),e(we,h7),e(we,dm),e(dm,d7),e(we,g7),e(we,gm),e(gm,m7),e(we,$7),e(I,q7),e(I,Ln),e(Ln,Kc),e(Kc,_7),e(Ln,v7),e(Ln,is),e(is,E7),e(is,mm),e(mm,y7),e(is,w7),e(I,b7),e(I,Un),e(Un,Fc),e(Fc,T7),e(Un,j7),e(Un,us),e(us,k7),e(us,$m),e($m,A7),e(us,D7),e(I,O7),e(I,zn),e(zn,Jc),e(Jc,qm),e(qm,P7),e(zn,R7),e(zn,Wc),e(Wc,S7),e(I,N7),e(I,Mn),e(Mn,Yc),e(Yc,x7),e(Mn,I7),e(Mn,cs),e(cs,H7),e(cs,_m),e(_m,B7),e(cs,C7),e(I,G7),e(I,Kn),e(Kn,Vc),e(Vc,L7),e(Kn,U7),e(Kn,fs),e(fs,z7),e(fs,vm),e(vm,M7),e(fs,K7),e(I,F7),e(I,Fn),e(Fn,Xc),e(Xc,J7),e(Fn,W7),e(Fn,ps),e(ps,Y7),e(ps,Em),e(Em,V7),e(ps,X7),m(a,$v,g),m(a,Qc,g),e(Qc,Q7),m(a,qv,g),E(hs,a,g),m(a,_v,g),m(a,ds,g),e(ds,ym),e(ym,Jn),e(Jn,Zc),e(Zc,Z7),e(Jn,e9),e(Jn,wm),e(ds,t9),e(ds,bm),e(bm,Wn),e(Wn,ef),e(ef,Tm),e(Tm,s9),e(Wn,a9),e(Wn,tf),e(tf,r9),m(a,vv,g),m(a,Ue,g),e(Ue,gs),e(gs,jm),E(Yn,jm,null),e(Ue,n9),e(Ue,km),e(km,o9),m(a,Ev,g),m(a,ms,g),e(ms,l9),e(ms,sf),e(sf,i9),e(ms,u9),m(a,yv,g),m(a,ze,g),e(ze,$s),e($s,Am),E(Vn,Am,null),e(ze,c9),e(ze,Dm),e(Dm,f9),m(a,wv,g),m(a,af,g),e(af,p9),m(a,bv,g),E(qs,a,g),m(a,Tv,g),m(a,Me,g),e(Me,h9),e(Me,Xn),e(Xn,d9),e(Me,g9),e(Me,Qn),e(Qn,m9),m(a,jv,g),m(a,rf,g),e(rf,$9),m(a,kv,g),E(_s,a,g),m(a,Av,g),m(a,nf,g),e(nf,q9),m(a,Dv,g),m(a,vs,g),e(vs,Om),e(Om,Zn),e(Zn,of),e(of,_9),e(Zn,v9),e(Zn,Pm),e(vs,E9),e(vs,F),e(F,eo),e(eo,to),e(to,Rm),e(Rm,y9),e(to,w9),e(eo,b9),e(eo,lf),e(lf,T9),e(F,j9),e(F,so),e(so,uf),e(uf,Sm),e(Sm,k9),e(so,A9),e(so,cf),e(cf,D9),e(F,O9),e(F,ao),e(ao,ff),e(ff,P9),e(ao,R9),e(ao,x),e(x,S9),e(x,Nm),e(Nm,N9),e(x,x9),e(x,I9),e(x,H9),e(x,xm),e(xm,B9),e(x,C9),e(x,G9),e(x,L9),e(x,Im),e(Im,U9),e(x,z9),e(x,M9),e(x,K9),e(x,Hm),e(Hm,F9),e(x,J9),e(x,Bm),e(Bm,W9),e(x,Y9),e(x,V9),e(x,X9),e(x,Cm),e(Cm,Q9),e(x,Z9),e(x,Gm),e(Gm,e8),e(x,t8),e(x,s8),e(x,a8),e(x,Lm),e(Lm,r8),e(x,n8),e(x,Um),e(Um,o8),e(x,l8),e(F,i8),e(F,ro),e(ro,pf),e(pf,zm),e(zm,u8),e(ro,c8),e(ro,hf),e(hf,f8),e(F,p8),e(F,no),e(no,df),e(df,h8),e(no,d8),e(no,Es),e(Es,g8),e(Es,Mm),e(Mm,m8),e(Es,$8),e(F,q8),e(F,oo),e(oo,gf),e(gf,_8),e(oo,v8),e(oo,ys),e(ys,E8),e(ys,Km),e(Km,y8),e(ys,w8),e(F,b8),e(F,lo),e(lo,mf),e(mf,T8),e(lo,j8),e(lo,ws),e(ws,k8),e(ws,Fm),e(Fm,A8),e(ws,D8),m(a,Ov,g),m(a,$f,g),e($f,O8),m(a,Pv,g),E(bs,a,g),m(a,Rv,g),m(a,Ts,g),e(Ts,Jm),e(Jm,io),e(io,qf),e(qf,P8),e(io,R8),e(io,Wm),e(Ts,S8),e(Ts,te),e(te,uo),e(uo,_f),e(_f,Ym),e(Ym,N8),e(uo,x8),e(uo,vf),e(vf,I8),e(te,H8),e(te,co),e(co,Ef),e(Ef,Vm),e(Vm,B8),e(co,C8),e(co,yf),e(yf,G8),e(te,L8),e(te,fo),e(fo,wf),e(wf,Xm),e(Xm,U8),e(fo,z8),e(fo,bf),e(bf,M8),e(te,K8),e(te,po),e(po,Tf),e(Tf,Qm),e(Qm,F8),e(po,J8),e(po,js),e(js,W8),e(js,Zm),e(Zm,Y8),e(js,V8),e(te,X8),e(te,ho),e(ho,jf),e(jf,e$),e(e$,Q8),e(ho,Z8),e(ho,ks),e(ks,eA),e(ks,t$),e(t$,tA),e(ks,sA),m(a,Sv,g),m(a,Ke,g),e(Ke,As),e(As,s$),E(go,s$,null),e(Ke,aA),e(Ke,a$),e(a$,rA),m(a,Nv,g),m(a,mo,g),e(mo,nA),e(mo,kf),e(kf,oA),m(a,xv,g),m(a,Fe,g),e(Fe,Ds),e(Ds,r$),E($o,r$,null),e(Fe,lA),e(Fe,n$),e(n$,iA),m(a,Iv,g),m(a,Af,g),e(Af,uA),m(a,Hv,g),E(Os,a,g),m(a,Bv,g),m(a,qo,g),e(qo,cA),e(qo,_o),e(_o,fA),m(a,Cv,g),m(a,Df,g),e(Df,pA),m(a,Gv,g),E(Ps,a,g),m(a,Lv,g),m(a,Of,g),e(Of,hA),m(a,Uv,g),m(a,Rs,g),e(Rs,o$),e(o$,vo),e(vo,Pf),e(Pf,dA),e(vo,gA),e(vo,l$),e(Rs,mA),e(Rs,se),e(se,Eo),e(Eo,yo),e(yo,i$),e(i$,$A),e(yo,qA),e(Eo,_A),e(Eo,Rf),e(Rf,vA),e(se,EA),e(se,wo),e(wo,Sf),e(Sf,u$),e(u$,yA),e(wo,wA),e(wo,Nf),e(Nf,bA),e(se,TA),e(se,bo),e(bo,xf),e(xf,jA),e(bo,kA),e(bo,Ss),e(Ss,AA),e(Ss,c$),e(c$,DA),e(Ss,OA),e(se,PA),e(se,To),e(To,If),e(If,RA),e(To,SA),e(To,Ns),e(Ns,NA),e(Ns,f$),e(f$,xA),e(Ns,IA),e(se,HA),e(se,jo),e(jo,Hf),e(Hf,BA),e(jo,CA),e(jo,xs),e(xs,GA),e(xs,p$),e(p$,LA),e(xs,UA),m(a,zv,g),m(a,Bf,g),e(Bf,zA),m(a,Mv,g),m(a,Is,g),e(Is,h$),e(h$,ko),e(ko,Cf),e(Cf,MA),e(ko,KA),e(ko,d$),e(Is,FA),e(Is,g$),e(g$,Ao),e(Ao,Gf),e(Gf,m$),e(m$,JA),e(Ao,WA),e(Ao,Lf),e(Lf,YA),m(a,Kv,g),m(a,Je,g),e(Je,Hs),e(Hs,$$),E(Do,$$,null),e(Je,VA),e(Je,q$),e(q$,XA),m(a,Fv,g),m(a,Uf,g),e(Uf,QA),m(a,Jv,g),E(Bs,a,g),m(a,Wv,g),m(a,Oo,g),e(Oo,ZA),e(Oo,Po),e(Po,eD),m(a,Yv,g),m(a,zf,g),e(zf,tD),m(a,Vv,g),E(Cs,a,g),m(a,Xv,g),m(a,Mf,g),e(Mf,sD),m(a,Qv,g),m(a,Gs,g),e(Gs,_$),e(_$,Ro),e(Ro,Kf),e(Kf,aD),e(Ro,rD),e(Ro,v$),e(Gs,nD),e(Gs,M),e(M,So),e(So,No),e(No,E$),e(E$,oD),e(No,lD),e(So,iD),e(So,Ff),e(Ff,uD),e(M,cD),e(M,xo),e(xo,Io),e(Io,y$),e(y$,fD),e(Io,pD),e(xo,hD),e(xo,Jf),e(Jf,dD),e(M,gD),e(M,Ho),e(Ho,Wf),e(Wf,mD),e(Ho,$D),e(Ho,be),e(be,qD),e(be,w$),e(w$,_D),e(be,vD),e(be,b$),e(b$,ED),e(be,yD),e(M,wD),e(M,Bo),e(Bo,Yf),e(Yf,bD),e(Bo,TD),e(Bo,Ls),e(Ls,jD),e(Ls,T$),e(T$,kD),e(Ls,AD),e(M,DD),e(M,Co),e(Co,Vf),e(Vf,j$),e(j$,OD),e(Co,PD),e(Co,Xf),e(Xf,RD),e(M,SD),e(M,Go),e(Go,Qf),e(Qf,ND),e(Go,xD),e(Go,Us),e(Us,ID),e(Us,k$),e(k$,HD),e(Us,BD),e(M,CD),e(M,Lo),e(Lo,Zf),e(Zf,GD),e(Lo,LD),e(Lo,zs),e(zs,UD),e(zs,A$),e(A$,zD),e(zs,MD),e(M,KD),e(M,Uo),e(Uo,ep),e(ep,FD),e(Uo,JD),e(Uo,Ms),e(Ms,WD),e(Ms,D$),e(D$,YD),e(Ms,VD),m(a,Zv,g),m(a,tp,g),e(tp,XD),m(a,e2,g),m(a,sp,g),e(sp,QD),m(a,t2,g),E(Ks,a,g),m(a,s2,g),m(a,Fs,g),e(Fs,O$),e(O$,zo),e(zo,ap),e(ap,ZD),e(zo,eO),e(zo,P$),e(Fs,tO),e(Fs,We),e(We,Mo),e(Mo,rp),e(rp,R$),e(R$,sO),e(Mo,aO),e(Mo,np),e(np,rO),e(We,nO),e(We,Ko),e(Ko,op),e(op,S$),e(S$,oO),e(Ko,lO),e(Ko,lp),e(lp,iO),e(We,uO),e(We,Fo),e(Fo,ip),e(ip,N$),e(N$,cO),e(Fo,fO),e(Fo,Js),e(Js,pO),e(Js,x$),e(x$,hO),e(Js,dO),m(a,a2,g),m(a,Ye,g),e(Ye,Ws),e(Ws,I$),E(Jo,I$,null),e(Ye,gO),e(Ye,H$),e(H$,mO),m(a,r2,g),m(a,up,g),e(up,$O),m(a,n2,g),E(Ys,a,g),m(a,o2,g),m(a,Wo,g),e(Wo,qO),e(Wo,Yo),e(Yo,_O),m(a,l2,g),m(a,cp,g),e(cp,vO),m(a,i2,g),E(Vs,a,g),m(a,u2,g),m(a,fp,g),e(fp,EO),m(a,c2,g),m(a,Xs,g),e(Xs,B$),e(B$,Vo),e(Vo,pp),e(pp,yO),e(Vo,wO),e(Vo,C$),e(Xs,bO),e(Xs,N),e(N,Xo),e(Xo,Qo),e(Qo,G$),e(G$,TO),e(Qo,jO),e(Xo,kO),e(Xo,L$),e(N,AO),e(N,Zo),e(Zo,hp),e(hp,DO),e(Zo,OO),e(Zo,dp),e(dp,PO),e(N,RO),e(N,el),e(el,gp),e(gp,SO),e(el,NO),e(el,mp),e(mp,xO),e(N,IO),e(N,tl),e(tl,$p),e($p,HO),e(tl,BO),e(tl,Qs),e(Qs,CO),e(Qs,U$),e(U$,GO),e(Qs,LO),e(N,UO),e(N,sl),e(sl,qp),e(qp,z$),e(z$,zO),e(sl,MO),e(sl,_p),e(_p,KO),e(N,FO),e(N,al),e(al,vp),e(vp,JO),e(al,WO),e(al,Te),e(Te,YO),e(Te,M$),e(M$,VO),e(Te,XO),e(Te,K$),e(K$,QO),e(Te,ZO),e(N,eP),e(N,rl),e(rl,Ep),e(Ep,tP),e(rl,sP),e(rl,je),e(je,aP),e(je,F$),e(F$,rP),e(je,nP),e(je,J$),e(J$,oP),e(je,lP),e(N,iP),e(N,nl),e(nl,yp),e(yp,uP),e(nl,cP),e(nl,ke),e(ke,fP),e(ke,W$),e(W$,pP),e(ke,hP),e(ke,Y$),e(Y$,dP),e(ke,gP),e(N,mP),e(N,ol),e(ol,wp),e(wp,$P),e(ol,qP),e(ol,ce),e(ce,_P),e(ce,V$),e(V$,vP),e(ce,EP),e(ce,X$),e(X$,yP),e(ce,wP),e(ce,Q$),e(Q$,bP),e(ce,TP),e(N,jP),e(N,ll),e(ll,bp),e(bp,kP),e(ll,AP),e(ll,fe),e(fe,DP),e(fe,Z$),e(Z$,OP),e(fe,PP),e(fe,eq),e(eq,RP),e(fe,SP),e(fe,tq),e(tq,NP),e(fe,xP),e(N,IP),e(N,il),e(il,Tp),e(Tp,HP),e(il,BP),e(il,Zs),e(Zs,CP),e(Zs,sq),e(sq,GP),e(Zs,LP),e(N,UP),e(N,ul),e(ul,jp),e(jp,zP),e(ul,MP),e(ul,ea),e(ea,KP),e(ea,aq),e(aq,FP),e(ea,JP),e(N,WP),e(N,cl),e(cl,kp),e(kp,rq),e(rq,YP),e(cl,VP),e(cl,Ap),e(Ap,XP),e(N,QP),e(N,fl),e(fl,Dp),e(Dp,ZP),e(fl,eR),e(fl,ta),e(ta,tR),e(ta,nq),e(nq,sR),e(ta,aR),e(N,rR),e(N,pl),e(pl,Op),e(Op,nR),e(pl,oR),e(pl,sa),e(sa,lR),e(sa,oq),e(oq,iR),e(sa,uR),e(N,cR),e(N,hl),e(hl,Pp),e(Pp,fR),e(hl,pR),e(hl,aa),e(aa,hR),e(aa,lq),e(lq,dR),e(aa,gR),m(a,f2,g),m(a,Rp,g),e(Rp,mR),m(a,p2,g),m(a,ra,g),e(ra,iq),e(iq,dl),e(dl,Sp),e(Sp,$R),e(dl,qR),e(dl,uq),e(ra,_R),e(ra,ge),e(ge,gl),e(gl,Np),e(Np,cq),e(cq,vR),e(gl,ER),e(gl,xp),e(xp,yR),e(ge,wR),e(ge,ml),e(ml,Ip),e(Ip,fq),e(fq,bR),e(ml,TR),e(ml,Hp),e(Hp,jR),e(ge,kR),e(ge,$l),e($l,Bp),e(Bp,AR),e($l,DR),e($l,Cp),e(Cp,OR),e(ge,PR),e(ge,ql),e(ql,Gp),e(Gp,RR),e(ql,SR),e(ql,Lp),e(Lp,NR),m(a,h2,g),m(a,Ve,g),e(Ve,na),e(na,pq),E(_l,pq,null),e(Ve,xR),e(Ve,hq),e(hq,IR),m(a,d2,g),m(a,Up,g),e(Up,HR),m(a,g2,g),E(oa,a,g),m(a,m2,g),m(a,Xe,g),e(Xe,BR),e(Xe,vl),e(vl,CR),e(Xe,GR),e(Xe,El),e(El,LR),m(a,$2,g),m(a,zp,g),e(zp,UR),m(a,q2,g),m(a,la,g),e(la,dq),e(dq,yl),e(yl,Mp),e(Mp,zR),e(yl,MR),e(yl,gq),e(la,KR),e(la,ae),e(ae,wl),e(wl,bl),e(bl,mq),e(mq,FR),e(bl,JR),e(wl,WR),e(wl,Kp),e(Kp,YR),e(ae,VR),e(ae,Tl),e(Tl,Fp),e(Fp,$q),e($q,XR),e(Tl,QR),e(Tl,Jp),e(Jp,ZR),e(ae,eS),e(ae,jl),e(jl,Wp),e(Wp,tS),e(jl,sS),e(jl,ia),e(ia,aS),e(ia,qq),e(qq,rS),e(ia,nS),e(ae,oS),e(ae,kl),e(kl,Yp),e(Yp,lS),e(kl,iS),e(kl,ua),e(ua,uS),e(ua,_q),e(_q,cS),e(ua,fS),e(ae,pS),e(ae,Al),e(Al,Vp),e(Vp,hS),e(Al,dS),e(Al,ca),e(ca,gS),e(ca,vq),e(vq,mS),e(ca,$S),m(a,_2,g),m(a,Xp,g),e(Xp,qS),m(a,v2,g),m(a,fa,g),e(fa,Eq),e(Eq,Dl),e(Dl,Qp),e(Qp,_S),e(Dl,vS),e(Dl,yq),e(fa,ES),e(fa,wq),e(wq,Ol),e(Ol,Zp),e(Zp,bq),e(bq,yS),e(Ol,wS),e(Ol,eh),e(eh,bS),m(a,E2,g),m(a,th,g),e(th,TS),m(a,y2,g),m(a,Qe,g),e(Qe,pa),e(pa,Tq),E(Pl,Tq,null),e(Qe,jS),e(Qe,jq),e(jq,kS),m(a,w2,g),m(a,Ze,g),e(Ze,ha),e(ha,kq),E(Rl,kq,null),e(Ze,AS),e(Ze,Aq),e(Aq,DS),m(a,b2,g),m(a,sh,g),e(sh,OS),m(a,T2,g),E(da,a,g),m(a,j2,g),E(ga,a,g),m(a,k2,g),m(a,me,g),e(me,PS),e(me,Sl),e(Sl,RS),e(me,SS),e(me,Nl),e(Nl,NS),e(me,xS),e(me,xl),e(xl,IS),m(a,A2,g),m(a,ah,g),e(ah,HS),m(a,D2,g),E(ma,a,g),m(a,O2,g),m(a,rh,g),e(rh,BS),m(a,P2,g),m(a,$a,g),e($a,Dq),e(Dq,Il),e(Il,nh),e(nh,CS),e(Il,GS),e(Il,Oq),e($a,LS),e($a,Pq),e(Pq,Hl),e(Hl,Bl),e(Bl,Rq),e(Rq,US),e(Bl,zS),e(Hl,MS),e(Hl,oh),e(oh,KS),m(a,R2,g),m(a,lh,g),e(lh,FS),m(a,S2,g),m(a,ih,g),e(ih,JS),m(a,N2,g),E(qa,a,g),m(a,x2,g),m(a,_a,g),e(_a,Sq),e(Sq,Cl),e(Cl,uh),e(uh,WS),e(Cl,YS),e(Cl,Nq),e(_a,VS),e(_a,xq),e(xq,Gl),e(Gl,ch),e(ch,Iq),e(Iq,XS),e(Gl,QS),e(Gl,fh),e(fh,ZS),m(a,I2,g),m(a,et,g),e(et,va),e(va,Hq),E(Ll,Hq,null),e(et,eN),e(et,Bq),e(Bq,tN),m(a,H2,g),m(a,ph,g),e(ph,sN),m(a,B2,g),E(Ea,a,g),m(a,C2,g),m(a,tt,g),e(tt,aN),e(tt,Ul),e(Ul,rN),e(tt,nN),e(tt,zl),e(zl,oN),m(a,G2,g),m(a,hh,g),e(hh,lN),m(a,L2,g),E(ya,a,g),m(a,U2,g),m(a,dh,g),e(dh,iN),m(a,z2,g),m(a,wa,g),e(wa,Cq),e(Cq,Ml),e(Ml,gh),e(gh,uN),e(Ml,cN),e(Ml,Gq),e(wa,fN),e(wa,Lq),e(Lq,Kl),e(Kl,Fl),e(Fl,Uq),e(Uq,pN),e(Fl,hN),e(Kl,dN),e(Kl,mh),e(mh,gN),m(a,M2,g),m(a,$h,g),e($h,mN),m(a,K2,g),E(ba,a,g),m(a,F2,g),m(a,Ta,g),e(Ta,zq),e(zq,Jl),e(Jl,qh),e(qh,$N),e(Jl,qN),e(Jl,Mq),e(Ta,_N),e(Ta,Wl),e(Wl,Yl),e(Yl,_h),e(_h,Kq),e(Kq,vN),e(Yl,EN),e(Yl,vh),e(vh,yN),e(Wl,wN),e(Wl,Vl),e(Vl,Eh),e(Eh,Fq),e(Fq,bN),e(Vl,TN),e(Vl,yh),e(yh,jN),m(a,J2,g),m(a,st,g),e(st,ja),e(ja,Jq),E(Xl,Jq,null),e(st,kN),e(st,Wq),e(Wq,AN),m(a,W2,g),m(a,at,g),e(at,ka),e(ka,Yq),E(Ql,Yq,null),e(at,DN),e(at,Vq),e(Vq,ON),m(a,Y2,g),m(a,wh,g),e(wh,PN),m(a,V2,g),E(Aa,a,g),m(a,X2,g),m(a,Zl,g),e(Zl,RN),e(Zl,ei),e(ei,SN),m(a,Q2,g),m(a,bh,g),e(bh,NN),m(a,Z2,g),E(Da,a,g),m(a,eE,g),m(a,Oa,g),e(Oa,xN),e(Oa,ti),e(ti,IN),e(Oa,HN),m(a,tE,g),m(a,Pa,g),e(Pa,Xq),e(Xq,si),e(si,Th),e(Th,BN),e(si,CN),e(si,Qq),e(Pa,GN),e(Pa,Zq),e(Zq,ai),e(ai,ri),e(ri,e_),e(e_,LN),e(ri,UN),e(ai,zN),e(ai,jh),e(jh,MN),m(a,sE,g),m(a,kh,g),e(kh,KN),m(a,aE,g),E(Ra,a,g),m(a,rE,g),m(a,Sa,g),e(Sa,t_),e(t_,ni),e(ni,Ah),e(Ah,FN),e(ni,JN),e(ni,s_),e(Sa,WN),e(Sa,oi),e(oi,li),e(li,Dh),e(Dh,a_),e(a_,YN),e(li,VN),e(li,Oh),e(Oh,XN),e(oi,QN),e(oi,ii),e(ii,Ph),e(Ph,r_),e(r_,ZN),e(ii,ex),e(ii,Rh),e(Rh,tx),m(a,nE,g),m(a,rt,g),e(rt,Na),e(Na,n_),E(ui,n_,null),e(rt,sx),e(rt,o_),e(o_,ax),m(a,oE,g),m(a,Sh,g),e(Sh,rx),m(a,lE,g),E(xa,a,g),m(a,iE,g),m(a,ci,g),e(ci,nx),e(ci,fi),e(fi,ox),m(a,uE,g),m(a,Nh,g),e(Nh,lx),m(a,cE,g),E(Ia,a,g),m(a,fE,g),m(a,Ha,g),e(Ha,ix),e(Ha,pi),e(pi,ux),e(Ha,cx),m(a,pE,g),m(a,Ba,g),e(Ba,l_),e(l_,hi),e(hi,xh),e(xh,fx),e(hi,px),e(hi,i_),e(Ba,hx),e(Ba,u_),e(u_,di),e(di,gi),e(gi,c_),e(c_,dx),e(gi,gx),e(di,mx),e(di,Ih),e(Ih,$x),m(a,hE,g),m(a,Hh,g),e(Hh,qx),m(a,dE,g),E(Ca,a,g),m(a,gE,g),m(a,Ga,g),e(Ga,f_),e(f_,mi),e(mi,Bh),e(Bh,_x),e(mi,vx),e(mi,p_),e(Ga,Ex),e(Ga,nt),e(nt,$i),e($i,Ch),e(Ch,h_),e(h_,yx),e($i,wx),e($i,Gh),e(Gh,bx),e(nt,Tx),e(nt,qi),e(qi,Lh),e(Lh,d_),e(d_,jx),e(qi,kx),e(qi,Uh),e(Uh,Ax),e(nt,Dx),e(nt,_i),e(_i,zh),e(zh,g_),e(g_,Ox),e(_i,Px),e(_i,Mh),e(Mh,Rx),m(a,mE,g),m(a,ot,g),e(ot,La),e(La,m_),E(vi,m_,null),e(ot,Sx),e(ot,$_),e($_,Nx),m(a,$E,g),m(a,Kh,g),e(Kh,xx),m(a,qE,g),E(Ua,a,g),m(a,_E,g),m(a,Ei,g),e(Ei,Ix),e(Ei,yi),e(yi,Hx),m(a,vE,g),m(a,Fh,g),e(Fh,Bx),m(a,EE,g),E(za,a,g),m(a,yE,g),m(a,Ma,g),e(Ma,Cx),e(Ma,wi),e(wi,Gx),e(Ma,Lx),m(a,wE,g),m(a,Ka,g),e(Ka,q_),e(q_,bi),e(bi,Jh),e(Jh,Ux),e(bi,zx),e(bi,__),e(Ka,Mx),e(Ka,v_),e(v_,Ti),e(Ti,ji),e(ji,E_),e(E_,Kx),e(ji,Fx),e(Ti,Jx),e(Ti,Wh),e(Wh,Wx),m(a,bE,g),m(a,Yh,g),e(Yh,Yx),m(a,TE,g),E(Fa,a,g),m(a,jE,g),m(a,Ja,g),e(Ja,y_),e(y_,ki),e(ki,Vh),e(Vh,Vx),e(ki,Xx),e(ki,w_),e(Ja,Qx),e(Ja,lt),e(lt,Ai),e(Ai,Xh),e(Xh,b_),e(b_,Zx),e(Ai,eI),e(Ai,Qh),e(Qh,tI),e(lt,sI),e(lt,Di),e(Di,Zh),e(Zh,T_),e(T_,aI),e(Di,rI),e(Di,ed),e(ed,nI),e(lt,oI),e(lt,Oi),e(Oi,td),e(td,j_),e(j_,lI),e(Oi,iI),e(Oi,sd),e(sd,uI),kE=!0},p(a,[g]){const Pi={};g&2&&(Pi.$$scope={dirty:g,ctx:a}),ht.$set(Pi);const k_={};g&2&&(k_.$$scope={dirty:g,ctx:a}),dt.$set(k_);const A_={};g&2&&(A_.$$scope={dirty:g,ctx:a}),_t.$set(A_);const D_={};g&2&&(D_.$$scope={dirty:g,ctx:a}),wt.$set(D_);const Ri={};g&2&&(Ri.$$scope={dirty:g,ctx:a}),bt.$set(Ri);const O_={};g&2&&(O_.$$scope={dirty:g,ctx:a}),St.$set(O_);const P_={};g&2&&(P_.$$scope={dirty:g,ctx:a}),Nt.$set(P_);const R_={};g&2&&(R_.$$scope={dirty:g,ctx:a}),xt.$set(R_);const S_={};g&2&&(S_.$$scope={dirty:g,ctx:a}),Gt.$set(S_);const N_={};g&2&&(N_.$$scope={dirty:g,ctx:a}),Lt.$set(N_);const Si={};g&2&&(Si.$$scope={dirty:g,ctx:a}),Ft.$set(Si);const x_={};g&2&&(x_.$$scope={dirty:g,ctx:a}),Yt.$set(x_);const I_={};g&2&&(I_.$$scope={dirty:g,ctx:a}),Vt.$set(I_);const H_={};g&2&&(H_.$$scope={dirty:g,ctx:a}),ts.$set(H_);const Ni={};g&2&&(Ni.$$scope={dirty:g,ctx:a}),rs.$set(Ni);const B_={};g&2&&(B_.$$scope={dirty:g,ctx:a}),ns.$set(B_);const C_={};g&2&&(C_.$$scope={dirty:g,ctx:a}),hs.$set(C_);const G_={};g&2&&(G_.$$scope={dirty:g,ctx:a}),qs.$set(G_);const L_={};g&2&&(L_.$$scope={dirty:g,ctx:a}),_s.$set(L_);const ad={};g&2&&(ad.$$scope={dirty:g,ctx:a}),bs.$set(ad);const U_={};g&2&&(U_.$$scope={dirty:g,ctx:a}),Os.$set(U_);const z_={};g&2&&(z_.$$scope={dirty:g,ctx:a}),Ps.$set(z_);const M_={};g&2&&(M_.$$scope={dirty:g,ctx:a}),Bs.$set(M_);const xi={};g&2&&(xi.$$scope={dirty:g,ctx:a}),Cs.$set(xi);const K_={};g&2&&(K_.$$scope={dirty:g,ctx:a}),Ks.$set(K_);const Ii={};g&2&&(Ii.$$scope={dirty:g,ctx:a}),Ys.$set(Ii);const F_={};g&2&&(F_.$$scope={dirty:g,ctx:a}),Vs.$set(F_);const re={};g&2&&(re.$$scope={dirty:g,ctx:a}),oa.$set(re);const Hi={};g&2&&(Hi.$$scope={dirty:g,ctx:a}),da.$set(Hi);const rd={};g&2&&(rd.$$scope={dirty:g,ctx:a}),ga.$set(rd);const J_={};g&2&&(J_.$$scope={dirty:g,ctx:a}),ma.$set(J_);const W_={};g&2&&(W_.$$scope={dirty:g,ctx:a}),qa.$set(W_);const Bi={};g&2&&(Bi.$$scope={dirty:g,ctx:a}),Ea.$set(Bi);const Y_={};g&2&&(Y_.$$scope={dirty:g,ctx:a}),ya.$set(Y_);const V_={};g&2&&(V_.$$scope={dirty:g,ctx:a}),ba.$set(V_);const X_={};g&2&&(X_.$$scope={dirty:g,ctx:a}),Aa.$set(X_);const Ci={};g&2&&(Ci.$$scope={dirty:g,ctx:a}),Da.$set(Ci);const Q_={};g&2&&(Q_.$$scope={dirty:g,ctx:a}),Ra.$set(Q_);const Gi={};g&2&&(Gi.$$scope={dirty:g,ctx:a}),xa.$set(Gi);const Z_={};g&2&&(Z_.$$scope={dirty:g,ctx:a}),Ia.$set(Z_);const Li={};g&2&&(Li.$$scope={dirty:g,ctx:a}),Ca.$set(Li);const e1={};g&2&&(e1.$$scope={dirty:g,ctx:a}),Ua.$set(e1);const Ui={};g&2&&(Ui.$$scope={dirty:g,ctx:a}),za.$set(Ui);const t1={};g&2&&(t1.$$scope={dirty:g,ctx:a}),Fa.$set(t1)},i(a){kE||(y(k.$$.fragment,a),y(Q.$$.fragment,a),y(nr.$$.fragment,a),y(or.$$.fragment,a),y(ht.$$.fragment,a),y(dt.$$.fragment,a),y(_t.$$.fragment,a),y(Er.$$.fragment,a),y(wt.$$.fragment,a),y(bt.$$.fragment,a),y(Lr.$$.fragment,a),y(St.$$.fragment,a),y(Nt.$$.fragment,a),y(xt.$$.fragment,a),y(Yr.$$.fragment,a),y(Gt.$$.fragment,a),y(Lt.$$.fragment,a),y(Ft.$$.fragment,a),y(hn.$$.fragment,a),y(Yt.$$.fragment,a),y(Vt.$$.fragment,a),y(ts.$$.fragment,a),y(kn.$$.fragment,a),y(rs.$$.fragment,a),y(ns.$$.fragment,a),y(hs.$$.fragment,a),y(Yn.$$.fragment,a),y(Vn.$$.fragment,a),y(qs.$$.fragment,a),y(_s.$$.fragment,a),y(bs.$$.fragment,a),y(go.$$.fragment,a),y($o.$$.fragment,a),y(Os.$$.fragment,a),y(Ps.$$.fragment,a),y(Do.$$.fragment,a),y(Bs.$$.fragment,a),y(Cs.$$.fragment,a),y(Ks.$$.fragment,a),y(Jo.$$.fragment,a),y(Ys.$$.fragment,a),y(Vs.$$.fragment,a),y(_l.$$.fragment,a),y(oa.$$.fragment,a),y(Pl.$$.fragment,a),y(Rl.$$.fragment,a),y(da.$$.fragment,a),y(ga.$$.fragment,a),y(ma.$$.fragment,a),y(qa.$$.fragment,a),y(Ll.$$.fragment,a),y(Ea.$$.fragment,a),y(ya.$$.fragment,a),y(ba.$$.fragment,a),y(Xl.$$.fragment,a),y(Ql.$$.fragment,a),y(Aa.$$.fragment,a),y(Da.$$.fragment,a),y(Ra.$$.fragment,a),y(ui.$$.fragment,a),y(xa.$$.fragment,a),y(Ia.$$.fragment,a),y(Ca.$$.fragment,a),y(vi.$$.fragment,a),y(Ua.$$.fragment,a),y(za.$$.fragment,a),y(Fa.$$.fragment,a),kE=!0)},o(a){w(k.$$.fragment,a),w(Q.$$.fragment,a),w(nr.$$.fragment,a),w(or.$$.fragment,a),w(ht.$$.fragment,a),w(dt.$$.fragment,a),w(_t.$$.fragment,a),w(Er.$$.fragment,a),w(wt.$$.fragment,a),w(bt.$$.fragment,a),w(Lr.$$.fragment,a),w(St.$$.fragment,a),w(Nt.$$.fragment,a),w(xt.$$.fragment,a),w(Yr.$$.fragment,a),w(Gt.$$.fragment,a),w(Lt.$$.fragment,a),w(Ft.$$.fragment,a),w(hn.$$.fragment,a),w(Yt.$$.fragment,a),w(Vt.$$.fragment,a),w(ts.$$.fragment,a),w(kn.$$.fragment,a),w(rs.$$.fragment,a),w(ns.$$.fragment,a),w(hs.$$.fragment,a),w(Yn.$$.fragment,a),w(Vn.$$.fragment,a),w(qs.$$.fragment,a),w(_s.$$.fragment,a),w(bs.$$.fragment,a),w(go.$$.fragment,a),w($o.$$.fragment,a),w(Os.$$.fragment,a),w(Ps.$$.fragment,a),w(Do.$$.fragment,a),w(Bs.$$.fragment,a),w(Cs.$$.fragment,a),w(Ks.$$.fragment,a),w(Jo.$$.fragment,a),w(Ys.$$.fragment,a),w(Vs.$$.fragment,a),w(_l.$$.fragment,a),w(oa.$$.fragment,a),w(Pl.$$.fragment,a),w(Rl.$$.fragment,a),w(da.$$.fragment,a),w(ga.$$.fragment,a),w(ma.$$.fragment,a),w(qa.$$.fragment,a),w(Ll.$$.fragment,a),w(Ea.$$.fragment,a),w(ya.$$.fragment,a),w(ba.$$.fragment,a),w(Xl.$$.fragment,a),w(Ql.$$.fragment,a),w(Aa.$$.fragment,a),w(Da.$$.fragment,a),w(Ra.$$.fragment,a),w(ui.$$.fragment,a),w(xa.$$.fragment,a),w(Ia.$$.fragment,a),w(Ca.$$.fragment,a),w(vi.$$.fragment,a),w(Ua.$$.fragment,a),w(za.$$.fragment,a),w(Fa.$$.fragment,a),kE=!1},d(a){t(r),a&&t(c),a&&t(s),b(k),a&&t(S),a&&t(D),b(Q),a&&t(rr),a&&t(Se),a&&t(o1),a&&t(Ki),a&&t(l1),a&&t(ut),a&&t(i1),a&&t(ct),a&&t(u1),a&&t(Ne),b(nr),a&&t(c1),a&&t(xe),b(or),a&&t(f1),a&&t(Fi),a&&t(p1),b(ht,a),a&&t(h1),a&&t(lr),a&&t(d1),a&&t(Ji),a&&t(g1),b(dt,a),a&&t(m1),a&&t(Wi),a&&t($1),a&&t(gt),a&&t(q1),a&&t(su),a&&t(_1),b(_t,a),a&&t(v1),a&&t(vt),a&&t(E1),a&&t(Ie),b(Er),a&&t(y1),a&&t(yt),a&&t(w1),b(wt,a),a&&t(b1),a&&t(yr),a&&t(T1),a&&t(hu),a&&t(j1),b(bt,a),a&&t(k1),a&&t(du),a&&t(A1),a&&t(Tt),a&&t(D1),a&&t(Pu),a&&t(O1),a&&t(Pt),a&&t(P1),a&&t(He),b(Lr),a&&t(R1),a&&t(xu),a&&t(S1),b(St,a),a&&t(N1),a&&t(Be),a&&t(x1),a&&t(Iu),a&&t(I1),b(Nt,a),a&&t(H1),a&&t(Hu),a&&t(B1),a&&t(Bu),a&&t(C1),b(xt,a),a&&t(G1),a&&t(It),a&&t(L1),a&&t(Ce),b(Yr),a&&t(U1),a&&t(Fu),a&&t(z1),b(Gt,a),a&&t(M1),a&&t(Vr),a&&t(K1),a&&t(Ju),a&&t(F1),b(Lt,a),a&&t(J1),a&&t(Wu),a&&t(W1),a&&t(Ut),a&&t(Y1),a&&t(nc),a&&t(V1),b(Ft,a),a&&t(X1),a&&t(Jt),a&&t(Q1),a&&t(Ge),b(hn),a&&t(Z1),a&&t(gc),a&&t(ev),b(Yt,a),a&&t(tv),a&&t(dn),a&&t(sv),a&&t(mc),a&&t(av),b(Vt,a),a&&t(rv),a&&t($c),a&&t(nv),a&&t(Xt),a&&t(ov),a&&t(Tc),a&&t(lv),b(ts,a),a&&t(iv),a&&t(ss),a&&t(uv),a&&t(Le),b(kn),a&&t(cv),a&&t(Pc),a&&t(fv),b(rs,a),a&&t(pv),a&&t(An),a&&t(hv),a&&t(Rc),a&&t(dv),b(ns,a),a&&t(gv),a&&t(Sc),a&&t(mv),a&&t(os),a&&t($v),a&&t(Qc),a&&t(qv),b(hs,a),a&&t(_v),a&&t(ds),a&&t(vv),a&&t(Ue),b(Yn),a&&t(Ev),a&&t(ms),a&&t(yv),a&&t(ze),b(Vn),a&&t(wv),a&&t(af),a&&t(bv),b(qs,a),a&&t(Tv),a&&t(Me),a&&t(jv),a&&t(rf),a&&t(kv),b(_s,a),a&&t(Av),a&&t(nf),a&&t(Dv),a&&t(vs),a&&t(Ov),a&&t($f),a&&t(Pv),b(bs,a),a&&t(Rv),a&&t(Ts),a&&t(Sv),a&&t(Ke),b(go),a&&t(Nv),a&&t(mo),a&&t(xv),a&&t(Fe),b($o),a&&t(Iv),a&&t(Af),a&&t(Hv),b(Os,a),a&&t(Bv),a&&t(qo),a&&t(Cv),a&&t(Df),a&&t(Gv),b(Ps,a),a&&t(Lv),a&&t(Of),a&&t(Uv),a&&t(Rs),a&&t(zv),a&&t(Bf),a&&t(Mv),a&&t(Is),a&&t(Kv),a&&t(Je),b(Do),a&&t(Fv),a&&t(Uf),a&&t(Jv),b(Bs,a),a&&t(Wv),a&&t(Oo),a&&t(Yv),a&&t(zf),a&&t(Vv),b(Cs,a),a&&t(Xv),a&&t(Mf),a&&t(Qv),a&&t(Gs),a&&t(Zv),a&&t(tp),a&&t(e2),a&&t(sp),a&&t(t2),b(Ks,a),a&&t(s2),a&&t(Fs),a&&t(a2),a&&t(Ye),b(Jo),a&&t(r2),a&&t(up),a&&t(n2),b(Ys,a),a&&t(o2),a&&t(Wo),a&&t(l2),a&&t(cp),a&&t(i2),b(Vs,a),a&&t(u2),a&&t(fp),a&&t(c2),a&&t(Xs),a&&t(f2),a&&t(Rp),a&&t(p2),a&&t(ra),a&&t(h2),a&&t(Ve),b(_l),a&&t(d2),a&&t(Up),a&&t(g2),b(oa,a),a&&t(m2),a&&t(Xe),a&&t($2),a&&t(zp),a&&t(q2),a&&t(la),a&&t(_2),a&&t(Xp),a&&t(v2),a&&t(fa),a&&t(E2),a&&t(th),a&&t(y2),a&&t(Qe),b(Pl),a&&t(w2),a&&t(Ze),b(Rl),a&&t(b2),a&&t(sh),a&&t(T2),b(da,a),a&&t(j2),b(ga,a),a&&t(k2),a&&t(me),a&&t(A2),a&&t(ah),a&&t(D2),b(ma,a),a&&t(O2),a&&t(rh),a&&t(P2),a&&t($a),a&&t(R2),a&&t(lh),a&&t(S2),a&&t(ih),a&&t(N2),b(qa,a),a&&t(x2),a&&t(_a),a&&t(I2),a&&t(et),b(Ll),a&&t(H2),a&&t(ph),a&&t(B2),b(Ea,a),a&&t(C2),a&&t(tt),a&&t(G2),a&&t(hh),a&&t(L2),b(ya,a),a&&t(U2),a&&t(dh),a&&t(z2),a&&t(wa),a&&t(M2),a&&t($h),a&&t(K2),b(ba,a),a&&t(F2),a&&t(Ta),a&&t(J2),a&&t(st),b(Xl),a&&t(W2),a&&t(at),b(Ql),a&&t(Y2),a&&t(wh),a&&t(V2),b(Aa,a),a&&t(X2),a&&t(Zl),a&&t(Q2),a&&t(bh),a&&t(Z2),b(Da,a),a&&t(eE),a&&t(Oa),a&&t(tE),a&&t(Pa),a&&t(sE),a&&t(kh),a&&t(aE),b(Ra,a),a&&t(rE),a&&t(Sa),a&&t(nE),a&&t(rt),b(ui),a&&t(oE),a&&t(Sh),a&&t(lE),b(xa,a),a&&t(iE),a&&t(ci),a&&t(uE),a&&t(Nh),a&&t(cE),b(Ia,a),a&&t(fE),a&&t(Ha),a&&t(pE),a&&t(Ba),a&&t(hE),a&&t(Hh),a&&t(dE),b(Ca,a),a&&t(gE),a&&t(Ga),a&&t(mE),a&&t(ot),b(vi),a&&t($E),a&&t(Kh),a&&t(qE),b(Ua,a),a&&t(_E),a&&t(Ei),a&&t(vE),a&&t(Fh),a&&t(EE),b(za,a),a&&t(yE),a&&t(Ma),a&&t(wE),a&&t(Ka),a&&t(bE),a&&t(Yh),a&&t(TE),b(Fa,a),a&&t(jE),a&&t(Ja)}}}const bY={local:"detailed-parameters",sections:[{local:"which-task-is-used-by-this-model",title:"Which task is used by this model ?"},{local:"natural-language-processing",sections:[{local:"fill-mask-task",title:"Fill Mask task"},{local:"summarization-task",title:"Summarization task"},{local:"question-answering-task",title:"Question Answering task"},{local:"table-question-answering-task",title:"Table Question Answering task"},{local:"text-classification-task",title:"Text Classification task"},{local:"text-generation-task",title:"Text Generation task"},{local:"text2text-generation-task",title:"Text2Text Generation task"},{local:"token-classification-task",title:"Token Classification task"},{local:"named-entity-recognition-ner-task",title:"Named Entity Recognition (NER) task"},{local:"translation-task",title:"Translation task"},{local:"zeroshot-classification-task",title:"Zero-Shot Classification task"},{local:"conversational-task",title:"Conversational task"},{local:"feature-extraction-task",title:"Feature Extraction task"}],title:"Natural Language Processing"},{local:"audio",sections:[{local:"automatic-speech-recognition-task",title:"Automatic Speech Recognition task"},{local:"audio-classification-task",title:"Audio Classification task"}],title:"Audio"},{local:"computer-vision",sections:[{local:"image-classification-task",title:"Image Classification task"},{local:"object-detection-task",title:"Object Detection task"},{local:"image-segmentation-task",title:"Image Segmentation task"}],title:"Computer Vision"}],title:"Detailed parameters"};function TY(q){return ZF(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class OY extends YF{constructor(r){super();VF(this,r,TY,wY,XF,{})}}export{OY as default,bY as metadata};
